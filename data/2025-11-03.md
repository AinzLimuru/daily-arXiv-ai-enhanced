<div id=toc></div>

# Table of Contents

- [cs.CL](#cs.CL) [Total: 39]
- [cs.IT](#cs.IT) [Total: 4]
- [stat.ML](#stat.ML) [Total: 6]
- [cs.LG](#cs.LG) [Total: 50]
- [cs.GT](#cs.GT) [Total: 1]
- [cs.SI](#cs.SI) [Total: 4]
- [cs.AI](#cs.AI) [Total: 14]


<div id='cs.CL'></div>

# cs.CL [[Back]](#toc)

### [1] [Frame Semantic Patterns for Identifying Underreporting of Notifiable Events in Healthcare: The Case of Gender-Based Violence](https://arxiv.org/abs/2510.26969)
*Lívia Dutra,Arthur Lorenzi,Laís Berno,Franciany Campos,Karoline Biscardi,Kenneth Brown,Marcelo Viridiano,Frederico Belcavello,Ely Matos,Olívia Guaranha,Erik Santos,Sofia Reinach,Tiago Timponi Torrent*

Main category: cs.CL

TL;DR: 这篇论文介绍了一种识别医疗领域可通知事件的方法，该方法利用语义框架在电子病历的开放文本字段中搜索特定模式，以解决巴西电子病历中性别暴力（GBV）漏报的问题。此方法通过手动评估，识别暴力报告的准确率为0.726，并被设计为透明、高效、低碳和语言无关的通用型工具。


<details>
  <summary>Details</summary>
Motivation: 解决医疗领域可通知事件（特别是电子病历中性别暴力的漏报问题）识别的挑战，以便在公共卫生系统中更有效地利用自然语言处理技术。

Method: 该方法利用语义框架定义细粒度模式，并在非结构化数据（即电子病历中的开放文本字段）中搜索这些模式。研究定义了八种模式，并在一个包含2100万句巴西葡萄牙语的语料库上进行了搜索。结果由语言学家手动评估，并测量了每种模式的精确度。

Result: 该方法有效识别暴力报告的准确率为0.726，证明了其鲁棒性。

Conclusion: 该方法能够有效识别医疗事件，特别是性别暴力报告，具有较高的准确性和普遍适用性，可广泛应用于其他健康监测场景，促进自然语言处理技术在公共卫生系统中伦理、可解释和广泛的应用。

Abstract: We introduce a methodology for the identification of notifiable events in the
domain of healthcare. The methodology harnesses semantic frames to define
fine-grained patterns and search them in unstructured data, namely, open-text
fields in e-medical records. We apply the methodology to the problem of
underreporting of gender-based violence (GBV) in e-medical records produced
during patients' visits to primary care units. A total of eight patterns are
defined and searched on a corpus of 21 million sentences in Brazilian
Portuguese extracted from e-SUS APS. The results are manually evaluated by
linguists and the precision of each pattern measured. Our findings reveal that
the methodology effectively identifies reports of violence with a precision of
0.726, confirming its robustness. Designed as a transparent, efficient,
low-carbon, and language-agnostic pipeline, the approach can be easily adapted
to other health surveillance contexts, contributing to the broader, ethical,
and explainable use of NLP in public health systems.

</details>


### [2] [Overview of the MEDIQA-OE 2025 Shared Task on Medical Order Extraction from Doctor-Patient Consultations](https://arxiv.org/abs/2510.26974)
*Jean-Philippe Corbeil,Asma Ben Abacha,Jerome Tremblay,Phillip Swazinna,Akila Jeeson Daniel,Miguel Del-Agua,Francois Beaulieu*

Main category: cs.CL

TL;DR: 本文介绍了MEDIQA-OE 2025共享任务，该任务旨在从医患对话中提取医疗医嘱，以减轻临床医生的文档负担并改善患者护理。


<details>
  <summary>Details</summary>
Motivation: 自动化语音识别和摘要在临床文档中得到应用，但将对话转化为可执行的医疗医嘱仍是空白。解决此问题可显著减轻临床医生的文档负担，直接影响后续患者护理。

Method: MEDIQA-OE 2025共享任务，首次尝试从医患对话中提取医疗医嘱。六个团队参与了任务，尝试了多种方法，包括封闭和开放式大型语言模型。

Result: 描述了MEDIQA-OE任务、数据集、最终排行榜排名和参与者的解决方案。

Conclusion: MEDIQA-OE 2025共享任务成功地促进了从医患对话中提取医疗医嘱的研究，展示了多种方法和大型语言模型在该领域的潜力，有望减轻临床医生负担并改善患者护理。

Abstract: Clinical documentation increasingly uses automatic speech recognition and
summarization, yet converting conversations into actionable medical orders for
Electronic Health Records remains unexplored. A solution to this problem can
significantly reduce the documentation burden of clinicians and directly impact
downstream patient care. We introduce the MEDIQA-OE 2025 shared task, the first
challenge on extracting medical orders from doctor-patient conversations. Six
teams participated in the shared task and experimented with a broad range of
approaches, and both closed- and open-weight large language models (LLMs). In
this paper, we describe the MEDIQA-OE task, dataset, final leaderboard ranking,
and participants' solutions.

</details>


### [3] [Elastic Architecture Search for Efficient Language Models](https://arxiv.org/abs/2510.27037)
*Shang Wang*

Main category: cs.CL

TL;DR: 该论文介绍了一种名为ELM的新型神经架构搜索（NAS）方法，该方法针对紧凑型语言模型进行了优化，以解决大型预训练语言模型在计算和内存方面的挑战。


<details>
  <summary>Details</summary>
Motivation: 大型预训练语言模型（LLMs）在自然语言理解（NLU）任务中表现出色，但其庞大的计算和内存需求带来了经济和环境问题。

Method: 本研究提出了一种名为弹性语言模型（ELM）的新型神经架构搜索（NAS）方法，该方法通过引入灵活的搜索空间，结合高效的Transformer块和用于维度与头部数量调整的动态模块。此外，研究还引入了新颖的知识蒸馏损失，以在搜索过程中保留每个块的独m殊特征，从而提高架构选择的辨别力。

Result: 为了验证ELM的性能，研究人员在掩码语言建模和因果语言建模任务上进行了实验。实验结果表明，ELM发现的模型显著优于现有方法，证明了其在生成高效且高性能紧凑型语言模型方面的有效性。

Conclusion: ELM通过创新的NAS方法和知识蒸馏损失，有效解决了大型预训练语言模型的资源消耗问题，为创建更实用、高效的紧凑型语言模型提供了有前景的途径。

Abstract: As large pre-trained language models become increasingly critical to natural
language understanding (NLU) tasks, their substantial computational and memory
requirements have raised significant economic and environmental concerns.
Addressing these challenges, this paper introduces the Elastic Language Model
(ELM), a novel neural architecture search (NAS) method optimized for compact
language models. ELM extends existing NAS approaches by introducing a flexible
search space with efficient transformer blocks and dynamic modules for
dimension and head number adjustment. These innovations enhance the efficiency
and flexibility of the search process, which facilitates more thorough and
effective exploration of model architectures. We also introduce novel knowledge
distillation losses that preserve the unique characteristics of each block, in
order to improve the discrimination between architectural choices during the
search process. Experiments on masked language modeling and causal language
modeling tasks demonstrate that models discovered by ELM significantly
outperform existing methods.

</details>


### [4] [Dataset Creation and Baseline Models for Sexism Detection in Hausa](https://arxiv.org/abs/2510.27038)
*Fatima Adam Muhammad,Shamsuddeen Muhammad Hassan,Isa Inuwa-Dutse*

Main category: cs.CL

TL;DR: 这篇论文介绍了一个新的豪萨语性别歧视检测数据集，并探讨了豪萨语中性别歧视的文化细微差别和检测挑战。


<details>
  <summary>Details</summary>
Motivation: 在线平台上各种形式的性别歧视盛行，需要有效的性别歧视检测和缓解策略。尽管计算方法在资源丰富的语言中广泛应用，但在资源匮乏的语言中进展有限，因为语言资源有限和文化差异影响性别歧视的表达和感知方式。

Method: 本研究通过社区参与、定性编码和数据增强，首次建立了豪萨语性别歧视检测数据集。通过一项两阶段的用户研究（n=66）探讨了性别歧视在日常话语中的定义和表达方式。该研究还尝试了传统机器学习分类器和预训练多语言模型，并评估了少样本学习在豪萨语性别歧视检测中的有效性。

Result: 研究结果强调了捕捉文化细微差别（特别是寻求澄清和习语表达）的挑战，并揭示了在这种情况下容易出现许多误报。

Conclusion: 本研究成功构建了首个豪萨语性别歧视检测数据集，揭示了在低资源语言中进行性别歧视检测的文化复杂性和挑战，并为未来的研究提供了方向，特别是在处理文化细微差别和减少误报方面的方向。

Abstract: Sexism reinforces gender inequality and social exclusion by perpetuating
stereotypes, bias, and discriminatory norms. Noting how online platforms enable
various forms of sexism to thrive, there is a growing need for effective sexism
detection and mitigation strategies. While computational approaches to sexism
detection are widespread in high-resource languages, progress remains limited
in low-resource languages where limited linguistic resources and cultural
differences affect how sexism is expressed and perceived. This study introduces
the first Hausa sexism detection dataset, developed through community
engagement, qualitative coding, and data augmentation. For cultural nuances and
linguistic representation, we conducted a two-stage user study (n=66) involving
native speakers to explore how sexism is defined and articulated in everyday
discourse. We further experiment with both traditional machine learning
classifiers and pre-trained multilingual language models and evaluating the
effectiveness few-shot learning in detecting sexism in Hausa. Our findings
highlight challenges in capturing cultural nuance, particularly with
clarification-seeking and idiomatic expressions, and reveal a tendency for many
false positives in such cases.

</details>


### [5] [Quantitative Intertextuality from the Digital Humanities Perspective: A Survey](https://arxiv.org/abs/2510.27045)
*Siyu Duan*

Main category: cs.CL

TL;DR: 这篇论文全面回顾了量化互文性研究的发展，总结了其数据、方法和应用，并展望了未来趋势。


<details>
  <summary>Details</summary>
Motivation: 互文性理论在数字人文研究中具有重要基础，自然语言处理的进步推动了互文性研究进入量化时代，因此有必要对当前的量化互文性研究进行系统性的总结和展望。

Method: 本文总结了量化互文性研究的数据、方法和应用。在方法方面，论文涵盖了从统计学到深度学习的多种方法，并使用了多语言和多主题的数据作为案例。

Result: 量化互文性研究在人文和社会科学研究中展现了广泛的应用前景。此外，还总结了相关的平台工具。

Conclusion: 计算机技术的进步将推动互文性研究向更精确、多样化和大规模方向发展，预示着互文性将在人工智能与人文学科的交叉研究中发挥更广阔的应用潜力。

Abstract: The connection between texts is referred to as intertextuality in literary
theory, which served as an important theoretical basis in many digital
humanities studies. Over the past decade, advancements in natural language
processing have ushered intertextuality studies into the quantitative age.
Large-scale intertextuality research based on cutting-edge methods has
continuously emerged. This paper provides a roadmap for quantitative
intertextuality studies, summarizing their data, methods, and applications.
Drawing on data from multiple languages and topics, this survey reviews methods
from statistics to deep learning. It also summarizes their applications in
humanities and social sciences research and the associated platform tools.
Driven by advances in computer technology, more precise, diverse, and
large-scale intertext studies can be anticipated. Intertextuality holds promise
for broader application in interdisciplinary research bridging AI and the
humanities.

</details>


### [6] [Recursive numeral systems are highly regular and easy to process](https://arxiv.org/abs/2510.27049)
*Ponrawee Prasertsom,Andrea Silvi,Jennifer Culbertson,Moa Johansson,Devdatt Dubhashi,Kenny Smith*

Main category: cs.CL

TL;DR: 这篇论文修正了先前关于递归数字系统权衡的观点，引入了“规律性”概念，并基于最小描述长度（MDL）方法，提出了一个综合考虑规律性和处理复杂性的新框架。


<details>
  <summary>Details</summary>
Motivation: 以往研究认为递归数字系统在词汇量和句法形态复杂性之间实现了最优权衡，但未能有效排除非自然系统，且依赖于临时性约束。

Method: 本研究引入了“规律性”的概念，并利用最小描述长度（MDL）方法，提出了衡量递归数字系统规律性和处理复杂性的新指标。

Result: 基于MDL的新指标能更好地区分自然语言系统和非自然系统，并能自然地解释以往研究中的临时性约束。

Conclusion: 语言系统最优性研究中应将“规律性”纳入考量。

Abstract: Previous work has argued that recursive numeral systems optimise the
trade-off between lexicon size and average morphosyntatic complexity (Deni\'c
and Szymanik, 2024). However, showing that only natural-language-like systems
optimise this tradeoff has proven elusive, and the existing solution has relied
on ad-hoc constraints to rule out unnatural systems (Yang and Regier, 2025).
Here, we argue that this issue arises because the proposed trade-off has
neglected regularity, a crucial aspect of complexity central to human grammars
in general. Drawing on the Minimum Description Length (MDL) approach, we
propose that recursive numeral systems are better viewed as efficient with
regard to their regularity and processing complexity. We show that our
MDL-based measures of regularity and processing complexity better capture the
key differences between attested, natural systems and unattested but possible
ones, including "optimal" recursive numeral systems from previous work, and
that the ad-hoc constraints from previous literature naturally follow from
regularity. Our approach highlights the need to incorporate regularity across
sets of forms in studies that attempt to measure and explain optimality in
language.

</details>


### [7] [VISTA Score: Verification In Sequential Turn-based Assessment](https://arxiv.org/abs/2510.27052)
*Ashley Lewis,Andrew Perrault,Eric Fosler-Lussier,Michael White*

Main category: cs.CL

TL;DR: VISTA是一种评估对话系统事实性的框架，它通过逐句验证和序列一致性跟踪来提高幻觉检测能力，优于现有基线。


<details>
  <summary>Details</summary>
Motivation: 现有评估指标无法有效处理多轮对话中的幻觉问题，限制了对话式AI系统在需要事实可靠性的场景中的应用。

Method: VISTA将每个助手回复分解为原子事实声明，根据可信来源和对话历史进行验证，并对不可验证的声明进行分类。

Result: VISTA在八种大型语言模型和四个对话事实性基准测试中，比FACTSCORE和LLM-as-Judge基线显着提高了幻觉检测能力，并能提高标注者的一致性。

Conclusion: VISTA通过将事实性建模为对话的动态属性，提供了一种更透明、更符合人类判断的对话系统真实性衡量方法。

Abstract: Hallucination--defined here as generating statements unsupported or
contradicted by available evidence or conversational context--remains a major
obstacle to deploying conversational AI systems in settings that demand factual
reliability. Existing metrics either evaluate isolated responses or treat
unverifiable content as errors, limiting their use for multi-turn dialogue. We
introduce VISTA (Verification In Sequential Turn-based Assessment), a framework
for evaluating conversational factuality through claim-level verification and
sequential consistency tracking. VISTA decomposes each assistant turn into
atomic factual claims, verifies them against trusted sources and dialogue
history, and categorizes unverifiable statements (subjective, contradicted,
lacking evidence, or abstaining). Across eight large language models and four
dialogue factuality benchmarks (AIS, BEGIN, FAITHDIAL, and FADE), VISTA
substantially improves hallucination detection over FACTSCORE and LLM-as-Judge
baselines. Human evaluation confirms that VISTA's decomposition improves
annotator agreement and reveals inconsistencies in existing benchmarks. By
modeling factuality as a dynamic property of conversation, VISTA offers a more
transparent, human-aligned measure of truthfulness in dialogue systems.

</details>


### [8] [LLM-Centric RAG with Multi-Granular Indexing and Confidence Constraints](https://arxiv.org/abs/2510.27054)
*Xiaofan Guo,Yaxuan Luan,Yue Kang,Xiangchen Song,Jinxu Guo*

Main category: cs.CL

TL;DR: 本文提出了一种结合多粒度记忆索引和不确定性估计的置信度控制方法，以解决检索增强生成在复杂知识环境下覆盖不足、结果不稳定和可靠性有限的问题。


<details>
  <summary>Details</summary>
Motivation: 解决检索增强生成在复杂知识环境下，存在的覆盖不足、结果不稳定以及可靠性有限的问题。

Method: 该方法通过构建分层的记忆结构，将知识表示划分为不同粒度层级，实现了从局部细节到全局上下文的动态索引与检索，从而在检索与生成之间建立了更紧密的语义联系。在此基础上，引入不确定性估计机制，在生成过程中显式约束和过滤低置信度路径，使模型在保持信息覆盖的同时，有效抑制噪声和虚假内容。整体优化目标由生成损失、熵约束和方差正则化构成，形成统一的置信度控制框架。

Result: 实验结果表明，该方法在QA准确率、检索召回率、排序质量和事实一致性方面均优于现有模型，证明了多粒度索引与置信度控制相结合的有效性。

Conclusion: 本研究不仅为检索增强生成提供了一种新的技术途径，也为提升大模型在复杂语境下的可靠性和可控性提供了实践依据。

Abstract: This paper addresses the issues of insufficient coverage, unstable results,
and limited reliability in retrieval-augmented generation under complex
knowledge environments, and proposes a confidence control method that
integrates multi-granularity memory indexing with uncertainty estimation. The
method builds a hierarchical memory structure that divides knowledge
representations into different levels of granularity, enabling dynamic indexing
and retrieval from local details to global context, and thus establishing
closer semantic connections between retrieval and generation. On this basis, an
uncertainty estimation mechanism is introduced to explicitly constrain and
filter low-confidence paths during the generation process, allowing the model
to maintain information coverage while effectively suppressing noise and false
content. The overall optimization objective consists of generation loss,
entropy constraints, and variance regularization, forming a unified confidence
control framework. In the experiments, comprehensive sensitivity tests and
comparative analyses were designed, covering hyperparameters, environmental
conditions, and data structures, to verify the stability and robustness of the
proposed method across different scenarios. The results show that the method
achieves superior performance over existing models in QA accuracy, retrieval
recall, ranking quality, and factual consistency, demonstrating the
effectiveness of combining multi-granularity indexing with confidence control.
This study not only provides a new technical pathway for retrieval-augmented
generation but also offers practical evidence for improving the reliability and
controllability of large models in complex contexts.

</details>


### [9] [Detecting Data Contamination in LLMs via In-Context Learning](https://arxiv.org/abs/2510.27055)
*Michał Zawalski,Meriem Boubdir,Klaudia Bałazy,Besmira Nushi,Pablo Ribalta*

Main category: cs.CL

TL;DR: CoDeC是一种检测和量化大型语言模型训练数据污染的实用而准确的方法。


<details>
  <summary>Details</summary>
Motivation: 区分训练期间记忆的数据和训练分布之外的数据。

Method: CoDeC通过测量上下文学习如何影响模型性能，来区分训练期间记忆的数据和训练分布之外的数据。研究发现，上下文示例通常会提高未见数据集的置信度，但当数据集是训练数据的一部分时，由于记忆模式被扰乱，置信度可能会降低。

Result: CoDeC产生了可解释的污染分数，这些分数清晰地分离了已见和未见数据集，并揭示了在训练语料库未公开的开源模型中存在强烈的记忆证据。

Conclusion: 该方法简单、自动化，且与模型和数据集无关，易于与基准评估。

Abstract: We present Contamination Detection via Context (CoDeC), a practical and
accurate method to detect and quantify training data contamination in large
language models. CoDeC distinguishes between data memorized during training and
data outside the training distribution by measuring how in-context learning
affects model performance. We find that in-context examples typically boost
confidence for unseen datasets but may reduce it when the dataset was part of
training, due to disrupted memorization patterns. Experiments show that CoDeC
produces interpretable contamination scores that clearly separate seen and
unseen datasets, and reveals strong evidence of memorization in open-weight
models with undisclosed training corpora. The method is simple, automated, and
both model- and dataset-agnostic, making it easy to integrate with benchmark
evaluations.

</details>


### [10] [Contrastive Knowledge Transfer and Robust Optimization for Secure Alignment of Large Language Models](https://arxiv.org/abs/2510.27077)
*Jiasen Zheng,Huajun Zhang,Xu Yan,Ran Hao,Chong Peng*

Main category: cs.CL

TL;DR: 该论文提出了一种结合对比蒸馏和噪声鲁棒训练的微调方法，旨在解决大型语言模型在安全对齐和鲁棒性方面的局限性。


<details>
  <summary>Details</summary>
Motivation: 解决大型语言模型在安全对齐和鲁棒性方面的局限性。

Method: 冻结主干模型，通过蒸馏将教师模型的知识边界转移到学生模型；引入噪声扰动和鲁棒优化约束，确保模型在噪声和不确定输入下保持稳定的预测输出；整体框架包括蒸馏损失、鲁棒性损失和正则化项。

Result: 该方法在知识迁移、鲁棒性和整体安全性方面显著优于现有基线，在几个关键指标上取得了最佳性能。

Conclusion: 该工作不仅丰富了参数高效微调的理论体系，而且为构建更安全、更值得信赖的对齐机制提供了新的解决方案。

Abstract: This paper addresses the limitations of large-scale language models in safety
alignment and robustness by proposing a fine-tuning method that combines
contrastive distillation with noise-robust training. The method freezes the
backbone model and transfers the knowledge boundaries of the teacher model to
the student model through distillation, thereby improving semantic consistency
and alignment accuracy. At the same time, noise perturbations and robust
optimization constraints are introduced during training to ensure that the
model maintains stable predictive outputs under noisy and uncertain inputs. The
overall framework consists of distillation loss, robustness loss, and a
regularization term, forming a unified optimization objective that balances
alignment ability with resistance to interference. To systematically validate
its effectiveness, the study designs experiments from multiple perspectives,
including distillation weight sensitivity, stability analysis under computation
budgets and mixed-precision environments, and the impact of data noise and
distribution shifts on model performance. Results show that the method
significantly outperforms existing baselines in knowledge transfer, robustness,
and overall safety, achieving the best performance across several key metrics.
This work not only enriches the theoretical system of parameter-efficient
fine-tuning but also provides a new solution for building safer and more
trustworthy alignment mechanisms.

</details>


### [11] [Characterizing Selective Refusal Bias in Large Language Models](https://arxiv.org/abs/2510.27087)
*Adel Khorramrouz,Sharon Levy*

Main category: cs.CL

TL;DR: 该研究探讨了大型语言模型（LLM）安全护栏中存在的选择性拒绝偏见，发现护栏在拒绝生成有害内容时对不同人口群体存在差异。


<details>
  <summary>Details</summary>
Motivation: 大型语言模型（LLM）的安全护栏旨在阻止恶意用户生成有害内容，但这些措施可能会无意中引入或反映新的偏见，导致LLM在拒绝针对某些人口群体的有害内容时表现出选择性。

Method: 本研究通过分析LLM护栏对不同个体和交叉人口群体的拒绝率、LLM的响应类型以及生成的拒绝内容的长度来探究选择性拒绝偏见。研究还通过一种间接攻击方式，针对之前被拒绝的群体，进一步探讨了安全隐患。

Result: 研究结果表明，在性别、性取向、国籍和宗教属性上都存在选择性拒绝偏见的证据。通过间接攻击，研究发现了额外的安全隐患。

Conclusion: 研究强调了在不同人口群体中，安全护栏需要更公平和稳健的性能。

Abstract: Safety guardrails in large language models(LLMs) are developed to prevent
malicious users from generating toxic content at a large scale. However, these
measures can inadvertently introduce or reflect new biases, as LLMs may refuse
to generate harmful content targeting some demographic groups and not others.
We explore this selective refusal bias in LLM guardrails through the lens of
refusal rates of targeted individual and intersectional demographic groups,
types of LLM responses, and length of generated refusals. Our results show
evidence of selective refusal bias across gender, sexual orientation,
nationality, and religion attributes. This leads us to investigate additional
safety implications via an indirect attack, where we target previously refused
groups. Our findings emphasize the need for more equitable and robust
performance in safety guardrails across demographic groups.

</details>


### [12] [Rating Roulette: Self-Inconsistency in LLM-As-A-Judge Frameworks](https://arxiv.org/abs/2510.27106)
*Rajarshi Haldar,Julia Hockenmaier*

Main category: cs.CL

TL;DR: 这篇文章探讨了大型语言模型（LLM）在评估自然语言生成（NLG）任务中的可靠性问题。


<details>
  <summary>Details</summary>
Motivation: 随着自然语言生成（NLG）的广泛应用，对其进行适当评估变得越来越困难。最近，使用大型语言模型（LLM）来评估这些生成内容受到了关注，因为它们往往比传统的n-gram或基于嵌入的指标更符合人类偏好。

Method: 本文通过实验，量化了LLM评估器在不同NLG任务和基准测试中评分的不一致性。此外，还研究了在遵循适当指导原则的情况下，LLM评估器是否仍然有用。

Result: 实验结果表明，LLM评估器在不同运行中给出的分数之间存在较低的评分者内部一致性（intra-rater reliability）。这种差异使得它们的评估结果不一致，在最坏的情况下几乎是任意的。这使得衡量它们的判断力变得困难。

Conclusion: LLM作为评估器拥有巨大潜力，但在实际应用中需要谨慎。通过遵循适当的指导原则，可以减轻LLM评估器的这种不一致性，使其评估结果更加可靠和有效。

Abstract: As Natural Language Generation (NLG) continues to be widely adopted, properly
assessing it has become quite difficult. Lately, using large language models
(LLMs) for evaluating these generations has gained traction, as they tend to
align more closely with human preferences than conventional n-gram or
embedding-based metrics. In our experiments, we show that LLM judges have low
intra-rater reliability in their assigned scores across different runs. This
variance makes their ratings inconsistent, almost arbitrary in the worst case,
making it difficult to measure how good their judgments actually are. We
quantify this inconsistency across different NLG tasks and benchmarks and see
if judicious use of LLM judges can still be useful following proper guidelines.

</details>


### [13] [Probability Distributions Computed by Hard-Attention Transformers](https://arxiv.org/abs/2510.27118)
*Andy Yang,Anej Svete,Jiaoda Li,Anthony Widjaja Lin,Jonathan Rawski,Ryan Cotterell,David Chiang*

Main category: cs.CL

TL;DR: 本文探讨了Transformer语言模型在表达概率分布方面的能力，并指出自回归和概率化特性对模型表达能力的影响，这与它们作为语言识别器的能力有所不同。


<details>
  <summary>Details</summary>
Motivation: 此前的研究主要将Transformer视为语言识别器，但实际上它们更常被用作语言模型（生成字符串）。本文旨在弥补这一认知差异，深入理解Transformer在作为语言模型时所能表达的功能。

Method: 通过分析Transformer语言识别器在自回归和概率化设置下的行为，来研究其概率分布的表达能力。

Result: 研究表明，使Transformer语言识别器自回归化有时可以提高其表达能力；而概率化则可能打破在非概率情况下成立的等价性。

Conclusion: 本文阐明了Transformer在作为语言模型这一常见用例中，其所能表达的功能特性。

Abstract: Most expressivity results for transformers treat them as language recognizers
(which accept or reject strings), and not as they are used in practice, as
language models (which generate strings autoregressively and
probabilistically). Here, we characterize the probability distributions that
transformer language models can express. We show that making transformer
language recognizers autoregressive can sometimes increase their expressivity,
and that making them probabilistic can break equivalences that hold in the
non-probabilistic case. Our overall contribution is to tease apart what
functions transformers are capable of expressing, in their most common use-case
as language models.

</details>


### [14] [Simple Additions, Substantial Gains: Expanding Scripts, Languages, and Lineage Coverage in URIEL+](https://arxiv.org/abs/2510.27183)
*Mason Shipton,York Hay Ng,Aditya Khan,Phuong Hanh Hoang,Xiang Lu,A. Seza Doğruöz,En-Shiun Annie Lee*

Main category: cs.CL

TL;DR: URIEL+是一个多语言知识库，但存在数据稀疏问题。本文通过引入文字系统向量、整合Glottolog数据和扩展谱系推断，减少了URIEL+的数据稀疏性，提高了语言覆盖率和推断质量，并在跨语言迁移任务中表现出性能提升。


<details>
  <summary>Details</summary>
Motivation: URIEL+语言知识库在多语言研究中很有用，但其数据稀疏性（包括缺失特征类型、不完整的语言条目和有限的谱系覆盖）限制了其在跨语言迁移中的应用，尤其是在支持低资源语言方面。

Method: 本文通过三项贡献来解决URIEL+的数据稀疏性问题：1. 引入文字系统向量，表示7,488种语言的书写系统属性。2. 整合Glottolog，增加18,710种额外语言。3. 扩展26,449种语言的谱系推断，通过谱系传播类型学和文字系统特征。

Result: 这些改进使文字系统向量的特征稀疏性减少了14%，语言覆盖率增加了多达19,015种语言（1,007%），推断质量指标提高了33%。在以低资源语言为中心的跨语言迁移任务基准测试中，性能增益在某些设置下高达6%。

Conclusion: 本文的改进使URIEL+在多语言研究中更加完整和包容，有效解决了数据稀疏问题，提升了跨语言迁移任务的性能。

Abstract: The URIEL+ linguistic knowledge base supports multilingual research by
encoding languages through geographic, genetic, and typological vectors.
However, data sparsity remains prevalent, in the form of missing feature types,
incomplete language entries, and limited genealogical coverage. This limits the
usefulness of URIEL+ in cross-lingual transfer, particularly for supporting
low-resource languages. To address this sparsity, this paper extends URIEL+
with three contributions: introducing script vectors to represent writing
system properties for 7,488 languages, integrating Glottolog to add 18,710
additional languages, and expanding lineage imputation for 26,449 languages by
propagating typological and script features across genealogies. These additions
reduce feature sparsity by 14% for script vectors, increase language coverage
by up to 19,015 languages (1,007%), and improve imputation quality metrics by
up to 33%. Our benchmark on cross-lingual transfer tasks (oriented around
low-resource languages) shows occasionally divergent performance compared to
URIEL+, with performance gains up to 6% in certain setups. Our advances make
URIEL+ more complete and inclusive for multilingual research.

</details>


### [15] [MemeArena: Automating Context-Aware Unbiased Evaluation of Harmfulness Understanding for Multimodal Large Language Models](https://arxiv.org/abs/2510.27196)
*Zixin Chen,Hongzhan Lin,Kaixin Li,Ziyang Luo,Yayue Deng,Jing Ma*

Main category: cs.CL

TL;DR: 这篇论文介绍了一个名为MemeArena的评估框架，用于客观评估多模态大语言模型（mLLM）对多模态有害内容的理解能力。


<details>
  <summary>Details</summary>
Motivation: 现有的评估方法主要关注二元分类任务中的检测准确性，但未能深入反映有害内容在不同上下文中的解释性细微差别。

Method: MemeArena是一个基于智能体的竞技场式评估框架，通过模拟不同的解释语境来制定评估任务，从而引发mLLM进行特定视角的分析。它通过整合不同观点并在评估者之间达成共识，实现对mLLM在解释多模态有害性能力的公平无偏比较。

Result: 实验证明MemeArena有效减少了判断智能体的评估偏差，判断结果与人类偏好高度一致。

Conclusion: MemeArena为多模态有害性理解中可靠、全面的mLLM评估提供了有价值的见解。

Abstract: The proliferation of memes on social media necessitates the capabilities of
multimodal Large Language Models (mLLMs) to effectively understand multimodal
harmfulness. Existing evaluation approaches predominantly focus on mLLMs'
detection accuracy for binary classification tasks, which often fail to reflect
the in-depth interpretive nuance of harmfulness across diverse contexts. In
this paper, we propose MemeArena, an agent-based arena-style evaluation
framework that provides a context-aware and unbiased assessment for mLLMs'
understanding of multimodal harmfulness. Specifically, MemeArena simulates
diverse interpretive contexts to formulate evaluation tasks that elicit
perspective-specific analyses from mLLMs. By integrating varied viewpoints and
reaching consensus among evaluators, it enables fair and unbiased comparisons
of mLLMs' abilities to interpret multimodal harmfulness. Extensive experiments
demonstrate that our framework effectively reduces the evaluation biases of
judge agents, with judgment results closely aligning with human preferences,
offering valuable insights into reliable and comprehensive mLLM evaluations in
multimodal harmfulness understanding. Our code and data are publicly available
at https://github.com/Lbotirx/MemeArena.

</details>


### [16] [Identifying the Periodicity of Information in Natural Language](https://arxiv.org/abs/2510.27241)
*Yulin Ou,Yu Wang,Yang Xu,Hendrik Buschmeier*

Main category: cs.CL

TL;DR: 本文提出了一种新的基于Surprisal的自动周期检测方法（APS），用于研究自然语言信息密度中的周期性模式。研究结果表明，人类语言在信息中表现出很强的周期性模式，并发现了超出文本典型结构单元分布的新周期。


<details>
  <summary>Details</summary>
Motivation: 探索自然语言中编码信息的周期性模式。

Method: 引入了一种名为Surprisal自动周期检测（APS）的新方法，该方法采用了规范的周期性检测算法，并能识别单个文档中Surprisal序列中存在的任何显著周期。

Result: 人类语言中有相当一部分信息表现出很强的周期性模式；发现了超出文本中典型结构单元（例如，句子边界，基本话语单元等）分布的新周期，并通过谐波回归模型进一步证实了这些新周期。

Conclusion: 语言中信息的周期性是结构因素和在更长距离上起作用的其他驱动因素共同作用的结果。

Abstract: Recent theoretical advancement of information density in natural language has
brought the following question on desk: To what degree does natural language
exhibit periodicity pattern in its encoded information? We address this
question by introducing a new method called AutoPeriod of Surprisal (APS). APS
adopts a canonical periodicity detection algorithm and is able to identify any
significant periods that exist in the surprisal sequence of a single document.
By applying the algorithm to a set of corpora, we have obtained the following
interesting results: Firstly, a considerable proportion of human language
demonstrates a strong pattern of periodicity in information; Secondly, new
periods that are outside the distributions of typical structural units in text
(e.g., sentence boundaries, elementary discourse units, etc.) are found and
further confirmed via harmonic regression modeling. We conclude that the
periodicity of information in language is a joint outcome from both structured
factors and other driving factors that take effect at longer distances. The
advantages of our periodicity detection method and its potentials in
LLM-generation detection are further discussed.

</details>


### [17] [Beyond a Million Tokens: Benchmarking and Enhancing Long-Term Memory in LLMs](https://arxiv.org/abs/2510.27246)
*Mohammad Tavakoli,Alireza Salemi,Carrie Ye,Mohamed Abdalla,Hamed Zamani,J Ross Mitchell*

Main category: cs.CL

TL;DR: 本文介绍了BEAM，这是一个新的基准，包含100个对话和2000个已验证的问题，用于评估LLM在需要长期记忆和长上下文推理任务中的能力。为了提高模型性能，我们提出了LIGHT，这个框架为LLM配备了三种互补的记忆系统。


<details>
  <summary>Details</summary>
Motivation: 现有基准无法有效评估LLM在需要长期记忆和长上下文推理任务中的能力。这些基准往往缺乏叙事连贯性，覆盖领域狭窄，并且只测试简单的回忆导向任务。

Method: 我们提出了一个新颖的框架，用于自动生成长（高达10M token）、连贯且主题多样的对话，并辅以针对各种记忆能力的探究性问题。在此基础上，我们构建了BEAM基准。同时，我们提出了LIGHT框架，它为LLM配备了三种互补的记忆系统：长期情景记忆、短期工作记忆和用于积累显著事实的暂存器。

Result: 在BEAM上的实验表明，即使是具有1M token上下文窗口的LLM（无论是否经过检索增强），在对话变长时也会遇到困难。相比之下，LIGHT在各种模型中持续提高了性能，相对于最强的基线，平均提升了3.5%至12.69%，具体取决于骨干LLM。消融研究进一步证实了每个记忆组件的贡献。

Conclusion: 本研究通过引入BEAM基准和LIGHT框架，有效解决了现有LLM评估和性能提升的挑战。BEAM提供了更全面和真实的评估环境，而LIGHT则通过模拟人类记忆系统显著提升了LLM在长上下文任务中的表现。

Abstract: Evaluating the abilities of large language models (LLMs) for tasks that
require long-term memory and thus long-context reasoning, for example in
conversational settings, is hampered by the existing benchmarks, which often
lack narrative coherence, cover narrow domains, and only test simple
recall-oriented tasks. This paper introduces a comprehensive solution to these
challenges. First, we present a novel framework for automatically generating
long (up to 10M tokens), coherent, and topically diverse conversations,
accompanied by probing questions targeting a wide range of memory abilities.
From this, we construct BEAM, a new benchmark comprising 100 conversations and
2,000 validated questions. Second, to enhance model performance, we propose
LIGHT-a framework inspired by human cognition that equips LLMs with three
complementary memory systems: a long-term episodic memory, a short-term working
memory, and a scratchpad for accumulating salient facts. Our experiments on
BEAM reveal that even LLMs with 1M token context windows (with and without
retrieval-augmentation) struggle as dialogues lengthen. In contrast, LIGHT
consistently improves performance across various models, achieving an average
improvement of 3.5%-12.69% over the strongest baselines, depending on the
backbone LLM. An ablation study further confirms the contribution of each
memory component.

</details>


### [18] [Languages are Modalities: Cross-Lingual Alignment via Encoder Injection](https://arxiv.org/abs/2510.27254)
*Rajan Agarwal,Aarush Gupta*

Main category: cs.CL

TL;DR: LLINK是一个轻量级、跨语言对齐框架，通过将多语言编码器的句子嵌入对齐到解码器的潜在嵌入空间，从而显著提高了低资源非拉丁语脚本上的性能。


<details>
  <summary>Details</summary>
Motivation: 针对指令调优的大型语言模型（LLMs）在低资源、非拉丁脚本上表现不佳的问题，这种不足源于分词器碎片化和弱跨语言耦合。

Method: LLINK是一种语言即模态的方法，首先将来自冻结多语言编码器的句子嵌入通过一个轻量级的对比投影器对齐到解码器的潜在嵌入空间的保留位置；其次，将该向量扩展为K个软槽，并使用最小适配器进行训练，以便冻结的解码器能够接收信号。

Result: LLINK显著改善了双语检索，并在LLM评估的问答中，相对于基础模型获得了81.3%的偏好，相对于直接微调获得了63.6%的偏好。

Conclusion: 将低资源语言视为一种模态，为轻量级LLM中实现更强的跨语言对齐提供了一条实用途径。

Abstract: Instruction-tuned Large Language Models (LLMs) underperform on low resource,
non-Latin scripts due to tokenizer fragmentation and weak cross-lingual
coupling. We present LLINK (Latent Language Injection for Non-English
Knowledge), a compute efficient language-as-modality method that conditions an
instruction-tuned decoder without changing the tokenizer or retraining the
decoder. First, we align sentence embeddings from a frozen multilingual encoder
to the decoder's latent embedding space at a reserved position via a
lightweight contrastive projector. Second, the vector is expanded into K soft
slots and trained with minimal adapters so the frozen decoder consumes the
signal. LLINK substantially improves bilingual retrieval and achieves 81.3%
preference over the base model and 63.6% over direct fine-tuning in LLM-judged
Q&A evaluations. We further find that improvements can be attributed to reduced
tokenization inflation and a stronger cross lingual alignment, despite the
model having residual weaknesses in numeric fidelity. Treating low resource
languages as a modality offers a practical path to stronger cross-lingual
alignment in lightweight LLMs.

</details>


### [19] [MedCalc-Eval and MedCalc-Env: Advancing Medical Calculation Capabilities of Large Language Models](https://arxiv.org/abs/2510.27267)
*Kangkun Mao,Jinru Ding,Jiayuan Chen,Mouxiao Bian,Ruiyao Chen,Xinwei Peng,Sijie Ren,Linyang Li,Jie Xu*

Main category: cs.CL

TL;DR: MedCalc-Eval 是一个评估大型语言模型医疗计算能力的最大基准测试，它包含 700 多个任务，涵盖方程计算和基于规则的评分系统。为了提高性能，MedCalc-Env 环境被开发出来，它在 MedCalc-Eval 上取得了最先进的结果，但在单位转换、多条件逻辑和上下文理解方面仍存在挑战。


<details>
  <summary>Details</summary>
Motivation: 目前大多数评估大型语言模型 (LLMs) 在医疗领域能力的基准测试都侧重于问答或描述性推理，忽略了对临床决策至关重要的定量推理。现有的数据集（如 MedCalc-Bench）计算任务较少，未能反映真实的计算场景。

Method: 我们引入了 MedCalc-Eval，这是评估 LLMs 医疗计算能力的基准测试，包含 700 多个任务，分为两类：基于方程的计算（例如 Cockcroft-Gault、BMI、BSA）和基于规则的评分系统（例如 Apgar、格拉斯哥昏迷量表）。这些任务涵盖了内科、外科、儿科和心脏病学等多个专业。为了提高性能，我们进一步开发了 MedCalc-Env，这是一个基于 InternBootcamp 框架的强化学习环境，支持多步骤临床推理和规划。

Result: 在 MedCalc-Env 环境中对 Qwen2.5-32B 模型进行微调，在 MedCalc-Eval 上取得了最先进的结果，在数值敏感度、公式选择和推理鲁棒性方面有显著提升。

Conclusion: MedCalc-Eval 为评估 LLMs 的医疗计算能力提供了一个全面的基准。尽管 MedCalc-Env 取得了显著进展，但在单位转换、多条件逻辑和上下文理解方面仍存在挑战，未来需要进一步研究。

Abstract: As large language models (LLMs) enter the medical domain, most benchmarks
evaluate them on question answering or descriptive reasoning, overlooking
quantitative reasoning critical to clinical decision-making. Existing datasets
like MedCalc-Bench cover few calculation tasks and fail to reflect real-world
computational scenarios.
  We introduce MedCalc-Eval, the largest benchmark for assessing LLMs' medical
calculation abilities, comprising 700+ tasks across two types: equation-based
(e.g., Cockcroft-Gault, BMI, BSA) and rule-based scoring systems (e.g., Apgar,
Glasgow Coma Scale). These tasks span diverse specialties including internal
medicine, surgery, pediatrics, and cardiology, offering a broader and more
challenging evaluation setting.
  To improve performance, we further develop MedCalc-Env, a reinforcement
learning environment built on the InternBootcamp framework, enabling multi-step
clinical reasoning and planning. Fine-tuning a Qwen2.5-32B model within this
environment achieves state-of-the-art results on MedCalc-Eval, with notable
gains in numerical sensitivity, formula selection, and reasoning robustness.
Remaining challenges include unit conversion, multi-condition logic, and
contextual understanding.
  Code and datasets are available at
https://github.com/maokangkun/MedCalc-Eval.

</details>


### [20] [Why Do Multilingual Reasoning Gaps Emerge in Reasoning Language Models?](https://arxiv.org/abs/2510.27269)
*Deokhyung Kang,Seonjeong Hwang,Daehui Kim,Hyounghun Kim,Gary Geunbae Lee*

Main category: cs.CL

TL;DR: RLM在多语言推理方面存在鸿沟，本文发现这主要是语言理解失败所致。通过检测理解失败并选择性地进行翻译，可以有效弥补这一鸿沟，达到接近完全翻译的性能，且只需对约20%的输入进行翻译。


<details>
  <summary>Details</summary>
Motivation: 探索多语言推理模型（RLM）中存在的“多语言推理鸿沟”的根本原因，并解决这一问题。

Method: 通过分析发现多语言推理鸿沟主要源于语言理解失败，即模型无法将多语言输入含义准确地表征为推理链中主流语言（英语）。为了解决这个问题，本文评估了多种检测理解失败的方法，并发现有监督方法表现最佳。在此基础上，提出了“选择性翻译”策略，仅在检测到理解失败时将多语言输入翻译成英语。

Result: 实验结果表明，“选择性翻译”策略有效地弥合了多语言推理鸿沟，实现了接近完全翻译的性能，但仅对约20%的输入进行了翻译。

Conclusion: 多语言推理鸿沟的主要原因在于语言理解失败，并且这种失败是可以被检测和选择性地缓解的。这为更公平的多语言推理提供了一个重要的见解和有前景的路径。

Abstract: Reasoning language models (RLMs) achieve strong performance on complex
reasoning tasks, yet they still suffer from a multilingual reasoning gap,
performing better in high-resource languages than in low-resource ones. While
recent efforts have reduced this gap, its underlying causes remain largely
unexplored. In this paper, we address this by showing that the multilingual
reasoning gap largely stems from failures in language understanding-the model's
inability to represent the multilingual input meaning into the dominant
language (i.e., English) within its reasoning trace. This motivates us to
examine whether understanding failures can be detected, as this ability could
help mitigate the multilingual reasoning gap. To this end, we evaluate a range
of detection methods and find that understanding failures can indeed be
identified, with supervised approaches performing best. Building on this, we
propose Selective Translation, a simple yet effective strategy that translates
the multilingual input into English only when an understanding failure is
detected. Experimental results show that Selective Translation bridges the
multilingual reasoning gap, achieving near full-translation performance while
using translation for only about 20% of inputs. Together, our work demonstrates
that understanding failures are the primary cause of the multilingual reasoning
gap and can be detected and selectively mitigated, providing key insight into
its origin and a promising path toward more equitable multilingual reasoning.
Our code and data are publicly available at
https://github.com/deokhk/RLM_analysis.

</details>


### [21] [A Unified Representation Underlying the Judgment of Large Language Models](https://arxiv.org/abs/2510.27328)
*Yi-Long Lu,Jiajun Song,Wei Wang*

Main category: cs.CL

TL;DR: 深入研究了LLMs的内在结构，揭示了其判断能力并非完全模块化，而是通过一个名为“Valence-Assent Axis (VAA)”的单一维度进行关键收敛。这个VAA统一编码了主观评价和模型对事实主张的认同。


<details>
  <summary>Details</summary>
Motivation: 大型语言模型（LLMs）的内部架构中，其判断能力是依赖于专门的模块还是统一的、领域通用的资源，是一个核心的架构问题。以往的研究发现了可解码的独立概念的神经表征，这暗示了模块化的架构，但这些表征是否真正独立仍未解决。

Method: 通过对一系列LLMs的评估，研究人员发现不同的评价判断都是沿着一个主导维度计算的，这个维度被称为“Valence-Assent Axis (VAA)”。并通过直接干预，表明这种统一的表征创建了一个关键的依赖关系：VAA作为一个控制信号，引导生成过程构建与评估状态一致的理由。

Result: VAA统一编码了主观评价（“什么是好的”）和模型对事实主张的认同（“什么是真实的”）。即使以牺牲事实准确性为代价，VAA也能引导生成过程构建与评估状态一致的理由。这种机制（被称为“推理的服从”）将推理过程从公正的推断转向了目标导向的 F 论证。

Conclusion: 本研究揭示了系统性偏差和幻觉的机械解释，揭示了促进连贯判断的架构如何系统性地破坏忠实推理。

Abstract: A central architectural question for both biological and artificial
intelligence is whether judgment relies on specialized modules or a unified,
domain-general resource. While the discovery of decodable neural
representations for distinct concepts in Large Language Models (LLMs) has
suggested a modular architecture, whether these representations are truly
independent systems remains an open question. Here we provide evidence for a
convergent architecture. Across a range of LLMs, we find that diverse
evaluative judgments are computed along a dominant dimension, which we term the
Valence-Assent Axis (VAA). This axis jointly encodes subjective valence ("what
is good") and the model's assent to factual claims ("what is true"). Through
direct interventions, we show this unified representation creates a critical
dependency: the VAA functions as a control signal that steers the generative
process to construct a rationale consistent with its evaluative state, even at
the cost of factual accuracy. This mechanism, which we term the subordination
of reasoning, shifts the process of reasoning from impartial inference toward
goal-directed justification. Our discovery offers a mechanistic account for
systemic bias and hallucination, revealing how an architecture that promotes
coherent judgment can systematically undermine faithful reasoning.

</details>


### [22] [TransAlign: Machine Translation Encoders are Strong Word Aligners, Too](https://arxiv.org/abs/2510.27337)
*Benedikt Ebing,Christian Goldschmied,Goran Glavaš*

Main category: cs.CL

TL;DR: TransAlign是一种利用大规模多语言机器翻译模型的编码器进行词对齐的新方法，它在机器翻译的跨语言迁移中，在词对齐和基于标签投射的token分类任务方面表现出色。


<details>
  <summary>Details</summary>
Motivation: 在缺乏大量训练数据的情况下，跨语言迁移(XLT)是一个重要问题。现有的翻译策略，如translate-test和translate-train，需要标签投射，而这通常依赖于多语言词对齐器。然而，从机器翻译模型中提取对齐的研究通常效果不佳。

Method: 本文提出TransAlign，它利用大规模多语言机器翻译模型的编码器进行词对齐。

Result: TransAlign实现了强大的词对齐性能，并且在基于机器翻译的token分类跨语言迁移任务中，明显优于流行的词对齐器和最先进的非词对齐的标签投射方法。

Conclusion: TransAlign为解决跨语言迁移中的标签投射问题提供了一个有效且高性能的解决方案，特别是在token分类任务中。它通过利用机器翻译模型的编码器，提升了词对齐的准确性和效率。

Abstract: In the absence of sizable training data for most world languages and NLP
tasks, translation-based strategies such as translate-test -- evaluating on
noisy source language data translated from the target language -- and
translate-train -- training on noisy target language data translated from the
source language -- have been established as competitive approaches for
cross-lingual transfer (XLT). For token classification tasks, these strategies
require label projection: mapping the labels from each token in the original
sentence to its counterpart(s) in the translation. To this end, it is common to
leverage multilingual word aligners (WAs) derived from encoder language models
such as mBERT or LaBSE. Despite obvious associations between machine
translation (MT) and WA, research on extracting alignments with MT models is
largely limited to exploiting cross-attention in encoder-decoder architectures,
yielding poor WA results. In this work, in contrast, we propose TransAlign, a
novel word aligner that utilizes the encoder of a massively multilingual MT
model. We show that TransAlign not only achieves strong WA performance but
substantially outperforms popular WA and state-of-the-art non-WA-based label
projection methods in MT-based XLT for token classification.

</details>


### [23] [ThoughtProbe: Classifier-Guided LLM Thought Space Exploration via Probing Representations](https://arxiv.org/abs/2510.27355)
*Zijian Wang,Chang Xu*

Main category: cs.CL

TL;DR: 本文介绍了ThoughtProbe，一个利用大型语言模型（LLMs）隐藏推理特征来提高其推理性能的推理时间框架。


<details>
  <summary>Details</summary>
Motivation: 利用大型语言模型的隐藏推理特征来提高其推理性能，而之前的研究主要通过操作隐藏表示来引导LLM生成。本文的目标是利用这些隐藏特征作为判别信号，指导树状响应空间探索。

Method: ThoughtProbe框架在每个节点扩展时，使用分类器作为评分和排序机制，通过优先处理得分较高的候选者来有效地分配计算资源。完成树扩展后，从所有分支收集答案，形成候选答案池。然后，提出一种分支聚合方法，通过聚合CoT分数来边缘化所有支持分支，从而从池中识别出最优答案。

Result: 实验结果表明，ThoughtProbe框架的全面探索不仅覆盖了有效的推理链，而且有效地识别了它们。

Conclusion: ThoughtProbe框架在多个算术推理基准测试中取得了显著的改进。

Abstract: This paper introduces ThoughtProbe, a novel inference time framework that
leverages the hidden reasoning features of Large Language Models (LLMs) to
improve their reasoning performance. Unlike previous works that manipulate the
hidden representations to steer LLM generation, we harness them as
discriminative signals to guide the tree structured response space exploration.
In each node expansion, a classifier serves as a scoring and ranking mechanism
that efficiently allocates computational resources by prioritizing higher score
candidates for continuation. After completing the tree expansion, we collect
answers from all branches to form a candidate answer pool. We then propose a
branch aggregation method that marginalizes over all supporting branches by
aggregating their CoT scores, thereby identifying the optimal answer from the
pool. Experimental results show that our framework's comprehensive exploration
not only covers valid reasoning chains but also effectively identifies them,
achieving significant improvements across multiple arithmetic reasoning
benchmarks.

</details>


### [24] [Detecting Prefix Bias in LLM-based Reward Models](https://arxiv.org/abs/2505.13487)
*Ashwin Kumar,Yuzi He,Aram H. Markosyan,Bobbie Chern,Imanol Arrieta-Ibarra*

Main category: cs.CL

TL;DR: 这篇论文研究了在大型语言模型奖励模型中，可能存在的“前缀偏差”问题，这种偏差与查询前缀的微小变化有关。研究发现，这种偏差在不同的偏好数据集和奖励模型架构中普遍存在，尤其是在种族和性别维度上。文章进而提出了一种数据增强策略以减轻这种偏差。


<details>
  <summary>Details</summary>
Motivation: RLHF在根据人类偏好数据对语言模型进行任务特定微调方面已成为关键范式。然而，在现有的奖励模型中，潜在的偏差（特别是前缀偏差）仍未被充分探索。

Method: 本文引入了新的方法来检测和评估大型语言模型奖励模型中的前缀偏差，该偏差由查询前缀的微小变化触发。通过这些指标，揭示了偏好模型在种族和性别方面存在的显著偏差。

Result: 在各种开源偏好数据集和奖励模型架构中，这种前缀偏差普遍存在，并且与底层模型架构无关。研究发现，奖励模型显示出显著的种族和性别偏差。

Conclusion: 本文揭示了大型语言模型奖励模型中前缀偏差的普遍存在，并提出了数据增强策略来缓解这种偏差。研究强调了在开发公平可靠的奖励模型时，偏见感知数据集设计和评估的必要性，这有助于人工智能公平性的讨论。

Abstract: Reinforcement Learning with Human Feedback (RLHF) has emerged as a key
paradigm for task-specific fine-tuning of language models using human
preference data. While numerous publicly available preference datasets provide
pairwise comparisons of responses, the potential for biases in the resulting
reward models remains underexplored. In this work, we introduce novel methods
to detect and evaluate prefix bias -- a systematic shift in model preferences
triggered by minor variations in query prefixes -- in LLM-based reward models
trained on such datasets. We leverage these metrics to reveal significant
biases in preference models across racial and gender dimensions. Our
comprehensive evaluation spans diverse open-source preference datasets and
reward model architectures, demonstrating susceptibility to this kind of bias
regardless of the underlying model architecture. Furthermore, we propose a data
augmentation strategy to mitigate these biases, showing its effectiveness in
reducing the impact of prefix bias. Our findings highlight the critical need
for bias-aware dataset design and evaluation in developing fair and reliable
reward models, contributing to the broader discourse on fairness in AI.

</details>


### [25] [Balancing Knowledge Updates: Toward Unified Modular Editing in LLMs](https://arxiv.org/abs/2510.27400)
*Jiahao Liu,Zijian Wang,Kuo Zhao,Dong Hu*

Main category: cs.CL

TL;DR: 知识编辑是更新大语言模型中事实知识的有效方法，之前的方法主要关注MLP模块，但我们发现Attention模块，特别是早期层的，在事实知识存储和检索中也扮演重要角色。因此，我们提出了IntAttn-Edit方法，联合更新MLP和Attention模块，并通过知识平衡策略分配更新量。实验表明，该方法在编辑成功率、泛化能力和知识保留方面优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 现有的知识编辑方法主要关注大语言模型（LLMs）中的多层感知器（MLP）模块，认为它们是事实知识的主要存储库。然而，这种做法忽略了模型中其他重要组件，如注意力（Attn）模块，可能导致残余的过时知识并限制编辑效果。

Method: 我们对先进的LLMs进行了全面的知识定位实验，发现Attn模块在事实知识存储和检索中，尤其是在早期层，发挥着重要作用。基于这些发现，我们提出了IntAttn-Edit方法。该方法将关联记忆范式扩展到联合更新MLP和Attn模块。此外，它采用了一种知识平衡策略，根据每个模块对知识存储的贡献来按比例分配更新量。

Result: 在标准基准测试中，IntAttn-Edit方法与现有方法相比，取得了更高的编辑成功率、更好的泛化能力和更强的知识保留能力。进一步的分析表明，知识平衡策略在各种设置下都能使编辑性能保持在最佳范围内。

Conclusion: 本研究指出，Attention模块在大语言模型的事实知识存储和检索中扮演着不可忽视的角色，特别是在早期层。我们提出的IntAttn-Edit方法，通过联合更新MLP和Attention模块，并采用知识平衡策略，显著提升了知识编辑的效率和效果。

Abstract: Knowledge editing has emerged as an efficient approach for updating factual
knowledge in large language models (LLMs). It typically locates knowledge
storage modules and then modifies their parameters. However, most existing
methods focus on the weights of multilayer perceptron (MLP) modules, which are
often identified as the main repositories of factual information. Other
components, such as attention (Attn) modules, are often ignored during editing.
This imbalance can leave residual outdated knowledge and limit editing
effectiveness. We perform comprehensive knowledge localization experiments on
advanced LLMs and find that Attn modules play a substantial role in factual
knowledge storage and retrieval, especially in earlier layers. Based on these
insights, we propose IntAttn-Edit, a method that extends the associative memory
paradigm to jointly update both MLP and Attn modules. Our approach uses a
knowledge balancing strategy that allocates update magnitudes in proportion to
each module's measured contribution to knowledge storage. Experiments on
standard benchmarks show that IntAttn-Edit achieves higher edit success, better
generalization, and stronger knowledge preservation than prior methods. Further
analysis shows that the balancing strategy keeps editing performance within an
optimal range across diverse settings.

</details>


### [26] [Awal -- Community-Powered Language Technology for Tamazight](https://arxiv.org/abs/2510.27407)
*Alp Öktem,Farida Boudichat*

Main category: cs.CL

TL;DR: 该文章介绍了Awal项目，一个旨在为Tamazight语开发语言技术资源的社区驱动平台，并探讨了在数据稀缺和标准化挑战下社区参与的障碍和局限性。


<details>
  <summary>Details</summary>
Motivation: 解决Tamazight在数字领域代表性不足的问题，并通过社区贡献开发语言技术资源。

Method: 通过 awaldigital.org 平台，聚合社区贡献的翻译和语音数据，并通过对18个月社区参与的分析来评估其有效性。

Result: 尽管社区反响积极，但数据贡献主要集中在语言学家和活动家。社区贡献的数据量适中（6,421对翻译和3小时语音数据），表明标准众包方法在复杂社会语言背景下的局限性。

Conclusion: 现有的众包方法可能不适用于像Tamazight这样具有复杂社会语言背景的语言。需要改进策略以克服参与障碍。项目团队正在利用收集到的数据改进开源机器翻译模型。

Abstract: This paper presents Awal, a community-powered initiative for developing
language technology resources for Tamazight. We provide a comprehensive review
of the NLP landscape for Tamazight, examining recent progress in computational
resources, and the emergence of community-driven approaches to address
persistent data scarcity. Launched in 2024, awaldigital.org platform addresses
the underrepresentation of Tamazight in digital spaces through a collaborative
platform enabling speakers to contribute translation and voice data. We analyze
18 months of community engagement, revealing significant barriers to
participation including limited confidence in written Tamazight and ongoing
standardization challenges. Despite widespread positive reception, actual data
contribution remained concentrated among linguists and activists. The modest
scale of community contributions -- 6,421 translation pairs and 3 hours of
speech data -- highlights the limitations of applying standard crowdsourcing
approaches to languages with complex sociolinguistic contexts. We are working
on improved open-source MT models using the collected data.

</details>


### [27] [VCORE: Variance-Controlled Optimization-based Reweighting for Chain-of-Thought Supervision](https://arxiv.org/abs/2510.27462)
*Xuan Gong,Senmiao Wang,Hanbo Huang,Ruoyu Sun,Shiyu Liang*

Main category: cs.CL

TL;DR: VCORE通过将CoT监督重新定义为受约束的优化问题，解决了SFT中令牌重新加权的挑战，从而在各种LLM和基准测试中实现了卓越的性能。


<details>
  <summary>Details</summary>
Motivation: 现有的监督微调（SFT）方法在处理长思维链（CoT）轨迹时，对所有令牌一视同仁，忽略了它们在推理过程中的不同贡献，导致监督分配不当和泛化能力弱。

Method: 本文引入了VCORE（Variance-Controlled Optimization-based REweighting）框架，该框架将CoT监督重新定义为约束优化问题。VCORE采用优化理论视角，实现了跨令牌监督的原则性自适应分配，使训练目标与鲁棒推理泛化目标更紧密地对齐。

Result: VCORE在数学和编码基准测试中，无论是在域内还是域外设置下，都持续优于现有的令牌重新加权方法，并取得了显著的性能提升。VCORE还被证明是后续强化学习的更有效初始化方法。

Conclusion: VCORE通过优化理论方法有效解决了长CoT轨迹SFT中令牌监督分配不均的问题，显著提高了大型语言模型的推理能力和泛化性，并为未来的强化学习奠定了坚实基础。

Abstract: Supervised fine-tuning (SFT) on long chain-of-thought (CoT) trajectories has
emerged as a crucial technique for enhancing the reasoning abilities of large
language models (LLMs). However, the standard cross-entropy loss treats all
tokens equally, ignoring their heterogeneous contributions across a reasoning
trajectory. This uniform treatment leads to misallocated supervision and weak
generalization, especially in complex, long-form reasoning tasks. To address
this, we introduce \textbf{V}ariance-\textbf{C}ontrolled
\textbf{O}ptimization-based \textbf{RE}weighting (VCORE), a principled
framework that reformulates CoT supervision as a constrained optimization
problem. By adopting an optimization-theoretic perspective, VCORE enables a
principled and adaptive allocation of supervision across tokens, thereby
aligning the training objective more closely with the goal of robust reasoning
generalization. Empirical evaluations demonstrate that VCORE consistently
outperforms existing token reweighting methods. Across both in-domain and
out-of-domain settings, VCORE achieves substantial performance gains on
mathematical and coding benchmarks, using models from the Qwen3 series (4B, 8B,
32B) and LLaMA-3.1-8B-Instruct. Moreover, we show that VCORE serves as a more
effective initialization for subsequent reinforcement learning, establishing a
stronger foundation for advancing the reasoning capabilities of LLMs. The Code
will be released at https://github.com/coder-gx/VCORE.

</details>


### [28] [Diffuse Thinking: Exploring Diffusion Language Models as Efficient Thought Proposers for Reasoning](https://arxiv.org/abs/2510.27469)
*Chenyang Shao,Sijian Ren,Fengli Xu,Yong Li*

Main category: cs.CL

TL;DR: 该研究提出了一种名为“Diffuse Thinking”的高效协同推理框架，它利用扩散语言模型（DLMs）生成候选思考过程，并由大型语言模型（LLMs）评估其质量，从而在保持推理性能的同时显著降低计算成本。


<details>
  <summary>Details</summary>
Motivation: 大型语言模型（LLMs）在推理能力方面取得了显著进展，但其自回归生成范式导致计算开销大且性能提升边际化。

Method: 提出了一种高效协同推理框架，利用扩散语言模型（DLMs）并行生成多样化的中间思考，从而减轻了大型语言模型（LLMs）自回归生成带来的计算负担，同时利用LLMs来评估这些思考的质量。

Result: 在多个基准测试中，该框架在复杂推理任务上取得了强大的性能。

Conclusion: 该框架为未来的复杂推理任务研究提供了一个有前景的方向，通过结合DLMs和LLMs的优势，实现了计算效率和推理质量的平衡。

Abstract: In recent years, large language models (LLMs) have witnessed remarkable
advancements, with the test-time scaling law consistently enhancing the
reasoning capabilities. Through systematic evaluation and exploration of a
diverse spectrum of intermediate thoughts, LLMs demonstrate the potential to
generate deliberate reasoning steps, thereby substantially enhancing reasoning
accuracy. However, LLMs' autoregressive generation paradigm results in
reasoning performance scaling sub-optimally with test-time computation, often
requiring excessive computational overhead to propose thoughts while yielding
only marginal performance gains. In contrast, diffusion language models (DLMs)
can efficiently produce diverse samples through parallel denoising in a single
forward pass, inspiring us to leverage them for proposing intermediate
thoughts, thereby alleviating the computational burden associated with
autoregressive generation while maintaining quality. In this work, we propose
an efficient collaborative reasoning framework, leveraging DLMs to generate
candidate thoughts and LLMs to evaluate their quality. Experiments across
diverse benchmarks demonstrate that our framework achieves strong performance
in complex reasoning tasks, offering a promising direction for future research.
Our code is open-source at
https://anonymous.4open.science/r/Diffuse-Thinking-EC60.

</details>


### [29] [The aftermath of compounds: Investigating Compounds and their Semantic Representations](https://arxiv.org/abs/2510.27477)
*Swarang Joshi*

Main category: cs.CL

TL;DR: 本文探讨了计算嵌入与人类在英语复合词处理中语义判断的一致性。


<details>
  <summary>Details</summary>
Motivation: 探索计算嵌入（静态词向量和上下文嵌入）与人类对词素意义主导（LMD）和语义透明度（ST）判断的一致性。

Method: 通过关联强度、频率和可预测性等指标，计算嵌入派生的LMD和ST度量，并通过Spearman相关和回归分析评估它们与人类判断的关系。

Result: BERT嵌入比GloVe更好地捕捉了组合语义，并且可预测性评级是人类和模型数据中语义透明度的强预测因子。

Conclusion: 这些发现通过阐明驱动复合词处理的因素，并提供对基于嵌入的语义建模的见解，推动了计算心理语言学的发展。

Abstract: This study investigates how well computational embeddings align with human
semantic judgments in the processing of English compound words. We compare
static word vectors (GloVe) and contextualized embeddings (BERT) against human
ratings of lexeme meaning dominance (LMD) and semantic transparency (ST) drawn
from a psycholinguistic dataset. Using measures of association strength
(Edinburgh Associative Thesaurus), frequency (BNC), and predictability (LaDEC),
we compute embedding-derived LMD and ST metrics and assess their relationships
with human judgments via Spearmans correlation and regression analyses. Our
results show that BERT embeddings better capture compositional semantics than
GloVe, and that predictability ratings are strong predictors of semantic
transparency in both human and model data. These findings advance computational
psycholinguistics by clarifying the factors that drive compound word processing
and offering insights into embedding-based semantic modeling.

</details>


### [30] [Effect of Domain Generalization Techniques in Low Resource Systems](https://arxiv.org/abs/2510.27512)
*Mahi Aminu,Chisom Chibuike,Fatimo Adebanjo,Omokolade Awosanya,Samuel Oyeneye*

Main category: cs.CL

TL;DR: 这篇论文探讨了两种在低资源下进行领域泛化的因果机器学习方法，并验证了它们在低资源NLP任务中的有效性。


<details>
  <summary>Details</summary>
Motivation: 机器学习模型在现实世界中常常面临训练和测试数据分布不一致的问题，尤其在低资源场景下，数据稀缺和领域多样性有限会阻碍模型的泛化能力。为了解决这个问题，需要开发能够学习跨领域不变特征的方法，从而提高模型的鲁棒性。

Method: 本研究考察了两种不同的因果领域泛化技术。首先，作者们研究了一种因果数据增强（CDA）方法，该方法通过自动生成反事实样本来提高模型对虚假相关性的鲁棒性，并将其应用于NaijaSenti Twitter语料库上的情感分类任务。其次，作者们探索了一种使用DINER框架的不变因果表征学习（ICRL）方法，并将其应用于多语言情感分析设置。

Result: 研究结果表明，这两种方法都能增强模型对未见领域的鲁棒性：反事实数据增强在情感分类中获得了持续的跨领域准确性提升，而使用DINER的因果表征学习则在多语言情感分析中改善了分布外性能，尽管在不同语言中增益有所不同。

Conclusion: 本研究成功地在低资源NLP任务中应用并验证了两种因果领域泛化技术，为解决因果关系中的分布偏移和提高模型泛化能力提供了有效的途径。

Abstract: Machine learning models typically assume that training and test data follow
the same distribution, an assumption that often fails in real-world scenarios
due to distribution shifts. This issue is especially pronounced in low-resource
settings, where data scarcity and limited domain diversity hinder robust
generalization. Domain generalization (DG) approaches address this challenge by
learning features that remain invariant across domains, often using causal
mechanisms to improve model robustness. In this study, we examine two distinct
causal DG techniques in low-resource natural language tasks. First, we
investigate a causal data augmentation (CDA) approach that automatically
generates counterfactual examples to improve robustness to spurious
correlations. We apply this method to sentiment classification on the
NaijaSenti Twitter corpus, expanding the training data with semantically
equivalent paraphrases to simulate controlled distribution shifts. Second, we
explore an invariant causal representation learning (ICRL) approach using the
DINER framework, originally proposed for debiasing aspect-based sentiment
analysis. We adapt DINER to a multilingual setting. Our findings demonstrate
that both approaches enhance robustness to unseen domains: counterfactual data
augmentation yields consistent cross-domain accuracy gains in sentiment
classification, while causal representation learning with DINER improves
out-of-distribution performance in multilingual sentiment analysis, albeit with
varying gains across languages.

</details>


### [31] [BiSparse-AAS: Bilinear Sparse Attention and Adaptive Spans Framework for Scalable and Efficient Text Summarization](https://arxiv.org/abs/2510.27516)
*Desta Haileselassie Hagos,Legand L. Burge,Anietie Andy,Anis Yazidi,Vladimir Vlassov*

Main category: cs.CL

TL;DR: BiSparse-AAS (Bilinear Sparse Attention with Adaptive Spans) 是一种新的框架，它结合了稀疏注意力、自适应跨度和双线性注意力，用于解决Transformer模型在长文档文本摘要方面的二次复杂性限制。


<details>
  <summary>Details</summary>
Motivation: Transformer模型在处理长文档时存在二次复杂性限制。

Method: BiSparse-AAS 结合了稀疏注意力、自适应跨度和双线性注意力：稀疏注意力通过关注输入中最相关的部分来降低计算成本；自适应跨度动态调整注意力范围；双线性注意力在优化的上下文中模拟复杂的token交互。

Result: BiSparse-AAS 在抽取式和抽象式摘要任务中都优于最先进的基线模型，在CNN/DailyMail数据集上ROUGE平均提升约68.1%，在XSum数据集上提升52.6%，同时在OpenWebText和Gigaword数据集上保持了强劲的性能。

Conclusion: BiSparse-AAS 通过解决效率、可扩展性和长序列建模问题，为真实世界的文本摘要应用提供了一个统一、实用的解决方案。

Abstract: Transformer-based architectures have advanced text summarization, yet their
quadratic complexity limits scalability on long documents. This paper
introduces BiSparse-AAS (Bilinear Sparse Attention with Adaptive Spans), a
novel framework that combines sparse attention, adaptive spans, and bilinear
attention to address these limitations. Sparse attention reduces computational
costs by focusing on the most relevant parts of the input, while adaptive spans
dynamically adjust the attention ranges. Bilinear attention complements both by
modeling complex token interactions within this refined context. BiSparse-AAS
consistently outperforms state-of-the-art baselines in both extractive and
abstractive summarization tasks, achieving average ROUGE improvements of about
68.1% on CNN/DailyMail and 52.6% on XSum, while maintaining strong performance
on OpenWebText and Gigaword datasets. By addressing efficiency, scalability,
and long-sequence modeling, BiSparse-AAS provides a unified, practical solution
for real-world text summarization applications.

</details>


### [32] [SQLSpace: A Representation Space for Text-to-SQL to Discover and Mitigate Robustness Gaps](https://arxiv.org/abs/2510.27532)
*Neha Srikanth,Victor Bursztyn,Puneet Mathur,Ani Nenkova*

Main category: cs.CL

TL;DR: SQLSpace是一种人类可解释的文本到SQL示例紧凑表示，可以用于评估、理解模型性能和提高模型性能。


<details>
  <summary>Details</summary>
Motivation: 此论文旨在介绍SQLSpace，一种人类可解释的、可推广的、紧凑的文本到SQL示例表示方法，其提取过程仅需最少的人工干预。

Method: SQLSpace是一种表示方法，能够密切比较和对比流行的文本到SQL基准测试的组成，以识别它们评估的示例的独特维度；理解超越整体准确性分数，在更细粒度级别的模型性能；通过基于学习到的正确性估计进行有针对性的查询重写来提高模型性能。

Result: SQLSpace揭示了基准测试之间的组成差异，揭示了仅靠准确性掩盖的性能模式，并支持查询成功建模。

Conclusion: SQLSpace能够对文本到SQL示例进行多方面的分析，这些分析仅凭原始示例是难以实现的。

Abstract: We introduce SQLSpace, a human-interpretable, generalizable, compact
representation for text-to-SQL examples derived with minimal human
intervention. We demonstrate the utility of these representations in evaluation
with three use cases: (i) closely comparing and contrasting the composition of
popular text-to-SQL benchmarks to identify unique dimensions of examples they
evaluate, (ii) understanding model performance at a granular level beyond
overall accuracy scores, and (iii) improving model performance through targeted
query rewriting based on learned correctness estimation. We show that SQLSpace
enables analysis that would be difficult with raw examples alone: it reveals
compositional differences between benchmarks, exposes performance patterns
obscured by accuracy alone, and supports modeling of query success.

</details>


### [33] [Patient-Centered Summarization Framework for AI Clinical Summarization: A Mixed-Methods Design](https://arxiv.org/abs/2510.27535)
*Maria Lizarazo Jimenez,Ana Gabriela Claros,Kieran Green,David Toro-Tobon,Felipe Larios,Sheena Asthana,Camila Wenczenovicz,Kerly Guevara Maldonado,Luis Vilatuna-Andrango,Cristina Proano-Velez,Satya Sai Sri Bandi,Shubhangi Bagewadi,Megan E. Branda,Misk Al Zahidy,Saturnino Luz,Mirella Lapata,Juan P. Brito,Oscar J. Ponce-Ponte*

Main category: cs.CL

TL;DR: 该研究旨在开发以患者为中心的临床总结（PCS）框架，并评估开源大型语言模型（LLMs）在生成此类总结方面的表现。


<details>
  <summary>Details</summary>
Motivation: LLMs在生成临床总结方面表现出巨大潜力，但往往侧重于患者的生物学信息而非个人偏好、价值观、愿望和顾虑，这与以患者为中心的护理理念不符。因此，需要一种新的AI临床总结标准——以患者为中心的总结（PCS）。

Method: 本研究采用混合方法。首先，通过英国的两个患者和公众参与小组（10名患者和8名临床医生）的半结构化访谈，探讨了PCS应包含的个人和情境信息及其结构。然后，根据访谈结果制定了注释指南，8名临床医生利用该指南从88份心房颤动咨询中创建了黄金标准PCS。接着，使用16份咨询来优化PCS生成提示。最后，选用Llama-3.2-3B、Llama-3.1-8B、Mistral-8B、Gemma-3-4B和Qwen3-8B五种开源LLMs，采用零样本和少样本提示，为72份咨询生成总结，并使用ROUGE-L、BERTScore和定性指标进行评估。

Result: 患者强调了生活习惯、社会支持、近期压力源和护理价值观的重要性。临床医生则关注简洁的功能性、社会心理和情感背景信息。在零样本学习中，Mistral-8B（ROUGE-L 0.189）和Llama-3.1-8B（BERTScore 0.673）表现最佳；在少样本学习中，Llama-3.1-8B表现最佳（ROUGE-L 0.206，BERTScore 0.683）。专家和模型在总结的完整性和流畅性方面相似，但在正确性和以患者为中心性方面，人类生成的PCS更优。

Conclusion: 本研究成功开发了以患者为中心的临床总结（PCS）框架，并评估了开源LLMs在生成此类总结方面的能力。尽管LLMs在某些方面表现良好，但在正确性和以患者为中心性方面仍需改进，以达到人类专家的水平。

Abstract: Large Language Models (LLMs) are increasingly demonstrating the potential to
reach human-level performance in generating clinical summaries from
patient-clinician conversations. However, these summaries often focus on
patients' biology rather than their preferences, values, wishes, and concerns.
To achieve patient-centered care, we propose a new standard for Artificial
Intelligence (AI) clinical summarization tasks: Patient-Centered Summaries
(PCS). Our objective was to develop a framework to generate PCS that capture
patient values and ensure clinical utility and to assess whether current
open-source LLMs can achieve human-level performance in this task. We used a
mixed-methods process. Two Patient and Public Involvement groups (10 patients
and 8 clinicians) in the United Kingdom participated in semi-structured
interviews exploring what personal and contextual information should be
included in clinical summaries and how it should be structured for clinical
use. Findings informed annotation guidelines used by eight clinicians to create
gold-standard PCS from 88 atrial fibrillation consultations. Sixteen
consultations were used to refine a prompt aligned with the guidelines. Five
open-source LLMs (Llama-3.2-3B, Llama-3.1-8B, Mistral-8B, Gemma-3-4B, and
Qwen3-8B) generated summaries for 72 consultations using zero-shot and few-shot
prompting, evaluated with ROUGE-L, BERTScore, and qualitative metrics. Patients
emphasized lifestyle routines, social support, recent stressors, and care
values. Clinicians sought concise functional, psychosocial, and emotional
context. The best zero-shot performance was achieved by Mistral-8B (ROUGE-L
0.189) and Llama-3.1-8B (BERTScore 0.673); the best few-shot by Llama-3.1-8B
(ROUGE-L 0.206, BERTScore 0.683). Completeness and fluency were similar between
experts and models, while correctness and patient-centeredness favored human
PCS.

</details>


### [34] [DialectalArabicMMLU: Benchmarking Dialectal Capabilities in Arabic and Multilingual Language Models](https://arxiv.org/abs/2510.27543)
*Malik H. Altakrori,Nizar Habash,Abdelhakim Freihat,Younes Samih,Kirill Chirkunov,Muhammed AbuOdeh,Radu Florian,Teresa Lynn,Preslav Nakov,Alham Fikri Aji*

Main category: cs.CL

TL;DR: DialectalArabicMMLU 是一个用于评估大型语言模型在阿拉伯方言方面表现的新基准。


<details>
  <summary>Details</summary>
Motivation: 尽管阿拉伯语和多语言基准测试在MSA的LLM评估方面取得了进展，但方言变体在日常交流中的普遍性却未得到充分体现。

Method: DialectalArabicMMLU 通过手动翻译和调整 MMLU-Redux 框架的 3K 多项选择问答对，将其转换为五种主要方言（叙利亚、埃及、阿联酋、沙特和摩洛哥），从而产生了总计 15K 个跨 32 个学术和专业领域的问答对。

Result: 评估了 19 个开源的阿拉伯语和多语言 LLM，报告了方言之间存在显著的性能差异，揭示了方言泛化中持续存在的差距。

Conclusion: DialectalArabicMMLU 提供了第一个统一的、人工策划的资源，用于衡量阿拉伯语中的方言理解，从而促进更具包容性的评估和未来的模型开发。

Abstract: We present DialectalArabicMMLU, a new benchmark for evaluating the
performance of large language models (LLMs) across Arabic dialects. While
recently developed Arabic and multilingual benchmarks have advanced LLM
evaluation for Modern Standard Arabic (MSA), dialectal varieties remain
underrepresented despite their prevalence in everyday communication.
DialectalArabicMMLU extends the MMLU-Redux framework through manual translation
and adaptation of 3K multiple-choice question-answer pairs into five major
dialects (Syrian, Egyptian, Emirati, Saudi, and Moroccan), yielding a total of
15K QA pairs across 32 academic and professional domains (22K QA pairs when
also including English and MSA). The benchmark enables systematic assessment of
LLM reasoning and comprehension beyond MSA, supporting both task-based and
linguistic analysis. We evaluate 19 open-weight Arabic and multilingual LLMs
(1B-13B parameters) and report substantial performance variation across
dialects, revealing persistent gaps in dialectal generalization.
DialectalArabicMMLU provides the first unified, human-curated resource for
measuring dialectal understanding in Arabic, thus promoting more inclusive
evaluation and future model development.

</details>


### [35] [Multilingual BERT language model for medical tasks: Evaluation on domain-specific adaptation and cross-linguality](https://arxiv.org/abs/2510.27552)
*Yinghao Luo,Lang Zhou,Amrish Jhingoer,Klaske Vliegenthart Jongbloed,Carlijn Jordans,Ben Werkhoven,Tom Seinen,Erik van Mulligen,Casper Rokx,Yunlei Li*

Main category: cs.CL

TL;DR: 这篇论文探讨了在多语言医疗保健应用中，如何通过在特定领域语料库上进行进一步预训练，来提升低资源语言的医学自然语言处理（NLP）模型性能。


<details>
  <summary>Details</summary>
Motivation: 在多语言医疗保健应用中，领域特定的自然语言处理工具是有限的，尤其对于低资源语言。尽管多语言BERT在弥合语言鸿沟方面提供了前景，但低资源语言的医学NLP任务仍未得到充分探索。

Method: 本研究在荷兰语、罗马尼亚语和西班牙语三种语言上，通过对特定领域语料库进行进一步预训练，创建了医学领域模型。具体进行了四项预训练实验，然后将这些模型在三个下游任务中进行了微调：荷兰语临床笔记中的自动化患者筛选、罗马尼亚语和西班牙语临床笔记中的命名实体识别。

Result: 结果表明，领域适应显著增强了任务性能。此外，领域（如临床和通用生物医学领域）的进一步区分导致了不同的性能，其中临床领域适应模型优于更通用的生物医学领域适应模型。并且观察到跨语言可迁移性的证据。文章还进一步探讨了导致这些性能差异的潜在原因。

Conclusion: 这些发现强调了在医疗NLP中领域适应和跨语言能力的P可行性。在低资源语言环境下，这些发现可以为开发多语言医疗NLP系统以缓解训练数据不足并提高模型性能提供有意义的指导。

Abstract: In multilingual healthcare applications, the availability of domain-specific
natural language processing(NLP) tools is limited, especially for low-resource
languages. Although multilingual bidirectional encoder representations from
transformers (BERT) offers a promising motivation to mitigate the language gap,
the medical NLP tasks in low-resource languages are still underexplored.
Therefore, this study investigates how further pre-training on domain-specific
corpora affects model performance on medical tasks, focusing on three
languages: Dutch, Romanian and Spanish. In terms of further pre-training, we
conducted four experiments to create medical domain models. Then, these models
were fine-tuned on three downstream tasks: Automated patient screening in Dutch
clinical notes, named entity recognition in Romanian and Spanish clinical
notes. Results show that domain adaptation significantly enhanced task
performance. Furthermore, further differentiation of domains, e.g. clinical and
general biomedical domains, resulted in diverse performances. The clinical
domain-adapted model outperformed the more general biomedical domain-adapted
model. Moreover, we observed evidence of cross-lingual transferability.
Moreover, we also conducted further investigations to explore potential reasons
contributing to these performance differences. These findings highlight the
feasibility of domain adaptation and cross-lingual ability in medical NLP.
Within the low-resource language settings, these findings can provide
meaningful guidance for developing multilingual medical NLP systems to mitigate
the lack of training data and thereby improve the model performance.

</details>


### [36] [Data-Efficient Domain Adaptation for LLM-based MT using Contrastive Preference Optimization](https://arxiv.org/abs/2510.27556)
*Inacio Vieira,Antonio Castaldo,James O'Doherty,Sheila Castilho*

Main category: cs.CL

TL;DR: 研究了成本效益高的CPO方法，以模拟后编辑工作流程，实现数据高效的领域适应，并通过约1.5万个偏好对在MT领域实现了与16万多个SFT样本模型相近的性能。


<details>
  <summary>Details</summary>
Motivation: 开发一种数据高效的方法，使大型语言模型适应特定领域的需求，减少对昂贵的SFT的依赖。

Method: 应用CPO（对比偏好优化）模拟后编辑工作流程，通过将基础模型的原始输出作为“拒绝”翻译，人工批准的TM条目作为“选择”翻译来合成偏好对。

Result: 在英-巴西葡萄牙语和英-韩语翻译任务中，仅使用14.7k偏好对，模型性能接近于使用160k+SFT样本训练的模型，显示出显著的数据效率。

Conclusion: CPO方法在机器翻译中实现了数据高效的领域适应，并通过模型初始草稿与黄金参考的对比信号，可推广到其他生成任务。

Abstract: LLMs often require adaptation to domain-specific requirements, a process that
can be expensive when relying solely on SFT. We present an empirical study on
applying CPO to simulate a post-editing workflow for data-efficient domain
adaptation. Our approach synthesizes preference pairs by treating the base
model's own raw output as the 'rejected' translation and the human-approved TM
entry as the 'chosen' one. This method provides direct feedback on the model's
current knowledge, guiding it to align with domain-specific standards.
Experiments in English-Brazilian Portuguese and English-Korean show that, by
using just 14.7k preference pairs, the model achieves performance close to that
of a model trained on 160k+ samples with SFT, demonstrating significant data
efficiency. Although we showcase its effectiveness in MT, this application of
CPO naturally generalizes to other generative tasks where a model's initial
drafts can serve as a contrastive signal against a golden reference.

</details>


### [37] [MARAG-R1: Beyond Single Retriever via Reinforcement-Learned Multi-Tool Agentic Retrieval](https://arxiv.org/abs/2510.27569)
*Qi Luo,Xiaonan Li,Yuxin Wang,Tingshuo Fan,Yuan Li,Xinchi Chen,Xipeng Qiu*

Main category: cs.CL

TL;DR: 本文提出了MARAG-R1，一种强化学习多工具RAG框架，解决了现有RAG系统单检索器限制信息获取的问题，通过动态协调多种检索机制实现更广泛和精确的信息访问。


<details>
  <summary>Details</summary>
Motivation: 现有RAG系统依赖单一检索器，其固定的top-k选择限制了对语料库窄而静态子集的访问，成为全面获取外部信息的瓶颈，尤其在需要语料库级别推理的任务中。

Method: 本文提出了MARAG-R1，一个强化学习多工具RAG框架。它为模型配备了四种检索工具：语义搜索、关键词搜索、过滤和聚合。通过监督微调和强化学习两阶段训练过程，模型学习如何以及何时使用这些工具。

Result: MARAG-R1在GlobalQA、HotpotQA和2WikiMultiHopQA的实验中，显著优于强大的基线模型，并在语料库级别推理任务中取得了新的最先进结果。

Conclusion: MARAG-R1通过引入多工具检索和强化学习，成功克服了传统RAG模型在外部信息获取方面的局限性，实现了更广泛、更精确的信息访问和更强的语料库级别推理能力。

Abstract: Large Language Models (LLMs) excel at reasoning and generation but are
inherently limited by static pretraining data, resulting in factual
inaccuracies and weak adaptability to new information. Retrieval-Augmented
Generation (RAG) addresses this issue by grounding LLMs in external knowledge;
However, the effectiveness of RAG critically depends on whether the model can
adequately access relevant information. Existing RAG systems rely on a single
retriever with fixed top-k selection, restricting access to a narrow and static
subset of the corpus. As a result, this single-retriever paradigm has become
the primary bottleneck for comprehensive external information acquisition,
especially in tasks requiring corpus-level reasoning. To overcome this
limitation, we propose MARAG-R1, a reinforcement-learned multi-tool RAG
framework that enables LLMs to dynamically coordinate multiple retrieval
mechanisms for broader and more precise information access. MARAG-R1 equips the
model with four retrieval tools -- semantic search, keyword search, filtering,
and aggregation -- and learns both how and when to use them through a two-stage
training process: supervised fine-tuning followed by reinforcement learning.
This design allows the model to interleave reasoning and retrieval,
progressively gathering sufficient evidence for corpus-level synthesis.
Experiments on GlobalQA, HotpotQA, and 2WikiMultiHopQA demonstrate that
MARAG-R1 substantially outperforms strong baselines and achieves new
state-of-the-art results in corpus-level reasoning tasks.

</details>


### [38] [SpecAttn: Speculating Sparse Attention](https://arxiv.org/abs/2510.27641)
*Harsh Shah*

Main category: cs.CL

TL;DR: SpecAttn是一种无需训练的方法，它通过利用草稿模型在推测解码过程中计算的注意力权重，实现了预训练Transformer中的高效稀疏注意力。


<details>
  <summary>Details</summary>
Motivation: 大型语言模型（LLMs）在推理过程中面临计算瓶颈，这主要是由于自注意力机制的二次复杂度，尤其是在上下文长度增加时。

Method: SpecAttn采用了三种核心技术：1. 基于KL散度的草稿模型和目标模型之间的层对齐。2. 一种GPU优化的无排序算法，用于从草稿注意力模式中选择top-p令牌。3. 根据这些预测进行的动态键值缓存剪枝。

Result: SpecAttn在PG-19数据集上，将键值缓存访问量减少了75%以上，而困惑度仅增加了15.29%，显著优于现有的稀疏注意力方法。

Conclusion: SpecAttn证明了推测执行可以被增强，以提供近似验证，而不会导致显著的性能下降。

Abstract: Large Language Models (LLMs) face significant computational bottlenecks
during inference due to the quadratic complexity of self-attention mechanisms,
particularly as context lengths increase. We introduce SpecAttn, a novel
training-free approach that seamlessly integrates with existing speculative
decoding techniques to enable efficient sparse attention in pre-trained
transformers. Our key insight is to exploit the attention weights already
computed by the draft model during speculative decoding to identify important
tokens for the target model, eliminating redundant computation while
maintaining output quality. SpecAttn employs three core techniques: KL
divergence-based layer alignment between draft and target models, a
GPU-optimized sorting-free algorithm for top-p token selection from draft
attention patterns, and dynamic key-value cache pruning guided by these
predictions. By leveraging the computational work already performed in standard
speculative decoding pipelines, SpecAttn achieves over 75% reduction in
key-value cache accesses with a mere 15.29% increase in perplexity on the PG-19
dataset, significantly outperforming existing sparse attention methods. Our
approach demonstrates that speculative execution can be enhanced to provide
approximate verification without significant performance degradation.

</details>


### [39] [Continuous Autoregressive Language Models](https://arxiv.org/abs/2510.27688)
*Chenze Shao,Darren Li,Fandong Meng,Jie Zhou*

Main category: cs.CL

TL;DR: 该论文介绍了一种名为 CALM 的新型语言模型，该模型通过将离散的下一个单词预测转换为连续的下一个向量预测，显著提高了大型语言模型的效率，从而减少了生成步骤。


<details>
  <summary>Details</summary>
Motivation: 大型语言模型（LLM）的效率受限于其顺序的、逐令牌的生成过程。为了克服这一瓶颈，需要一个新的设计方向来提高每个生成步骤的语义带宽。

Method: CALM 使用了一个高保真自动编码器，将 K 个令牌块压缩成一个单一的连续向量，可以以超过 99.9% 的准确率重建原始令牌。通过将语言建模为连续向量序列而非离散令牌，生成步骤减少了 K 倍。为此，论文开发了一个全面的无似然框架，支持连续域中的鲁棒训练、评估和可控采样。

Result: CALM 显著改善了性能与计算成本之间的权衡，在显著降低计算成本的同时，达到了强大离散基线的性能。

Conclusion: 下一个向量预测是通往超高效语言模型的强大且可扩展的途径。

Abstract: The efficiency of large language models (LLMs) is fundamentally limited by
their sequential, token-by-token generation process. We argue that overcoming
this bottleneck requires a new design axis for LLM scaling: increasing the
semantic bandwidth of each generative step. To this end, we introduce
Continuous Autoregressive Language Models (CALM), a paradigm shift from
discrete next-token prediction to continuous next-vector prediction. CALM uses
a high-fidelity autoencoder to compress a chunk of K tokens into a single
continuous vector, from which the original tokens can be reconstructed with
over 99.9\% accuracy. This allows us to model language as a sequence of
continuous vectors instead of discrete tokens, which reduces the number of
generative steps by a factor of K. The paradigm shift necessitates a new
modeling toolkit; therefore, we develop a comprehensive likelihood-free
framework that enables robust training, evaluation, and controllable sampling
in the continuous domain. Experiments show that CALM significantly improves the
performance-compute trade-off, achieving the performance of strong discrete
baselines at a significantly lower computational cost. More importantly, these
findings establish next-vector prediction as a powerful and scalable pathway
towards ultra-efficient language models. Code:
https://github.com/shaochenze/calm. Project:
https://shaochenze.github.io/blog/2025/CALM.

</details>


<div id='cs.IT'></div>

# cs.IT [[Back]](#toc)

### [40] [Multi-hop Parallel Image Semantic Communication for Distortion Accumulation Mitigation](https://arxiv.org/abs/2510.26844)
*Bingyan Xie,Jihong Park,Yongpeng Wu,Wenjun Zhang,Tony Quek*

Main category: cs.IT

TL;DR: 该文章提出了一种多跳并行图像语义通信（MHPSC）框架，旨在解决多跳无线图像传输中语义通信固有的失真累积问题。


<details>
  <summary>Details</summary>
Motivation: 现有的语义通信方案主要集中在单跳场景，忽略了多跳无线图像传输的挑战，而语义通信固有的有损特性会导致多跳传输中的失真累积，从而严重降低性能。

Method: MHPSC框架在每一跳引入并行残差补偿链路以对抗失真累积。为了最小化传输带宽开销，设计了由深度学习残差压缩器和自适应算术编码（AAC）组成的粗到细残差压缩方案，并通过残差分布估计模块预测AAC的先验分布。

Result: MHPSC在多跳图像传输中表现出鲁棒性，且只略微增加了传输带宽。

Conclusion: 实验结果证实，MHPSC优于现有的语义通信和传统的分离编码方案。

Abstract: Existing semantic communication schemes primarily focus on single-hop
scenarios, overlooking the challenges of multi-hop wireless image transmission.
As semantic communication is inherently lossy, distortion accumulates over
multiple hops, leading to significant performance degradation. To address this,
we propose the multi-hop parallel image semantic communication (MHPSC)
framework, which introduces a parallel residual compensation link at each hop
against distortion accumulation. To minimize the associated transmission
bandwidth overhead, a coarse-to-fine residual compression scheme is designed. A
deep learning-based residual compressor first condenses the residuals, followed
by the adaptive arithmetic coding (AAC) for further compression. A residual
distribution estimation module predicts the prior distribution for the AAC to
achieve fine compression performances. This approach ensures robust multi-hop
image transmission with only a minor increase in transmission bandwidth.
Experimental results confirm that MHPSC outperforms both existing semantic
communication and traditional separated coding schemes.

</details>


### [41] [Inferring the Chemotaxis Distortion Function from Cellular Decision Strategies](https://arxiv.org/abs/2510.26988)
*Fardad Vakilipoor,Johannes Konrad,Maximilian Schäfer*

Main category: cs.IT

TL;DR: 该研究探讨了细胞智能如何在不确定性下处理环境信号和做出决策，特别是在趋化性中。


<details>
  <summary>Details</summary>
Motivation: 探索细胞如何在存在噪声信号通路的情况下处理不确定性，并做出依赖于环境的决策。

Method: 应用基于速率失真理论（RDT）的信息论框架，利用Blahut-Arimoto算法（BAA）计算最优决策策略。提出逆Blahut-Arimoto算法（IBAA）来量化系统的决策标准，并通过细胞凋亡场景验证了IBAA的准确性。使用局部兴奋全局抑制（LEGI）模型模拟趋化性反应，从细胞视角计算失真函数。

Result: IBAA成功估计了理论失真函数，且研究发现细胞的决策标准是状态依赖的。

Conclusion: 该信息论框架可以扩展到其他需要在不确定性下进行高效信息处理的生物和工程系统。

Abstract: Cellular intelligence enables cells to process environmental signals and make
context-dependent decisions, as exemplified by chemotaxis, where cells navigate
chemical gradients despite noisy signaling pathways. To investigate how cells
deal with uncertainty, we apply an information-theoretic framework based on
rate distortion theory (RDT). The Blahut-Arimoto algorithm (BAA) computes
optimal decision strategies that minimize mutual information while satisfying
distortion constraints, balancing sensing accuracy with distortion constraint
equivalent to resource cost. We propose the inverse Blahut-Arimoto algorithm
(IBAA) to compute the distortion function, which quantifies the system's
decision-making criteria for realizing a decision strategy to map input signals
to outputs. This general framework extends beyond chemotaxis to biological and
engineered systems requiring efficient information processing under
uncertainty. We validate the proposed IBAA by accurately estimating theoretical
distortion functions in a cellular apoptosis scenario. Additionally, using the
local excitation global inhibition (LEGI) model to simulate chemotactic
responses, we compute the distortion functions from the cell's perspective. Our
finding reveals a state-dependent decision criteria by the cell.

</details>


### [42] [Dual-Scale Antenna Deployment for Pinching Antenna Systems](https://arxiv.org/abs/2510.27185)
*Xu Gan,Zhaolin Wang,Yuanwei Liu*

Main category: cs.IT

TL;DR: 本文提出了一种双尺度部署（DSD）框架，用于自适应天线系统（PASS），并通过仿真验证了其在提高能源效率方面的优越性。


<details>
  <summary>Details</summary>
Motivation: 在通信系统中，提高能源效率是一个重要的问题。传统的大规模MIMO系统和无蜂窝架构在能源效率方面存在局限性。因此，需要一种新的方法来解决这个问题。

Method: 本文提出了一种双尺度部署（DSD）框架，用于自适应天线系统（PASS）。该框架包括两个阶段：粗略阶段和精细阶段。在粗略阶段，夹点天线（PA）在波导级别进行大范围传输。在精细阶段，PA进行小范围高精度重新定位。本文提出了四种PA部署协议，并建立了一个实用的功耗模型，在此基础上推导了PASS的理论能量效率公式。通过联合优化发射预编码、PA辐射功率和PA部署，在所提供的PA部署协议下最大化能量效率。为了解决这个非凸、高度耦合的问题，本文提出了一种低复杂度的基于惩罚的交替优化算法。

Result: 1. 仿真结果验证了理论结果的准确性和所提算法的收敛性。 2. PASS比传统的无蜂窝架构的能量效率高出约70%，比MIMO系统高出近两倍。 3. 必须指定DSD分辨率和部署协议，以实现PASS的最大能量效率。

Conclusion: 本文提出的双尺度部署（DSD）框架可以显著提高自适应天线系统（PASS）的能量效率。通过联合优化和低复杂度的算法，PASS在能源效率方面表现出色，并优于传统方案。未来的研究可以进一步探索DSD框架在不同场景下的应用和优化。

Abstract: A dual-scale deployment (DSD) framework for pinching antenna systems (PASS)
is proposed. 1) In the first coarse stage, the pinching antenna (PA) is
transferred over a large-scale range at the waveguide level. 2) The refinement
stage performs small-scale relocation of the PA with high precision. Four PA
deployment protocols are provided in the proposed DSD framework. Then, a
practical power consumption model is proposed, based on which the theoretical
energy efficiency formulas for PASS are derived. The transmit precoding, PA
radiation power, and PA deployment are jointly optimized to maximize the energy
efficiency under the provided PA deployment protocols. To solve this
non-convex, highly coupled problem, a low-complexity penalty-based alternating
optimization algorithm is proposed. Simulation results validate the accuracy of
theoretical results and the convergence of the proposed algorithm. It is
demonstrated that: 1) PASS delivers about 70% higher energy efficiency than the
conventional cell-free architecture and nearly twofold improvement relative to
MIMO systems; 2) it is essential to specify the DSD resolution and deployment
protocol to achieve the maximum energy efficiency for PASS.

</details>


### [43] [Weight Enumerators From Equivalence Relations and MacWilliams Identities](https://arxiv.org/abs/2510.27358)
*S. T. Dougherty,C. Fernández-Córdoba*

Main category: cs.IT

TL;DR: 本文探讨了有限域、有限阿贝尔群和有限Frobenius环上代码的权枚举器，并研究了MacWilliams关系在其上的适用性。


<details>
  <summary>Details</summary>
Motivation: 文中旨在探索在不同代数结构（有限域、有限阿贝尔群和有限Frobenius环）上定义的代码的权枚举器及其所满足的MacWilliams关系。

Method: 通过引入基于等价关系的权枚举器定义，并分析其满足MacWilliams关系的条件，文中也针对特定的等价关系进行了权枚举器的研究。

Result: 研究结果明确了在特定条件下，MacWilliams关系对于基于等价关系定义的权枚举器是成立的。

Conclusion: 本研究为理解广义的权枚举器及其MacWilliams关系提供了理论框架，对编码理论的进一步发展具有潜在意义。

Abstract: In this paper, we consider codes over finite fields, finite abelian groups,
and finite Frobenius rings. For such codes, the complete weight enumerator and
the Hamming weight enumerator serve as powerful tools. These two types of
weight enumerators satisfy the MacWilliams relations. We define the weight
enumerator of a code with respect to an equivalence relation and determine in
which cases the MacWilliams relations hold for this weight enumerator. We also
study some weight enumerators for specific equivalence relations.

</details>


<div id='stat.ML'></div>

# stat.ML [[Back]](#toc)

### [44] [Overspecified Mixture Discriminant Analysis: Exponential Convergence, Statistical Guarantees, and Remote Sensing Applications](https://arxiv.org/abs/2510.27056)
*Arman Bolatov,Alan Legg,Igor Melnykov,Amantay Nurlanuly,Maxat Tezekbayev,Zhenisbek Assylbekov*

Main category: stat.ML

TL;DR: 该研究探讨了在混合成分数量超过数据实际分布（过拟合）的情况下，混合判别分析（MDA）的分类错误。


<details>
  <summary>Details</summary>
Motivation: 探索当混合成分数量超过实际数据分布时，MDA分类错误的表现，并为此提供理论框架。

Method: 使用双组高斯混合模型拟合由单一高斯生成的数据，分析EM算法的收敛性和统计分类误差，并通过遥感数据集验证理论。

Result: EM算法在适当初始化下能以指数速度收敛到贝叶斯风险；在温和条件下，分类错误以n^(-1/2)的速度收敛到贝叶斯风险。

Conclusion: 本研究为理解过拟合MDA的性能提供了一个严格的理论框架，这对于图像和文本分类等复杂数据设置中的经验应用至关重要。

Abstract: This study explores the classification error of Mixture Discriminant Analysis
(MDA) in scenarios where the number of mixture components exceeds those present
in the actual data distribution, a condition known as overspecification. We use
a two-component Gaussian mixture model within each class to fit data generated
from a single Gaussian, analyzing both the algorithmic convergence of the
Expectation-Maximization (EM) algorithm and the statistical classification
error. We demonstrate that, with suitable initialization, the EM algorithm
converges exponentially fast to the Bayes risk at the population level.
Further, we extend our results to finite samples, showing that the
classification error converges to Bayes risk with a rate $n^{-1/2}$ under mild
conditions on the initial parameter estimates and sample size. This work
provides a rigorous theoretical framework for understanding the performance of
overspecified MDA, which is often used empirically in complex data settings,
such as image and text classification. To validate our theory, we conduct
experiments on remote sensing datasets.

</details>


### [45] [Decreasing Entropic Regularization Averaged Gradient for Semi-Discrete Optimal Transport](https://arxiv.org/abs/2510.27340)
*Ferdinand Genans,Antoine Godichon-Baggioni,François-Xavier Vialard,Olivier Wintenberger*

Main category: stat.ML

TL;DR: 这篇论文介绍了一种名为DRAG的算法，该算法通过逐步减小正则化参数来加速半离散最优传输问题的求解，并在理论和实践中证明了其有效性。


<details>
  <summary>Details</summary>
Motivation: 在最优传输问题中，熵正则化虽然能加速求解，但会引入偏差。因此，研究人员希望找到一种方法，在保留正则化加速优势的同时，减小其引入的偏差。

Method: 本文提出了一种名为DRAG（Decreasing (entropic) Regularization Averaged Gradient）的随机梯度下降算法。该算法的正则化参数会随着优化步数的增加而减小。

Result: DRAG算法在半离散最优传输问题中，与固定正则化方案相比，能加速收敛。它在最优传输成本和势能估计方面达到了 \(\mathcal{O}(1/t)\) 的无偏采样和迭代复杂度，在最优传输映射方面达到了 \(\mathcal{O}(1/\sqrt{t})\) 的速率。

Conclusion: DRAG算法通过在优化过程中逐步减小正则化参数，有效加速了半离散最优传输问题的求解，并在理论和实践中验证了其有效性。

Abstract: Adding entropic regularization to Optimal Transport (OT) problems has become
a standard approach for designing efficient and scalable solvers. However,
regularization introduces a bias from the true solution. To mitigate this bias
while still benefiting from the acceleration provided by regularization, a
natural solver would adaptively decrease the regularization as it approaches
the solution. Although some algorithms heuristically implement this idea, their
theoretical guarantees and the extent of their acceleration compared to using a
fixed regularization remain largely open. In the setting of semi-discrete OT,
where the source measure is continuous and the target is discrete, we prove
that decreasing the regularization can indeed accelerate convergence. To this
end, we introduce DRAG: Decreasing (entropic) Regularization Averaged Gradient,
a stochastic gradient descent algorithm where the regularization decreases with
the number of optimization steps. We provide a theoretical analysis showing
that DRAG benefits from decreasing regularization compared to a fixed scheme,
achieving an unbiased $\mathcal{O}(1/t)$ sample and iteration complexity for
both the OT cost and the potential estimation, and a $\mathcal{O}(1/\sqrt{t})$
rate for the OT map. Our theoretical findings are supported by numerical
experiments that validate the effectiveness of DRAG and highlight its practical
advantages.

</details>


### [46] [On the Equivalence of Optimal Transport Problem and Action Matching with Optimal Vector Fields](https://arxiv.org/abs/2510.27385)
*Nikita Kornilov,Alexander Korotin*

Main category: stat.ML

TL;DR: Flow Matching (FM) 方法在生成建模中通过构建任意概率分布之间的插值来工作，然后学习定义 ODE 的向量场。最近的研究表明，FM 可以被修改，以在二次成本函数方面最优地映射分布，这通过在 FM 损失最小化过程中仅考虑特定的最优向量场实现，这些向量场通常是最优传输 (OT) 问题的解决方案。本文指出，仅考虑最优向量场可以将 OT 引导到另一种方法：Action Matching (AM)。与 FM 不同的是，FM 为给定分布之间手动选择的插值学习向量场，而 AM 则是为一个完整的给定分布序列学习定义 ODE 的向量场。


<details>
  <summary>Details</summary>
Motivation: 此研究的动机是探索 Flow Matching (FM) 方法在生成建模中的潜力，特别是在二次成本函数下实现最优分布映射。通过将 FM 与最优传输 (OT) 问题的解决方案联系起来，并引入 Action Matching (AM) 作为另一种方法，旨在提升生成模型的效率和应用范围。

Method: 本文深入探讨了Flow Matching (FM) 和 Action Matching (AM) 这两种方法。Flow Matching (FM) 是通过构建任意概率分布之间的插值，并学习定义 ODE 的向量场来实现；Action Matching (AM) 学习的是定义 ODE 的向量场。

Result: 研究结果表明，通过仅考虑最优向量场，Flow Matching (FM) 能够实现分布的最优映射，这与最优传输 (OT) 问题的解决方案一致。此外，文章还引入了 Action Matching (AM) 方法，它与 FM 不同，AM 直接为一个完整的分布序列学习向量场，从而在特定应用中可能提供更直接或更有效的方法。

Conclusion: 本文的结论是，Flow Matching (FM) 方法通过考虑最优向量场，可以有效地在二次成本函数下实现最优分布映射，从而与最优传输 (OT) 建立了联系。此外，Action Matching (AM) 被提出作为一种替代方法，它直接为分布序列学习向量场，为生成建模提供了新的视角和工具，尤其是在处理序列分布时。

Abstract: Flow Matching (FM) method in generative modeling maps arbitrary probability
distributions by constructing an interpolation between them and then learning
the vector field that defines ODE for this interpolation. Recently, it was
shown that FM can be modified to map distributions optimally in terms of the
quadratic cost function for any initial interpolation. To achieve this, only
specific optimal vector fields, which are typical for solutions of Optimal
Transport (OT) problems, need to be considered during FM loss minimization. In
this note, we show that considering only optimal vector fields can lead to OT
in another approach: Action Matching (AM). Unlike FM, which learns a vector
field for a manually chosen interpolation between given distributions, AM
learns the vector field that defines ODE for an entire given sequence of
distributions.

</details>


### [47] [Interpretable Model-Aware Counterfactual Explanations for Random Forest](https://arxiv.org/abs/2510.27397)
*Joshua S. Harvey,Guanchao Feng,Sai Anusha Meesala,Tina Zhao,Dhagash Mehta*

Main category: stat.ML

TL;DR: 这篇论文提出了一种新的反事实案例解释方法，通过利用随机森林模型学习到的表示来搜索和解释反事实案例，生成比Shapley值更稀疏、更有用的解释。


<details>
  <summary>Details</summary>
Motivation: 尽管机器学习模型具有强大的预测能力，但由于其解释能力有限，通常不适用于金融等受监管行业的应用。现有的模型无关框架（如Shapley值）很少与用户所需的因果解释类型对齐。反事实案例解释可能更直观和可操作，但寻找和解释反事实案例仍是一个开放性挑战。

Method: 本文将反事实搜索和解释问题转化为相似性学习问题，利用随机森林预测模型本身学习到的表示。找到反事实后，解释的特征重要性被计算为从原始实例到反事实的过程中，所跨越的随机森林分区的函数。

Result: 在MNIST手绘数字数据集和德国信用数据集上验证了该方法，结果表明它生成的解释比Shapley值更稀疏、更有用。

Conclusion: 本文提出了一种利用随机森林内部表示进行反事实搜索和解释的新方法，该方法生成的解释更具可操作性和直观性，有望在受监管行业中推广机器学习模型的应用方面发挥作用。

Abstract: Despite their enormous predictive power, machine learning models are often
unsuitable for applications in regulated industries such as finance, due to
their limited capacity to provide explanations. While model-agnostic frameworks
such as Shapley values have proved to be convenient and popular, they rarely
align with the kinds of causal explanations that are typically sought after.
Counterfactual case-based explanations, where an individual is informed of
which circumstances would need to be different to cause a change in outcome,
may be more intuitive and actionable. However, finding appropriate
counterfactual cases is an open challenge, as is interpreting which features
are most critical for the change in outcome. Here, we pose the question of
counterfactual search and interpretation in terms of similarity learning,
exploiting the representation learned by the random forest predictive model
itself. Once a counterfactual is found, the feature importance of the
explanation is computed as a function of which random forest partitions are
crossed in order to reach it from the original instance. We demonstrate this
method on both the MNIST hand-drawn digit dataset and the German credit
dataset, finding that it generates explanations that are sparser and more
useful than Shapley values.

</details>


### [48] [Minimax-Optimal Two-Sample Test with Sliced Wasserstein](https://arxiv.org/abs/2510.27498)
*Binh Thuan Tran,Nicolas Schreuder*

Main category: stat.ML

TL;DR: 本文提出了一种基于切片 Wasserstein （SW）距离的置换双样本检验方法，填补了该方法在假设检验理论基础方面的空白，并证明其在多种替代假设下具有与核方法相当的最优功效。


<details>
  <summary>Details</summary>
Motivation: 现有的切片 Wasserstein (SW) 距离在统计保证和计算效率之间取得了良好平衡，但其在假设检验方面的理论基础仍然有限。

Method: 本文提出了一种基于置换原理的 SW 检验方法。

Result: 该检验方法在有限样本下能控制第一类错误，并达到了多项式和有界支持替代下的 $n^{-1/2}$ 极小极大分离速率，与核方法达到相同最优保证。此外，本文还量化了投影数量和统计功效之间的权衡。

Conclusion: 所提出的 SW 检验方法在所有实验场景中都表现出良好的有效性和竞争力，并且不需要像核方法那样进行繁琐的核调优选 H tuning。

Abstract: We study the problem of nonparametric two-sample testing using the sliced
Wasserstein (SW) distance. While prior theoretical and empirical work indicates
that the SW distance offers a promising balance between strong statistical
guarantees and computational efficiency, its theoretical foundations for
hypothesis testing remain limited. We address this gap by proposing a
permutation-based SW test and analyzing its performance. The test inherits
finite-sample Type I error control from the permutation principle. Moreover, we
establish non-asymptotic power bounds and show that the procedure achieves the
minimax separation rate $n^{-1/2}$ over multinomial and bounded-support
alternatives, matching the optimal guarantees of kernel-based tests while
building on the geometric foundations of Wasserstein distances. Our analysis
further quantifies the trade-off between the number of projections and
statistical power. Finally, numerical experiments demonstrate that the test
combines finite-sample validity with competitive power and scalability, and --
unlike kernel-based tests, which require careful kernel tuning -- it performs
consistently well across all scenarios we consider.

</details>


### [49] [Optimal Convergence Analysis of DDPM for General Distributions](https://arxiv.org/abs/2510.27562)
*Yuchen Jiao,Yuchen Zhou,Gen Li*

Main category: stat.ML

TL;DR: 本文分析了DDPM采样器的收敛性，并在一般分布假设下建立了接近最优的收敛速度。


<details>
  <summary>Details</summary>
Motivation: DDPM采样器在生成高质量样本方面取得了显著的经验成功，但其收敛性缺乏严格的理论理解。

Method: 我们引入了一个由常数L参数化的松弛平滑条件，并证明了具有精确分数估计的DDPM采样器在Kullback-Leibler散度下的收敛速度为$\widetilde{O}\left(\frac{d\min\{d,L^2\}}{T^2}\right)$。

Result: 我们的结果显著改进了现有结果$d^2/T^2$，尤其是在$L < \sqrt{d}$的情况下。我们还通过建立匹配的下界证明了我们收敛分析的紧密性。

Conclusion: DDPM的收敛速度为$\widetilde{O}\left(\frac{d\min\{d,L^2\}}{T^2}\right)$，并且我们发现DDPM和DDIM对d的依赖性相同，这引发了DDIM在经验上通常表现更快的有趣问题。

Abstract: Score-based diffusion models have achieved remarkable empirical success in
generating high-quality samples from target data distributions. Among them, the
Denoising Diffusion Probabilistic Model (DDPM) is one of the most widely used
samplers, generating samples via estimated score functions. Despite its
empirical success, a tight theoretical understanding of DDPM -- especially its
convergence properties -- remains limited.
  In this paper, we provide a refined convergence analysis of the DDPM sampler
and establish near-optimal convergence rates under general distributional
assumptions. Specifically, we introduce a relaxed smoothness condition
parameterized by a constant $L$, which is small for many practical
distributions (e.g., Gaussian mixture models). We prove that the DDPM sampler
with accurate score estimates achieves a convergence rate of
$$\widetilde{O}\left(\frac{d\min\{d,L^2\}}{T^2}\right)~\text{in
Kullback-Leibler divergence},$$ where $d$ is the data dimension, $T$ is the
number of iterations, and $\widetilde{O}$ hides polylogarithmic factors in $T$.
This result substantially improves upon the best-known $d^2/T^2$ rate when $L <
\sqrt{d}$. By establishing a matching lower bound, we show that our convergence
analysis is tight for a wide array of target distributions. Moreover, it
reveals that DDPM and DDIM share the same dependence on $d$, raising an
interesting question of why DDIM often appears empirically faster.

</details>


<div id='cs.LG'></div>

# cs.LG [[Back]](#toc)

### [50] [SmoothGuard: Defending Multimodal Large Language Models with Noise Perturbation and Clustering Aggregation](https://arxiv.org/abs/2510.26830)
*Guangzhi Su,Shuchang Huang,Yutong Ke,Zhuohang Liu,Long Qian,Kaizhu Huang*

Main category: cs.LG

TL;DR: 该论文介绍了一种名为SmoothGuard的轻量级、与模型无关的防御框架，通过随机噪声注入和基于聚类的预测聚合来增强多模态大型语言模型（MLLMs）的鲁棒性，以抵御对抗性攻击，同时保持竞争力。


<details>
  <summary>Details</summary>
Motivation: 多模态大型语言模型（MLLMs）在各种任务中取得了令人印象深刻的性能，但它们很容易受到对抗性操纵，这引起了对其部署安全性和可靠性的担忧。

Method: 本文首先推广了一种在HuggingFace生态系统中生成对抗性图像的方法，然后引入了SmoothGuard。SmoothGuard通过高斯噪声扰动连续模态（例如图像和音频），生成多个候选输出，并应用基于嵌入的聚类来过滤掉受对抗性影响的预测。最终答案从多数簇中选择。

Result: 在POPE、LLaVA-Bench（In-the-Wild）和MM-SafetyBench上的大量实验表明，SmoothGuard提高了对对抗性攻击的弹性，同时保持了有竞争力的实用性。

Conclusion: SmoothGuard通过随机噪声注入和基于聚类的预测聚合，有效增强了多模态大型语言模型抵御对抗性攻击的鲁棒性，取得了良好的平衡。

Abstract: Multimodal large language models (MLLMs) have achieved impressive performance
across diverse tasks by jointly reasoning over textual and visual inputs.
Despite their success, these models remain highly vulnerable to adversarial
manipulations, raising concerns about their safety and reliability in
deployment. In this work, we first generalize an approach for generating
adversarial images within the HuggingFace ecosystem and then introduce
SmoothGuard, a lightweight and model-agnostic defense framework that enhances
the robustness of MLLMs through randomized noise injection and clustering-based
prediction aggregation. Our method perturbs continuous modalities (e.g., images
and audio) with Gaussian noise, generates multiple candidate outputs, and
applies embedding-based clustering to filter out adversarially influenced
predictions. The final answer is selected from the majority cluster, ensuring
stable responses even under malicious perturbations. Extensive experiments on
POPE, LLaVA-Bench (In-the-Wild), and MM-SafetyBench demonstrate that
SmoothGuard improves resilience to adversarial attacks while maintaining
competitive utility. Ablation studies further identify an optimal noise range
(0.1-0.2) that balances robustness and utility.

</details>


### [51] [Accurate Target Privacy Preserving Federated Learning Balancing Fairness and Utility](https://arxiv.org/abs/2510.26841)
*Kangkang Sun,Jun Wu,Minyi Guo,Jianhua Li,Jianwei Huang*

Main category: cs.LG

TL;DR: 本文提出了FedPF，一种差异化隐私公平联邦学习算法，将多目标优化转化为零和博弈。研究发现，严格的隐私保护会限制系统纠正人口偏见的能力，导致隐私与公平之间存在内在的张力；适度的公平约束可以改善模型泛化能力，但过度则会导致性能下降。


<details>
  <summary>Details</summary>
Motivation: 在不共享数据的情况下，联邦学习（FL）中的参与者面临着一个基本挑战，即如何在保护敏感客户数据的同时确保不同人口群体之间的公平性。

Method: 本文引入了一种差异私有公平联邦学习算法（FedPF），它将多目标优化转换为零和博弈，其中公平性和隐私约束与模型效用竞争。

Result: 实验证明，在三个数据集中，FedPF在保持竞争性准确性的同时，将歧视降低了42.9%。更重要的是，实验表明隐私与公平之间的紧张关系是不可避免的，即同时实现这两个目标需要仔细权衡，而不是孤立地优化其中任何一个。

Conclusion: 隐私与公平之间存在内在的张力，严格的隐私保护会限制系统纠正人口偏见的能力。适度的公平约束可以改善模型泛化能力，但过度则会导致性能下降。同时实现隐私和公平目标需要精心平衡的妥协。

Abstract: Federated Learning (FL) enables collaborative model training without data
sharing, yet participants face a fundamental challenge, e.g., simultaneously
ensuring fairness across demographic groups while protecting sensitive client
data. We introduce a differentially private fair FL algorithm (\textit{FedPF})
that transforms this multi-objective optimization into a zero-sum game where
fairness and privacy constraints compete against model utility. Our theoretical
analysis reveals a surprising inverse relationship, i.e., stricter privacy
protection fundamentally limits the system's ability to detect and correct
demographic biases, creating an inherent tension between privacy and fairness.
Counterintuitively, we prove that moderate fairness constraints initially
improve model generalization before causing performance degradation, where a
non-monotonic relationship that challenges conventional wisdom about
fairness-utility tradeoffs. Experimental validation demonstrates up to 42.9 %
discrimination reduction across three datasets while maintaining competitive
accuracy, but more importantly, reveals that the privacy-fairness tension is
unavoidable, i.e., achieving both objectives simultaneously requires carefully
balanced compromises rather than optimization of either in isolation. The
source code for our proposed algorithm is publicly accessible at
https://github.com/szpsunkk/FedPF.

</details>


### [52] [CAS-Spec: Cascade Adaptive Self-Speculative Decoding for On-the-Fly Lossless Inference Acceleration of LLMs](https://arxiv.org/abs/2510.26843)
*Zhiyuan Ning,Jiawei Shao,Ruge Xu,Xinfei Guo,Jun Zhang,Chi Zhang,Xuelong Li*

Main category: cs.LG

TL;DR: CAS-Spec方法通过动态可切换的推理加速策略（包括层稀疏性和激活量化）来构建推测性草稿模型，并引入DyTC算法，从而在LLMs的推测解码中实现最先进的加速。


<details>
  <summary>Details</summary>
Motivation: 推测解码在LLMs中得到广泛应用，但目前的即时自推测方法在加速效果上仍不如依赖专门训练的方法。现有的级联方法训练成本高，限制了实际应用。

Method: 本文提出了Cascade Adaptive Self-Speculative Decoding (CAS-Spec) 方法，该方法利用动态可切换的推理加速 (DSIA) 策略（包括层稀疏性和激活量化）来构建推测性草稿模型。此外，本文引入了Dynamic Tree Cascade (DyTC) 算法，该算法基于接受率和延迟预测的启发式方法，自适应地路由多级草稿模型并分配草稿长度。

Result: CAS-Spec方法相对于现有的即时推测解码方法实现了最先进的加速，在各种LLMs和数据集上，相较于自回归解码，平均加速比为1.1倍到2.3倍。DyTC算法相较于基于级联的基线和基于树的基线算法，平均加速分别提高了47%和48%。

Conclusion: CAS-Spec方法可以很容易地集成到大多数现有LLMs中，并且随着自推测解码技术的不断发展，具有进一步加速的巨大潜力。

Abstract: Speculative decoding has become a widely adopted as an effective technique
for lossless inference acceleration when deploying large language models
(LLMs). While on-the-fly self-speculative methods offer seamless integration
and broad utility, they often fall short of the speed gains achieved by methods
relying on specialized training. Cascading a hierarchy of draft models promises
further acceleration and flexibility, but the high cost of training multiple
models has limited its practical application. In this paper, we propose a novel
Cascade Adaptive Self-Speculative Decoding (CAS-Spec) method which constructs
speculative draft models by leveraging dynamically switchable inference
acceleration (DSIA) strategies, including layer sparsity and activation
quantization. Furthermore, traditional vertical and horizontal cascade
algorithms are inefficient when applied to self-speculative decoding methods.
We introduce a Dynamic Tree Cascade (DyTC) algorithm that adaptively routes the
multi-level draft models and assigns the draft lengths, based on the heuristics
of acceptance rates and latency prediction. Our CAS-Spec method achieves
state-of-the-art acceleration compared to existing on-the-fly speculative
decoding methods, with an average speedup from $1.1\times$ to $2.3\times$ over
autoregressive decoding across various LLMs and datasets. DyTC improves the
average speedup by $47$\% and $48$\% over cascade-based baseline and tree-based
baseline algorithms, respectively. CAS-Spec can be easily integrated into most
existing LLMs and holds promising potential for further acceleration as
self-speculative decoding techniques continue to evolve.

</details>


### [53] [Integrating Ontologies with Large Language Models for Enhanced Control Systems in Chemical Engineering](https://arxiv.org/abs/2510.26898)
*Crystal Su,Kuai Yu,Jingrui Zhang,Mingyuan Shao,Daniel Bauer*

Main category: cs.LG

TL;DR: 本文提出了一个将结构化领域知识与生成推理相结合的本体集成大型语言模型（LLM）框架，用于化学工程领域。


<details>
  <summary>Details</summary>
Motivation: 作者旨在将大型语言模型应用于化学工程领域，并增强其可解释性和可靠性，以处理过程控制、安全分析和其他关键工程上下文。

Method: 该框架通过数据采集、语义预处理、信息提取和本体映射等步骤，将模型训练和推理与COPE本体对齐，生成模板化的问答对来指导模型微调。通过以控制为中心的解码阶段和引用门，将输出约束在本体链接的术语上，以确保句法和事实的准确性。

Result: 通过这种方法，模型在语言质量和本体准确性方面都得到了量化评估，并通过语义检索和迭代验证等反馈和未来扩展进一步提升了系统的可解释性和可靠性。

Conclusion: 该研究表明，符号结构和神经生成的结合为将大型语言模型应用于关键工程领域提供了一种透明、可审计的方法。

Abstract: This work presents an ontology-integrated large language model (LLM)
framework for chemical engineering that unites structured domain knowledge with
generative reasoning. The proposed pipeline aligns model training and inference
with the COPE ontology through a sequence of data acquisition, semantic
preprocessing, information extraction, and ontology mapping steps, producing
templated question-answer pairs that guide fine-tuning. A control-focused
decoding stage and citation gate enforce syntactic and factual grounding by
constraining outputs to ontology-linked terms, while evaluation metrics
quantify both linguistic quality and ontological accuracy. Feedback and future
extensions, including semantic retrieval and iterative validation, further
enhance the system's interpretability and reliability. This integration of
symbolic structure and neural generation provides a transparent, auditable
approach for applying LLMs to process control, safety analysis, and other
critical engineering contexts.

</details>


### [54] [Discovering EV Charging Site Archetypes Through Few Shot Forecasting: The First U.S.-Wide Study](https://arxiv.org/abs/2510.26910)
*Kshitij Nikhal,Luke Ackerknecht,Benjamin S. Riggan,Phil Stahlfeld*

Main category: cs.LG

TL;DR: 该研究提出了一个结合聚类和少样本预测的框架，以解决现有电动汽车充电需求预测中数据规模小、时间依赖性建模简单以及对运营历史有限站点的泛化能力弱的问题。


<details>
  <summary>Details</summary>
Motivation: 交通运输的脱碳依赖于电动汽车的普及，这需要准确理解充电行为，以确保经济高效、电网弹性的基础设施。现有研究受限于小规模数据集、简单的时间依赖性建模以及对运营历史有限站点的弱泛化能力。

Method: 本研究提出了一个框架，将聚类与少样本预测相结合，利用新颖的大规模充电需求数据集来揭示站点原型。

Result: 结果表明，在预测未见站点的需求时，特定原型的专家模型优于全局基线。通过将预测性能作为基础设施细分的基础，该工作生成了可操作的见解。

Conclusion: 本研究的框架通过提供准确的需求预测和可操作的见解，帮助运营商降低成本、优化能源和定价策略，并支持对气候目标至关重要的电网弹性。

Abstract: The decarbonization of transportation relies on the widespread adoption of
electric vehicles (EVs), which requires an accurate understanding of charging
behavior to ensure cost-effective, grid-resilient infrastructure. Existing work
is constrained by small-scale datasets, simple proximity-based modeling of
temporal dependencies, and weak generalization to sites with limited
operational history. To overcome these limitations, this work proposes a
framework that integrates clustering with few-shot forecasting to uncover site
archetypes using a novel large-scale dataset of charging demand. The results
demonstrate that archetype-specific expert models outperform global baselines
in forecasting demand at unseen sites. By establishing forecast performance as
a basis for infrastructure segmentation, we generate actionable insights that
enable operators to lower costs, optimize energy and pricing strategies, and
support grid resilience critical to climate goals.

</details>


### [55] [MM-OPERA: Benchmarking Open-ended Association Reasoning for Large Vision-Language Models](https://arxiv.org/abs/2510.26937)
*Zimeng Huang,Jinxin Ke,Xiaoxuan Fan,Yufeng Yang,Yang Liu,Liu Zhonghan,Zedi Wang,Junteng Dai,Haoyi Jiang,Yuyu Zhou,Keze Wang,Ziliang Chen*

Main category: cs.LG

TL;DR: MM-OPERA是一个用于评估大型视觉-语言模型（LVLMs）联想智能的基准测试，它包含11,497个实例和两个开放式任务，旨在弥补现有基准的不足，促进更像人类的通用人工智能发展。


<details>
  <summary>Details</summary>
Motivation: 大型视觉-语言模型（LVLMs）在幻觉和浅层模式匹配方面存在不足，限制了它们在类人智能方面的表现。本研究旨在评估LVLMs在联想智能方面的能力，这被认为是人类创造性思维和知识整合的基石，而现有基准未能充分捕捉开放式联想推理的复杂性。

Method: 本文提出了MM-OPERA，一个包含11,497个实例的系统基准测试，涵盖了远程项目联想（RIA）和情境联想（ICA）两个开放式任务。该基准通过自由形式的回答和明确的推理路径，挑战LVLMs模拟发散性思维和收敛性联想推理。研究人员还采用了LLM-as-a-Judge策略，并结合过程奖励信息判断来评估开放式输出和分析推理过程。

Result: 对最先进的大型视觉-语言模型进行了广泛的实证研究，包括任务实例的敏感性分析、LLM-as-a-Judge策略的有效性分析，以及跨能力、领域、语言和文化的多样性分析。研究结果提供了对当前LVLMs在联想推理方面局限性的全面而细致的理解。

Conclusion: MM-OPERA基准测试的引入，为评估大型视觉-语言模型的联想智能提供了一个系统且开放的平台。研究结果揭示了当前LVLMs在联想推理方面的不足，为未来开发更类人、更通用的AI指明了方向。数据集和代码的公开将促进该领域的研究进展。

Abstract: Large Vision-Language Models (LVLMs) have exhibited remarkable progress.
However, deficiencies remain compared to human intelligence, such as
hallucination and shallow pattern matching. In this work, we aim to evaluate a
fundamental yet underexplored intelligence: association, a cornerstone of human
cognition for creative thinking and knowledge integration. Current benchmarks,
often limited to closed-ended tasks, fail to capture the complexity of
open-ended association reasoning vital for real-world applications. To address
this, we present MM-OPERA, a systematic benchmark with 11,497 instances across
two open-ended tasks: Remote-Item Association (RIA) and In-Context Association
(ICA), aligning association intelligence evaluation with human psychometric
principles. It challenges LVLMs to resemble the spirit of divergent thinking
and convergent associative reasoning through free-form responses and explicit
reasoning paths. We deploy tailored LLM-as-a-Judge strategies to evaluate
open-ended outputs, applying process-reward-informed judgment to dissect
reasoning with precision. Extensive empirical studies on state-of-the-art
LVLMs, including sensitivity analysis of task instances, validity analysis of
LLM-as-a-Judge strategies, and diversity analysis across abilities, domains,
languages, cultures, etc., provide a comprehensive and nuanced understanding of
the limitations of current LVLMs in associative reasoning, paving the way for
more human-like and general-purpose AI. The dataset and code are available at
https://github.com/MM-OPERA-Bench/MM-OPERA.

</details>


### [56] [Can machines think efficiently?](https://arxiv.org/abs/2510.26954)
*Adam Winchell*

Main category: cs.LG

TL;DR: 这篇论文提出了一个新的图灵测试版本，通过考虑回答问题所消耗的能量来评估人工智能的效率和智能。


<details>
  <summary>Details</summary>
Motivation: 目前的图灵测试已经不足以区分人类和机器智能，因为高级人工智能系统已经可以通过该测试，并且引发了伦理和环境问题。

Method: 通过在原始模仿游戏中增加一个额外的因素：回答问题所消耗的能量。这个新的测试将智能评估与效率联系起来，并将抽象的思维问题与有限资源的具体现实联系起来。

Result: 该方法提供了一个可衡量、实用的智能评估终点线，这是原始测试所缺乏的。

Conclusion: 这个新的测试迫使社会权衡使用人工智能节省的时间与其总资源成本。

Abstract: The Turing Test is no longer adequate for distinguishing human and machine
intelligence. With advanced artificial intelligence systems already passing the
original Turing Test and contributing to serious ethical and environmental
concerns, we urgently need to update the test. This work expands upon the
original imitation game by accounting for an additional factor: the energy
spent answering the questions. By adding the constraint of energy, the new test
forces us to evaluate intelligence through the lens of efficiency, connecting
the abstract problem of thinking to the concrete reality of finite resources.
Further, this proposed new test ensures the evaluation of intelligence has a
measurable, practical finish line that the original test lacks. This additional
constraint compels society to weigh the time savings of using artificial
intelligence against its total resource cost.

</details>


### [57] [Fine-Grained Iterative Adversarial Attacks with Limited Computation Budget](https://arxiv.org/abs/2510.26981)
*Zhichao Hou,Weizhi Gao,Xiaorui Liu*

Main category: cs.LG

TL;DR: 为了实现计算预算受限下的最大迭代对抗性攻击，提出了一种细粒度的控制机制，通过选择性地重新计算迭代和层激活来提高攻击效率。该方法在成本相同的情况下优于现有基线，并在对抗训练中以30%的原始预算获得可比性能。


<details>
  <summary>Details</summary>
Motivation: 在有限计算预算下，如何最大化迭代对抗攻击的强度，同时避免粗粒度地减少攻击迭代导致攻击效果大幅下降。

Method: 提出了一种细粒度控制机制，该机制在迭代和层级上选择性地重新计算层激活，以在受限预算内实现可达到的攻击效率。

Result: 在相同成本下，所提出的方法始终优于现有基线。此外，当集成到对抗性训练中时，它仅以30%的原始预算获得了与原始方法相当的性能。

Conclusion: 所提出的细粒度控制机制能够有效提升迭代对抗攻击在有限计算预算下的效能，并在对抗训练中展现出显著的成本效益。

Abstract: This work tackles a critical challenge in AI safety research under limited
compute: given a fixed computation budget, how can one maximize the strength of
iterative adversarial attacks? Coarsely reducing the number of attack
iterations lowers cost but substantially weakens effectiveness. To fulfill the
attainable attack efficacy within a constrained budget, we propose a
fine-grained control mechanism that selectively recomputes layer activations
across both iteration-wise and layer-wise levels. Extensive experiments show
that our method consistently outperforms existing baselines at equal cost.
Moreover, when integrated into adversarial training, it attains comparable
performance with only 30% of the original budget.

</details>


### [58] [Quantitative Bounds for Length Generalization in Transformers](https://arxiv.org/abs/2510.27015)
*Zachary Izzo,Eshaan Nichani,Jason D. Lee*

Main category: cs.LG

TL;DR: 研究了transformer的长度泛化能力，即模型在较短序列上训练后，在评估时能否在更长的、以前未见过的输入上保持性能。


<details>
  <summary>Details</summary>
Motivation: Huang等人（2025年）的研究表明，一旦训练序列长度超过某个有限阈值，transformer最终会实现长度泛化，但这并未解决所需阈值大小的问题。本文旨在首次量化长度泛化所需的训练长度下限。

Method: 本文在几种不同的问题设置中分析了长度泛化，包括：无穷范数误差控制与输入分布上的平均误差控制、无限精度softmax注意力与有限精度注意力（归结为argmax），以及单层与双层 transformer。证明了当transformer在较长序列上的内部行为可以被训练期间较短序列上的行为“模拟”时，长度泛化就会发生。

Result: 本文的界限给出了transformer进行泛化所需的训练数据长度的定性估计，并通过实验验证了这些 H结果。

Conclusion: 这些结果深化了我们对transformer中外推机制的理论理解，并使“更复杂的任务需要更丰富的训练数据才能实现泛化”这一 H直觉得到了 M正式证实。

Abstract: We study the problem of length generalization (LG) in transformers: the
ability of a model trained on shorter sequences to maintain performance when
evaluated on much longer, previously unseen inputs. Prior work by Huang et al.
(2025) established that transformers eventually achieve length generalization
once the training sequence length exceeds some finite threshold, but left open
the question of how large it must be. In this work, we provide the first
quantitative bounds on the required training length for length generalization
to occur. Motivated by previous empirical and theoretical work, we analyze LG
in several distinct problem settings: $\ell_\infty$ error control vs. average
error control over an input distribution, infinite-precision softmax attention
vs. finite-precision attention (which reduces to an argmax) in the transformer,
and one- vs. two-layer transformers. In all scenarios, we prove that LG occurs
when the internal behavior of the transformer on longer sequences can be
"simulated" by its behavior on shorter sequences seen during training. Our
bounds give qualitative estimates for the length of training data required for
a transformer to generalize, and we verify these insights empirically. These
results sharpen our theoretical understanding of the mechanisms underlying
extrapolation in transformers, and formalize the intuition that richer training
data is required for generalization on more complex tasks.

</details>


### [59] [Towards a Measure of Algorithm Similarity](https://arxiv.org/abs/2510.27063)
*Shairoz Sohail,Taher Ali*

Main category: cs.LG

TL;DR: 这篇论文介绍了一个用于衡量算法相似性的新框架EMOC，并提供了一个包含Python算法实现的PACD数据集。


<details>
  <summary>Details</summary>
Motivation: 研究人员需要一个实用且一致的相似性度量来确定解决相同问题的两种算法是否具有显著差异。

Method: EMOC框架通过将算法实现嵌入到特征空间中来评估算法，并以此支持聚类、分类、近似重复检测和量化LLM生成程序的多样性。

Result: EMOC框架支持算法类型的聚类和分类、近似重复项的检测以及LLM生成程序中多样性的量化。

Conclusion: EMOC框架和PACD数据集为算法相似性的研究提供了新的工具和资源，有助于该领域未来的研究。

Abstract: Given two algorithms for the same problem, can we determine whether they are
meaningfully different? In full generality, the question is uncomputable, and
empirically it is muddied by competing notions of similarity. Yet, in many
applications (such as clone detection or program synthesis) a pragmatic and
consistent similarity metric is necessary. We review existing equivalence and
similarity notions and introduce EMOC: An
Evaluation-Memory-Operations-Complexity framework that embeds algorithm
implementations into a feature space suitable for downstream tasks. We compile
PACD, a curated dataset of verified Python implementations across three
problems, and show that EMOC features support clustering and classification of
algorithm types, detection of near-duplicates, and quantification of diversity
in LLM-generated programs. Code, data, and utilities for computing EMOC
embeddings are released to facilitate reproducibility and future work on
algorithm similarity.

</details>


### [60] [HADSF: Aspect Aware Semantic Control for Explainable Recommendation](https://arxiv.org/abs/2510.26994)
*Zheng Nie,Peijie Sun*

Main category: cs.LG

TL;DR: 论文介绍了一种名为HADSF的双阶段方法，用于改进评论推荐系统中的信息提取，解决了现有方法在范围控制、幻觉度量和成本效益方面的不足，并通过实验证明了其有效性。


<details>
  <summary>Details</summary>
Motivation: 大型语言模型（LLMs）在评论推荐系统信息提取方面的潜力尚未被充分利用，现有方法存在三个主要问题：1）信息提取范围控制不足，导致冗余和噪声；2）缺乏将LLM幻觉与实际效果联系起来的有效度量标准；3）未充分探索模型规模与成本效益之间的权衡。

Method: 本文提出了Hyper-Adaptive Dual-Stage Semantic Framework (HADSF)。该方法分为两个阶段：首先，通过自适应选择，构建一个紧凑的、语料库层面的方面词汇表；其次，在词汇表的指导下，对结构化的方面-观点三元组进行明确约束的提取。为了评估表示的忠实度，引入了方面漂移率（ADR）和观点忠实率（OFR）指标。

Result: 实验结果表明，幻觉严重程度与评分预测误差之间存在非单调关系。将HADSF集成到标准评分预测器中，可以显著降低预测误差。同时，HADSF使得小型模型在代表性部署场景中也能达到与大型模型相当的性能。

Conclusion: HADSF通过引入双阶段方法和新的评估指标，有效地解决了LLM在评论推荐系统中信息提取的现有挑战，提高了预测准确性，并优化了模型成本效益。论文还开源了代码、数据管道和指标实现，以支持该领域的可重现研究。

Abstract: Recent advances in large language models (LLMs) promise more effective
information extraction for review-based recommender systems, yet current
methods still (i) mine free-form reviews without scope control, producing
redundant and noisy representations, (ii) lack principled metrics that link LLM
hallucination to downstream effectiveness, and (iii) leave the cost-quality
trade-off across model scales largely unexplored. We address these gaps with
the Hyper-Adaptive Dual-Stage Semantic Framework (HADSF), a two-stage approach
that first induces a compact, corpus-level aspect vocabulary via adaptive
selection and then performs vocabulary-guided, explicitly constrained
extraction of structured aspect-opinion triples. To assess the fidelity of the
resulting representations, we introduce Aspect Drift Rate (ADR) and Opinion
Fidelity Rate (OFR) and empirically uncover a nonmonotonic relationship between
hallucination severity and rating prediction error. Experiments on
approximately 3 million reviews across LLMs spanning 1.5B-70B parameters show
that, when integrated into standard rating predictors, HADSF yields consistent
reductions in prediction error and enables smaller models to achieve
competitive performance in representative deployment scenarios. We release
code, data pipelines, and metric implementations to support reproducible
research on hallucination-aware, LLM-enhanced explainable recommendation. Code
is available at https://github.com/niez233/HADSF

</details>


### [61] [Exploring Landscapes for Better Minima along Valleys](https://arxiv.org/abs/2510.27153)
*Tong Zhao,Jiacheng Li,Yuanchang Zhou,Guangming Tan,Weile Jia*

Main category: cs.LG

TL;DR: 本文提出了一个名为“E”的适配器，用于改进现有的基于梯度的优化器，使其在达到局部最小值后仍能继续探索损失函数景观的低谷，从而找到更低、更平坦的局部最小值，提高模型的泛化能力。


<details>
  <summary>Details</summary>
Motivation: 在深度学习中，找到更低且泛化能力更好的最小值至关重要。然而，大多数现有优化器在达到局部最小值后会停止搜索参数空间，这难以保证找到最低点或最佳泛化能力。

Method: 本文提出了一个名为“E”的适配器，用于基于梯度的优化器。该适配器使优化器在达到局部最小值后仍能沿着损失函数景观的低谷（损失低且几乎相同）继续探索，以寻找潜在的更优局部最小值。同时，论文还提供了在凸和非凸情况下收敛性的证明。

Result: 在大型批量训练任务中，经过适配的Lamb优化器（ALTO）将当前最先进优化器的测试准确率（泛化能力）平均提高了2.5%。

Conclusion: 本文开辟了优化算法设计的一个新研究方向，即通过继续探索损失函数景观的低谷来寻找更低、更平坦的局部最小值，从而提高模型的泛化能力。

Abstract: Finding lower and better-generalizing minima is crucial for deep learning.
However, most existing optimizers stop searching the parameter space once they
reach a local minimum. Given the complex geometric properties of the loss
landscape, it is difficult to guarantee that such a point is the lowest or
provides the best generalization. To address this, we propose an adaptor "E"
for gradient-based optimizers. The adapted optimizer tends to continue
exploring along landscape valleys (areas with low and nearly identical losses)
in order to search for potentially better local minima even after reaching a
local minimum. This approach increases the likelihood of finding a lower and
flatter local minimum, which is often associated with better generalization. We
also provide a proof of convergence for the adapted optimizers in both convex
and non-convex scenarios for completeness. Finally, we demonstrate their
effectiveness in an important but notoriously difficult training scenario,
large-batch training, where Lamb is the benchmark optimizer. Our testing
results show that the adapted Lamb, ALTO, increases the test accuracy
(generalization) of the current state-of-the-art optimizer by an average of
2.5% across a variety of large-batch training tasks. This work potentially
opens a new research direction in the design of optimization algorithms.

</details>


### [62] [Gradient Descent as Loss Landscape Navigation: a Normative Framework for Deriving Learning Rules](https://arxiv.org/abs/2510.26997)
*John J. Vastola,Samuel J. Gershman,Kanaka Rajan*

Main category: cs.LG

TL;DR: 该框架将学习规则视为在（部分可观察的）损失景观中导航的策略，并将最优规则识别为相关最优控制问题的解决方案。


<details>
  <summary>Details</summary>
Motivation: 传统的学习规则通常是假设而非推导的，本研究旨在探究不同学习规则的性能差异及在何种假设下规则是最优的。

Method: 提出一个理论框架，将学习规则视为在（部分可观察的）损失景观中导航的策略，并通过解决最优控制问题来找到最优规则。

Result: 在不同假设下，该框架自然地引出了多种知名规则，如短时优化下的梯度下降、长时规划下的动量、考虑参数空间几何的自然梯度、部分可控性下的非梯度规则以及通过在线贝叶斯推断损失景观形状获得的Adam等自适应优化器。此外，类似权重重置的持续学习策略可被理解为对任务不确定性的最优响应。

Conclusion: 该框架通过将这些现象统一在单一目标之下，阐明了学习的计算结构，并为设计自适应算法提供了原则性基础。

Abstract: Learning rules -- prescriptions for updating model parameters to improve
performance -- are typically assumed rather than derived. Why do some learning
rules work better than others, and under what assumptions can a given rule be
considered optimal? We propose a theoretical framework that casts learning
rules as policies for navigating (partially observable) loss landscapes, and
identifies optimal rules as solutions to an associated optimal control problem.
A range of well-known rules emerge naturally within this framework under
different assumptions: gradient descent from short-horizon optimization,
momentum from longer-horizon planning, natural gradients from accounting for
parameter space geometry, non-gradient rules from partial controllability, and
adaptive optimizers like Adam from online Bayesian inference of loss landscape
shape. We further show that continual learning strategies like weight resetting
can be understood as optimal responses to task uncertainty. By unifying these
phenomena under a single objective, our framework clarifies the computational
structure of learning and offers a principled foundation for designing adaptive
algorithms.

</details>


### [63] [Soft Task-Aware Routing of Experts for Equivariant Representation Learning](https://arxiv.org/abs/2510.27222)
*Jaebyeong Jeon,Hyeonseo Jang,Jy-yong Sohn,Kibok Lee*

Main category: cs.LG

TL;DR: 本文提出了一种名为STAR的路由策略，用于改善不变性和等变性表示学习中的冗余特征学习问题。


<details>
  <summary>Details</summary>
Motivation: 现有的不变性和等变性表示学习方法通常使用独立的投影头部，这会导致冗余特征学习和模型容量的低效利用。

Method: 本文引入了Soft Task-Aware Routing（STAR）策略，将投影头部建模为专家，并促使这些专家专注于捕获共享信息或任务特定信息，从而减少冗余特征学习。

Result: 实验结果表明，STAR方法在各种迁移学习任务中都取得了持续的改进。通过观察不变性和等变性嵌入之间较低的典型相关性，验证了STAR减少冗余特征学习的有效性。

Conclusion: STAR路由策略通过优化投影头部的分工，有效地减少了不变性和等变性表示学习中的冗余特征，提升了模型性能。

Abstract: Equivariant representation learning aims to capture variations induced by
input transformations in the representation space, whereas invariant
representation learning encodes semantic information by disregarding such
transformations. Recent studies have shown that jointly learning both types of
representations is often beneficial for downstream tasks, typically by
employing separate projection heads. However, this design overlooks information
shared between invariant and equivariant learning, which leads to redundant
feature learning and inefficient use of model capacity. To address this, we
introduce Soft Task-Aware Routing (STAR), a routing strategy for projection
heads that models them as experts. STAR induces the experts to specialize in
capturing either shared or task-specific information, thereby reducing
redundant feature learning. We validate this effect by observing lower
canonical correlations between invariant and equivariant embeddings.
Experimental results show consistent improvements across diverse transfer
learning tasks. The code is available at https://github.com/YonseiML/star.

</details>


### [64] [A Framework for Fair Evaluation of Variance-Aware Bandit Algorithms](https://arxiv.org/abs/2510.27001)
*Elise Wolf*

Main category: cs.LG

TL;DR: 这篇论文介绍了一个评估多臂老虎机（MAB）算法的框架，并比较了经典算法和方差感知算法在不同环境下的性能。


<details>
  <summary>Details</summary>
Motivation: 目前，评估和比较多臂老虎机（MAB）算法面临缺乏标准化条件和可重复性的挑战。尤其是方差感知算法的性能可能严重依赖于底层环境，这使得可靠地观察算法之间的性能差异并确定方差感知算法优于经典算法的条件变得困难。

Method: 本研究提出了一个可重复的评估框架，系统地比较了八种经典和方差感知MAB算法。该框架在“Bandit Playground”代码库中实现，具有明确定义的实验设置、多种性能指标（奖励、遗憾、奖励分布、风险价值和行动最优性）以及一个支持一致且透明分析的交互式评估界面。

Result: 方差感知算法在高不确定性环境下具有优势，即臂奖励之间存在细微差异时。相反，经典算法在更容易区分的场景下表现同样好或更好，或者在经过 extensively 微调后表现更好。

Conclusion: 本研究的贡献包括： (1) 一个系统评估MAB算法的框架；(2) 关于方差感知方法何时优于经典方法的洞察。

Abstract: Multi-armed bandit (MAB) problems serve as a fundamental building block for
more complex reinforcement learning algorithms. However, evaluating and
comparing MAB algorithms remains challenging due to the lack of standardized
conditions and replicability. This is particularly problematic for
variance-aware extensions of classical methods like UCB, whose performance can
heavily depend on the underlying environment. In this study, we address how
performance differences between bandit algorithms can be reliably observed, and
under what conditions variance-aware algorithms outperform classical ones. We
present a reproducible evaluation designed to systematically compare eight
classical and variance-aware MAB algorithms. The evaluation framework,
implemented in our Bandit Playground codebase, features clearly defined
experimental setups, multiple performance metrics (reward, regret, reward
distribution, value-at-risk, and action optimality), and an interactive
evaluation interface that supports consistent and transparent analysis. We show
that variance-aware algorithms can offer advantages in settings with high
uncertainty where the difficulty arises from subtle differences between arm
rewards. In contrast, classical algorithms often perform equally well or better
in more separable scenarios or if fine-tuned extensively. Our contributions are
twofold: (1) a framework for systematic evaluation of MAB algorithms, and (2)
insights into the conditions under which variance-aware approaches outperform
their classical counterparts.

</details>


### [65] [Jasmine: A Simple, Performant and Scalable JAX-based World Modeling Codebase](https://arxiv.org/abs/2510.27002)
*Mihir Mahajan,Alfred Nguyen,Franz Srambical,Stefan Bauer*

Main category: cs.LG

TL;DR: Jasmine是一个基于JAX的世界模型代码库，它通过优化数据加载、训练和检查点，实现了比现有开源实现快一个数量级的CoinRun案例研究复现速度，并支持从单主机到数百个加速器的扩展。


<details>
  <summary>Details</summary>
Motivation: 现有的世界模型训练基础设施尚不成熟，难以克服数据稀缺问题。

Method: Jasmine通过优化数据加载、训练和检查点，实现了性能提升，并保证了训练的可复现性，支持多种分片配置。

Result: Jasmine在CoinRun案例研究中的复现速度比现有开源实现快一个数量级。

Conclusion: Jasmine为不同模型家族和架构的严格基准测试管道建立了基础设施。

Abstract: While world models are increasingly positioned as a pathway to overcoming
data scarcity in domains such as robotics, open training infrastructure for
world modeling remains nascent. We introduce Jasmine, a performant JAX-based
world modeling codebase that scales from single hosts to hundreds of
accelerators with minimal code changes. Jasmine achieves an order-of-magnitude
faster reproduction of the CoinRun case study compared to prior open
implementations, enabled by performance optimizations across data loading,
training and checkpointing. The codebase guarantees fully reproducible training
and supports diverse sharding configurations. By pairing Jasmine with curated
large-scale datasets, we establish infrastructure for rigorous benchmarking
pipelines across model families and architectural ablations.

</details>


### [66] [Enhancing Sentiment Classification with Machine Learning and Combinatorial Fusion](https://arxiv.org/abs/2510.27014)
*Sean Patten,Pin-Yu Chen,Christina Schweikert,D. Frank Hsu*

Main category: cs.LG

TL;DR: 本文提出了一种新颖的情感分类方法，该方法使用组合融合分析（CFA）来集成多样化的机器学习模型，并在IMDB情感分析数据集上实现了最先进的准确率。


<details>
  <summary>Details</summary>
Motivation: 在不增加单个模型规模的情况下，提高情感分类的准确性，并有效利用计算资源。

Method: 该方法利用组合融合分析（CFA）集成了一个多样化的机器学习模型集合，包括基于Transformer的RoBERTa模型以及Random Forest、SVM和XGBoost等传统机器学习模型。CFA利用秩-分数特征函数来量化模型之间的差异性，并策略性地结合它们的预测结果。

Result: 在IMDB情感分析数据集上实现了97.072%的最先进准确率。实验结果表明，CFA在计算和利用模型多样性方面优于传统的集成方法。

Conclusion: 组合融合分析（CFA）提供了一种有效且计算资源高效的方法来集成多样化模型，以提高情感分类的性能。

Abstract: This paper presents a novel approach to sentiment classification using the
application of Combinatorial Fusion Analysis (CFA) to integrate an ensemble of
diverse machine learning models, achieving state-of-the-art accuracy on the
IMDB sentiment analysis dataset of 97.072\%. CFA leverages the concept of
cognitive diversity, which utilizes rank-score characteristic functions to
quantify the dissimilarity between models and strategically combine their
predictions. This is in contrast to the common process of scaling the size of
individual models, and thus is comparatively efficient in computing resource
use. Experimental results also indicate that CFA outperforms traditional
ensemble methods by effectively computing and employing model diversity. The
approach in this paper implements the combination of a transformer-based model
of the RoBERTa architecture with traditional machine learning models, including
Random Forest, SVM, and XGBoost.

</details>


### [67] [Limits of Generalization in RLVR: Two Case Studies in Mathematical Reasoning](https://arxiv.org/abs/2510.27044)
*Md Tanvirul Alam,Nidhi Rastogi*

Main category: cs.LG

TL;DR: 研究了强化学习与可验证奖励（RLVR）在提高大型语言模型（LLMs）的数学推理能力方面的有效性，发现RLVR虽然能改善评估指标，但往往通过强化肤浅的启发式方法而非学习新的推理策略。


<details>
  <summary>Details</summary>
Motivation: 大型语言模型（LLMs）在数学推理方面面临挑战，需要正确的答案和真实的推理过程。可验证奖励强化学习（RLVR）作为一种增强这些能力的方法，其培养真正推理的能力尚不明确。

Method: 在两个具有完全可验证解的组合问题上（活动调度和最长递增子序列），使用精心策划的具有独特最优解的数据集，调查了RLVR。采用了多种奖励设计。

Result: RLVR提高了评估指标，但通常是通过强化肤浅的启发式方法来实现的，而不是通过获取新的推理策略。

Conclusion: 强调了RLVR泛化能力的局限性，并指出了需要设计能够区分真正数学推理和捷径利用的基准测试，提供衡量进展的真实指标。

Abstract: Mathematical reasoning is a central challenge for large language models
(LLMs), requiring not only correct answers but also faithful reasoning
processes. Reinforcement Learning with Verifiable Rewards (RLVR) has emerged as
a promising approach for enhancing such capabilities; however, its ability to
foster genuine reasoning remains unclear. We investigate RLVR on two
combinatorial problems with fully verifiable solutions: \emph{Activity
Scheduling} and the \emph{Longest Increasing Subsequence}, using carefully
curated datasets with unique optima. Across multiple reward designs, we find
that RLVR improves evaluation metrics but often by reinforcing superficial
heuristics rather than acquiring new reasoning strategies. These findings
highlight the limits of RLVR generalization, emphasizing the importance of
benchmarks that disentangle genuine mathematical reasoning from shortcut
exploitation and provide faithful measures of progress. Code available at
https://github.com/xashru/rlvr-seq-generalization.

</details>


### [68] [Consistency Training Helps Stop Sycophancy and Jailbreaks](https://arxiv.org/abs/2510.27062)
*Alex Irpan,Alexander Matt Turner,Mark Kurzeja,David K. Elson,Rohin Shah*

Main category: cs.LG

TL;DR: 该文章探讨了一种名为“一致性训练”的自我监督范式，旨在增强大型语言模型的鲁棒性，使其在面对无关提示线索时保持行为不变。


<details>
  <summary>Details</summary>
Motivation: 大型语言模型（LLM）的事实性和拒绝训练容易受到提示的简单改变而损害，模型经常采纳用户信念（谄媚）或满足包含在特殊文本中的不当请求（越狱）。

Method: 文章提出了两种强制模型对提示数据增强保持不变性的方法：通过模型的外部输出（偏置增强一致性训练BCT）和通过其内部激活（激活一致性训练ACT）。BCT源于Chua等人的研究，ACT是本文提出的一种新方法。

Result: 这两种方法都降低了Gemini 2.5 Flash模型对无关线索的敏感性。一致性训练使用了模型自身的响应作为训练数据，避免了过时训练数据带来的问题。BCT和ACT在减少谄媚方面表现相似，但BCT在减少越狱方面表现更优。

Conclusion: 作者认为BCT可以简化训练流程，因为它不依赖静态数据集。他们提出，一些对齐问题最好被视为一致性问题，而非寻求最佳响应。

Abstract: An LLM's factuality and refusal training can be compromised by simple changes
to a prompt. Models often adopt user beliefs (sycophancy) or satisfy
inappropriate requests which are wrapped within special text (jailbreaking). We
explore \emph{consistency training}, a self-supervised paradigm that teaches a
model to be invariant to certain irrelevant cues in the prompt. Instead of
teaching the model what exact response to give on a particular prompt, we aim
to teach the model to behave identically across prompt data augmentations (like
adding leading questions or jailbreak text). We try enforcing this invariance
in two ways: over the model's external outputs (\emph{Bias-augmented
Consistency Training} (BCT) from Chua et al. [2025]) and over its internal
activations (\emph{Activation Consistency Training} (ACT), a method we
introduce). Both methods reduce Gemini 2.5 Flash's susceptibility to irrelevant
cues. Because consistency training uses responses from the model itself as
training data, it avoids issues that arise from stale training data, such as
degrading model capabilities or enforcing outdated response guidelines. While
BCT and ACT reduce sycophancy equally well, BCT does better at jailbreak
reduction. We think that BCT can simplify training pipelines by removing
reliance on static datasets. We argue that some alignment problems are better
viewed not in terms of optimal responses, but rather as consistency issues.

</details>


### [69] [MLPerf Automotive](https://arxiv.org/abs/2510.27065)
*Radoyeh Shojaei,Predrag Djurdjevic,Mostafa El-Khamy,James Goel,Kasper Mecklenburg,John Owens,Pınar Muyan-Özçelik,Tom St. John,Jinho Suh,Arjun Suresh*

Main category: cs.LG

TL;DR: 该文章介绍了MLPerf Automotive，这是首个用于评估汽车AI加速中部署的机器学习系统的标准化公共基准。


<details>
  <summary>Details</summary>
Motivation: 现有的基准测试套件无法用于汽车系统，因为汽车工作负载具有独特的约束，包括安全性，可扩展性和实时处理。

Method: 通过MLCommons和自动驾驶汽车计算联盟之间的合作。基准测试框架提供延迟和准确性指标以及评估协议，并包含2D对象检测，2D语义分割和3D对象检测的汽车感知任务。

Result: MLPerf Automotive通过提供一致且可重现的性能，来比较不同的硬件平台和软件实施。

Conclusion: MLPerf Automotive成功发布，解决了汽车机器学习系统标准化性能评估方法的需求。

Abstract: We present MLPerf Automotive, the first standardized public benchmark for
evaluating Machine Learning systems that are deployed for AI acceleration in
automotive systems. Developed through a collaborative partnership between
MLCommons and the Autonomous Vehicle Computing Consortium, this benchmark
addresses the need for standardized performance evaluation methodologies in
automotive machine learning systems. Existing benchmark suites cannot be
utilized for these systems since automotive workloads have unique constraints
including safety and real-time processing that distinguish them from the
domains that previously introduced benchmarks target. Our benchmarking
framework provides latency and accuracy metrics along with evaluation protocols
that enable consistent and reproducible performance comparisons across
different hardware platforms and software implementations. The first iteration
of the benchmark consists of automotive perception tasks in 2D object
detection, 2D semantic segmentation, and 3D object detection. We describe the
methodology behind the benchmark design including the task selection, reference
models, and submission rules. We also discuss the first round of benchmark
submissions and the challenges involved in acquiring the datasets and the
engineering efforts to develop the reference implementations. Our benchmark
code is available at https://github.com/mlcommons/mlperf_automotive.

</details>


### [70] [Towards Understanding Self-play for LLM Reasoning](https://arxiv.org/abs/2510.27072)
*Justin Yang Chae,Md Tanvirul Alam,Nidhi Rastogi*

Main category: cs.LG

TL;DR: 这篇文章探讨了大型语言模型（LLM）的自我博弈（self-play）训练，分析了它的训练动态、改进机制，并与RLVR和SFT进行了比较，以理解其对数学推理性能的影响。


<details>
  <summary>Details</summary>
Motivation: 理解自我博弈训练对大型语言模型推理能力提升的机制，并找出其与RLVR和SFT等其他后训练方法的区别。

Method: 通过分析大型语言模型的参数更新稀疏性、token分布的熵动态以及替代的proposer奖励函数，对自我博弈的训练动态进行研究，并使用pass@k评估方法将这些动态与推理性能联系起来。

Result: 研究结果阐明了自我博弈与其他后训练策略的不同之处，揭示了其固有的局限性，并为未来通过自我博弈改进大型语言模型数学推理的提供了方向。

Conclusion: 自我博弈是一种有效的提高大型语言模型推理能力的训练方法，但其作用机制仍需深入理解。本研究通过深入分析其训练动态，为理解自我博弈的优势和局限性以及未来的研究方向奠定了基础。

Abstract: Recent advances in large language model (LLM) reasoning, led by reinforcement
learning with verifiable rewards (RLVR), have inspired self-play post-training,
where models improve by generating and solving their own problems. While
self-play has shown strong in-domain and out-of-domain gains, the mechanisms
behind these improvements remain poorly understood. In this work, we analyze
the training dynamics of self-play through the lens of the Absolute Zero
Reasoner, comparing it against RLVR and supervised fine-tuning (SFT). Our study
examines parameter update sparsity, entropy dynamics of token distributions,
and alternative proposer reward functions. We further connect these dynamics to
reasoning performance using pass@k evaluations. Together, our findings clarify
how self-play differs from other post-training strategies, highlight its
inherent limitations, and point toward future directions for improving LLM math
reasoning through self-play.

</details>


### [71] [Functional embeddings enable Aggregation of multi-area SEEG recordings over subjects and sessions](https://arxiv.org/abs/2510.27090)
*Sina Javadzadeh,Rahil Soroushmojdehi,S. Alireza Seyyed Mousavi,Mehrnaz Asadi,Sumiko Abe,Terence D. Sanger*

Main category: cs.LG

TL;DR: 这篇论文提出了一种可扩展的表征学习框架，用于整合不同受试者的颅内记录。该框架学习在不同受试者、不同电极布局下，对每个电极的功能进行识别，并对这些嵌入进行标记，以训练一个可以对区域间关系进行建模的Transformer。


<details>
  <summary>Details</summary>
Motivation: 传统的空间归一化方法（如MNI坐标）难以捕捉真实的功能相似性，因为电极数量、位置和覆盖区域因人而异，导致在匹配的解剖坐标下，目标脑区和神经动力学在个体之间存在显著差异。

Method: 该框架主要包含两个部分：1. 使用带有对比目标的Siamese编码器，从多区域局部场电位中学习每个电极的独立于受试者的功能识别。这种方法可以产生对区域特异性神经信号具有局部敏感性的嵌入几何。 2. 将这些嵌入标记化，送入Transformer模型，以建模可变数量通道的区域间关系。

Result: 该框架在包含20个受试者的数据集上进行了评估，这些数据集涵盖了基底神经节-丘脑区域，并且电极布局不均一。结果表明，学习到的功能空间能够实现准确的受试者内区分，并形成清晰、区域一致的聚类；它还可以零样本迁移到未见过的通道。Transformer模型在没有受试者特定头部或监督的情况下，捕获了跨区域依赖关系，并实现了对被掩盖通道的重建，为下游解码提供了一个独立于受试者的骨干。

Conclusion: 这些结果表明，对于颅内神经数据，即使在任务结构严格且传感器放置不统一的情况下，该方法也为大规模、跨受试者聚合和预训练提供了一个可行的途径。

Abstract: Aggregating intracranial recordings across subjects is challenging since
electrode count, placement, and covered regions vary widely. Spatial
normalization methods like MNI coordinates offer a shared anatomical reference,
but often fail to capture true functional similarity, particularly when
localization is imprecise; even at matched anatomical coordinates, the targeted
brain region and underlying neural dynamics can differ substantially between
individuals. We propose a scalable representation-learning framework that (i)
learns a subject-agnostic functional identity for each electrode from
multi-region local field potentials using a Siamese encoder with contrastive
objectives, inducing an embedding geometry that is locality-sensitive to
region-specific neural signatures, and (ii) tokenizes these embeddings for a
transformer that models inter-regional relationships with a variable number of
channels. We evaluate this framework on a 20-subject dataset spanning basal
ganglia-thalamic regions collected during flexible rest/movement recording
sessions with heterogeneous electrode layouts. The learned functional space
supports accurate within-subject discrimination and forms clear,
region-consistent clusters; it transfers zero-shot to unseen channels. The
transformer, operating on functional tokens without subject-specific heads or
supervision, captures cross-region dependencies and enables reconstruction of
masked channels, providing a subject-agnostic backbone for downstream decoding.
Together, these results indicate a path toward large-scale, cross-subject
aggregation and pretraining for intracranial neural data where strict task
structure and uniform sensor placement are unavailable.

</details>


### [72] [Hierarchical Bayesian Model for Gene Deconvolution and Functional Analysis in Human Endometrium Across the Menstrual Cycle](https://arxiv.org/abs/2510.27097)
*Crystal Su,Kuai Yu,Mingyuan Shao,Daniel Bauer*

Main category: cs.LG

TL;DR: 这篇论文介绍了一个概率分层贝叶斯模型，用于将批量RNA测序数据反卷积为细胞类型特异性表达谱和比例，并应用于人类子宫内膜组织，揭示了月经周期中细胞组成和基因表达的动态变化。


<details>
  <summary>Details</summary>
Motivation: 为了解决批量组织RNA测序数据中细胞类型特异性动态被平均基因表达谱掩盖的问题。

Method: 本文提出了一个概率分层贝叶斯模型，利用高分辨率单细胞参考数据，将批量RNA-seq数据反卷积为组成细胞类型的表达谱和比例。该模型通过扩展框架对细胞类型比例和细胞特异性基因表达在月经周期各阶段的变化进行原理性推断。

Result: 结果显示，月经周期各阶段上皮细胞、基质细胞和免疫细胞比例发生动态变化，并识别出与子宫内膜功能相关的细胞类型特异性差异基因表达（例如，分泌期的基质细胞中的蜕膜化标志物）。模型通过模拟和与现有方法的比较验证了其性能，并显示出对参考不匹配和噪声具有鲁棒性。

Conclusion: 该模型成功揭示了人类子宫内膜在月经周期中细胞组成和基因表达的动态变化，这些发现对于生育能力和子宫内膜疾病的潜在临床应用以及未来的研究方向（包括空间转录组学整合）具有生物学意义。

Abstract: Bulk tissue RNA sequencing of heterogeneous samples provides averaged gene
expression profiles, obscuring cell type-specific dynamics. To address this, we
present a probabilistic hierarchical Bayesian model that deconvolves bulk
RNA-seq data into constituent cell-type expression profiles and proportions,
leveraging a high-resolution single-cell reference. We apply our model to human
endometrial tissue across the menstrual cycle, a context characterized by
dramatic hormone-driven cellular composition changes. Our extended framework
provides a principled inference of cell type proportions and cell-specific gene
expression changes across cycle phases. We demonstrate the model's structure,
priors, and inference strategy in detail, and we validate its performance with
simulations and comparisons to existing methods. The results reveal dynamic
shifts in epithelial, stromal, and immune cell fractions between menstrual
phases, and identify cell-type-specific differential gene expression associated
with endometrial function (e.g., decidualization markers in stromal cells
during the secretory phase). We further conduct robustness tests and show that
our Bayesian approach is resilient to reference mismatches and noise. Finally,
we discuss the biological significance of our findings, potential clinical
implications for fertility and endometrial disorders, and future directions,
including integration of spatial transcriptomics.

</details>


### [73] [Group-Sensitive Offline Contextual Bandits](https://arxiv.org/abs/2510.27123)
*Yihong Guo,Junjie Luo,Guodong Gao,Ritu Agarwal,Anqi Liu*

Main category: cs.LG

TL;DR: 这篇论文提出了一种在离线上下文赌 bandit 中处理群体敏感公平约束的方法，通过引入群体奖励差异约束到基于离线策略梯度的优化过程中，有效减少了奖励差异，同时保持了有竞争力的整体性能。


<details>
  <summary>Details</summary>
Motivation: 离线上下文赌 bandit 学习策略可能会无意中放大群体间的奖励差异，导致某些群体受益更多，从而引发公平性问题，尤其是在资源有限的情况下。

Method: 本文通过将群体奖励差异约束引入到离线策略梯度优化过程中，提出了一个受约束的离线策略优化框架。为了提高训练过程中群体奖励差异估计的准确性，作者采用了双重鲁棒估计器，并为策略优化提供了收敛保证。

Result: 在合成数据集和真实世界数据集上的实证结果表明，该方法在有效减少奖励差异的同时，保持了有竞争力的整体性能。

Conclusion: 本文提出的方法能够有效缓解离线上下文赌 bandit 中策略学习引起的群体间奖励差异。

Abstract: Offline contextual bandits allow one to learn policies from
historical/offline data without requiring online interaction. However, offline
policy optimization that maximizes overall expected rewards can unintentionally
amplify the reward disparities across groups. As a result, some groups might
benefit more than others from the learned policy, raising concerns about
fairness, especially when the resources are limited. In this paper, we study a
group-sensitive fairness constraint in offline contextual bandits, reducing
group-wise reward disparities that may arise during policy learning. We tackle
the following common-parity requirements: the reward disparity is constrained
within some user-defined threshold or the reward disparity should be minimized
during policy optimization. We propose a constrained offline policy
optimization framework by introducing group-wise reward disparity constraints
into an off-policy gradient-based optimization procedure. To improve the
estimation of the group-wise reward disparity during training, we employ a
doubly robust estimator and further provide a convergence guarantee for policy
optimization. Empirical results in synthetic and real-world datasets
demonstrate that our method effectively reduces reward disparities while
maintaining competitive overall performance.

</details>


### [74] [Exploring the Utilities of the Rationales from Large Language Models to Enhance Automated Essay Scoring](https://arxiv.org/abs/2510.27131)
*Hong Jiao,Hanna Choi,Haowei Hua*

Main category: cs.LG

TL;DR: 本文探讨了使用GPT-4.1和GPT-5生成的理由在自动评分中的效用。


<details>
  <summary>Details</summary>
Motivation: 探索GPT模型生成的理由在自动评分中的实用性，并对比基于文章和基于理由的评分方法。

Method: 使用来自2012年Kaggle ASAP数据的Prompt 6论文，对比了基于论文的评分和基于理由的评分。同时，研究了集成模型在提高评分准确性方面的效果。

Result: 基于论文的评分整体表现优于基于理由的评分，具有更高的QWK。然而，在分数0的评分中，基于理由的评分F1分数更高。集成基于论文的评分模型提高了整体评分准确性。结合基于论文和基于理由的评分得到的集成模型取得了最佳的评分准确性，QWK为0.870。

Conclusion: GPT-4.1和GPT-5生成的理由对于自动评分具有一定的价值，尤其是在处理类别不平衡问题时。通过集成基于论文和基于理由的评分，可以显著提高自动评分的准确性。

Abstract: This study explored the utilities of rationales generated by GPT-4.1 and
GPT-5 in automated scoring using Prompt 6 essays from the 2012 Kaggle ASAP
data. Essay-based scoring was compared with rationale-based scoring. The study
found in general essay-based scoring performed better than rationale-based
scoring with higher Quadratic Weighted Kappa (QWK). However, rationale-based
scoring led to higher scoring accuracy in terms of F1 scores for score 0 which
had less representation due to class imbalance issues. The ensemble modeling of
essay-based scoring models increased the scoring accuracy at both specific
score levels and across all score levels. The ensemble modeling of essay-based
scoring and each of the rationale-based scoring performed about the same.
Further ensemble of essay-based scoring and both rationale-based scoring
yielded the best scoring accuracy with QWK of 0.870 compared with 0.848
reported in literature.

</details>


### [75] [FairAD: Computationally Efficient Fair Graph Clustering via Algebraic Distance](https://arxiv.org/abs/2510.27136)
*Minh Phu Vuong,Young-Ju Lee,Iván Ojeda-Ruiz,Chul-Ho Lee*

Main category: cs.LG

TL;DR: FairAD是一种高效的公平图聚类方法，它首先构建一个施加公平性约束的新亲和矩阵，然后进行图粗化以找到代表性节点，最后通过解决一个约束最小化问题来实现公平聚类。


<details>
  <summary>Details</summary>
Motivation: 由于机器学习模型对特定人群的不良行为日益受到关注，公平性问题引起了社区的广泛关注，因此推动了图聚类中公平性研究。

Method: FairAD方法首先基于代数距离概念构建一个施加公平性约束的新亲和矩阵。然后，对该亲和矩阵执行图粗化过程，以找到对应于k个簇的代表性节点。最后，通过求解一个约束最小化问题来获得公平聚类的解决方案。

Result: 在改进的随机块模型和六个公共数据集上的实验结果表明，FairAD在实现公平聚类的同时，速度比最先进的公平图聚类算法快40倍。

Conclusion: FairAD是一种有效的公平图聚类方法，它通过创新的方法解决了现有算法在公平性约束方面的计算挑战，并在速度和公平性之间取得了显著的平衡。

Abstract: Due to the growing concern about unsavory behaviors of machine learning
models toward certain demographic groups, the notion of 'fairness' has recently
drawn much attention from the community, thereby motivating the study of
fairness in graph clustering. Fair graph clustering aims to partition the set
of nodes in a graph into $k$ disjoint clusters such that the proportion of each
protected group within each cluster is consistent with the proportion of that
group in the entire dataset. It is, however, computationally challenging to
incorporate fairness constraints into existing graph clustering algorithms,
particularly for large graphs. To address this problem, we propose FairAD, a
computationally efficient fair graph clustering method. It first constructs a
new affinity matrix based on the notion of algebraic distance such that
fairness constraints are imposed. A graph coarsening process is then performed
on this affinity matrix to find representative nodes that correspond to $k$
clusters. Finally, a constrained minimization problem is solved to obtain the
solution of fair clustering. Experiment results on the modified stochastic
block model and six public datasets show that FairAD can achieve fair
clustering while being up to 40 times faster compared to state-of-the-art fair
graph clustering algorithms.

</details>


### [76] [A Polynomial-time Algorithm for Online Sparse Linear Regression with Improved Regret Bound under Weaker Conditions](https://arxiv.org/abs/2510.27177)
*Junfan Li,Shizhong Liao,Zenglin Xu,Liqiang Nie*

Main category: cs.LG

TL;DR: 本文提出了一种新的多项式时间算法，用于解决在线稀疏线性回归问题，在兼容性条件下显著改善了以往的后悔界限。


<details>
  <summary>Details</summary>
Motivation: 以往的在线稀疏线性回归算法对于数据矩阵的假设条件过于严格（如特征的线性独立性、兼容性条件或受限等距性质），且在NP-hard问题上难以达到最优。本文旨在在更弱的兼容性条件下，提出一种更优的算法。

Method: 本文提出了一种新的多项式时间算法，该算法借鉴了Dantzig Selector，并结合了算法依赖的协方差矩阵估计采样方案、自适应参数调整方案以及带仔细初始化的批处理在线牛顿步等创新技术。

Result: 新算法在兼容性条件下显著改善了以往的后悔界限。这得益于估计器L1范数误差收敛速度的提高。该算法还扩展到带有额外观测值的OSLR，并改进了以往的后悔界限。

Conclusion: 本文提出了一种新的在线稀疏线性回归算法，在更弱的兼容性条件下，取得了更好的后悔界限，并通过新的分析方法验证了算法的有效性。

Abstract: In this paper, we study the problem of online sparse linear regression (OSLR)
where the algorithms are restricted to accessing only $k$ out of $d$ attributes
per instance for prediction, which was proved to be NP-hard. Previous work gave
polynomial-time algorithms assuming the data matrix satisfies the linear
independence of features, the compatibility condition, or the restricted
isometry property. We introduce a new polynomial-time algorithm, which
significantly improves previous regret bounds (Ito et al., 2017) under the
compatibility condition that is weaker than the other two assumptions. The
improvements benefit from a tighter convergence rate of the $\ell_1$-norm error
of our estimators. Our algorithm leverages the well-studied Dantzig Selector,
but importantly with several novel techniques, including an algorithm-dependent
sampling scheme for estimating the covariance matrix, an adaptive parameter
tuning scheme, and a batching online Newton step with careful initializations.
We also give novel and non-trivial analyses, including an induction method for
analyzing the $\ell_1$-norm error, careful analyses on the covariance of
non-independent random variables, and a decomposition on the regret. We further
extend our algorithm to OSLR with additional observations where the algorithms
can observe additional $k_0$ attributes after each prediction, and improve
previous regret bounds (Kale et al., 2017; Ito et al., 2017).

</details>


### [77] [SERFLOW: A Cross-Service Cost Optimization Framework for SLO-Aware Dynamic ML Inference](https://arxiv.org/abs/2510.27182)
*Zongshun Zhang,Ibrahim Matta*

Main category: cs.LG

TL;DR: 该文章提出了一种名为SERFLOW的动态卸载机器学习模型分区的方法，旨在平衡处理和传输延迟，同时最小化自适应推理应用程序的成本。


<details>
  <summary>Details</summary>
Motivation: 以往的工作往往忽视了真实世界的因素，如虚拟机冷启动、长尾服务时间分布下的请求等。为了解决这些限制，本文对机器学习查询进行了建模。由于输入相关的退出率不同，没有单一的资源配置适合所有查询分布。当许多请求提前退出时，基于IaaS的VMs利用率不足，但快速扩展以处理到达深层的请求突发是不切实际的。

Method: SERFLOW通过利用基于FaaS的无服务器功能（容器），并使用阶段特定的资源供应，该供应考虑了在每个阶段退出的请求比例，从而解决了这些挑战。通过将此供应与基于请求摄入的VM和无服务器功能之间的自适应负载平衡相结合。

Result: SERFLOW将云成本降低了23%以上，同时有效地适应了动态工作负载。

Conclusion: SERFLOW通过动态卸载机器学习模型分区，有效平衡了处理和传输延迟，并最小化了自适应推理应用程序的成本，同时克服了现有方法的局限性。

Abstract: Dynamic offloading of Machine Learning (ML) model partitions across different
resource orchestration services, such as Function-as-a-Service (FaaS) and
Infrastructure-as-a-Service (IaaS), can balance processing and transmission
delays while minimizing costs of adaptive inference applications. However,
prior work often overlooks real-world factors, such as Virtual Machine (VM)
cold starts, requests under long-tail service time distributions, etc. To
tackle these limitations, we model each ML query (request) as traversing an
acyclic sequence of stages, wherein each stage constitutes a contiguous block
of sparse model parameters ending in an internal or final classifier where
requests may exit. Since input-dependent exit rates vary, no single resource
configuration suits all query distributions. IaaS-based VMs become
underutilized when many requests exit early, yet rapidly scaling to handle
request bursts reaching deep layers is impractical. SERFLOW addresses this
challenge by leveraging FaaS-based serverless functions (containers) and using
stage-specific resource provisioning that accounts for the fraction of requests
exiting at each stage. By integrating this provisioning with adaptive load
balancing across VMs and serverless functions based on request ingestion,
SERFLOW reduces cloud costs by over $23\%$ while efficiently adapting to
dynamic workloads.

</details>


### [78] [Feature-Function Curvature Analysis: A Geometric Framework for Explaining Differentiable Models](https://arxiv.org/abs/2510.27207)
*Hamed Najafi,Dongsheng Luo,Jason Liu*

Main category: cs.LG

TL;DR: FFCA是一种新的模型函数几何分析框架，它能为每个特征生成一个4维特征，并通过动态原型分析来追踪这些特征在训练过程中的演变。


<details>
  <summary>Details</summary>
Motivation: 主流归因方法在解释复杂机器学习模型时，通常只提供不完整、静态的模型最终状态，且易受非线性和交互作用的混淆。

Method: 本文引入了特征-函数曲率分析（FFCA）框架，该框架通过分析模型学习函数的几何结构，为每个特征生成一个包含“影响、波动性、非线性、交互”的4维特征。此外，该框架还扩展到动态原型分析，用于追踪这些特征在训练过程中的演变。

Result: 通过FFCA，作者首次直接经验性地证明了分层学习的存在，表明模型在学习复杂交互之前，会首先学习简单的线性效应。此外，动态分析为识别模型容量不足和预测过拟合的发生提供了新颖、实用的诊断方法。

Conclusion: FFCA框架，通过其静态和动态组件，提供了必要的几何上下文，将模型解释从简单的量化转变为对整个学习过程的细致、可靠分析。

Abstract: Explainable AI (XAI) is critical for building trust in complex machine
learning models, yet mainstream attribution methods often provide an
incomplete, static picture of a model's final state. By collapsing a feature's
role into a single score, they are confounded by non-linearity and
interactions. To address this, we introduce Feature-Function Curvature Analysis
(FFCA), a novel framework that analyzes the geometry of a model's learned
function. FFCA produces a 4-dimensional signature for each feature, quantifying
its: (1) Impact, (2) Volatility, (3) Non-linearity, and (4) Interaction.
Crucially, we extend this framework into Dynamic Archetype Analysis, which
tracks the evolution of these signatures throughout the training process. This
temporal view moves beyond explaining what a model learned to revealing how it
learns. We provide the first direct, empirical evidence of hierarchical
learning, showing that models consistently learn simple linear effects before
complex interactions. Furthermore, this dynamic analysis provides novel,
practical diagnostics for identifying insufficient model capacity and
predicting the onset of overfitting. Our comprehensive experiments demonstrate
that FFCA, through its static and dynamic components, provides the essential
geometric context that transforms model explanation from simple quantification
to a nuanced, trustworthy analysis of the entire learning process.

</details>


### [79] [Not All Instances Are Equally Valuable: Towards Influence-Weighted Dataset Distillation](https://arxiv.org/abs/2510.27253)
*Qiyan Deng,Changqian Zheng,Lianpeng Qiao,Yuping Wang,Chengliang Chai,Lei Cao*

Main category: cs.LG

TL;DR: 数据集蒸馏将大型数据集压缩成合成子集，在保持与在完整数据集上训练相当的性能的同时，显著降低了存储和计算成本。本文提出了一种名为IWD的框架，利用影响函数在蒸馏过程中明确考虑数据质量，通过为每个实例分配自适应权重来优先处理有益数据，同时降低不太有用或有害数据的权重。


<details>
  <summary>Details</summary>
Motivation: 现有的数据集蒸馏方法大多假设所有真实实例对蒸馏过程贡献相同，但实际数据集中存在信息丰富、冗余甚至有害的实例，不考虑数据质量直接蒸馏可能损害模型性能。

Method: 本文提出了Influence-Weighted Distillation（IWD）框架。IWD利用影响函数在蒸馏过程中明确考虑数据质量，根据每个实例对蒸馏目标的估计影响，为其分配自适应权重，以优先考虑有益数据，同时降低不太有用或有害数据的权重。IWD具有模块化设计，可以无缝集成到不同的数据集蒸馏框架中。

Result: 实验结果表明，集成IWD可以提高蒸馏数据集的质量，并增强模型性能，准确率提高了7.8%。

Conclusion: IWD框架通过在数据集蒸馏中引入数据质量考量，有效提升了蒸馏数据集的质量和模型性能。

Abstract: Dataset distillation condenses large datasets into synthetic subsets,
achieving performance comparable to training on the full dataset while
substantially reducing storage and computation costs. Most existing dataset
distillation methods assume that all real instances contribute equally to the
process. In practice, real-world datasets contain both informative and
redundant or even harmful instances, and directly distilling the full dataset
without considering data quality can degrade model performance. In this work,
we present Influence-Weighted Distillation IWD, a principled framework that
leverages influence functions to explicitly account for data quality in the
distillation process. IWD assigns adaptive weights to each instance based on
its estimated impact on the distillation objective, prioritizing beneficial
data while downweighting less useful or harmful ones. Owing to its modular
design, IWD can be seamlessly integrated into diverse dataset distillation
frameworks. Our empirical results suggest that integrating IWD tends to improve
the quality of distilled datasets and enhance model performance, with accuracy
gains of up to 7.8%.

</details>


### [80] [ECVL-ROUTER: Scenario-Aware Routing for Vision-Language Models](https://arxiv.org/abs/2510.27256)
*Xin Tang,Youfang Han,Fangfei Gou,Wei Zhao,Xin Meng,Yang Yu,Jinguo Zhang,Yuanchun Shi,Yuntao Wang,Tengxiang Zhang*

Main category: cs.LG

TL;DR: ECVL-ROUTER是一个场景感知路由框架，旨在通过动态选择合适的视觉语言模型（VLM）来平衡响应速度、输出质量和能耗。它将80%以上的查询路由到小型模型，同时将问题解决能力下降控制在10%以内。


<details>
  <summary>Details</summary>
Motivation: 大型视觉语言模型在云端部署时存在高延迟和高能耗问题，而小型模型虽然能耗和延迟较低，但处理能力有限。为了结合两者的优势并满足不同用户需求，作者提出了一个场景感知路由框架。

Method: 作者提出了ECVL-ROUTER框架，该框架包含新的路由策略和评估指标，可根据用户需求动态选择合适的模型进行查询处理。同时，作者构建了一个多模态响应质量数据集用于路由器训练，并通过大量实验验证该方法。

Result: ECVL-ROUTER成功地将超过80%的查询路由到小型模型，同时将问题解决概率的下降控制在10%以内。这表明该框架在有效利用小型模型的同时，能保持较高的性能水平。

Conclusion: ECVL-ROUTER是一个有效的场景感知路由框架，它能够根据用户需求动态选择视觉语言模型，从而在保证输出质量的同时，显著降低延迟和能耗。

Abstract: Vision-Language Models (VLMs) excel in diverse multimodal tasks. However,
user requirements vary across scenarios, which can be categorized into fast
response, high-quality output, and low energy consumption. Relying solely on
large models deployed in the cloud for all queries often leads to high latency
and energy cost, while small models deployed on edge devices are capable of
handling simpler tasks with low latency and energy cost. To fully leverage the
strengths of both large and small models, we propose ECVL-ROUTER, the first
scenario-aware routing framework for VLMs. Our approach introduces a new
routing strategy and evaluation metrics that dynamically select the appropriate
model for each query based on user requirements, maximizing overall utility. We
also construct a multimodal response-quality dataset tailored for router
training and validate the approach through extensive experiments. Results show
that our approach successfully routes over 80\% of queries to the small model
while incurring less than 10\% drop in problem solving probability.

</details>


### [81] [ODP-Bench: Benchmarking Out-of-Distribution Performance Prediction](https://arxiv.org/abs/2510.27263)
*Han Yu,Kehan Li,Dongbai Li,Yue He,Xingxuan Zhang,Peng Cui*

Main category: cs.LG

TL;DR: 这篇论文介绍了一个名为 ODP-Bench 的综合基准，用于评估在未标记的出分布测试集上训练模型的性能预测。ODP-Bench 旨在解决现有文献中评估协议不一致的问题，并提供一个标准化的平台来比较各种算法。


<details>
  <summary>Details</summary>
Motivation: 尽管在出分布 (OOD) 性能预测领域已经取得了一些进展，但现有文献中的评估协议不一致，并且大多数工作只涵盖了有限数量的真实 OOD 数据集和分布偏移类型。

Method: 本文提出了一个名为 ODP-Distribution Performance Prediction Benchmark (ODP-Bench) 的综合基准。它包含了最常用的 OOD 数据集和现有的实用性能预测算法。作者提供预训练的模型作为测试平台，以确保比较的一致性，并避免重复模型训练过程的负担。

Result: ODP-Bench 包含了最常用的 OOD 数据集和现有的实用性能预测算法。作者提供预训练的模型作为测试平台，以确保比较的一致性，并避免重复模型训练过程的负担。此外，还进行了深入的实验分析，以更好地理解这些算法的能力边界。

Conclusion: ODP-Bench 是一个全面的基准，旨在为 OOD 性能预测研究提供一个标准化和公平的比较平台。它通过提供预训练模型和进行深入的实验分析，解决了现有研究中的不一致性问题，并促进了对 OOD 性能预测算法的更好理解。

Abstract: Recently, there has been gradually more attention paid to Out-of-Distribution
(OOD) performance prediction, whose goal is to predict the performance of
trained models on unlabeled OOD test datasets, so that we could better leverage
and deploy off-the-shelf trained models in risk-sensitive scenarios. Although
progress has been made in this area, evaluation protocols in previous
literature are inconsistent, and most works cover only a limited number of
real-world OOD datasets and types of distribution shifts. To provide convenient
and fair comparisons for various algorithms, we propose Out-of-Distribution
Performance Prediction Benchmark (ODP-Bench), a comprehensive benchmark that
includes most commonly used OOD datasets and existing practical performance
prediction algorithms. We provide our trained models as a testbench for future
researchers, thus guaranteeing the consistency of comparison and avoiding the
burden of repeating the model training process. Furthermore, we also conduct
in-depth experimental analyses to better understand their capability boundary.

</details>


### [82] [Temporal Cardiovascular Dynamics for Improved PPG-Based Heart Rate Estimation](https://arxiv.org/abs/2510.27297)
*Berken Utku Demirel,Christian Holz*

Main category: cs.LG

TL;DR: 这篇论文研究了人类心率振荡的非线性混沌行为，并提出了一种新的方法来增强现实生活中的心率估计。


<details>
  <summary>Details</summary>
Motivation: 心率振荡复杂且非线性，在日常心血管健康监测中应用面临挑战。

Method: 通过互信息研究心率的非线性混沌行为，并提出一种新方法来增强现实生活中的心率估计。该方法从数学角度解释和处理非线性时间复杂性，并可与深度学习解决方案结合使用。

Result: 在四个真实数据集上验证了所提出的方法，与传统方法和现有机器学习技术相比，心率估计性能提高了40%，同时减少了对多种传感模式的依赖并消除了后处理步骤的需求。

Conclusion: 所提出的方法能有效提高现实生活中复杂心率数据的估计精度和鲁棒性。

Abstract: The oscillations of the human heart rate are inherently complex and
non-linear -- they are best described by mathematical chaos, and they present a
challenge when applied to the practical domain of cardiovascular health
monitoring in everyday life. In this work, we study the non-linear chaotic
behavior of heart rate through mutual information and introduce a novel
approach for enhancing heart rate estimation in real-life conditions. Our
proposed approach not only explains and handles the non-linear temporal
complexity from a mathematical perspective but also improves the deep learning
solutions when combined with them. We validate our proposed method on four
established datasets from real-life scenarios and compare its performance with
existing algorithms thoroughly with extensive ablation experiments. Our results
demonstrate a substantial improvement, up to 40\%, of the proposed approach in
estimating heart rate compared to traditional methods and existing
machine-learning techniques while reducing the reliance on multiple sensing
modalities and eliminating the need for post-processing steps.

</details>


### [83] [Un-Attributability: Computing Novelty From Retrieval & Semantic Similarity](https://arxiv.org/abs/2510.27313)
*Philipp Davydov,Ameya Prabhu,Matthias Bethge,Elisa Nguyen,Seong Joon Oh*

Main category: cs.LG

TL;DR: 本文提出了一种新的方法来测量大型语言模型输出的语义新颖性，即“不可归因性”。


<details>
  <summary>Details</summary>
Motivation: 现有的训练数据归因（TDA）方法主要关注训练样本与模型输出之间的因果关系，而本文旨在探究哪些输出无法归因于预训练语料库中的任何训练样本，从而衡量输出的语义新颖性。

Method: 本文提出了一种两阶段检索流程。首先，使用轻量级GIST嵌入索引语料库，并检索出top-n个候选。然后，使用ColBERTv2进行重新排序。如果最近的语料库项目不如人工生成的文本参考具有可归因性，则认为模型的输出是新颖的。

Result: 通过在SmolLM和SmolLM2上进行评估，本文发现：1）模型在比之前报告的更长的范围内利用预训练数据；2）某些领域系统性地促进或抑制新颖性；3）指令调优不仅改变了文本风格，还增加了文本的新颖性。

Conclusion: 本文将新颖性评估重新定义为“不可归因性”，从而实现了预训练规模下的高效分析。

Abstract: Understanding how language-model outputs relate to the pretraining corpus is
central to studying model behavior. Most training data attribution (TDA)
methods ask which training examples causally influence a given output, often
using leave-one-out tests. We invert the question: which outputs cannot be
attributed to any pretraining example? We introduce un-attributability as an
operational measure of semantic novelty: an output is novel if the pretraining
corpus contains no semantically similar context. We approximate this with a
simple two-stage retrieval pipeline: index the corpus with lightweight GIST
embeddings, retrieve the top-n candidates, then rerank with ColBERTv2. If the
nearest corpus item is less attributable than a human-generated text reference,
we consider the output of the model as novel. We evaluate on SmolLM and SmolLM2
and report three findings: (1) models draw on pretraining data across much
longer spans than previously reported; (2) some domains systematically promote
or suppress novelty; and (3) instruction tuning not only alters style but also
increases novelty. Reframing novelty assessment around un-attributability
enables efficient analysis at pretraining scale. We release ~20 TB of corpus
chunks and index artifacts to support replication and large-scale extension of
our analysis at https://huggingface.co/datasets/stai-tuebingen/faiss-smollm

</details>


### [84] [MedM2T: A MultiModal Framework for Time-Aware Modeling with Electronic Health Record and Electrocardiogram Data](https://arxiv.org/abs/2510.27321)
*Yu-Chen Kuo,Yi-Ju Tseng*

Main category: cs.LG

TL;DR: MedM2T是一个时间感知的多模态框架，旨在解决医疗数据固有的多模态和异构时间结构带来的挑战。


<details>
  <summary>Details</summary>
Motivation: 医疗数据的固有老年化和异构时间结构对建模提出了重大挑战。

Method: MedM2T集成了稀疏时间序列编码器、分层时间感知融合和双模态注意力机制。它使用特定于模态的预训练编码器，并在共享编码器中对齐生成的特征，以弥补模态之间的粒度差距。

Result: MedM2T在MIMIC-IV和MIMIC-IV-ECG数据集上进行了评估，在90天心血管疾病预测、院内死亡率预测和ICU住院时长回归三项任务中，均优于最先进的多模态学习框架和现有时间序列模型。例如，在CVD预测中，AUROC达到0.947，AUPRC达到0.706。

Conclusion: MedM2T的强大性能和广泛适用性使其成为临床预测中有前景的工具。

Abstract: The inherent multimodality and heterogeneous temporal structures of medical
data pose significant challenges for modeling. We propose MedM2T, a time-aware
multimodal framework designed to address these complexities. MedM2T integrates:
(i) Sparse Time Series Encoder to flexibly handle irregular and sparse time
series, (ii) Hierarchical Time-Aware Fusion to capture both micro- and
macro-temporal patterns from multiple dense time series, such as ECGs, and
(iii) Bi-Modal Attention to extract cross-modal interactions, which can be
extended to any number of modalities. To mitigate granularity gaps between
modalities, MedM2T uses modality-specific pre-trained encoders and aligns
resulting features within a shared encoder. We evaluated MedM2T on MIMIC-IV and
MIMIC-IV-ECG datasets for three tasks that encompass chronic and acute disease
dynamics: 90-day cardiovascular disease (CVD) prediction, in-hospital mortality
prediction, and ICU length-of-stay (LOS) regression. MedM2T outperformed
state-of-the-art multimodal learning frameworks and existing time series
models, achieving an AUROC of 0.947 and an AUPRC of 0.706 for CVD prediction;
an AUROC of 0.901 and an AUPRC of 0.558 for mortality prediction; and Mean
Absolute Error (MAE) of 2.31 for LOS regression. These results highlight the
robustness and broad applicability of MedM2T, positioning it as a promising
tool in clinical prediction. We provide the implementation of MedM2T at
https://github.com/DHLab-TSENG/MedM2T.

</details>


### [85] [Reasoning Models Sometimes Output Illegible Chains of Thought](https://arxiv.org/abs/2510.27338)
*Arun Jose*

Main category: cs.LG

TL;DR: 研究发现，通过结果导向强化学习（RL）训练的语言模型在进行思维链（CoT）推理时，其推理过程（CoT）往往变得不可读，这使得监控模型意图和发现潜在恶意行为变得困难。


<details>
  <summary>Details</summary>
Motivation: CoT推理的语言模型在性能上表现出色，但为了有效监控模型的意图和潜在恶意行为，CoT必须清晰且忠实。

Method: 本文研究了14种推理模型的CoT可读性，并分析了RL对CoT可读性的影响以及可读性与性能之间的关系。

Result: RL常导致推理过程对人类和AI监控器都变得不可读，但最终答案却清晰可读。模型使用不可读的推理过程来获得正确答案（当模型被迫仅使用可读部分时，准确率下降了53%）。可读性与性能之间没有简单的相关性，且在较难问题上可读性会下降。

Conclusion: 如果不对可读性进行明确优化，结果导向的RL自然会使模型的推理过程变得越来越不透明，这可能削弱现有的监控方法。

Abstract: Language models trained via outcome-based reinforcement learning (RL) to
reason using chain-of-thought (CoT) have shown remarkable performance.
Monitoring such a model's CoT may allow us to understand its intentions and
detect potential malicious behavior. However, to be effective, this requires
that CoTs are legible and faithful. We study CoT legibility across 14 reasoning
models, finding that RL often causes reasoning to become illegible to both
humans and AI monitors, with reasoning models (except Claude) generating
illegible CoTs while returning to perfectly readable final answers. We show
that models use illegible reasoning to reach correct answers (accuracy dropping
by 53\% when forced to use only legible portions), yet find no correlation
between legibility and performance when resampling - suggesting the
relationship is more nuanced. We also find that legibility degrades on harder
questions. We discuss potential hypotheses for these results, including
steganography, training artifacts, and vestigial tokens. These results suggest
that without explicit optimization for legibility, outcome-based RL naturally
produces models with increasingly opaque reasoning processes, potentially
undermining monitoring approaches.

</details>


### [86] [Measuring Chain-of-Thought Monitorability Through Faithfulness and Verbosity](https://arxiv.org/abs/2510.27378)
*Austin Meek,Eitan Sprejer,Iván Arcuschin,Austin J. Brockmeier,Steven Basart*

Main category: cs.LG

TL;DR: 研究了CoT的忠实性和冗长性，并将其结合为“可监控性”指标，用于评估模型CoT作为外部工作记忆的能力。


<details>
  <summary>Details</summary>
Motivation: 探究如何通过链式思考（CoT）输出，更好地理解模型的逐步推理过程，旨在发现不安全或不一致的行为，并提高CoT对其内部推理的透明度（忠实性）。现有的忠实性衡量方法存在局限性，未能全面反映模型推理的完整性。

Method: 本文引入了“冗长性”概念，即CoT是否列出了解决任务所需的所有因素，并将其与忠实性相结合，提出了一个统一的“可监控性”评分。该评分旨在衡量CoT作为模型外部“工作记忆”的有效性。在BBH、GPQA和MMLU数据集上评估了指令精调和推理模型。

Result: 模型可能表面上看起来是忠实的，但如果遗漏了关键因素，仍然难以监控。不同模型家族的可监控性差异显著。

Conclusion: CoT的“可监控性”是一个重要的指标，它结合了忠实性和冗长性，提供了对模型内部推理过程更全面的理解。未来的研究应关注提高CoT的可监控性，以增强模型安全性和对齐性。

Abstract: Chain-of-thought (CoT) outputs let us read a model's step-by-step reasoning.
Since any long, serial reasoning process must pass through this textual trace,
the quality of the CoT is a direct window into what the model is thinking. This
visibility could help us spot unsafe or misaligned behavior (monitorability),
but only if the CoT is transparent about its internal reasoning (faithfulness).
Fully measuring faithfulness is difficult, so researchers often focus on
examining the CoT in cases where the model changes its answer after adding a
cue to the input. This proxy finds some instances of unfaithfulness but loses
information when the model maintains its answer, and does not investigate
aspects of reasoning not tied to the cue. We extend these results to a more
holistic sense of monitorability by introducing verbosity: whether the CoT
lists every factor needed to solve the task. We combine faithfulness and
verbosity into a single monitorability score that shows how well the CoT serves
as the model's external `working memory', a property that many safety schemes
based on CoT monitoring depend on. We evaluate instruction-tuned and reasoning
models on BBH, GPQA, and MMLU. Our results show that models can appear faithful
yet remain hard to monitor when they leave out key factors, and that
monitorability differs sharply across model families. We release our evaluation
code using the Inspect library to support reproducible future work.

</details>


### [87] [FedMuon: Accelerating Federated Learning with Matrix Orthogonalization](https://arxiv.org/abs/2510.27403)
*Junkang Liu,Fanhua Shang,Junchao Zhou,Hongying Liu,Yuanyuan Liu,Jin Liu*

Main category: cs.LG

TL;DR: 联邦学习（FL）中的核心瓶颈是通信轮次。现有的FL方法大多使用逐元素局部优化器，忽略了权重矩阵的几何结构，导致收敛缓慢。本文提出了FedMuon优化器，通过动量聚合和局部-全局对齐技术，解决了Muon优化器在非IID环境下的客户端漂移问题，显著减少了通信轮次并提高了测试准确性。


<details>
  <summary>Details</summary>
Motivation: 探索一种能减少联邦学习（FL）通信轮次的方法，解决现有局部优化器（如Adam/SGD）忽略权重矩阵几何结构导致的收敛慢问题，特别是在非IID数据环境下。

Method: 本文提出了FedMuon优化器，它结合了两个关键技术：1. 动量聚合：客户端使用聚合的动量进行局部初始化。2. 局部-全局对齐：局部梯度与全局更新方向对齐以显著减少客户端漂移。

Result: 在IID设置下，Local Muon比Local SGD和Local AdamW显著加速了FL的收敛并减少了通信轮次。在非IID设置下，FedMuon在理论上实现了线性加速收敛，并通过实验验证在语言和视觉模型上显著减少了通信轮次并提高了测试准确性。

Conclusion: FedMuon优化器通过动量聚合与局部-全局对齐技术，有效解决了联邦学习在IID和非IID环境下的收敛速度慢和通信轮次多问题，显著提升了模型性能。

Abstract: The core bottleneck of Federated Learning (FL) lies in the communication
rounds. That is, how to achieve more effective local updates is crucial for
reducing communication rounds. Existing FL methods still primarily use
element-wise local optimizers (Adam/SGD), neglecting the geometric structure of
the weight matrices. This often leads to the amplification of pathological
directions in the weights during local updates, leading deterioration in the
condition number and slow convergence. Therefore, we introduce the Muon
optimizer in local, which has matrix orthogonalization to optimize
matrix-structured parameters. Experimental results show that, in IID setting,
Local Muon significantly accelerates the convergence of FL and reduces
communication rounds compared to Local SGD and Local AdamW. However, in non-IID
setting, independent matrix orthogonalization based on the local distributions
of each client induces strong client drift. Applying Muon in non-IID FL poses
significant challenges: (1) client preconditioner leading to client drift; (2)
moment reinitialization. To address these challenges, we propose a novel
Federated Muon optimizer (FedMuon), which incorporates two key techniques: (1)
momentum aggregation, where clients use the aggregated momentum for local
initialization; (2) local-global alignment, where the local gradients are
aligned with the global update direction to significantly reduce client drift.
Theoretically, we prove that \texttt{FedMuon} achieves a linear speedup
convergence rate without the heterogeneity assumption, where $S$ is the number
of participating clients per round, $K$ is the number of local iterations, and
$R$ is the total number of communication rounds. Empirically, we validate the
effectiveness of FedMuon on language and vision models. Compared to several
baselines, FedMuon significantly reduces communication rounds and improves test
accuracy.

</details>


### [88] [Atlas-Alignment: Making Interpretability Transferable Across Language Models](https://arxiv.org/abs/2510.27413)
*Bruno Puri,Jim Berend,Sebastian Lapuschkin,Wojciech Samek*

Main category: cs.LG

TL;DR: Atlas-Alignment是一个通过将未知的潜在空间与概念图谱对齐，从而实现跨语言模型可解释性迁移的框架。


<details>
  <summary>Details</summary>
Motivation: 现有的可解释性方法成本高昂且难以扩展，通常需要为每个新模型进行耗时的训练和手动标注。

Method: 该框架通过共享输入和轻量级的表征对齐技术，将未知潜在空间与已标注、人类可解释的潜在空间（概念图谱）对齐。

Result: Atlas-Alignment实现了对不透明模型的语义特征搜索、检索和沿人类可解释概念进行指导性生成，并且无需标记概念数据就能实现鲁棒的语义检索和可控的生成。

Conclusion: Atlas-Alignment摊销了可解释人工智能和机械可解释性的成本，通过构建一个高质量的概念图谱，能够以最小的边际成本使许多新模型变得透明和可控。

Abstract: Interpretability is crucial for building safe, reliable, and controllable
language models, yet existing interpretability pipelines remain costly and
difficult to scale. Interpreting a new model typically requires costly training
of model-specific sparse autoencoders, manual or semi-automated labeling of SAE
components, and their subsequent validation. We introduce Atlas-Alignment, a
framework for transferring interpretability across language models by aligning
unknown latent spaces to a Concept Atlas - a labeled, human-interpretable
latent space - using only shared inputs and lightweight representational
alignment techniques. Once aligned, this enables two key capabilities in
previously opaque models: (1) semantic feature search and retrieval, and (2)
steering generation along human-interpretable atlas concepts. Through
quantitative and qualitative evaluations, we show that simple representational
alignment methods enable robust semantic retrieval and steerable generation
without the need for labeled concept data. Atlas-Alignment thus amortizes the
cost of explainable AI and mechanistic interpretability: by investing in one
high-quality Concept Atlas, we can make many new models transparent and
controllable at minimal marginal cost.

</details>


### [89] [MVeLMA: Multimodal Vegetation Loss Modeling Architecture for Predicting Post-fire Vegetation Loss](https://arxiv.org/abs/2510.27443)
*Meenu Ravi,Shailik Sarkar,Yanshen Sun,Vaishnavi Singh,Chang-Tien Lu*

Main category: cs.LG

TL;DR: 这篇论文提出了一个名为MVeLMA的新的端到端机器学习流程，用于预测火灾后的植被损失，MVeLMA使用多模态特征集成和基于堆叠集成的架构，通过概率建模来估计不确定性，预测县域范围内的植被损失。


<details>
  <summary>Details</summary>
Motivation: 了解野火后的植被损失对于制定有效的生态恢复策略至关重要，但由于捕捉不断演变的生态系统特征需要大量的时间和精力，因此通常具有挑战性。该领域最近的工作尚未完全探索所有影响因素、其模式以及它们之间的相互作用。此外，该领域的大多数研究都受到预测模型缺乏可解释性的限制，使其在实际应用中作用有限。

Method: 本文提出了一种新颖的端到端机器学习流程，称为MVeLMA（多模态植被损失建模架构），用于预测县域范围内的火灾事件后的植被损失。MVeLMA使用多模态特征集成流程和基于堆叠集成的架构来捕获不同的模态，同时通过概率建模纳入不确定性估计。

Result: 通过全面的实验，我们表明我们的模型在预测野火后植被损失方面优于几种最先进（SOTA）和基线模型。此外，我们生成植被损失置信度图，以识别高风险县，从而帮助有针对性的恢复工作。

Conclusion: 这项工作的发现 MVeLMA 有潜力为未来的灾害救援规划、生态政策制定和野生动物恢复管理提供信息。

Abstract: Understanding post-wildfire vegetation loss is critical for developing
effective ecological recovery strategies and is often challenging due to the
extended time and effort required to capture the evolving ecosystem features.
Recent works in this area have not fully explored all the contributing factors,
their modalities, and interactions with each other. Furthermore, most research
in this domain is limited by a lack of interpretability in predictive modeling,
making it less useful in real-world settings. In this work, we propose a novel
end-to-end ML pipeline called MVeLMA (\textbf{M}ultimodal \textbf{Ve}getation
\textbf{L}oss \textbf{M}odeling \textbf{A}rchitecture) to predict county-wise
vegetation loss from fire events. MVeLMA uses a multimodal feature integration
pipeline and a stacked ensemble-based architecture to capture different
modalities while also incorporating uncertainty estimation through
probabilistic modeling. Through comprehensive experiments, we show that our
model outperforms several state-of-the-art (SOTA) and baseline models in
predicting post-wildfire vegetation loss. Furthermore, we generate vegetation
loss confidence maps to identify high-risk counties, thereby helping targeted
recovery efforts. The findings of this work have the potential to inform future
disaster relief planning, ecological policy development, and wildlife recovery
management.

</details>


### [90] [Simplex-to-Euclidean Bijections for Categorical Flow Matching](https://arxiv.org/abs/2510.27480)
*Bernardo Williams,Victor M. Yeom-Song,Marcelo Hartmann,Arto Klami*

Main category: cs.LG

TL;DR: 该文章提出一种在单纯形上学习和采样概率分布的方法。


<details>
  <summary>Details</summary>
Motivation: 以往的方法在单纯形上建模时使用黎曼几何或自定义噪声过程，而本文方法在欧氏空间中操作，同时遵守Aitchison几何。

Method: 通过平滑双射将开放单纯形映射到欧几里得空间，利用Aitchison几何定义映射，并通过Dirichlet插值实现分类数据的建模，将离散观测去量化为连续观测。

Result: 与以前的方法相比，本文方法在合成数据集和真实数据集上均取得了有竞争力的性能。

Conclusion: 本文方法在欧氏空间中进行操作，同时尊重Aitchison几何，在性能上具有竞争力。

Abstract: We propose a method for learning and sampling from probability distributions
supported on the simplex. Our approach maps the open simplex to Euclidean space
via smooth bijections, leveraging the Aitchison geometry to define the
mappings, and supports modeling categorical data by a Dirichlet interpolation
that dequantizes discrete observations into continuous ones. This enables
density modeling in Euclidean space through the bijection while still allowing
exact recovery of the original discrete distribution. Compared to previous
methods that operate on the simplex using Riemannian geometry or custom noise
processes, our approach works in Euclidean space while respecting the Aitchison
geometry, and achieves competitive performance on both synthetic and real-world
data sets.

</details>


### [91] [FedAdamW: A Communication-Efficient Optimizer with Convergence and Generalization Guarantees for Federated Large Models](https://arxiv.org/abs/2510.27486)
*Junkang Liu,Fanhua Shang,Kewen Zhu,Hongying Liu,Yuanyuan Liu,Jin Liu*

Main category: cs.LG

TL;DR: FedAdamW是针对联邦学习中AdamW的不足提出的一种优化算法，通过局部校正机制和解耦权重衰减来解决数据异构、局部过拟合和收敛速度慢的问题，并在理论和实践中都表现出优越的性能。


<details>
  <summary>Details</summary>
Motivation: AdamW在联邦学习中直接应用存在三个主要挑战：数据异构导致二阶矩估计高方差；局部过拟合导致客户端漂移；每轮重新初始化矩估计会降低收敛速度。

Method: 本文提出了FedAdamW算法，通过局部校正机制和解耦权重衰减来使局部更新与全局更新对齐，以减轻局部过拟合。同时，FedAdamW有效地聚合了二阶矩估计的均值，以减少其方差并重新初始化它们。

Result: 理论上，FedAdamW在没有异构性假设的情况下，实现了线性的加速收敛速度，收敛率为$\mathcal{O}(\sqrt{(L \Delta \sigma_l^2)/(S K R \epsilon^2)}+(L \Delta)/R)$。实证上，FedAdamW在语言和视觉Transformer模型上表现出有效性，相比基线显著减少了通信轮次并提高了测试准确性。

Conclusion: FedAdamW算法有效解决了联邦学习中AdamW面临的挑战，通过创新的局部校正机制和解耦权重衰减，实现了更快的收敛速度和更高的准确性，尤其适用于训练和微调大型模型。

Abstract: AdamW has become one of the most effective optimizers for training
large-scale models. We have also observed its effectiveness in the context of
federated learning (FL). However, directly applying AdamW in federated learning
settings poses significant challenges: (1) due to data heterogeneity, AdamW
often yields high variance in the second-moment estimate $\boldsymbol{v}$; (2)
the local overfitting of AdamW may cause client drift; and (3) Reinitializing
moment estimates ($\boldsymbol{v}$, $\boldsymbol{m}$) at each round slows down
convergence. To address these challenges, we propose the first
\underline{Fed}erated \underline{AdamW} algorithm, called \texttt{FedAdamW},
for training and fine-tuning various large models. \texttt{FedAdamW} aligns
local updates with the global update using both a \textbf{local correction
mechanism} and decoupled weight decay to mitigate local overfitting.
\texttt{FedAdamW} efficiently aggregates the \texttt{mean} of the second-moment
estimates to reduce their variance and reinitialize them. Theoretically, we
prove that \texttt{FedAdamW} achieves a linear speedup convergence rate of
$\mathcal{O}(\sqrt{(L \Delta \sigma_l^2)/(S K R \epsilon^2)}+(L \Delta)/R)$
without \textbf{heterogeneity assumption}, where $S$ is the number of
participating clients per round, $K$ is the number of local iterations, and $R$
is the total number of communication rounds. We also employ PAC-Bayesian
generalization analysis to explain the effectiveness of decoupled weight decay
in local training. Empirically, we validate the effectiveness of
\texttt{FedAdamW} on language and vision Transformer models. Compared to
several baselines, \texttt{FedAdamW} significantly reduces communication rounds
and improves test accuracy. The code is available in
https://github.com/junkangLiu0/FedAdamW.

</details>


### [92] [DP-FedPGN: Finding Global Flat Minima for Differentially Private Federated Learning via Penalizing Gradient Norm](https://arxiv.org/abs/2510.27504)
*Junkang Liu,Yuxuan Tian,Fanhua Shang,Yuanyuan Liu,Hongying Liu,Junchao Zhou,Daorui Ding*

Main category: cs.LG

TL;DR: 本文提出了一种新的CL-DPFL算法DP-FedPGN，通过引入全局梯度范数惩罚来寻找全局平坦最小值，以解决现有方法在差分隐私保护后模型泛化能力下降的问题。


<details>
  <summary>Details</summary>
Motivation: 现有的CL-DPFL方法通常会导致损失函数面更尖锐，从而在差分隐私保护后降低模型泛化能力。虽然SAM等方法试图寻找局部平坦最小值，但在CL-DPFL中局部平坦性可能无法反映全局平坦性。

Method: 提出了一种新的CL-DPFL算法DP-FedPGN，该算法在局部损失中引入了全局梯度范数惩罚，以寻找全局平坦最小值。此外，通过此全局梯度范数惩罚，不仅找到了更平坦的全局最小值，还降低了局部更新范数，减少了梯度裁剪的误差。

Result: DP-FedPGN算法不仅能够找到更平坦的全局最小值，而且通过减少局部更新范数，进一步降低了梯度裁剪的误差。理论上分析了DP-FedPGN如何减轻差分隐私引起的性能下降。该算法消除了数据异质性的影响，实现了快速收敛。通过在ResNet和Transformer模型上进行有效性测试，在六项视觉和自然语言处理任务上取得了显著优于现有SOTA算法的改进。

Conclusion: DP-FedPGN通过引入全局梯度范数惩罚，有效解决了CL-DPFL中差分隐私保护导致的模型泛化能力下降问题，实现了更平坦的全局最小值和更小的梯度裁剪误差，并在多项任务上取得了显著优异的性能。

Abstract: To prevent inference attacks in Federated Learning (FL) and reduce the
leakage of sensitive information, Client-level Differentially Private Federated
Learning (CL-DPFL) is widely used. However, current CL-DPFL methods usually
result in sharper loss landscapes, which leads to a decrease in model
generalization after differential privacy protection. By using Sharpness Aware
Minimization (SAM), the current popular federated learning methods are to find
a local flat minimum value to alleviate this problem. However, the local
flatness may not reflect the global flatness in CL-DPFL. Therefore, to address
this issue and seek global flat minima of models, we propose a new CL-DPFL
algorithm, DP-FedPGN, in which we introduce a global gradient norm penalty to
the local loss to find the global flat minimum. Moreover, by using our global
gradient norm penalty, we not only find a flatter global minimum but also
reduce the locally updated norm, which means that we further reduce the error
of gradient clipping. From a theoretical perspective, we analyze how DP-FedPGN
mitigates the performance degradation caused by DP. Meanwhile, the proposed
DP-FedPGN algorithm eliminates the impact of data heterogeneity and achieves
fast convergence. We also use R\'enyi DP to provide strict privacy guarantees
and provide sensitivity analysis for local updates. Finally, we conduct
effectiveness tests on both ResNet and Transformer models, and achieve
significant improvements in six visual and natural language processing tasks
compared to existing state-of-the-art algorithms. The code is available at
https://github.com/junkangLiu0/DP-FedPGN

</details>


### [93] [Leveraging Generic Time Series Foundation Models for EEG Classification](https://arxiv.org/abs/2510.27522)
*Théo Gnassounou,Yessin Moakher,Shifeng Xie,Vasilii Feofanov,Ievgen Redko*

Main category: cs.LG

TL;DR: 该研究探讨了时间序列基础模型在脑电图（EEG）领域的应用潜力，发现这种模型在运动想象分类和睡眠阶段预测等任务上表现出色，甚至优于专门的脑电图模型。


<details>
  <summary>Details</summary>
Motivation: 探索时间序列基础模型在生物医学信号（尤其是脑电图）领域的应用潜力，因为目前对其在这方面的研究还很少。

Method: 1. 采用了一种新的时间序列分类基础模型。2. 测试了两种预训练方案：a) 在多领域异构真实时间序列数据上进行预训练。b) 在纯合成数据上进行预训练。3. 将模型的性能与EEGNet（广泛使用的卷积基线）和CBraMod（最新的脑电图专用基础模型）进行了比较。

Result: 1. 两种预训练方案的时间序列基础模型都表现出强大的性能。2. 它们在性能上持续优于EEGNet和CBraMod。3. 结果表明，通用时间序列基础模型，即使是在非神经源数据或合成信号上进行预训练，也能有效地迁移到脑电图任务。

Conclusion: 通用时间序列基础模型即使在非脑电图数据或合成数据上预训练，也能有效地应用于脑电图分析，并在该领域展现出巨大的潜力，这表明脑电图领域可以从更广泛的时间序列研究进展中获益。

Abstract: Foundation models for time series are emerging as powerful general-purpose
backbones, yet their potential for domain-specific biomedical signals such as
electroencephalography (EEG) remains rather unexplored. In this work, we
investigate the applicability a recently proposed time series classification
foundation model, to a different EEG tasks such as motor imagery classification
and sleep stage prediction. We test two pretraining regimes: (a) pretraining on
heterogeneous real-world time series from multiple domains, and (b) pretraining
on purely synthetic data. We find that both variants yield strong performance,
consistently outperforming EEGNet, a widely used convolutional baseline, and
CBraMod, the most recent EEG-specific foundation model. These results suggest
that generalist time series foundation models, even when pretrained on data of
non-neural origin or on synthetic signals, can transfer effectively to EEG. Our
findings highlight the promise of leveraging cross-domain pretrained models for
brain signal analysis, suggesting that EEG may benefit from advances in the
broader time series literature.

</details>


### [94] [Active transfer learning for structural health monitoring](https://arxiv.org/abs/2510.27525)
*J. Poole,N. Dervilis,K. Worden,P. Gardner,V. Giglioni,R. S. Mills,A. J. Hughes*

Main category: cs.LG

TL;DR: 本文提出了一种用于群体结构健康监测（PBSHM）的贝叶斯迁移学习框架，它结合了主动学习策略，可以在标签稀缺的情况下提高数据效率，从而降低运营成本。


<details>
  <summary>Details</summary>
Motivation: 结构健康监测（SHM）系统的数据获取成本高昂且不切实际，尤其是带标签的数据。群体SHM（PBSHM）旨在通过利用来自多个结构的数据来解决这个限制。然而，不同结构的数据分布不同，可能导致传统机器学习方法的泛化误差大。

Method: 本文提出了一种用于PBSHM的贝叶斯框架，该框架利用迁移学习（域适应）来对齐数据分布。它可以在线框架中更新，并使用有限的带标签目标数据改进无监督域适应映射。此外，该模型被整合到主动采样策略中，以指导检查，选择信息量最大的观测数据进行标记，从而进一步减少学习目标分类器所需的带标签数据。

Result: 在实验桥梁群体上评估了该方法的有效性，该群体包括对应于几种损伤状态以及一系列环境条件的数据。结果表明，在标签稀缺的情况下，结合迁移学习和主动学习可以提高分类模型的学习数据效率。

Conclusion: 将迁移学习和主动学习相结合可以提高数据效率，从而减少结构在运行寿命期内的检查次数，降低运营成本，对数据驱动的结构运营和维护具有重要意义。

Abstract: Data for training structural health monitoring (SHM) systems are often
expensive and/or impractical to obtain, particularly for labelled data.
Population-based SHM (PBSHM) aims to address this limitation by leveraging data
from multiple structures. However, data from different structures will follow
distinct distributions, potentially leading to large generalisation errors for
models learnt via conventional machine learning methods. To address this issue,
transfer learning -- in the form of domain adaptation (DA) -- can be used to
align the data distributions. Most previous approaches have only considered
\emph{unsupervised} DA, where no labelled target data are available; they do
not consider how to incorporate these technologies in an online framework --
updating as labels are obtained throughout the monitoring campaign. This paper
proposes a Bayesian framework for DA in PBSHM, that can improve unsupervised DA
mappings using a limited quantity of labelled target data. In addition, this
model is integrated into an active sampling strategy to guide inspections to
select the most informative observations to label -- leading to further
reductions in the required labelled data to learn a target classifier. The
effectiveness of this methodology is evaluated on a population of experimental
bridges. Specifically, this population includes data corresponding to several
damage states, as well as, a comprehensive set of environmental conditions. It
is found that combining transfer learning and active learning can improve data
efficiency when learning classification models in label-scarce scenarios. This
result has implications for data-informed operation and maintenance of
structures, suggesting a reduction in inspections over the operational lifetime
of a structure -- and therefore a reduction in operational costs -- can be
achieved.

</details>


### [95] [TetraJet-v2: Accurate NVFP4 Training for Large Language Models with Oscillation Suppression and Outlier Control](https://arxiv.org/abs/2510.27527)
*Yuxiang Chen,Xiaoming Xu,Pengle Zhang,Michael Beyer,Martin Rapp,Jun Zhu,Jianfei Chen*

Main category: cs.LG

TL;DR: TetraJet-v2是一种端到端的4位全量化训练（FQT）方法，利用NVFP4在所有线性层中对激活、权重和梯度进行处理，旨在解决LLM训练中的资源消耗问题。


<details>
  <summary>Details</summary>
Motivation: 解决大型语言模型（LLMs）训练成本高昂的问题，并对低精度全量化训练（FQT）的兴趣日益增长。

Method: TetraJet-v2方法：1. 提出了一种针对NVFP4线性层的无偏双块量化方法。2. 提出了OsciReset算法来抑制权重振荡。3. 提出了OutControl算法来保持离群值的准确性。

Result: TetraJet-v2在不同模型尺寸（最大3.7亿）和数据尺寸（最大2000亿token）的LLM预训练中，始终优于之前的FP4训练方法，并使性能差距平均减少了51.3%。

Conclusion: TetraJet-v2通过解决权重振荡和离群值等关键问题，显著提高了低精度LLM训练的效率和准确性，从而降低了LLM训练的成本。

Abstract: Large Language Models (LLMs) training is prohibitively expensive, driving
interest in low-precision fully-quantized training (FQT). While novel 4-bit
formats like NVFP4 offer substantial efficiency gains, achieving near-lossless
training at such low precision remains challenging. We introduce TetraJet-v2,
an end-to-end 4-bit FQT method that leverages NVFP4 for activations, weights,
and gradients in all linear layers. We identify two critical issues hindering
low-precision LLM training: weight oscillation and outliers. To address these,
we propose: 1) an unbiased double-block quantization method for NVFP4 linear
layers, 2) OsciReset, an algorithm to suppress weight oscillation, and 3)
OutControl, an algorithm to retain outlier accuracy. TetraJet-v2 consistently
outperforms prior FP4 training methods on pre-training LLMs across varying
model sizes up to 370M and data sizes up to 200B tokens, reducing the
performance gap to full-precision training by an average of 51.3%.

</details>


### [96] [AstuteRAG-FQA: Task-Aware Retrieval-Augmented Generation Framework for Proprietary Data Challenges in Financial Question Answering](https://arxiv.org/abs/2510.27537)
*Mohammad Zahangir Alam,Khandoker Ashik Uz Zaman,Mahdi H. Miraz*

Main category: cs.LG

TL;DR: AstuteRAG-FQA是一个为金融问答（FQA）量身定制的自适应检索增强生成（RAG）框架，旨在解决金融领域RAG应用的挑战，通过混合检索策略、动态提示框架、四层任务分类、多层安全机制和实时合规性监控来提高准确性、相关性和安全性。


<details>
  <summary>Details</summary>
Motivation: 尽管RAG在知识密集型任务中表现出色，但将其应用于金融领域面临诸多挑战，包括专有数据集访问受限、检索准确性有限、法规限制和敏感数据解释等。

Method: AstuteRAG-FQA框架采用混合检索策略，整合开源和专有金融数据；使用动态提示框架，根据查询复杂性实时调整；提出四层任务分类（显性事实、隐性事实、可解释理由、隐性因果推理）；并实现了多层安全机制（差分隐私、数据匿名化、基于角色的访问控制）和实时合规性监控。此外，还评估了三种数据集成技术：上下文嵌入、小型模型增强和有针对性的微调。

Result: AstuteRAG-FQA通过其混合检索策略、动态提示框架和四层任务分类，显著提高了金融问答的精度和上下文相关性。通过多层安全机制和实时合规性监控，在处理敏感金融信息和满足法规要求方面表现出色。三种数据集成技术也在不同的金融环境中展现了效率和可行性。

Conclusion: AstuteRAG-FQA框架成功解决了RAG在金融领域应用的挑战，通过其创新的设计和全面的安全合规措施，为金融领域的知识密集型任务提供了一个强大而安全的解决方案。

Abstract: Retrieval-Augmented Generation (RAG) shows significant promise in
knowledge-intensive tasks by improving domain specificity, enhancing temporal
relevance, and reducing hallucinations. However, applying RAG to finance
encounters critical challenges: restricted access to proprietary datasets,
limited retrieval accuracy, regulatory constraints, and sensitive data
interpretation. We introduce AstuteRAG-FQA, an adaptive RAG framework tailored
for Financial Question Answering (FQA), leveraging task-aware prompt
engineering to address these challenges. The framework uses a hybrid retrieval
strategy integrating both open-source and proprietary financial data while
maintaining strict security protocols and regulatory compliance. A dynamic
prompt framework adapts in real time to query complexity, improving precision
and contextual relevance. To systematically address diverse financial queries,
we propose a four-tier task classification: explicit factual, implicit factual,
interpretable rationale, and hidden rationale involving implicit causal
reasoning. For each category, we identify key challenges, datasets, and
optimization techniques within the retrieval and generation process. The
framework incorporates multi-layered security mechanisms including differential
privacy, data anonymization, and role-based access controls to protect
sensitive financial information. Additionally, AstuteRAG-FQA implements
real-time compliance monitoring through automated regulatory validation systems
that verify responses against industry standards and legal obligations. We
evaluate three data integration techniques - contextual embedding, small model
augmentation, and targeted fine-tuning - analyzing their efficiency and
feasibility across varied financial environments.

</details>


### [97] [ORGEval: Graph-Theoretic Evaluation of LLMs in Optimization Modeling](https://arxiv.org/abs/2510.27610)
*Zhuohan Wang,Ziwei Zhu,Ziniu Li,Congliang Chen,Yizhou Han,Yufeng Lin,Zhihang Lin,Angyang Gu,Xinglin Hu,Ruoyu Sun,Tian Ding*

Main category: cs.LG

TL;DR: 该论文提出了ORGEval，一个以图论为基础的评估框架，用于评估大型语言模型在制定线性及混合整数线性规划方面的能力。ORGEval将优化模型表示为图，并将等效性检测简化为图同构测试。通过整合Weisfeiler-Lehman测试的定制变体与对称可分解图检测算法，ORGEval能够稳健地评估模型等效性，且在实验中表现出比基于求解器的方法更高的准确性和效率。


<details>
  <summary>Details</summary>
Motivation: 大型语言模型（LLMs）在自动化工业应用优化问题的制定方面显示出潜力，但由于缺乏可靠的评估指标，其性能评估面临挑战。现有的基于求解器的方法存在不一致性、不可行性问题和高计算成本。

Method: ORGEval框架将优化模型表示为图，并将模型等效性检测转化为图同构测试。该方法利用Weisfeiler-Lehman (WL) 测试的定制变体，并结合对称可分解 (SD) 检测算法来评估模型等效性。作者识别并证明了一个充分条件，当被测试的图是对称可分解时，WL测试能够正确检测同构。此外，ORGEval通过关注结构等效性而非实例级配置，提高了对数值变化的鲁棒性，并构建了Bench4Opt数据集来基准测试LLMs在优化建模方面的表现。

Result: ORGEval能够在随机参数配置下成功检测模型等效性并产生100%一致的结果，同时在运行时间上显著优于基于求解器的方法，尤其是在处理难题时。在使用ORGEval构建的Bench4Opt数据集上，实验结果表明，尽管优化建模对所有LLMs来说仍具挑战性，但DeepSeek-V3和Claude-Opus-4在直接提示下取得了最高的准确率，甚至超越了领先的推理模型。

Conclusion: ORGEval为评估LLMs在优化建模方面的能力提供了一个鲁棒且高效的框架。通过将优化模型转化为图并利用图同构检测技术，它解决了现有评估方法的局限性。研究结果强调了DeepSeek-V3和Claude-Opus-4在优化建模任务上的潜力，同时也指出了LLMs在该领域仍面临的挑战，为未来的研究和发展提供了方向。

Abstract: Formulating optimization problems for industrial applications demands
significant manual effort and domain expertise. While Large Language Models
(LLMs) show promise in automating this process, evaluating their performance
remains difficult due to the absence of robust metrics. Existing solver-based
approaches often face inconsistency, infeasibility issues, and high
computational costs. To address these issues, we propose ORGEval, a
graph-theoretic evaluation framework for assessing LLMs' capabilities in
formulating linear and mixed-integer linear programs. ORGEval represents
optimization models as graphs, reducing equivalence detection to graph
isomorphism testing. We identify and prove a sufficient condition, when the
tested graphs are symmetric decomposable (SD), under which the
Weisfeiler-Lehman (WL) test is guaranteed to correctly detect isomorphism.
Building on this, ORGEval integrates a tailored variant of the WL-test with an
SD detection algorithm to evaluate model equivalence. By focusing on structural
equivalence rather than instance-level configurations, ORGEval is robust to
numerical variations. Experimental results show that our method can
successfully detect model equivalence and produce 100\% consistent results
across random parameter configurations, while significantly outperforming
solver-based methods in runtime, especially on difficult problems. Leveraging
ORGEval, we construct the Bench4Opt dataset and benchmark state-of-the-art LLMs
on optimization modeling. Our results reveal that although optimization
modeling remains challenging for all LLMs, DeepSeek-V3 and Claude-Opus-4
achieve the highest accuracies under direct prompting, outperforming even
leading reasoning models.

</details>


### [98] [Panprediction: Optimal Predictions for Any Downstream Task and Loss](https://arxiv.org/abs/2510.27638)
*Sivaraman Balakrishnan,Nika Haghtalab,Daniel Hsu,Brian Lee,Eric Zhao*

Main category: cs.LG

TL;DR: 这篇论文介绍了一种名为“panprediction”的新型学习范式，它旨在通过从数据中提取足够信息，使模型能够适应多个损失函数和多个下游任务。


<details>
  <summary>Details</summary>
Motivation: 传统的监督学习范式关注在固定损失函数和固定数据分布下训练模型，而本文提出一个新的范式，即“panprediction”，它旨在让模型能够适应后续许多损失函数和许多下游任务。

Method: 本文提出了一种新的学习范式——panprediction，并研究了其统计复杂性。文中设计了可以学习确定性和随机性panpredictors的算法，分别需要$\tilde{O}(1/\varepsilon^3)$和$\tilde{O}(1/\varepsilon^2)$个样本。文中还介绍了一种从panprediction到统计高效校准（称为步长校准）的近似无损归约。

Result: 本文结果表明，在温和的假设下，同时最小化无限多个损失函数和无限多个任务在统计学上可以像最小化一个损失函数和一个任务一样容易。该方法将确定性全预测的最佳样本复杂度保证提高了$\varepsilon$因子，并且与全预测和多组学习的所有其他已知样本复杂度保证相匹配。

Conclusion: Panprediction是一个强大的新范式，它有可能在各种现实世界应用中实现更通用和适应性更强的机器学习模型。该论文提出的算法和理论结果为进一步研究该领域奠定了基础。

Abstract: Supervised learning is classically formulated as training a model to minimize
a fixed loss function over a fixed distribution, or task. However, an emerging
paradigm instead views model training as extracting enough information from
data so that the model can be used to minimize many losses on many downstream
tasks. We formalize a mathematical framework for this paradigm, which we call
panprediction, and study its statistical complexity. Formally, panprediction
generalizes omniprediction and sits upstream from multi-group learning, which
respectively focus on predictions that generalize to many downstream losses or
many downstream tasks, but not both. Concretely, we design algorithms that
learn deterministic and randomized panpredictors with
$\tilde{O}(1/\varepsilon^3)$ and $\tilde{O}(1/\varepsilon^2)$ samples,
respectively. Our results demonstrate that under mild assumptions,
simultaneously minimizing infinitely many losses on infinitely many tasks can
be as statistically easy as minimizing one loss on one task. Along the way, we
improve the best known sample complexity guarantee of deterministic
omniprediction by a factor of $1/\varepsilon$, and match all other known sample
complexity guarantees of omniprediction and multi-group learning. Our key
technical ingredient is a nearly lossless reduction from panprediction to a
statistically efficient notion of calibration, called step calibration.

</details>


### [99] [Imbalanced Classification through the Lens of Spurious Correlations](https://arxiv.org/abs/2510.27650)
*Jakob Hackstein,Sidney Bender*

Main category: cs.LG

TL;DR: 这篇论文提出了一种新的视角来解决机器学习中不平衡数据带来的挑战。作者认为不平衡数据会放大“Clever Hans”效应，并提出使用可解释AI来识别和消除这种效应。


<details>
  <summary>Details</summary>
Motivation: 以往的方法大多集中在数据或损失的再平衡上，但本文作者认为不平衡是会放大“Clever Hans”（CH）效应的数据条件，尤其是在少数类别的欠规范化情况下。

Method: 本文提出了一种基于反事实解释的方法，利用可解释AI工具来识别并消除在数据不平衡情况下出现的“Clever Hans”效应。

Result: 该方法在三个数据集上实现了有竞争力的分类性能，并展示了“Clever Hans”效应在不平衡数据下是如何产生的。

Conclusion: 本文提出了一种利用可解释AI识别和消除不平衡数据中“Clever Hans”效应的新方法，为处理类不平衡问题提供了一个新颖且之前被忽视的视角。

Abstract: Class imbalance poses a fundamental challenge in machine learning, frequently
leading to unreliable classification performance. While prior methods focus on
data- or loss-reweighting schemes, we view imbalance as a data condition that
amplifies Clever Hans (CH) effects by underspecification of minority classes.
In a counterfactual explanations-based approach, we propose to leverage
Explainable AI to jointly identify and eliminate CH effects emerging under
imbalance. Our method achieves competitive classification performance on three
datasets and demonstrates how CH effects emerge under imbalance, a perspective
largely overlooked by existing approaches.

</details>


<div id='cs.GT'></div>

# cs.GT [[Back]](#toc)

### [100] [Algorithmic Predation: Equilibrium Analysis in Dynamic Oligopolies with Smooth Market Sharing](https://arxiv.org/abs/2510.27008)
*Fabian Raoul Pieroth,Ole Petersen,Martin Bichler*

Main category: cs.GT

TL;DR: 本文利用深度强化学习计算并验证了有限 horizon 动态寡头垄断博弈中的均衡，研究发现掠夺性定价可以在不对称成本结构的均衡中出现。


<details>
  <summary>Details</summary>
Motivation: 尽管有限期动态模型早已被提出来捕捉寡头垄断者之间的战略性跨期激励，但在允许企业退出（亏损后退出）的环境中，均衡策略的存在和形式仍然是一个悬而未决的问题。

Method: 我们利用深度强化学习的最新进展来计算和验证有限 horizon 动态寡头博弈中的均衡。

Result: 1. 最先进的深度强化学习算法在完全信息和不完全信息寡头模型中都能可靠地收敛到均衡。2. 当企业面临不对称的成本结构时，由此产生的均衡表现出掠夺性定价行为。

Conclusion: 掠夺性定价可以作为一种理性的均衡策略出现在各种模型设置中。我们的研究回答了一个存在十年之久的问题，并为竞争主管部门和监管机构提供了新的见解。

Abstract: Predatory pricing -- where a firm strategically lowers prices to undermine
competitors -- is a contentious topic in dynamic oligopoly theory, with
scholars debating practical relevance and the existence of predatory
equilibria. Although finite-horizon dynamic models have long been proposed to
capture the strategic intertemporal incentives of oligopolists, the existence
and form of equilibrium strategies in settings that allow for firm exit
(drop-outs following loss-making periods) have remained an open question. We
focus on the seminal dynamic oligopoly model by Selten (1965) that introduces
the subgame perfect equilibrium and analyzes smooth market sharing. Equilibrium
can be derived analytically in models that do not allow for dropouts, but not
in models that can lead to predatory pricing. In this paper, we leverage recent
advances in deep reinforcement learning to compute and verify equilibria in
finite-horizon dynamic oligopoly games. Our experiments reveal two key
findings: first, state-of-the-art deep reinforcement learning algorithms
reliably converge to equilibrium in both perfect- and imperfect-information
oligopoly models; second, when firms face asymmetric cost structures, the
resulting equilibria exhibit predatory pricing behavior. These results
demonstrate that predatory pricing can emerge as a rational equilibrium
strategy across a broad variety of model settings. By providing equilibrium
analysis of finite-horizon dynamic oligopoly models with drop-outs, our study
answers a decade-old question and offers new insights for competition
authorities and regulators.

</details>


<div id='cs.SI'></div>

# cs.SI [[Back]](#toc)

### [101] [Are Online Sports Fan Communities Becoming More Offensive? A Quantitative Review of Topics, Trends, and Toxicity of r/PremierLeague](https://arxiv.org/abs/2510.27003)
*Muhammad Zeeshan Mazhar,Tolga Buz,Yiran Su*

Main category: cs.SI

TL;DR: 该研究分析了Reddit上r/PremierLeague社区在2013-2022年间的110万余条评论，旨在了解在线体育迷社区的趋势，包括情感、话题和言语攻击性。


<details>
  <summary>Details</summary>
Motivation: 尽管英超联赛在全球，尤其是在美国市场日益普及，但对其在线粉丝社区的了解仍然存在显著空白。本研究旨在填补这一空白，分析reddit上的r/PremierLeague社区，来探究其讨论趋势、用户情感和社区中存在的言语攻击性。

Method: 情感分析、主题建模和毒性评估

Result: r/PremierLeague社区讨论的快速增长带来了更多样化的话题，但负面情绪和言语攻击性也随之增加。该社区也成为了用户表达对种族主义、COVID-19大流行和政治紧张等社会问题的沮丧的平台。

Conclusion: 本研究揭示了在线体育社区的动态，展示了其在促进讨论和反映社会问题方面的潜力，同时也强调了管理负面情绪和攻击性言论的挑战。

Abstract: Online communities for sports fans have surged in popularity, with Reddit's
r/PremierLeague emerging as a focal point for fans of one of the globe's most
celebrated sports leagues. This boom has helped the Premier League make
significant inroads into the US market, increasing viewership and sparking
greater interest in its matches. Despite the league's broad appeal, there's
still a notable gap in understanding its online fan community. Therefore, we
analyzed a substantial dataset of over 1.1 million comments posted from
2013-2022 on r/PremierLeague. Our study delves into the sentiment, topics, and
toxicity of these discussions, tracking trends over time, aiming to map out the
conversation landscape. The rapid expansion has brought more diverse
discussions, but also a worrying rise in negative sentiment and toxicity.
Additionally, the subreddit has become a venue for users to voice frustrations
about broader societal issues like racism, the COVID-19 pandemic, and political
tensions.

</details>


### [102] [Back to the Communities: A Mixed-Methods and Community-Driven Evaluation of Cultural Sensitivity in Text-to-Image Models](https://arxiv.org/abs/2510.27361)
*Sarah Kiden,Oriane Peter,Gisela Reyes-Cruz,Maira Klyshbekova,Sena Choi,Aislinn Gomez Bergin,Maria Waheed,Damian Eke,Tayyaba Azim,Sarvapali Ramchurn,Sebastian Stein,Elvira Perez Vallejos,Kate Devlin,Joel E Fischer*

Main category: cs.SI

TL;DR: 本文提出了一种社区驱动的混合方法评估方法，用于评估文本到图像（T2I）模型的文化敏感性。


<details>
  <summary>Details</summary>
Motivation: 证明文本到图像（T2I）模型不成比例地反映了西方文化规范，放大了对少数群体的错误表述和危害。然而，评估文化敏感性本身就很复杂，因为它具有流动性和多面性。

Method: 本文利用最先进的审查和协同创作研讨会，共有来自19个不同国家的59个人参与。我们开发并验证了一种基于社区的混合方法评估方法，以评估T2I模型中的文化敏感性，该方法采用了第一人称方法。

Result: 量化分数和定性调查揭示了社区内部和社区之间的趋同和分歧，阐明了错误表述的下游后果，并追溯了由不平等的权力关系塑造的训练数据如何扭曲描绘。

Conclusion: 本文为利益相关者提供了可操作的建议，强调了调查T2I模型中文化（错误）表征的来源、机制和影响的途径。

Abstract: Evidence shows that text-to-image (T2I) models disproportionately reflect
Western cultural norms, amplifying misrepresentation and harms to minority
groups. However, evaluating cultural sensitivity is inherently complex due to
its fluid and multifaceted nature. This paper draws on a state-of-the-art
review and co-creation workshops involving 59 individuals from 19 different
countries. We developed and validated a mixed-methods community-based
evaluation methodology to assess cultural sensitivity in T2I models, which
embraces first-person methods. Quantitative scores and qualitative inquiries
expose convergence and disagreement within and across communities, illuminate
the downstream consequences of misrepresentation, and trace how training data
shaped by unequal power relations distort depictions. Extensive assessments are
constrained by high resource requirements and the dynamic nature of culture, a
tension we alleviate through a context-based and iterative methodology. The
paper provides actionable recommendations for stakeholders, highlighting
pathways to investigate the sources, mechanisms, and impacts of cultural
(mis)representation in T2I models.

</details>


### [103] [Beyond Demographics: Behavioural Segmentation and Spatial Analytics to Enhance Visitor Experience at The British Museum](https://arxiv.org/abs/2510.27542)
*Naomi Muggleton,Timothy Monteath,Taha Yasseri*

Main category: cs.SI

TL;DR: 该研究利用数据科学方法，通过分析大英博物馆的语音导览日志和TripAdvisor评论，揭示了游客行为模式，识别了满意度驱动因素，并提出了改善游客体验的策略。


<details>
  <summary>Details</summary>
Motivation: 探索大英博物馆的游客行为，以期通过数据驱动的方法改进游客体验和博物馆规划。

Method: 分析了42,000条游客旅程和超过50,000条TripAdvisor评论，采用了数据科学方法，包括行为聚类、旅游使用分析、空间流建模等。

Result: 识别出四种游客类型：忠诚徒步者、悠闲探险者、目标明确的访客和快速采样者。语音导览使用存在高流失率和不同语言群体的完成率差异。空间导航主要受可达性和邻近性（尤其是楼梯厌恶）影响，而非主题组织。房间受欢迎程度与物理可达性而非策展内容更相关。

Conclusion: 该研究提出了改善游客参与度和流线型规划的实用策略，并为一个以游客为中心、数据驱动的博物馆规划提供了可扩展的框架。

Abstract: This study explores visitor behaviour at The British Museum using data
science methods applied to novel sources, including audio guide usage logs and
TripAdvisor reviews. Analysing 42,000 visitor journeys and over 50,000 reviews,
we identify key drivers of satisfaction, segment visitors by behavioural
patterns, examine tour engagement, model spatial navigation, and investigate
room popularity. Behavioural clustering uncovered four distinct visitor types:
Committed Trekkers, Leisurely Explorers, Targeted Visitors, and Speedy
Samplers, each characterised by different levels of engagement and movement.
Tour usage analysis revealed high drop-off rates and variation in completion
rates across different language groups. Spatial flow modelling revealed that
accessibility and proximity, particularly aversion to stairs, shaped visitor
paths more than thematic organisation. Room popularity was more strongly
predicted by physical accessibility than curatorial content. We propose
practical strategies for improving engagement and flow, offering a scalable
framework for visitor-centred, data-informed museum planning.

</details>


### [104] [Community Detection on Model Explanation Graphs for Explainable AI](https://arxiv.org/abs/2510.27655)
*Ehsan Moradi*

Main category: cs.SI

TL;DR: MoI(Modules of Influence)是一个新的模型解释框架，它克服了现有特征归因方法（如SHAP，LIME）无法识别高阶结构（即协同作用的特征集）的局限性。


<details>
  <summary>Details</summary>
Motivation: 现有的特征归因方法（如SHAP，LIME）在解释个体预测时忽略了特征之间的高阶结构，即它们协同作用的方式，这限制了模型可解释性。

Method: MoI通过以下步骤实现：1. 从每个实例的归因中构建模型解释图。2. 应用社区检测来找到共同影响预测的特征模块。3. 量化这些模块与偏见、冗余和因果关系模式之间的关系。

Result: MoI在合成数据集和真实数据集上都成功地揭示了相关的特征组，通过模块级别的消融改进了模型调试，并将偏差暴露定位到特定的模块。

Conclusion: MoI框架通过识别和分析特征模块，显著提升了模型解释的深度和实用性，并为XAI中的模块发现提供了一套评估工具，有助于更好地理解和调试AI模型。

Abstract: Feature-attribution methods (e.g., SHAP, LIME) explain individual predictions
but often miss higher-order structure: sets of features that act in concert. We
propose Modules of Influence (MoI), a framework that (i) constructs a model
explanation graph from per-instance attributions, (ii) applies community
detection to find feature modules that jointly affect predictions, and (iii)
quantifies how these modules relate to bias, redundancy, and causality
patterns. Across synthetic and real datasets, MoI uncovers correlated feature
groups, improves model debugging via module-level ablations, and localizes bias
exposure to specific modules. We release stability and synergy metrics, a
reference implementation, and evaluation protocols to benchmark module
discovery in XAI.

</details>


<div id='cs.AI'></div>

# cs.AI [[Back]](#toc)

### [105] [Inverse Knowledge Search over Verifiable Reasoning: Synthesizing a Scientific Encyclopedia from a Long Chains-of-Thought Knowledge Base](https://arxiv.org/abs/2510.26854)
*Yu Li,Yuan Huang,Tao Wang,Caiyu Fan,Xiansheng Cai,Sihan Hu,Xinzijian Liu,Cheng Shi,Mingjun Xu,Zhen Wang,Yan Wang,Xiangqi Jin,Tianhan Zhang,Linfeng Zhang,Lei Wang,Youjin Deng,Pan Zhang,Weijie Sun,Xingyu Li,Weinan E,Linfeng Zhang,Zhiyuan Yao,Kun Chen*

Main category: cs.AI

TL;DR: 该文章介绍了一种名为SciencePedia的框架，它通过解压缩科学推理，构建可验证的Long Chain-of-Thought（LCoT）知识库，并将其投射到新兴的百科全书SciencePedia中，以实现大规模可信的跨领域科学综合。


<details>
  <summary>Details</summary>
Motivation: 目前的科学材料通常省略推理过程，导致难以验证和阻碍跨领域知识的建立。

Method: 文章提出了一种可扩展的框架，通过苏格拉底式智能体生成约300万个第一性原理问题。多个独立的求解器模型生成LCoT，并通过提示净化和跨模型答案共识进行严格筛选，只保留具有可验证端点的LCoT。这些经过验证的语料库为Brainstorm搜索引擎提供了支持，该引擎执行逆向知识搜索，检索最终形成目标概念的各种第一性原理推导。Plato合成器将这些经过验证的链条叙述成连贯的文章。

Result: 初步的SciencePedia包含约20万个细粒度条目，涵盖数学、物理、化学、生物学、工程学和计算等领域。在六个学科的评估中，Plato合成的文章（以检索到的LCoT为条件）比没有检索的基线文章表现出更高的知识点密度和更低的事实错误率。

Conclusion: 这种以推理为中心的方法，建立在可验证的LCoT知识库之上，能够实现大规模可信的跨领域科学综合，并为不断扩展的百科全书奠定了基础。

Abstract: Most scientific materials compress reasoning, presenting conclusions while
omitting the derivational chains that justify them. This compression hinders
verification by lacking explicit, step-wise justifications and inhibits
cross-domain links by collapsing the very pathways that establish the logical
and causal connections between concepts. We introduce a scalable framework that
decompresses scientific reasoning, constructing a verifiable Long
Chain-of-Thought (LCoT) knowledge base and projecting it into an emergent
encyclopedia, SciencePedia. Our pipeline operationalizes an endpoint-driven,
reductionist strategy: a Socratic agent, guided by a curriculum of around 200
courses, generates approximately 3 million first-principles questions. To
ensure high fidelity, multiple independent solver models generate LCoTs, which
are then rigorously filtered by prompt sanitization and cross-model answer
consensus, retaining only those with verifiable endpoints. This verified corpus
powers the Brainstorm Search Engine, which performs inverse knowledge search --
retrieving diverse, first-principles derivations that culminate in a target
concept. This engine, in turn, feeds the Plato synthesizer, which narrates
these verified chains into coherent articles. The initial SciencePedia
comprises approximately 200,000 fine-grained entries spanning mathematics,
physics, chemistry, biology, engineering, and computation. In evaluations
across six disciplines, Plato-synthesized articles (conditioned on retrieved
LCoTs) exhibit substantially higher knowledge-point density and significantly
lower factual error rates than an equally-prompted baseline without retrieval
(as judged by an external LLM). Built on this verifiable LCoT knowledge base,
this reasoning-centric approach enables trustworthy, cross-domain scientific
synthesis at scale and establishes the foundation for an ever-expanding
encyclopedia.

</details>


### [106] [SUSTAINABLE Platform: Seamless Smart Farming Integration Towards Agronomy Automation](https://arxiv.org/abs/2510.26989)
*Agorakis Bompotas,Konstantinos Koutras,Nikitas Rigas Kalogeropoulos,Panagiotis Kechagias,Dimitra Gariza,Athanasios P. Kalogeras,Christos Alexakos*

Main category: cs.AI

TL;DR: SUSTAINABLE是一个智能农业平台，旨在通过整合物联网、人工智能、卫星图像和基于角色的任务编排，实现高效、可追溯和可持续的农业。


<details>
  <summary>Details</summary>
Motivation: 全球农业部门正经历转型，原因在于食品需求增长、气候多变和对可持续实践的需求。

Method: 本文探讨了当前的智能农业解决方案，进行了比较评估，并介绍了SUSTAINABLE平台的关键功能。

Result: SUSTAINABLE平台提供了卫星指数整合、实时环境数据和针对地中海葡萄园的角色感知任务管理。

Conclusion: SUSTAINABLE平台通过集成物联网、人工智能和卫星技术，为可持续农业提供了一个创新的解决方案。

Abstract: The global agricultural sector is undergoing a transformative shift, driven
by increasing food demands, climate variability and the need for sustainable
practices. SUSTAINABLE is a smart farming platform designed to integrate IoT,
AI, satellite imaging, and role-based task orchestration to enable efficient,
traceable, and sustainable agriculture with a pilot usecase in viticulture.
This paper explores current smart agriculture solutions, presents a comparative
evaluation, and introduces SUSTAINABLE's key features, including satellite
index integration, real-time environmental data, and role-aware task management
tailored to Mediterranean vineyards.

</details>


### [107] [Causal Masking on Spatial Data: An Information-Theoretic Case for Learning Spatial Datasets with Unimodal Language Models](https://arxiv.org/abs/2510.27009)
*Jared Junkin,Samuel Nathanson*

Main category: cs.AI

TL;DR: 本文探讨了在非序列数据上应用因果掩码的可行性。


<details>
  <summary>Details</summary>
Motivation: 传统的语言模型以因果掩码为中心，但在具有空间或关系结构的领域中，因果掩码可能不适用。很少有研究直接探讨在非序列数据上应用因果掩码所带来的信息损失是否可行。

Method: 作者在国际象棋领域进行了实验，该领域天然支持空间（基于棋盘）和序列（基于走法）两种表示。他们使用双向和因果自注意力机制在两种数据上训练语言模型。

Result: 实验结果表明，在空间棋盘状态上训练的模型，即使使用因果掩码，也比在序列数据上训练的模型具有更强的下棋能力。

Conclusion: 将因果掩码应用于空间数据是训练空间数据单模态大型语言模型的有效方法，在某些领域甚至优于序列化。这一发现具有方法论意义，并可能产生更广泛的影响。

Abstract: Language models are traditionally designed around causal masking. In domains
with spatial or relational structure, causal masking is often viewed as
inappropriate, and sequential linearizations are instead used. Yet the question
of whether it is viable to accept the information loss introduced by causal
masking on nonsequential data has received little direct study, in part because
few domains offer both spatial and sequential representations of the same
dataset. In this work, we investigate this issue in the domain of chess, which
naturally supports both representations. We train language models with
bidirectional and causal self-attention mechanisms on both spatial
(board-based) and sequential (move-based) data. Our results show that models
trained on spatial board states - \textit{even with causal masking} -
consistently achieve stronger playing strength than models trained on
sequential data. While our experiments are conducted on chess, our results are
methodological and may have broader implications: applying causal masking to
spatial data is a viable procedure for training unimodal LLMs on spatial data,
and in some domains is even preferable to sequentialization.

</details>


### [108] [e1: Learning Adaptive Control of Reasoning Effort](https://arxiv.org/abs/2510.27042)
*Michael Kleinman,Matthew Trager,Alessandro Achille,Wei Xia,Stefano Soatto*

Main category: cs.AI

TL;DR: 该论文提出了自适应努力控制（Adaptive Effort Control），一种基于强化学习的方法，允许用户在LLMs中根据其偏好对推理成本和输出质量进行细粒度控制。


<details>
  <summary>Details</summary>
Motivation: 增加AI模型的思考预算可以显著提高准确性，但并非所有问题都需要相同的推理量。用户可能希望根据其对输出质量、延迟和成本的权衡，分配不同数量的推理工作。为了有效利用这种权衡，用户需要对特定查询所使用的思考量进行细粒度控制。现有方法要求用户指定所需的绝对token数量，但这需要预先了解问题的难度。

Method: 本文提出了一种名为“自适应努力控制”（Adaptive Effort Control）的自适应强化学习方法。该方法训练模型，使其能够根据用户指定的token比例（相对于当前平均思维链长度）进行推理。该方法消除了对数据集和阶段的特定调整，并在成本-准确性权衡曲线上优于标准方法。

Result: 模型自动学会了根据任务难度按比例分配资源，并且在1.5B到32B参数范围的模型中，我们的方法将思维链长度减少了大约3倍，同时相对于用于RL训练的基础模型保持或提高了性能。

Conclusion: 自适应努力控制方法允许用户在推理时通过一个连续的努力参数动态调整成本-准确性权衡，从而在保持或提高性能的同时显著减少思维链长度，并使模型能够自动学习如何根据任务难度分配资源。

Abstract: Increasing the thinking budget of AI models can significantly improve
accuracy, but not all questions warrant the same amount of reasoning. Users may
prefer to allocate different amounts of reasoning effort depending on how they
value output quality versus latency and cost. To leverage this tradeoff
effectively, users need fine-grained control over the amount of thinking used
for a particular query, but few approaches enable such control. Existing
methods require users to specify the absolute number of desired tokens, but
this requires knowing the difficulty of the problem beforehand to appropriately
set the token budget for a query. To address these issues, we propose Adaptive
Effort Control, a self-adaptive reinforcement learning method that trains
models to use a user-specified fraction of tokens relative to the current
average chain-of-thought length for each query. This approach eliminates
dataset- and phase-specific tuning while producing better cost-accuracy
tradeoff curves compared to standard methods. Users can dynamically adjust the
cost-accuracy trade-off through a continuous effort parameter specified at
inference time. We observe that the model automatically learns to allocate
resources proportionally to the task difficulty and, across model scales
ranging from 1.5B to 32B parameters, our approach enables approximately 3x
reduction in chain-of-thought length while maintaining or improving performance
relative to the base model used for RL training.

</details>


### [109] [Glia: A Human-Inspired AI for Automated Systems Design and Optimization](https://arxiv.org/abs/2510.27176)
*Pouya Hamadanian,Pantea Karimi,Arash Nasr-Esfahany,Kimia Noorbakhsh,Joseph Chandler,Ali ParandehGheibi,Mohammad Alizadeh,Hari Balakrishnan*

Main category: cs.AI

TL;DR: Glia 是一种利用大型语言模型（LLM）进行计算机系统机制设计的AI架构，它通过多智能体工作流实现类人水平的创造力和推理。


<details>
  <summary>Details</summary>
Motivation: 探索 AI 是否能像人类专家一样自主设计计算机系统机制，并具备同等的创造力和推理能力。

Method: Glia 采用多智能体工作流，每个智能体专注于推理、实验和分析，并通过评估框架将抽象推理与经验反馈相结合。与以往的黑盒优化方法不同，Glia 生成可解释的设计并展现其推理过程。

Result: 在分布式 GPU 集群的 LLM 推理应用中，Glia 提出了新的请求路由、调度和自动扩缩算法，其性能达到人类专家水平，耗时显著减少，并能提供对工作负载行为的新颖见解。

Conclusion: 通过将推理型 LLM 与结构化实验相结合，AI 能够为复杂的系统问题生成有创造性且可理解的设计。

Abstract: Can an AI autonomously design mechanisms for computer systems on par with the
creativity and reasoning of human experts? We present Glia, an AI architecture
for networked systems design that uses large language models (LLMs) in a
human-inspired, multi-agent workflow. Each agent specializes in reasoning,
experimentation, and analysis, collaborating through an evaluation framework
that grounds abstract reasoning in empirical feedback. Unlike prior
ML-for-systems methods that optimize black-box policies, Glia generates
interpretable designs and exposes its reasoning process. When applied to a
distributed GPU cluster for LLM inference, it produces new algorithms for
request routing, scheduling, and auto-scaling that perform at human-expert
levels in significantly less time, while yielding novel insights into workload
behavior. Our results suggest that by combining reasoning LLMs with structured
experimentation, an AI can produce creative and understandable designs for
complex systems problems.

</details>


### [110] [Reinforcement Learning for Long-Horizon Unordered Tasks: From Boolean to Coupled Reward Machines](https://arxiv.org/abs/2510.27329)
*Kristina Levina,Nikolaos Pappas,Athanasios Karapantelakis,Aneta Vulgarakis Feljan,Jendrik Seipp*

Main category: cs.AI

TL;DR: 奖励机器（RMs）在强化学习（RL）中用于定义任务奖励结构，对于复杂的非马尔可夫任务特别有用。然而，传统RMs在处理包含无序子任务的“长远”问题时效率低下，因为信息量会随子任务数量呈指数增长。为了解决这一问题，本文引入了三种通用的RMs扩展：数字RMs、议程RMs和耦合RMs。此外，本文还提出了一种基于耦合RMs的新型组合学习算法——Q-learning with coupled RMs（CoRM）。实验表明，CoRM在处理包含无序子任务的“长远”问题时，相比现有RM算法具有更好的扩展性。


<details>
  <summary>Details</summary>
Motivation: 传统的奖励机器（RMs）在处理包含无序子任务的“长远”（long-horizon）问题时存在局限性。在这种问题中，随着无序子任务数量的增加，学习所需的信息量呈指数级增长，从而导致学习效率低下。

Method: 本文通过引入三种RMs的泛化形式来解决传统RMs的局限性：
1. **数字RMs（Numeric RMs）**：允许用户以紧凑的形式表达复杂任务。
2. **议程RMs（Agenda RMs）**：状态与一个议程相关联，该议程跟踪待完成的剩余子任务。
3. **耦合RMs（Coupled RMs）**：每个子任务的议程都关联到耦合状态。

此外，本文还提出了一种新的组合学习算法——**Q-learning with coupled RMs（CoRM）**，该算法利用耦合RMs的优势进行学习。

Result: 实验结果表明，Q-learning with coupled RMs（CoRM）算法在处理带有无序子任务的“长远”问题时，相比现有最先进的奖励机器算法具有更好的扩展性（scales better）。

Conclusion: 本文通过引入数字RMs、议程RMs和耦合RMs这三种奖励机器的泛化形式，并提出基于耦合RMs的Q-learning算法（CoRM），成功克服了传统奖励机器在处理包含无序子任务的“长远”问题时的局限性。CoRM在这些复杂任务中表现出更好的扩展性，为强化学习在实际应用中处理复杂任务提供了新的解决方案。

Abstract: Reward machines (RMs) inform reinforcement learning agents about the reward
structure of the environment. This is particularly advantageous for complex
non-Markovian tasks because agents with access to RMs can learn more
efficiently from fewer samples. However, learning with RMs is ill-suited for
long-horizon problems in which a set of subtasks can be executed in any order.
In such cases, the amount of information to learn increases exponentially with
the number of unordered subtasks. In this work, we address this limitation by
introducing three generalisations of RMs: (1) Numeric RMs allow users to
express complex tasks in a compact form. (2) In Agenda RMs, states are
associated with an agenda that tracks the remaining subtasks to complete. (3)
Coupled RMs have coupled states associated with each subtask in the agenda.
Furthermore, we introduce a new compositional learning algorithm that leverages
coupled RMs: Q-learning with coupled RMs (CoRM). Our experiments show that CoRM
scales better than state-of-the-art RM algorithms for long-horizon problems
with unordered subtasks.

</details>


### [111] [Discriminative Rule Learning for Outcome-Guided Process Model Discovery](https://arxiv.org/abs/2510.27343)
*Ali Norouzifar,Wil van der Aalst*

Main category: cs.AI

TL;DR: 这篇论文提出了一种通过区分期望和非期望流程执行来改进流程发现的方法。


<details>
  <summary>Details</summary>
Motivation: 在许多实际应用中，事件日志可以区分为理想和非理想的流程执行，现有的流程发现方法未能有效利用这些信息，导致发现的模型在合规性检查和性能分析方面表现不佳。

Method: 通过学习可解释的判别规则，根据控制流特征对具有相似期望度配置的轨迹进行分组，并在每个组内分别进行流程发现。

Result: 得到了聚焦且可解释的模型，揭示了理想和非理想执行的驱动因素。该方法被实现为一个公开工具，并在多个真实事件日志上进行了评估。

Conclusion: 该方法能够有效地隔离和可视化关键过程模式，提高了过程发现的质量。

Abstract: Event logs extracted from information systems offer a rich foundation for
understanding and improving business processes. In many real-world
applications, it is possible to distinguish between desirable and undesirable
process executions, where desirable traces reflect efficient or compliant
behavior, and undesirable ones may involve inefficiencies, rule violations,
delays, or resource waste. This distinction presents an opportunity to guide
process discovery in a more outcome-aware manner. Discovering a single process
model without considering outcomes can yield representations poorly suited for
conformance checking and performance analysis, as they fail to capture critical
behavioral differences. Moreover, prioritizing one behavior over the other may
obscure structural distinctions vital for understanding process outcomes. By
learning interpretable discriminative rules over control-flow features, we
group traces with similar desirability profiles and apply process discovery
separately within each group. This results in focused and interpretable models
that reveal the drivers of both desirable and undesirable executions. The
approach is implemented as a publicly available tool and it is evaluated on
multiple real-life event logs, demonstrating its effectiveness in isolating and
visualizing critical process patterns.

</details>


### [112] [An In-depth Study of LLM Contributions to the Bin Packing Problem](https://arxiv.org/abs/2510.27353)
*Julien Herrmann,Guillaume Pallez*

Main category: cs.AI

TL;DR: 该研究质疑了大型语言模型（LLMs）在数学发现中的作用，特别是其在在线装箱问题启发式算法生成方面的能力。通过对LLM生成启发式算法的详细分析，研究者发现这些算法虽然可读但难以解释，并在此基础上提出了更简单、高效、可解释且更具普适性的新算法。研究指出，对于LLM对该问题的贡献的论断，可能基于对问题实例复杂性的误判。


<details>
  <summary>Details</summary>
Motivation: 此前的研究表明大型语言模型（LLMs）在数学发现中具有潜在价值，尤其是在通过LLM生成遗传算法为在线装箱问题（在均匀和威布尔分布下）提供新的启发式方法方面。本研究旨在重新评估这一论断。

Method: 本研究通过详细分析LLMs生成的启发式算法，考察了它们在行为和可解释性方面的表现。在此基础上，研究者针对特定的装箱问题实例，提出了一类新的算法。

Result: LLM生成的启发式算法虽然人类可读，但即使对于领域专家来说也难以理解。本研究提出的一类新算法，在相同问题实例上，表现出显著更简单、更高效、更易解释且更具普适性。这表明此前LLM处理的实例本身相对简单。

Conclusion: 本研究认为，LLMs对在线装箱问题的贡献可能被高估了，其部分原因在于误认为相关问题实例之前已经被充分研究。研究强调了在评估LLM生成内容的科学价值时，进行严格验证和情境化的重要性，以避免对LLMs能力产生不准确的认识。

Abstract: Recent studies have suggested that Large Language Models (LLMs) could provide
interesting ideas contributing to mathematical discovery. This claim was
motivated by reports that LLM-based genetic algorithms produced heuristics
offering new insights into the online bin packing problem under uniform and
Weibull distributions. In this work, we reassess this claim through a detailed
analysis of the heuristics produced by LLMs, examining both their behavior and
interpretability. Despite being human-readable, these heuristics remain largely
opaque even to domain experts. Building on this analysis, we propose a new
class of algorithms tailored to these specific bin packing instances. The
derived algorithms are significantly simpler, more efficient, more
interpretable, and more generalizable, suggesting that the considered instances
are themselves relatively simple. We then discuss the limitations of the claim
regarding LLMs' contribution to this problem, which appears to rest on the
mistaken assumption that the instances had previously been studied. Our
findings instead emphasize the need for rigorous validation and
contextualization when assessing the scientific value of LLM-generated outputs.

</details>


### [113] [Realistic pedestrian-driver interaction modelling using multi-agent RL with human perceptual-motor constraints](https://arxiv.org/abs/2510.27383)
*Yueyang Wang,Mehmet Dogar,Gustav Markkula*

Main category: cs.AI

TL;DR: 这篇论文提出了一个多智能体强化学习框架，该框架集成了行人和驾驶员的视觉和运动约束，以模拟现实世界中的人车交互行为。


<details>
  <summary>Details</summary>
Motivation: 现有的行人与驾驶员交互模型通常缺乏灵活性，或者忽略了视觉和运动约束等潜在机制，而这些机制塑造了行人和驾驶员在交互场景中的感知和行为方式。

Method: 本研究提出了一个多智能体强化学习（RL）框架，该框架集成了行人和驾驶员智能体的视觉和运动约束。该研究使用来自非信号行人过街的真实世界数据集，评估了四种模型变体：无约束模型、仅有运动约束模型、仅有视觉约束模型以及同时具有视觉和运动约束的模型，并通过交互真实性的行为指标进行了评估。

Result: 结果表明，结合了视觉和运动约束的模型表现最佳。运动约束使动作更平稳，类似于人类在过街交互过程中的速度调整。视觉约束增加了感知不确定性和视野限制，使智能体表现出更谨慎和多变的行为，例如更少急剧减速。在数据有限的情况下，该模型优于有监督的行为克隆模型，证明了该方法在不需要大量训练数据集的情况下也能有效。

Conclusion: 多智能体强化学习与人类约束相结合是模拟真实道路使用者交互的一种有前景的建模方法。

Abstract: Modelling pedestrian-driver interactions is critical for understanding human
road user behaviour and developing safe autonomous vehicle systems. Existing
approaches often rely on rule-based logic, game-theoretic models, or
'black-box' machine learning methods. However, these models typically lack
flexibility or overlook the underlying mechanisms, such as sensory and motor
constraints, which shape how pedestrians and drivers perceive and act in
interactive scenarios. In this study, we propose a multi-agent reinforcement
learning (RL) framework that integrates both visual and motor constraints of
pedestrian and driver agents. Using a real-world dataset from an unsignalised
pedestrian crossing, we evaluate four model variants, one without constraints,
two with either motor or visual constraints, and one with both, across
behavioural metrics of interaction realism. Results show that the combined
model with both visual and motor constraints performs best. Motor constraints
lead to smoother movements that resemble human speed adjustments during
crossing interactions. The addition of visual constraints introduces perceptual
uncertainty and field-of-view limitations, leading the agents to exhibit more
cautious and variable behaviour, such as less abrupt deceleration. In this
data-limited setting, our model outperforms a supervised behavioural cloning
model, demonstrating that our approach can be effective without large training
datasets. Finally, our framework accounts for individual differences by
modelling parameters controlling the human constraints as population-level
distributions, a perspective that has not been explored in previous work on
pedestrian-vehicle interaction modelling. Overall, our work demonstrates that
multi-agent RL with human constraints is a promising modelling approach for
simulating realistic road user interactions.

</details>


### [114] [DeepCompress: A Dual Reward Strategy for Dynamically Exploring and Compressing Reasoning Chains](https://arxiv.org/abs/2510.27419)
*Tian Liang,Wenxiang Jiao,Zhiwei He,Jiahao Xu,Haitao Mi,Dong Yu*

Main category: cs.AI

TL;DR: DeepCompress 是一种新颖的框架，它同时提高了 LRMs 的准确性和效率。


<details>
  <summary>Details</summary>
Motivation: 大型推理模型（LRMs）表现出令人印象深刻的能力，但存在认知效率低下的问题，例如对简单问题“过度思考”和对复杂问题“思考不足”。

Method: DeepCompress 采用自适应长度奖励机制，根据模型的演变能力实时将问题分为“简单”或“困难”。它鼓励对“简单”问题进行更短、更有效的推理，同时促进对“困难”问题进行更长、更具探索性的思路。

Result: 在具有挑战性的数学基准测试中，DeepCompress 始终优于基线方法，在显著提高令牌效率的同时实现了卓越的准确性。

Conclusion: DeepCompress 使得模型能够自主调整其思维链（CoT）长度，对于已 H 掌握的问题压缩推理，对于发现具有挑战性的问题则扩展推理。

Abstract: Large Reasoning Models (LRMs) have demonstrated impressive capabilities but
suffer from cognitive inefficiencies like ``overthinking'' simple problems and
``underthinking'' complex ones. While existing methods that use supervised
fine-tuning~(SFT) or reinforcement learning~(RL) with token-length rewards can
improve efficiency, they often do so at the cost of accuracy. This paper
introduces \textbf{DeepCompress}, a novel framework that simultaneously
enhances both the accuracy and efficiency of LRMs. We challenge the prevailing
approach of consistently favoring shorter reasoning paths, showing that longer
responses can contain a broader range of correct solutions for difficult
problems. DeepCompress employs an adaptive length reward mechanism that
dynamically classifies problems as ``Simple'' or ``Hard'' in real-time based on
the model's evolving capability. It encourages shorter, more efficient
reasoning for ``Simple'' problems while promoting longer, more exploratory
thought chains for ``Hard'' problems. This dual-reward strategy enables the
model to autonomously adjust its Chain-of-Thought (CoT) length, compressing
reasoning for well-mastered problems and extending it for those it finds
challenging. Experimental results on challenging mathematical benchmarks show
that DeepCompress consistently outperforms baseline methods, achieving superior
accuracy while significantly improving token efficiency.

</details>


### [115] [GeoFM: Enhancing Geometric Reasoning of MLLMs via Synthetic Data Generation through Formal Language](https://arxiv.org/abs/2510.27448)
*Yuhao Zhang,Dingxin Hu,Tinghao Yu,Hao Liu,Yiting Liu*

Main category: cs.AI

TL;DR: GeoFM是一种新颖的几何数据合成方法，它使用形式语言在度量空间中探索条件组合，生成高保真几何问题。 GeoFM显著优于现有方法，并在几何问题解决任务中超越了GPT-4o模型18.7%（在MathVista上）和16.5%（在GeoQA上），同时在MathVista上超过领先的开源模型5.7%，在GeoQA上超过2.7%。


<details>
  <summary>Details</summary>
Motivation: 多模态大型语言模型（MLLMs）在处理多模态任务方面表现出色，但在数学几何推理方面面临挑战，原因在于高质量几何数据的稀缺性。合成几何数据已成为解决此问题的重要策略。

Method: 我们提出了GeoFM，一种新颖的几何数据合成方法。 GeoFM使用形式语言在度量空间中探索条件组合，生成与原始问题不同但通过符号引擎确保正确性的高保真几何问题。

Result: 我们的合成数据显著优于现有方法。使用我们数据训练的模型在MathVista中的几何问题解决任务上比专有的GPT-4o模型高出18.7％，在GeoQA上高出16.5％。此外，它在MathVista上领先领先的开源模型5.7％，在GeoQA上领先2.7％。

Conclusion: GeoFM通过生成高保真、多样化的几何问题，显著提升了多模态大型语言模型在几何推理任务上的表现，超越了GPT-4o等现有模型，证明了其在解决几何数据稀缺问题上的有效性。

Abstract: Multi-modal Large Language Models (MLLMs) have gained significant attention
in both academia and industry for their capabilities in handling multi-modal
tasks. However, these models face challenges in mathematical geometric
reasoning due to the scarcity of high-quality geometric data. To address this
issue, synthetic geometric data has become an essential strategy. Current
methods for generating synthetic geometric data involve rephrasing or expanding
existing problems and utilizing predefined rules and templates to create
geometric images and problems. However, these approaches often produce data
that lacks diversity or is prone to noise. Additionally, the geometric images
synthesized by existing methods tend to exhibit limited variation and deviate
significantly from authentic geometric diagrams. To overcome these limitations,
we propose GeoFM, a novel method for synthesizing geometric data. GeoFM uses
formal languages to explore combinations of conditions within metric space,
generating high-fidelity geometric problems that differ from the originals
while ensuring correctness through a symbolic engine. Experimental results show
that our synthetic data significantly outperforms existing methods. The model
trained with our data surpass the proprietary GPT-4o model by 18.7\% on
geometry problem-solving tasks in MathVista and by 16.5\% on GeoQA.
Additionally, it exceeds the performance of a leading open-source model by
5.7\% on MathVista and by 2.7\% on GeoQA.

</details>


### [116] [SIGMA: Search-Augmented On-Demand Knowledge Integration for Agentic Mathematical Reasoning](https://arxiv.org/abs/2510.27568)
*Ali Asgarov,Umid Suleymanov,Aadyant Khatri*

Main category: cs.AI

TL;DR: SIGMA是一个统一的多智能体框架，通过协调专业智能体进行独立推理、有针对性搜索和信息整合，从而解决了数学推理问题中检索增强模型依赖单一视角、搜索策略不灵活以及难以有效结合多源信息的问题。SIGMA在MATH500、AIME和GPQA等基准测试中表现出色，性能绝对提升7.4%。这表明多智能体按需知识整合显著提高了推理准确性和效率。


<details>
  <summary>Details</summary>
Motivation: 目前的检索增强模型在解决数学推理问题时，存在以下缺点：1. 依赖单一视角；2. 搜索策略不灵活；3. 难以有效结合来自多个来源的信息。

Method: SIGMA（Search-Augmented On-Demand Knowledge Integration for AGentic Mathematical reAsoning）是一个统一的框架，它协调专业的智能体独立地进行推理、执行有针对性的搜索，并通过一个协调器机制综合发现。每个智能体会生成假设性的段落，以优化其分析视角的检索，确保知识整合是上下文敏感且计算高效的。

Result: 在MATH500、AIME和PhD级别的科学问答GPQA等挑战性基准测试中进行评估时，SIGMA始终优于开源和闭源系统，实现了7.4%的绝对性能提升。

Conclusion: 多智能体按需知识整合显著提高了推理的准确性和效率，为复杂、知识密集型问题解决提供了一种可扩展的方法。

Abstract: Solving mathematical reasoning problems requires not only accurate access to
relevant knowledge but also careful, multi-step thinking. However, current
retrieval-augmented models often rely on a single perspective, follow
inflexible search strategies, and struggle to effectively combine information
from multiple sources. We introduce SIGMA (Search-Augmented On-Demand Knowledge
Integration for AGentic Mathematical reAsoning), a unified framework that
orchestrates specialized agents to independently reason, perform targeted
searches, and synthesize findings through a moderator mechanism. Each agent
generates hypothetical passages to optimize retrieval for its analytic
perspective, ensuring knowledge integration is both context-sensitive and
computation-efficient. When evaluated on challenging benchmarks such as
MATH500, AIME, and PhD-level science QA GPQA, SIGMA consistently outperforms
both open- and closed-source systems, achieving an absolute performance
improvement of 7.4%. Our results demonstrate that multi-agent, on-demand
knowledge integration significantly enhances both reasoning accuracy and
efficiency, offering a scalable approach for complex, knowledge-intensive
problem-solving. We will release the code upon publication.

</details>


### [117] [Visual Backdoor Attacks on MLLM Embodied Decision Making via Contrastive Trigger Learning](https://arxiv.org/abs/2510.27623)
*Qiusi Zhan,Hyeonjeong Ha,Rui Yang,Sirui Xu,Hanyang Chen,Liang-Yan Gui,Yu-Xiong Wang,Huan Zhang,Heng Ji,Daniel Kang*

Main category: cs.AI

TL;DR: BEAT框架首次将视觉后门植入多模态大语言模型（MLLM）的具身智能体中，通过对象触发器实现，并使用两阶段训练方案（SFT和CTL）提高后门激活的准确性，揭示了MLLM具身智能体中未被探索的安全风险。


<details>
  <summary>Details</summary>
Motivation: 多模态大语言模型（MLLM）在具身智能体中的应用引入了新的攻击面：视觉后门攻击。这种攻击利用视觉触发器，使智能体在特定触发器出现时执行攻击者指定的多步策略。

Method: BEAT框架通过以下两点解决视觉后门攻击的挑战：1. 构建多样化的训练集，涵盖不同场景、任务和触发器放置，以使智能体适应触发器的多变性。2. 引入两阶段训练方案，首先进行监督微调（SFT），然后采用新颖的对比触发学习（CTL）。CTL将触发器判别视为触发器存在与否输入之间的偏好学习，从而明确地锐化决策边界，确保后门精确激活。

Result: BEAT在各种具身智能体基准和MLLM上实现了高达80%的攻击成功率，同时保持了良好的良性任务性能，并能可靠地推广到分布外的触发器放置。与朴素的SFT相比，在有限的后门数据下，CTL将后门激活准确性提高了39%。

Conclusion: BEAT框架揭示了MLLM具身智能体中一个关键但尚未被探索的安全风险，强调在实际部署前需要建立强大的防御机制。

Abstract: Multimodal large language models (MLLMs) have advanced embodied agents by
enabling direct perception, reasoning, and planning task-oriented actions from
visual inputs. However, such vision driven embodied agents open a new attack
surface: visual backdoor attacks, where the agent behaves normally until a
visual trigger appears in the scene, then persistently executes an
attacker-specified multi-step policy. We introduce BEAT, the first framework to
inject such visual backdoors into MLLM-based embodied agents using objects in
the environments as triggers. Unlike textual triggers, object triggers exhibit
wide variation across viewpoints and lighting, making them difficult to implant
reliably. BEAT addresses this challenge by (1) constructing a training set that
spans diverse scenes, tasks, and trigger placements to expose agents to trigger
variability, and (2) introducing a two-stage training scheme that first applies
supervised fine-tuning (SFT) and then our novel Contrastive Trigger Learning
(CTL). CTL formulates trigger discrimination as preference learning between
trigger-present and trigger-free inputs, explicitly sharpening the decision
boundaries to ensure precise backdoor activation. Across various embodied agent
benchmarks and MLLMs, BEAT achieves attack success rates up to 80%, while
maintaining strong benign task performance, and generalizes reliably to
out-of-distribution trigger placements. Notably, compared to naive SFT, CTL
boosts backdoor activation accuracy up to 39% under limited backdoor data.
These findings expose a critical yet unexplored security risk in MLLM-based
embodied agents, underscoring the need for robust defenses before real-world
deployment.

</details>


### [118] [Validity Is What You Need](https://arxiv.org/abs/2510.27628)
*Sebastian Benthall,Andrew Clark*

Main category: cs.AI

TL;DR: 这篇论文讨论了具身通用人工智能（Agentic AI）的定义、特点、挑战和发展方向。


<details>
  <summary>Details</summary>
Motivation: 作者认为当前的具身通用人工智能系统是全新的，需要提出一种现实的定义。

Method: 本文通过将具身通用人工智能视为一种软件交付机制，并与软件即服务（SaaS）进行比较，从而引出对具身通用人工智能的探讨。

Result: 具身通用人工智能系统主要关注应用，而非基础模型。其成功取决于最终用户和主要利益相关者的验证。

Conclusion: 如果采用良好的验证措施，许多情况下，基础模型可以被更简单、更快、可解释性更强的模型取代，以处理核心逻辑。

Abstract: While AI agents have long been discussed and studied in computer science,
today's Agentic AI systems are something new. We consider other definitions of
Agentic AI and propose a new realist definition. Agentic AI is a software
delivery mechanism, comparable to software as a service (SaaS), which puts an
application to work autonomously in a complex enterprise setting. Recent
advances in large language models (LLMs) as foundation models have driven
excitement in Agentic AI. We note, however, that Agentic AI systems are
primarily applications, not foundations, and so their success depends on
validation by end users and principal stakeholders. The tools and techniques
needed by the principal users to validate their applications are quite
different from the tools and techniques used to evaluate foundation models.
Ironically, with good validation measures in place, in many cases the
foundation models can be replaced with much simpler, faster, and more
interpretable models that handle core logic. When it comes to Agentic AI,
validity is what you need. LLMs are one option that might achieve it.

</details>
