<div id=toc></div>

# Table of Contents

- [cs.CL](#cs.CL) [Total: 33]
- [cs.MA](#cs.MA) [Total: 1]
- [cs.IT](#cs.IT) [Total: 11]
- [cs.LG](#cs.LG) [Total: 45]
- [cs.GT](#cs.GT) [Total: 3]
- [cs.AI](#cs.AI) [Total: 19]
- [stat.ML](#stat.ML) [Total: 1]


<div id='cs.CL'></div>

# cs.CL [[Back]](#toc)

### [1] [Human-Level and Beyond: Benchmarking Large Language Models Against Clinical Pharmacists in Prescription Review](https://arxiv.org/abs/2512.02024)
*Yan Yang,Mouxiao Bian,Peiling Li,Bingjian Wen,Ruiyao Chen,Kangkun Mao,Xiaojun Ye,Tianbin Li,Pengcheng Chen,Bing Han,Jie Xu,Kaifeng Qiu,Junyan Wu*

Main category: cs.CL

TL;DR: RxBench是一个用于评估LLM在处方审核方面表现的基准测试，它涵盖了常见的处方审核类别和14种常见的处方错误类型。


<details>
  <summary>Details</summary>
Motivation: 大型语言模型（LLM）的快速发展加速了它们在临床决策支持中的整合，特别是在处方审核方面。为了实现系统和细粒度的评估，需要一个全面的基准。

Method: 开发了RxBench，一个综合基准，涵盖了常见的处方审核类别，并整合了来自权威药学参考资料的14种常见处方错误类型。RxBench包含1,150个单项选择题、230个多项选择题和879个简答题，所有题目都经过经验丰富的临床药师审核。

Result: 对18个最先进的LLM进行了基准测试，发现它们在任务中表现出明显的分层。Gemini-2.5-pro-preview-05-06、Grok-4-0709和DeepSeek-R1-0528持续位居第一梯队，在准确性和鲁棒性方面优于其他模型。与执业药师的比较表明，领先的LLM在某些任务中可以达到或超过人类的表现。通过对中等水平模型进行有针对性的微调，获得了在简答题任务中可与领先的通用LLM媲美的专用模型。

Conclusion: RxBench建立了一个标准化的、面向错误类型的框架，揭示了前沿LLM在处方审核中的能力和局限性，并为构建更可靠和专业的临床工具提供了基础资源。

Abstract: The rapid advancement of large language models (LLMs) has accelerated their integration into clinical decision support, particularly in prescription review. To enable systematic and fine-grained evaluation, we developed RxBench, a comprehensive benchmark that covers common prescription review categories and consolidates 14 frequent types of prescription errors drawn from authoritative pharmacy references. RxBench consists of 1,150 single-choice, 230 multiple-choice, and 879 short-answer items, all reviewed by experienced clinical pharmacists. We benchmarked 18 state-of-the-art LLMs and identified clear stratification of performance across tasks. Notably, Gemini-2.5-pro-preview-05-06, Grok-4-0709, and DeepSeek-R1-0528 consistently formed the first tier, outperforming other models in both accuracy and robustness. Comparisons with licensed pharmacists indicated that leading LLMs can match or exceed human performance in certain tasks. Furthermore, building on insights from our benchmark evaluation, we performed targeted fine-tuning on a mid-tier model, resulting in a specialized model that rivals leading general-purpose LLMs in performance on short-answer question tasks. The main contribution of RxBench lies in establishing a standardized, error-type-oriented framework that not only reveals the capabilities and limitations of frontier LLMs in prescription review but also provides a foundational resource for building more reliable and specialized clinical tools.

</details>


### [2] [Deep Research: A Systematic Survey](https://arxiv.org/abs/2512.02038)
*Zhengliang Shi,Yiqun Chen,Haitao Li,Weiwei Sun,Shiyu Ni,Yougang Lyu,Run-Ze Fan,Bowen Jin,Yixuan Weng,Minjun Zhu,Qiujie Xie,Xinyu Guo,Qu Yang,Jiayi Wu,Jujia Zhao,Xiaqiang Tang,Xinbei Ma,Cunxiang Wang,Jiaxin Mao,Qingyao Ai,Jen-Tse Huang,Wenxuan Wang,Yue Zhang,Yiming Yang,Zhaopeng Tu,Zhaochun Ren*

Main category: cs.CL

TL;DR: 这篇综述探讨了深度研究（Deep Research，DR）领域，该领域旨在结合大型语言模型（LLMs）的推理能力与外部工具，使其能够执行复杂的、开放式的研究任务。


<details>
  <summary>Details</summary>
Motivation: 探索深度研究（Deep Research, DR），它结合了大型语言模型（LLMs）的推理能力和外部工具，以完成超越单一提示或标准检索增强生成的、需要批判性思维、多源和可验证输出的开放式复杂任务。

Method: 本文提出并形式化了一个三阶段路线图，区分了深度研究与其他相关范式。介绍了查询规划、信息获取、记忆管理和答案生成四个关键组成部分，并辅以细粒度子分类。总结了包括提示工程、监督微调和智能体强化学习在内的优化技术。整合了评估标准和开放性挑战。

Result: 通过对现有深度研究系统的全面而系统的概述，本文为该领域提供了一个清晰的路线图、基础组件、实践实现技术、重要挑战和未来方向。

Conclusion: 深度研究是一个快速发展的领域，有望通过结合LLM的推理能力和外部工具来解决复杂的开放任务。该综述为未来该领域的发展提供了全面的框架和指导。

Abstract: Large language models (LLMs) have rapidly evolved from text generators into powerful problem solvers. Yet, many open tasks demand critical thinking, multi-source, and verifiable outputs, which are beyond single-shot prompting or standard retrieval-augmented generation. Recently, numerous studies have explored Deep Research (DR), which aims to combine the reasoning capabilities of LLMs with external tools, such as search engines, thereby empowering LLMs to act as research agents capable of completing complex, open-ended tasks. This survey presents a comprehensive and systematic overview of deep research systems, including a clear roadmap, foundational components, practical implementation techniques, important challenges, and future directions. Specifically, our main contributions are as follows: (i) we formalize a three-stage roadmap and distinguish deep research from related paradigms; (ii) we introduce four key components: query planning, information acquisition, memory management, and answer generation, each paired with fine-grained sub-taxonomies; (iii) we summarize optimization techniques, including prompting, supervised fine-tuning, and agentic reinforcement learning; and (iv) we consolidate evaluation criteria and open challenges, aiming to guide and facilitate future development. As the field of deep research continues to evolve rapidly, we are committed to continuously updating this survey to reflect the latest progress in this area.

</details>


### [3] [Beyond Confidence: Adaptive and Coherent Decoding for Diffusion Language Models](https://arxiv.org/abs/2512.02044)
*Kecheng Chen,Ziru Liu,Xijia Tao,Hui Liu,Xinyu Fu,Suiyun Zhang,Dandan Tu,Lingpeng Kong,Rui Liu,Haoliang Li*

Main category: cs.CL

TL;DR: 该论文提出了一种名为Coherent Contextual Decoding (CCD)的新颖推理框架，通过轨迹校正机制和自适应抽样策略，在提高生成质量的同时加速了扩散语言模型的推理过程。


<details>
  <summary>Details</summary>
Motivation: 现有的扩散语言模型推理方法通常依赖于局部、即时步骤的度量，这导致采样轨迹不一致和生成质量不佳。

Method: *** Coherent Contextual Decoding (CCD)框架：***
1. ***轨迹校正机制：*** 利用历史上下文信息来增强序列连贯性，从而能够及早拒绝次优路径。这在理论上等同于通过上下文和令牌预测之间的条件互信息来建模历史步骤的一致性。
2. ***自适应抽样策略：*** 针对传统均匀解码预算效率低下的问题，引入了一种自适应的采样策略，根据一致性度量动态调整每个步骤的unmasking预算。

Result: 在Dream和LLaDA等多个基准测试中，CCD方法同时提升了推理速度和性能，实现了高达3.48倍的速度提升和3.91%的性能改进。

Conclusion: 通过引入轨迹校正机制和自适应采样策略，Coherent Contextual Decoding (CCD)框架有效地解决了现有扩散语言模型推理方法的局限性，显著提高了生成质量并加速了采样过程。

Abstract: Diffusion Language Models (DLMs) have recently achieved significant success due to their any-order generation capabilities. However, existing inference methods typically rely on local, immediate-step metrics such as confidence or entropy which inherently lack a more reliable perspective. This limitation frequently leads to inconsistent sampling trajectories and suboptimal generation quality. To address this, we propose Coherent Contextual Decoding (CCD), a novel inference framework built upon two core innovations. First, CCD employs a trajectory rectification mechanism that leverages historical context to enhance sequence coherence, enabling the early rejection of suboptimal paths. We demonstrate that this mechanism is theoretically equivalent to modeling the consistency of historical steps via the conditional mutual information between context and token predictions. Building on this theoretical insight, we further address the inefficiency of conventional uniform decoding budgets. Instead of rigid allocations based on diffusion steps, we introduce an adaptive sampling strategy that dynamically adjusts the unmasking budget for each step according to our consistency metric. Consequently, our method significantly improves the quality of generation trajectories while accelerating the sampling process. Empirically, our method achieves a simultaneous enhancement in both inference speed and performance across diverse benchmarks on Dream and LLaDA, delivering up to 3.48x speedup alongside 3.91% performance improvement.

</details>


### [4] [Reversing Large Language Models for Efficient Training and Fine-Tuning](https://arxiv.org/abs/2512.02056)
*Eshed Gal,Moshe Eliasof,Javier Turek,Uri Ascher,Eran Treister,Eldad Haber*

Main category: cs.CL

TL;DR: 本文介绍了一种内存高效、可逆的LLM架构，灵感来自对称和辛微分方程，与传统架构不同，它无需存储所有中间激活。


<details>
  <summary>Details</summary>
Motivation: 大型语言模型（LLMs）的训练成本高昂且耗时，因此通常会针对特定任务对预训练的LLMs进行微调。本文旨在解决LLMs训练和微调过程中的内存消耗问题。

Method: 本文提出了一种内存高效、可逆的LLM架构，其灵感来源于对称和辛微分方程。该模型利用时间可逆动力学在反向传播过程中检索隐藏状态，从而无需存储激活。此外，本文还提出了一种将现有非可逆LLMs转换为可逆架构的有效微调方法。

Result: 实验结果表明，在多个数据集和基准测试中，该方法在几种LLMs上取得了相当或更好的性能。

Conclusion: 本文提出了一种可扩展且高效的方法，可以显著降低LLMs从头开始训练和微调相关的内存和计算成本。

Abstract: Large Language Models (LLMs) are known for their expensive and time-consuming training. Thus, oftentimes, LLMs are fine-tuned to address a specific task, given the pretrained weights of a pre-trained LLM considered a foundation model. In this work, we introduce memory-efficient, reversible architectures for LLMs, inspired by symmetric and symplectic differential equations, and investigate their theoretical properties. Different from standard, baseline architectures that store all intermediate activations, the proposed models use time-reversible dynamics to retrieve hidden states during backpropagation, relieving the need to store activations. This property allows for a drastic reduction in memory consumption, allowing for the processing of larger batch sizes for the same available memory, thereby offering improved throughput. In addition, we propose an efficient method for converting existing, non-reversible LLMs into reversible architectures through fine-tuning, rendering our approach practical for exploiting existing pre-trained models. Our results show comparable or improved performance on several datasets and benchmarks, on several LLMs, building a scalable and efficient path towards reducing the memory and computational costs associated with both training from scratch and fine-tuning of LLMs.

</details>


### [5] [Dialect Identification Using Resource-Efficient Fine-Tuning Approaches](https://arxiv.org/abs/2512.02074)
*Zirui Lin,Haris Gulzar,Monnika Roslianna Busto,Akiko Masaki,Takeharu Eda,Kazuhiro Nakadai*

Main category: cs.CL

TL;DR: 这篇论文探讨了方言识别（DI）任务中参数和内存效率问题，提出并评估了内存高效微调（MEFT）方法，以降低计算成本并提高训练速度，同时保持准确性。


<details>
  <summary>Details</summary>
Motivation: 方言识别（DI）对于提高下游语音相关任务的性能至关重要，尤其是在说话者方言口音较重的情况下。然而，对语音模型进行微调，例如DI，计算成本高昂，内存需求大。现有的参数高效微调（PEFT）方法虽然参数高效，但在内存效率和训练速度方面改进有限。

Method: 本文提出并评估了内存高效微调（MEFT）方法，该方法最初是为语言处理而设计的，并将其应用于通用的预训练语音模型。通过综合分析MEFT方法，研究者评估了GPU内存使用情况和微调速度，并以Whisper模型在KeSpeech数据集中识别六种普通话子方言为例进行了案例研究。

Result: MEFT方法成功地将GPU内存使用量降低了73.25%，训练速度提高了2.1倍，同时保持了与传统微调和PEFT方法相当的准确性。

Conclusion: MEFT方法在方言识别任务中表现出色，显著降低了计算成本并缩短了训练时间，同时保持了高准确性。这些发现为未来的语音模型微调和方言识别研究提供了有价值的参考。

Abstract: Dialect Identification (DI) is a task to recognize different dialects within the same language from a speech signal. DI can help to improve the downstream speech related tasks even when speakers have a strong dialect. However, fine-tuning a speech model for tasks like DI is expensive in terms of computation cost and memory requirement. Recent studies have explored fine-tuning pre-trained speech models for tasks like DI using Parameter-Efficient Fine-Tuning (PEFT) methods, which offer parameter efficiency but limited improvement in memory efficiency and training speed. To address these challenges, we explore Memory-Efficient Fine-Tuning (MEFT) methods, originally proposed for language processing, and apply them to the general-purpose pre-trained speech model. We then comprehensively analyze the GPU memory usage and fine-tuning speed based on various MEFT methods. As a case study, we fine-tune the Whisper model to identify six Mandarin subdialects from the KeSpeech dataset, reducing GPU memory usage by up to 73.25% and accelerating training speed by a factor of 2.1, while maintaining accuracy comparable to vanilla fine-tuning and PEFT methods.

</details>


### [6] [Feature Selection Empowered BERT for Detection of Hate Speech with Vocabulary Augmentation](https://arxiv.org/abs/2512.02141)
*Pritish N. Desai,Tanay Kewalramani,Srimanta Mandal*

Main category: cs.CL

TL;DR: 这篇论文提出了一种数据高效的BERT微调策略，用于仇恨言论分类，该策略通过基于TF-IDF的样本选择机制和特定领域词汇增强来减少训练集大小，同时保持性能，从而提高了计算效率。


<details>
  <summary>Details</summary>
Motivation: 社交媒体上的辱骂性言论带来了持续的挑战，因为新的俚语和模糊词汇不断出现，旨在规避检测系统。

Method: 本研究采用了一种数据高效的策略，通过以下方式对BERT进行仇恨言论分类的微调：1. 使用基于TF-IDF的样本选择机制，仅保留75%信息量最大的样本，以显著减少训练集大小。2. 扩充BERT的分词器，加入辱骂性语境中常见的领域特定俚语和词汇变体，以解决BERT原生词汇在捕捉不断演变的仇恨言论术语方面的局限性。

Result: 在广泛使用的仇恨言论数据集上的实验结果表明，该方法在提高计算效率的同时，取得了具有竞争力的性能。

Conclusion: 该研究提出的方法在不损害性能的前提下，显著减少了训练数据集的大小，并通过领域特定词汇增强解决了BERT在处理新兴仇恨言论方面的局限性。这表明了其在可扩展和自适应的辱骂内容审核方面的潜力。

Abstract: Abusive speech on social media poses a persistent and evolving challenge, driven by the continuous emergence of novel slang and obfuscated terms designed to circumvent detection systems. In this work, we present a data efficient strategy for fine tuning BERT on hate speech classification by significantly reducing training set size without compromising performance. Our approach employs a TF IDF-based sample selection mechanism to retain only the most informative 75 percent of examples, thereby minimizing training overhead. To address the limitations of BERT's native vocabulary in capturing evolving hate speech terminology, we augment the tokenizer with domain-specific slang and lexical variants commonly found in abusive contexts. Experimental results on a widely used hate speech dataset demonstrate that our method achieves competitive performance while improving computational efficiency, highlighting its potential for scalable and adaptive abusive content moderation.

</details>


### [7] [Swivuriso: The South African Next Voices Multilingual Speech Dataset](https://arxiv.org/abs/2512.02201)
*Vukosi Marivatee,Kayode Olaleye,Sitwala Mundia,Andinda Bakainga,Unarine Netshifhefhe,Mahmooda Milanzie,Tsholofelo Hope Mogale,Thapelo Sindane,Zainab Abdulrasaq,Kesego Mokgosi,Chijioke Okorie,Nia Zion Van Wyk,Graham Morrissey,Dale Dunbar,Francois Smit,Tsosheletso Chidi,Rooweither Mabuya,Andiswa Bukula,Respect Mlambo,Tebogo Macucwa,Idris Abdulmumin,and Seani Rananga*

Main category: cs.CL

TL;DR: Swivuriso是一个包含七种南非语言的3000小时多语言语音数据集，旨在支持自动语音识别（ASR）技术的开发和基准测试，并弥补现有ASR数据集的空白。


<details>
  <summary>Details</summary>
Motivation: 开发和基准测试七种南非语言的自动语音识别（ASR）技术，并解决现有ASR数据集中的显著空白。

Method: 设计了Swivuriso数据集，遵循特定的设计原则、伦理考量和数据收集程序。

Result: 创建了Swivuriso数据集，涵盖农业、医疗保健和通用领域主题，并提供了使用该数据训练/微调ASR模型的基线结果，并与其他ASR数据集进行了比较。

Conclusion: Swivuriso数据集的发布有望促进南非语言ASR技术的发展，并为未来的研究提供基准。

Abstract: This paper introduces Swivuriso, a 3000-hour multilingual speech dataset developed as part of the African Next Voices project, to support the development and benchmarking of automatic speech recognition (ASR) technologies in seven South African languages. Covering agriculture, healthcare, and general domain topics, Swivuriso addresses significant gaps in existing ASR datasets. We describe the design principles, ethical considerations, and data collection procedures that guided the dataset creation. We present baseline results of training/finetuning ASR models with this data and compare to other ASR datasets for the langauges concerned.

</details>


### [8] [Lightweight Latent Reasoning for Narrative Tasks](https://arxiv.org/abs/2512.02240)
*Alexander Gurung,Nikolay Malkin,Mirella Lapata*

Main category: cs.CL

TL;DR: LiteReason是一种轻量级的潜在推理方法，它通过训练推理投影模块生成连续的潜在token，从而跳过推理步骤，大大减少了大型语言模型在处理复杂任务时的计算成本，并在保持性能的同时，将推理长度缩短了77-92%。


<details>
  <summary>Details</summary>
Motivation: 大型语言模型在处理复杂任务时，通常需要生成长的思维链或“推理轨迹”，这会带来高昂的计算成本，尤其是在叙述相关任务中。

Method: LiteReason方法训练了一个轻量级的推理投影模块（Reasoning Projector），该模块产生连续的潜在token，帮助模型跳过推理步骤。在强化学习（RL）过程中，策略模型决定何时激活投影器，根据需要切换潜在推理和离散推理。

Result: LiteReason在情节漏洞检测和书籍章节生成任务上超越了现有的潜在推理基线模型，并且性能接近非潜在的RL训练，同时将最终推理长度减少了77-92%。

Conclusion: LiteReason方法能够有效地将强化学习训练引导到性能与计算权衡曲线中更有效的部分，提高了大型语言模型处理复杂任务的效率。

Abstract: Large language models (LLMs) tackle complex tasks by generating long chains of thought or "reasoning traces" that act as latent variables in the generation of an output given a query. A model's ability to generate such traces can be optimized with reinforcement learning (RL) to improve their utility in predicting an answer. This optimization comes at a high computational cost, especially for narrative-related tasks that involve retrieving and processing many tokens. To this end, we propose LiteReason, a latent reasoning method that can be interleaved with standard token sampling and easily combined with RL techniques. LiteReason employs a lightweight Reasoning Projector module, trained to produce continuous latent tokens that help the model 'skip' reasoning steps. During RL, the policy model decides when to activate the projector, switching between latent and discrete reasoning as needed. Experimental results on plot hole detection and book chapter generation show that our method outperforms latent reasoning baselines and comes close to matching non-latent RL training, while reducing final reasoning length by 77-92%. Overall, LiteReason guides RL training to a more efficient part of the performance-computation tradeoff curve.

</details>


### [9] [DETAIL Matters: Measuring the Impact of Prompt Specificity on Reasoning in Large Language Models](https://arxiv.org/abs/2512.02246)
*Olivia Kim*

Main category: cs.CL

TL;DR: 一个用于评估大语言模型在不同prompt特异性水平下表现的框架。


<details>
  <summary>Details</summary>
Motivation: Prompt设计在大语言模型推理性能中起着关键作用，但prompt特异性（prompt的详细或模糊程度）的影响仍未得到充分研究。

Method: 本文介绍了DETAIL框架，用于评估不同prompt特异性水平下的LLM性能。使用GPT-4生成多级prompt，通过困惑度量化特异性，并使用基于GPT的语义等价性评估正确性。

Result: 在GPT-4和O3-mini上对30个新颖推理任务进行的实验表明，特异性可以提高准确性，特别是对于较小的模型和程序性任务。

Conclusion: 实验结果强调了自适应prompt策略的需求，并为进一步研究提供了工具和数据。

Abstract: Prompt design plays a critical role in the reasoning performance of large language models (LLMs), yet the impact of prompt specificity - how detailed or vague a prompt is - remains understudied. This paper introduces DETAIL, a framework for evaluating LLM performance across varying levels of prompt specificity. We generate multi-level prompts using GPT-4, quantify specificity via perplexity, and assess correctness using GPT-based semantic equivalence. Experiments on 30 novel reasoning tasks across GPT-4 and O3-mini reveal that specificity improves accuracy, especially for smaller models and procedural tasks. Our results highlight the need for adaptive prompting strategies and provide tools and data to support further research.

</details>


### [10] [HealthContradict: Evaluating Biomedical Knowledge Conflicts in Language Models](https://arxiv.org/abs/2512.02299)
*Boya Zhang,Alban Bornet,Rui Yang,Nan Liu,Douglas Teodoro*

Main category: cs.CL

TL;DR: 本文探讨了语言模型如何利用上下文信息回答健康问题，并评估了它们在面对矛盾上下文时的表现。


<details>
  <summary>Details</summary>
Motivation: 评估语言模型在面对矛盾上下文时，如何利用上下文信息回答健康问题。

Method: 使用HealthContradict数据集（包含920个健康问题、事实答案和两个矛盾文档），在多种提示设置下（正确、不正确、矛盾上下文）评估语言模型。

Result: HealthContradict数据集能更好地区分语言模型的上下文推理能力。实验表明，经过微调的生物医学语言模型不仅依赖预训练获得的参数知识，还能有效利用正确上下文并抵制不正确上下文。

Conclusion: 语言模型在医疗问答中利用上下文信息的能力是其关键优势，尤其是在处理矛盾信息时。

Abstract: How do language models use contextual information to answer health questions? How are their responses impacted by conflicting contexts? We assess the ability of language models to reason over long, conflicting biomedical contexts using HealthContradict, an expert-verified dataset comprising 920 unique instances, each consisting of a health-related question, a factual answer supported by scientific evidence, and two documents presenting contradictory stances. We consider several prompt settings, including correct, incorrect or contradictory context, and measure their impact on model outputs. Compared to existing medical question-answering evaluation benchmarks, HealthContradict provides greater distinctions of language models' contextual reasoning capabilities. Our experiments show that the strength of fine-tuned biomedical language models lies not only in their parametric knowledge from pretraining, but also in their ability to exploit correct context while resisting incorrect context.

</details>


### [11] [When Does Verification Pay Off? A Closer Look at LLMs as Solution Verifiers](https://arxiv.org/abs/2512.02304)
*Jack Lu,Ryan Teehan,Jinran Jin,Mengye Ren*

Main category: cs.CL

TL;DR: 本文对各种大型语言模型在9个基准测试中作为问题解决者和解决方案验证者的性能进行了系统研究，发现跨家族验证特别有效，后训练会减少自我改进但会增强跨家族改进，并且数学和逻辑任务表现出最高的内在可验证性。


<details>
  <summary>Details</summary>
Motivation: 以前关于求解器-验证器交互的研究有限，主要集中在自我验证，很少检查验证器如何判断来自其自身或另一个模型家族的输出，并且现代大型语言模型也经历了广泛的后训练，但其对验证的影响仍不清楚。

Method: 本文对37个模型进行了系统研究，这些模型涵盖了多个家族、大小以及基础和后训练变体，并在9个涵盖逻辑推理、结构化难题、符号计算、数学、常识、事实回忆和领域知识的基准上进行了评估。此外，引入并凭经验验证了验证器增益（verifier gain），这是一种预测基于测试时验证器拒绝采样性能改进的指标。

Result: 研究发现，跨家族验证特别有效；后训练减少了自我改进，但增强了跨家族改进；数学和逻辑任务表现出最高的内在可验证性。

Conclusion: 跨家族验证是提高大型语言模型性能的有效策略，而后训练在增强跨家族验证方面发挥着关键作用，并且某些任务类型（如数学和逻辑）更适合通过验证机制进行改进。

Abstract: Large language models (LLMs) can act as both problem solvers and solution verifiers, with verifiers improving solver performance by selecting high-quality answers from a pool of candidates. However, prior studies of solver-verifier interactions have been limited, focusing mainly on self-verification and rarely examining how verifiers judge outputs from models in their own or in another model family. Modern LLMs also undergo extensive post-training, but its effect on verification remains unclear. We present a systematic study across 37 models spanning multiple families, sizes, and base vs. post-trained variants, evaluated on 9 benchmarks covering logical reasoning, structured puzzles, symbolic computation, mathematics, commonsense, factual recall, and domain knowledge. We compare self-verification with verification within the same family and across different families. To support this, we introduce and empirically validate verifier gain, a metric that predicts the performance improvements from test-time verifier-based rejection sampling. We analyze how metrics like verifier gain and false positive rate scale with model size and post-training, and characterize differences in dataset verifiability. Our findings show that cross-family verification is especially effective; post-training reduces self-improvement but strengthens cross-family improvement; and mathematical and logical tasks exhibit the highest inherent verifiability.

</details>


### [12] [A Concise Review of Hallucinations in LLMs and their Mitigation](https://arxiv.org/abs/2512.02527)
*Parth Pulkundwar,Vivek Dhanawade,Rohit Yadav,Minal Sonkar,Medha Asurlekar,Sarita Rathod*

Main category: cs.CL

TL;DR: 本文对大型语言模型中幻觉的类型、来源及其缓解方法进行了简洁的概述，以期为读者提供全面的理解。


<details>
  <summary>Details</summary>
Motivation: 传统语言模型中的“幻觉”现象对自然语言处理领域构成了重大挑战，因此，理解幻觉的类型、来源和缓解方法至关重要。

Method: 通过对现有文献的总结和归纳，清晰地阐述了幻觉的发生原因及其解决方案。

Result: 提供了一个关于幻觉的全面性资源，涵盖了其种类、产生根源及减轻策略。

Conclusion: 幻觉是当前语言模型面临的核心问题，通过对本文的阅读，读者可以系统地了解并掌握缓解幻觉的有效途径。

Abstract: Traditional language models face a challenge from hallucinations. Their very presence casts a large, dangerous shadow over the promising realm of natural language processing. It becomes crucial to understand the various kinds of hallucinations that occur nowadays, their origins, and ways of reducing them. This document provides a concise and straightforward summary of that. It serves as a one-stop resource for a general understanding of hallucinations and how to mitigate them.

</details>


### [13] [What Signals Really Matter for Misinformation Tasks? Evaluating Fake-News Detection and Virality Prediction under Real-World Constraints](https://arxiv.org/abs/2512.02552)
*Francesco Paolo Savatteri,Chahan Vidal-Gorène,Florian Cafiero*

Main category: cs.CL

TL;DR: 本文对假新闻检测和病毒性预测两个在线错误信息任务进行了评估驱动研究，比较了文本嵌入和轻量级数字特征，并分析了它们在不同任务中的表现。


<details>
  <summary>Details</summary>
Motivation: 在线错误信息（包括假新闻检测和病毒性预测）在实际操作环境中需要快速反应，因此本文旨在对这两种任务进行评估驱动研究。

Method: 本文使用EVONS和FakeNewsNet数据集，比较了文本嵌入（RoBERTa，并使用Mistral进行对照）与轻量级数字特征（时间、关注者数量、验证情况、点赞数）和序列模型（GRU、门控架构、Transformer编码器）。

Result: 文本内容是假新闻检测的强大判别器，而当语言模型不可用或计算受限时，纯数字管道仍然可行。病毒性预测比假新闻检测更难，并且对标签构建高度敏感。维度降低分析表明，非线性结构对病毒性的信息量大于对假新闻检测的信息量。用Mistral嵌入替换RoBERTa嵌入只会产生适度的变化，不影响结论。

Conclusion: 本文讨论了评估设计的影响和报告的可重复性限制，并提供了度量选择的指导。假新闻检测和病毒性预测任务在不同的特征和模型下表现不同，其中文本内容在假新闻检测中表现出色，而病毒性预测则更具挑战性并受标签构建的影响。

Abstract: We present an evaluation-driven study of two practical tasks regarding online misinformation: (i) fake-news detection and (ii) virality prediction in the context of operational settings, with the necessity for rapid reaction. Using the EVONS and FakeNewsNet datasets, we compare textual embeddings (RoBERTa; with a control using Mistral) against lightweight numeric features (timing, follower counts, verification, likes) and sequence models (GRU, gating architectures, Transformer encoders). We show that textual content alone is a strong discriminator for fake-news detection, while numeric-only pipelines remain viable when language models are unavailable or compute is constrained. Virality prediction is markedly harder than fake-news detection and is highly sensitive to label construction; in our setup, a median-based ''viral'' split (<50 likes) is pragmatic but underestimates real-world virality, and time-censoring for engagement features is desirable yet difficult under current API limits. Dimensionality-reduction analyses suggest non-linear structure is more informative for virality than for fake-news detection (t-SNE > PCA on numeric features). Swapping RoBERTa for Mistral embeddings yields only modest deltas, leaving conclusions unchanged. We discuss implications for evaluation design and report reproducibility constraints that realistically affect the field. We release splits and code where possible and provide guidance for metric selection.

</details>


### [14] [ADORE: Autonomous Domain-Oriented Relevance Engine for E-commerce](https://arxiv.org/abs/2512.02555)
*Zheng Fang,Donghao Xie,Ming Pang,Chunyuan Yuan,Xue Jiang,Changping Peng,Zhangang Lin,Zheng Luo*

Main category: cs.CL

TL;DR: ADORE是一个应对电商搜索相关性建模挑战的框架，它通过LLM生成数据、对抗性样本生成和知识蒸馏，解决了数据稀缺问题，提高了推理能力，并通过实验和在线A/B测试验证了其有效性。


<details>
  <summary>Details</summary>
Motivation: 电商搜索中的相关性建模面临挑战，主要原因是术语匹配方法（如BM25）存在语义鸿沟，以及神经模型依赖稀缺的领域特定硬样本。

Method: ADORE框架包含三个创新模块：1. 规则感知相关性判别模块：利用思维链LLM生成与意图对齐的训练数据，并通过Kahneman-Tversky优化（KTO）与用户行为对齐。2. 错误类型感知数据合成模块：自动生成对抗性样本以增强模型的鲁棒性。3. 关键属性增强知识蒸馏模块：将领域特定的属性层次结构注入到可部署的学生模型中。ADORE实现了自动化标注、对抗性生成和蒸馏。

Result: ADORE克服了数据稀缺问题，增强了模型的推理能力。大规模实验和在线A/B测试验证了ADORE的有效性。

Conclusion: ADORE框架为工业应用中资源高效、认知对齐的相关性建模建立了一种新范式。

Abstract: Relevance modeling in e-commerce search remains challenged by semantic gaps in term-matching methods (e.g., BM25) and neural models' reliance on the scarcity of domain-specific hard samples. We propose ADORE, a self-sustaining framework that synergizes three innovations: (1) A Rule-aware Relevance Discrimination module, where a Chain-of-Thought LLM generates intent-aligned training data, refined via Kahneman-Tversky Optimization (KTO) to align with user behavior; (2) An Error-type-aware Data Synthesis module that auto-generates adversarial examples to harden robustness; and (3) A Key-attribute-enhanced Knowledge Distillation module that injects domain-specific attribute hierarchies into a deployable student model. ADORE automates annotation, adversarial generation, and distillation, overcoming data scarcity while enhancing reasoning. Large-scale experiments and online A/B testing verify the effectiveness of ADORE. The framework establishes a new paradigm for resource-efficient, cognitively aligned relevance modeling in industrial applications.

</details>


### [15] [DeepSeek-V3.2: Pushing the Frontier of Open Large Language Models](https://arxiv.org/abs/2512.02556)
*DeepSeek-AI,Aixin Liu,Aoxue Mei,Bangcai Lin,Bing Xue,Bingxuan Wang,Bingzheng Xu,Bochao Wu,Bowei Zhang,Chaofan Lin,Chen Dong,Chengda Lu,Chenggang Zhao,Chengqi Deng,Chenhao Xu,Chong Ruan,Damai Dai,Daya Guo,Dejian Yang,Deli Chen,Erhang Li,Fangqi Zhou,Fangyun Lin,Fucong Dai,Guangbo Hao,Guanting Chen,Guowei Li,H. Zhang,Hanwei Xu,Hao Li,Haofen Liang,Haoran Wei,Haowei Zhang,Haowen Luo,Haozhe Ji,Honghui Ding,Hongxuan Tang,Huanqi Cao,Huazuo Gao,Hui Qu,Hui Zeng,Jialiang Huang,Jiashi Li,Jiaxin Xu,Jiewen Hu,Jingchang Chen,Jingting Xiang,Jingyang Yuan,Jingyuan Cheng,Jinhua Zhu,Jun Ran,Junguang Jiang,Junjie Qiu,Junlong Li,Junxiao Song,Kai Dong,Kaige Gao,Kang Guan,Kexin Huang,Kexing Zhou,Kezhao Huang,Kuai Yu,Lean Wang,Lecong Zhang,Lei Wang,Liang Zhao,Liangsheng Yin,Lihua Guo,Lingxiao Luo,Linwang Ma,Litong Wang,Liyue Zhang,M. S. Di,M. Y Xu,Mingchuan Zhang,Minghua Zhang,Minghui Tang,Mingxu Zhou,Panpan Huang,Peixin Cong,Peiyi Wang,Qiancheng Wang,Qihao Zhu,Qingyang Li,Qinyu Chen,Qiushi Du,Ruiling Xu,Ruiqi Ge,Ruisong Zhang,Ruizhe Pan,Runji Wang,Runqiu Yin,Runxin Xu,Ruomeng Shen,Ruoyu Zhang,S. H. Liu,Shanghao Lu,Shangyan Zhou,Shanhuang Chen,Shaofei Cai,Shaoyuan Chen,Shengding Hu,Shengyu Liu,Shiqiang Hu,Shirong Ma,Shiyu Wang,Shuiping Yu,Shunfeng Zhou,Shuting Pan,Songyang Zhou,Tao Ni,Tao Yun,Tian Pei,Tian Ye,Tianyuan Yue,Wangding Zeng,Wen Liu,Wenfeng Liang,Wenjie Pang,Wenjing Luo,Wenjun Gao,Wentao Zhang,Xi Gao,Xiangwen Wang,Xiao Bi,Xiaodong Liu,Xiaohan Wang,Xiaokang Chen,Xiaokang Zhang,Xiaotao Nie,Xin Cheng,Xin Liu,Xin Xie,Xingchao Liu,Xingkai Yu,Xingyou Li,Xinyu Yang,Xinyuan Li,Xu Chen,Xuecheng Su,Xuehai Pan,Xuheng Lin,Xuwei Fu,Y. Q. Wang,Yang Zhang,Yanhong Xu,Yanru Ma,Yao Li,Yao Li,Yao Zhao,Yaofeng Sun,Yaohui Wang,Yi Qian,Yi Yu,Yichao Zhang,Yifan Ding,Yifan Shi,Yiliang Xiong,Ying He,Ying Zhou,Yinmin Zhong,Yishi Piao,Yisong Wang,Yixiao Chen,Yixuan Tan,Yixuan Wei,Yiyang Ma,Yiyuan Liu,Yonglun Yang,Yongqiang Guo,Yongtong Wu,Yu Wu,Yuan Cheng,Yuan Ou,Yuanfan Xu,Yuduan Wang,Yue Gong,Yuhan Wu,Yuheng Zou,Yukun Li,Yunfan Xiong,Yuxiang Luo,Yuxiang You,Yuxuan Liu,Yuyang Zhou,Z. F. Wu,Z. Z. Ren,Zehua Zhao,Zehui Ren,Zhangli Sha,Zhe Fu,Zhean Xu,Zhenda Xie,Zhengyan Zhang,Zhewen Hao,Zhibin Gou,Zhicheng Ma,Zhigang Yan,Zhihong Shao,Zhixian Huang,Zhiyu Wu,Zhuoshu Li,Zhuping Zhang,Zian Xu,Zihao Wang,Zihui Gu,Zijia Zhu,Zilin Li,Zipeng Zhang,Ziwei Xie,Ziyi Gao,Zizheng Pan,Zongqing Yao,Bei Feng,Hui Li,J. L. Cai,Jiaqi Ni,Lei Xu,Meng Li,Ning Tian,R. J. Chen,R. L. Jin,S. S. Li,Shuang Zhou,Tianyu Sun,X. Q. Li,Xiangyue Jin,Xiaojin Shen,Xiaosha Chen,Xinnan Song,Xinyi Zhou,Y. X. Zhu,Yanping Huang,Yaohui Li,Yi Zheng,Yuchen Zhu,Yunxian Ma,Zhen Huang,Zhipeng Xu,Zhongyu Zhang,Dongjie Ji,Jian Liang,Jianzhong Guo,Jin Chen,Leyi Xia,Miaojun Wang,Mingming Li,Peng Zhang,Ruyi Chen,Shangmian Sun,Shaoqing Wu,Shengfeng Ye,T. Wang,W. L. Xiao,Wei An,Xianzu Wang,Xiaowen Sun,Xiaoxiang Wang,Ying Tang,Yukun Zha,Zekai Zhang,Zhe Ju,Zhen Zhang,Zihua Qu*

Main category: cs.CL

TL;DR: DeepSeek-V3.2是一个计算效率高、推理和Agent性能卓越的模型，它引入了DeepSeek稀疏注意力机制、可扩展的强化学习框架和大规模Agent任务合成流水线，使其在长上下文处理、性能上与GPT-5相当，甚至在特定版本中超越GPT-5，并在数学和信息学奥赛中取得金牌表现。


<details>
  <summary>Details</summary>
Motivation: 开发一个在计算效率、推理能力和Agent性能方面都表现卓越的模型，并解决长上下文处理中的计算复杂性问题，以及在工具使用场景中集成推理能力。

Method: 1. **DeepSeek稀疏注意力（DSA）**：引入一种高效的注意力机制，显著降低计算复杂性，同时在长上下文场景中保持模型性能。2. **可扩展的强化学习框架**：通过实施强大的强化学习协议并扩展训练后计算，提升模型性能，使其能与GPT-5媲美。3. **大规模Agent任务合成流水线**：开发一种新颖的合成流水线，系统地大规模生成训练数据，以促进可扩展的Agent训练，提高在复杂交互环境中的泛化能力和指令遵循鲁棒性。

Result: 1. DeepSeek-V3.2在计算效率、推理和Agent性能上表现出色。2. DeepSeek稀疏注意力机制在长上下文场景中显著降低计算复杂性的同时保持了模型性能。3. 通过可扩展的强化学习框架，DeepSeek-V3.2的性能可与GPT-5媲美。4. 高计算量的DeepSeek-V3.2-Speciale版本超越了GPT-5，推理能力与Gemini-3.0-Pro相当，并在2025年国际数学奥林匹克（IMO）和国际信息学奥林匹克（IOI）中获得金牌。5. 大规模Agent任务合成流水线显著提高了模型在复杂交互环境中的泛化能力和指令遵循鲁棒性。

Conclusion: DeepSeek-V3.2通过创新的稀疏注意力机制、可扩展的强化学习方法和Agent任务合成流水线，成功实现了计算效率与卓越推理及Agent性能的结合，有望在AI领域，特别是在长上下文理解和复杂任务处理方面，设立新的性能标准。特别是其Speciale版本在奥林匹克竞赛中取得的金牌表现，进一步证明了其强大的能力。

Abstract: We introduce DeepSeek-V3.2, a model that harmonizes high computational efficiency with superior reasoning and agent performance. The key technical breakthroughs of DeepSeek-V3.2 are as follows: (1) DeepSeek Sparse Attention (DSA): We introduce DSA, an efficient attention mechanism that substantially reduces computational complexity while preserving model performance in long-context scenarios. (2) Scalable Reinforcement Learning Framework: By implementing a robust reinforcement learning protocol and scaling post-training compute, DeepSeek-V3.2 performs comparably to GPT-5. Notably, our high-compute variant, DeepSeek-V3.2-Speciale, surpasses GPT-5 and exhibits reasoning proficiency on par with Gemini-3.0-Pro, achieving gold-medal performance in both the 2025 International Mathematical Olympiad (IMO) and the International Olympiad in Informatics (IOI). (3) Large-Scale Agentic Task Synthesis Pipeline: To integrate reasoning into tool-use scenarios, we developed a novel synthesis pipeline that systematically generates training data at scale. This methodology facilitates scalable agentic post-training, yielding substantial improvements in generalization and instruction-following robustness within complex, interactive environments.

</details>


### [16] [From Imitation to Discrimination: Toward A Generalized Curriculum Advantage Mechanism Enhancing Cross-Domain Reasoning Tasks](https://arxiv.org/abs/2512.02580)
*Changpeng Yang,Jinyang Wu,Yuchen Liu,Shuai Zhang,Yang Li,Qiliang Liang,Hongzhen Wang,Shuai Nie,Jiaming Xu,Runyu Shi,Ying Huang,Guoquan Zhang*

Main category: cs.CL

TL;DR: CAPO是一种基于优势信号的自适应课程机制，它通过先用正向优势样本进行模仿学习，然后引入负向信号来提高判别能力，从而提升大型语言模型在数学推理和多模态GUI推理任务中的性能。


<details>
  <summary>Details</summary>
Motivation: 大型语言模型训练中，现有方法将正向和负向信号混合使用，特别是在早期阶段，可能导致模糊的指导和有限的收益

Method: CAPO（Curriculum Advantage Policy Optimization）通过模仿学习，首先利用纯正向优势样本建立稳健的基础，随后引入负向信号以培养判别能力，从而改善复杂场景下的泛化能力。

Result: CAPO在数学推理任务中持续实现稳定且显著的改进，并有效地推广到多模态图形用户界面（GUI）推理场景。

Conclusion: CAPO作为一种通用的、稳健的优化框架，能够兼容多种优化方法，并显著提升LLM在推理任务上的性能。

Abstract: Reinforcement learning has emerged as a paradigm for post-training large language models, boosting their reasoning capabilities. Such approaches compute an advantage value for each sample, reflecting better or worse performance than expected, thereby yielding both positive and negative signals for training. However, the indiscriminate mixing of the two signals in existing methods, especially from the early stages, may lead to ambiguous guidance and limited gains. To address this issue, we propose **CAPO** (**C**urriculum **A**dvantage **P**olicy **O**ptimization), an adaptive curriculum mechanism based on advantage signals. The proposed mechanism bootstraps imitation learning with positive-only advantage samples to establish robust foundations, and subsequently introduces negative signals to cultivate discriminative capabilities, thereby improving generalization across complex scenarios. Compatible with diverse optimization methods including GRPO, PPO, RLOO, and Reinforce++, our method consistently achieves stable and significant improvements in mathematical reasoning tasks, and further generalizes effectively to multimodal Graphical User Interface (GUI) reasoning scenarios, establishing itself as a versatile and robust optimization framework.

</details>


### [17] [Input Order Shapes LLM Semantic Alignment in Multi-Document Summarization](https://arxiv.org/abs/2512.02665)
*Jing Ma*

Main category: cs.CL

TL;DR: 该研究探讨了大型语言模型（LLMs）在处理多文档摘要时是否存在首位效应，发现LLMs更倾向于与第一个输入文档在语义上保持一致，这可能对LLM的应用带来风险。


<details>
  <summary>Details</summary>
Motivation: 探索大型语言模型在多文档摘要中对输入文档的处理方式，特别是它们是否平等地权衡所有输入，以及是否存在首位效应。

Method: 通过构建40个堕胎相关新闻文章三联体（包括支持、中立、反对立场），将每个三联体以六种不同顺序排列，并使用Gemini 2.5 Flash生成中立的概述。使用ROUGE-L、BERTScore和SummaC评估摘要与源文章的一致性。

Result: 单向ANOVA分析显示，在所有立场中，BERTScore都存在显著的首位效应，表明摘要在语义上与第一个看到的文章更一致。成对比较进一步证实，第一个位置与第二、三个位置显著不同，而第二、三个位置之间没有显著差异。

Conclusion: LLMs在生成多文档摘要时存在显著的首位效应，即它们会优先考虑第一个输入文档。这对于依赖LLM生成概述的应用以及Agentic AI系统构成了风险，因为LLM的步骤可能会不成比例地影响后续行动。

Abstract: Large language models (LLMs) are now used in settings such as Google's AI Overviews, where it summarizes multiple long documents. However, it remains unclear whether they weight all inputs equally. Focusing on abortion-related news, we construct 40 pro-neutral-con article triplets, permute each triplet into six input orders, and prompt Gemini 2.5 Flash to generate a neutral overview. We evaluate each summary against its source articles using ROUGE-L (lexical overlap), BERTScore (semantic similarity), and SummaC (factual consistency). One-way ANOVA reveals a significant primacy effect for BERTScore across all stances, indicating that summaries are more semantically aligned with the first-seen article. Pairwise comparisons further show that Position 1 differs significantly from Positions 2 and 3, while the latter two do not differ from each other, confirming a selective preference for the first document. The findings present risks for applications that rely on LLM-generated overviews and for agentic AI systems, where the steps involving LLMs can disproportionately influence downstream actions.

</details>


### [18] [An Empirical Survey of Model Merging Algorithms for Social Bias Mitigation](https://arxiv.org/abs/2512.02689)
*Daiki Shirafuji,Tatsuhiko Saito,Yasutomo Kimura*

Main category: cs.CL

TL;DR: 本文探讨了通过模型合并方法编辑大型语言模型（LLMs）参数以减轻社会偏见，并对七种算法进行了实证比较，发现偏见降低与下游任务性能之间存在权衡。


<details>
  <summary>Details</summary>
Motivation: 大型语言模型（LLMs）继承并可能放大预训练语料库中的社会偏见，这威胁到公平性和社会信任；现有工作探索了“编辑”LLM参数以减轻社会偏见，但缺乏实证比较。

Method: 本文实证调查了七种算法（Linear, Karcher Mean, SLERP, NuSLERP, TIES, DELLA, 和 Nearswap），并将它们应用于GPT, LLaMA, 和 Qwen系列的13个开源模型。评估使用三个偏见数据集（BBQ, BOLD, 和 HONEST）以及SuperGLUE基准测试来衡量对LLM下游任务性能的影响。

Result: 研究发现偏见减少与下游性能之间存在权衡：实现更大偏见缓解的方法会降低准确性，尤其是在需要阅读理解、常识和因果推理的任务上。在合并算法中，Linear, SLERP, 和 Nearswap始终能减少偏见并保持整体性能，其中SLERP在中等插值权重下是平衡性最好的选择。

Conclusion: 模型合并算法在偏见缓解方面具有潜力，但过度去偏见或不当的合并方法可能导致LLMs重要的语言能力下降。

Abstract: Large language models (LLMs) are known to inherit and even amplify societal biases present in their pre-training corpora, threatening fairness and social trust. To address this issue, recent work has explored ``editing'' LLM parameters to mitigate social bias with model merging approaches; however, there is no empirical comparison. In this work, we empirically survey seven algorithms: Linear, Karcher Mean, SLERP, NuSLERP, TIES, DELLA, and Nearswap, applying 13 open weight models in the GPT, LLaMA, and Qwen families. We perform a comprehensive evaluation using three bias datasets (BBQ, BOLD, and HONEST) and measure the impact of these techniques on LLM performance in downstream tasks of the SuperGLUE benchmark. We find a trade-off between bias reduction and downstream performance: methods achieving greater bias mitigation degrade accuracy, particularly on tasks requiring reading comprehension and commonsense and causal reasoning. Among the merging algorithms, Linear, SLERP, and Nearswap consistently reduce bias while maintaining overall performance, with SLERP at moderate interpolation weights emerging as the most balanced choice. These results highlight the potential of model merging algorithms for bias mitigation, while indicating that excessive debiasing or inappropriate merging methods may lead to the degradation of important linguistic abilities.

</details>


### [19] [CREST: Universal Safety Guardrails Through Cluster-Guided Cross-Lingual Transfer](https://arxiv.org/abs/2512.02711)
*Lavish Bansal,Naman Mishra*

Main category: cs.CL

TL;DR: CREST是一个高效的多语言安全分类模型，支持100种语言，参数量仅为0.5B，通过在13种高资源语言的子集上进行训练，实现了跨语言安全传输，并在多项安全基准测试中表现优异。


<details>
  <summary>Details</summary>
Motivation: 现有大语言模型（LLM）的安全防护措施主要针对高资源语言，导致使用低资源语言的人群被忽视，阻碍了LLM在现实世界应用中的部署。

Method: 本文提出了CREST模型，一个参数高效的多语言安全分类模型，支持100种语言，仅包含0.5B参数。该模型通过在一个策略性选择的13种高资源语言子集上进行训练，利用基于聚类的跨语言迁移，从少数语言泛化到100种语言，从而有效推广到未见过的高资源和低资源语言。

Result: CREST在六个安全基准测试中进行了全面评估，结果表明它优于同等规模的现有最先进的安全防护措施，并与参数量显著更大（2.5B及以上）的模型取得了有竞争力的结果。

Conclusion: 语言特定的安全防护措施存在局限性，开发能够有效扩展以服务全球人口的通用、语言无关的安全系统至关重要。

Abstract: Ensuring content safety in large language models (LLMs) is essential for their deployment in real-world applications. However, existing safety guardrails are predominantly tailored for high-resource languages, leaving a significant portion of the world's population underrepresented who communicate in low-resource languages. To address this, we introduce CREST (CRoss-lingual Efficient Safety Transfer), a parameter-efficient multilingual safety classification model that supports 100 languages with only 0.5B parameters. By training on a strategically chosen subset of only 13 high-resource languages, our model utilizes cluster-based cross-lingual transfer from a few to 100 languages, enabling effective generalization to both unseen high-resource and low-resource languages. This approach addresses the challenge of limited training data in low-resource settings. We conduct comprehensive evaluations across six safety benchmarks to demonstrate that CREST outperforms existing state-of-the-art guardrails of comparable scale and achieves competitive results against models with significantly larger parameter counts (2.5B parameters and above). Our findings highlight the limitations of language-specific guardrails and underscore the importance of developing universal, language-agnostic safety systems that can scale effectively to serve global populations.

</details>


### [20] [Emergent Bayesian Behaviour and Optimal Cue Combination in LLMs](https://arxiv.org/abs/2512.02719)
*Julian Ma,Jun Wang,Zafeirios Fountas*

Main category: cs.CL

TL;DR: 这篇论文探讨了大型语言模型（LLMs）在多模态整合任务中，是否像人类一样运用贝叶斯策略进行隐性计算。研究通过引入一个名为 BayesBench 的行为基准，评估了九种 LLMs 在文本和图像上的四种量级估算任务中的表现。结果显示，虽然模型在某些方面表现出与贝叶斯一致的行为，但高准确度并不总是意味着鲁棒的贝叶斯整合能力。


<details>
  <summary>Details</summary>
Motivation: 探索大型语言模型（LLMs）是否像人类一样，在没有明确训练或指示的情况下，通过近乎最优的贝叶斯策略直观地处理和整合噪声信号，尤其是在多模态任务中。

Method: 1. 引入行为基准 BayesBench：包含基于经典心理物理学的四种文本和图像量级估算任务（长度、位置、距离和持续时间）。
2. 评估对象：对九种 LLMs 和人类判断进行评估和校准。
3. 实验控制：通过控制噪声、上下文和指令提示进行消融实验。
4. 评估指标：测量性能、行为和效率，并引入贝叶斯一致性分数（Bayesian Consistency Score）。

Result: 1. 某些能力强的模型表现出贝叶斯一致的行为适应性。
2. 高准确度不保证鲁棒性，例如 GPT-5 Mini 在文本方面准确率很高，但在有效整合视觉线索方面表现不佳。
3. 模型的性能和贝叶斯倾向之间存在相关性。
4. 准确性中心基准可能过分强调性能，而忽略了模型对不确定性的脆弱处理能力。

Conclusion: LLMs 表现出处理不确定性的新兴原则性能力，但准确性与贝叶斯整合策略之间存在解离。高准确度并不意味着模型能像人类一样鲁棒地进行多模态信息整合。本研究发布的基准和一致性度量工具将有助于未来多模态架构的设计。

Abstract: Large language models (LLMs) excel at explicit reasoning, but their implicit computational strategies remain underexplored. Decades of psychophysics research show that humans intuitively process and integrate noisy signals using near-optimal Bayesian strategies in perceptual tasks. We ask whether LLMs exhibit similar behaviour and perform optimal multimodal integration without explicit training or instruction. Adopting the psychophysics paradigm, we infer computational principles of LLMs from systematic behavioural studies. We introduce a behavioural benchmark - BayesBench: four magnitude estimation tasks (length, location, distance, and duration) over text and image, inspired by classic psychophysics, and evaluate a diverse set of nine LLMs alongside human judgments for calibration. Through controlled ablations of noise, context, and instruction prompts, we measure performance, behaviour and efficiency in multimodal cue-combination. Beyond accuracy and efficiency metrics, we introduce a Bayesian Consistency Score that detects Bayes-consistent behavioural shifts even when accuracy saturates. Our results show that while capable models often adapt in Bayes-consistent ways, accuracy does not guarantee robustness. Notably, GPT-5 Mini achieves perfect text accuracy but fails to integrate visual cues efficiently. This reveals a critical dissociation between capability and strategy, suggesting accuracy-centric benchmarks may over-index on performance while missing brittle uncertainty handling. These findings reveal emergent principled handling of uncertainty and highlight the correlation between accuracy and Bayesian tendencies. We release our psychophysics benchmark and consistency metric (https://bayes-bench.github.io) as evaluation tools and to inform future multimodal architecture designs.

</details>


### [21] [PEFT-Factory: Unified Parameter-Efficient Fine-Tuning of Autoregressive Large Language Models](https://arxiv.org/abs/2512.02764)
*Robert Belanec,Ivan Srba,Maria Bielikova*

Main category: cs.CL

TL;DR: PEFT-Factory是一个统一框架，用于高效微调LLM，支持即用型和自定义PEFT方法，提供19种PEFT方法、27个数据集和评估指标，旨在提高PEFT方法的可复现性和基准测试。


<details>
  <summary>Details</summary>
Motivation: 解决当前Parameter-Efficient Fine-Tuning (PEFT) 方法难以复现、部署和相互比较的挑战。

Method: 开发了一个名为PEFT-Factory的统一框架，它具有模块化设计以支持可扩展性，并原生集成了19种代表性的PEFT方法、27个分类和文本生成数据集（涵盖12项任务）以及标准和PEFT特定的评估指标。该框架源自LLaMA-Factory。

Result: PEFT-Factory提供了一个即用型、受控且稳定的环境，显著提高了PEFT方法的可复现性和基准测试能力。

Conclusion: PEFT-Factory通过提供一个统一、全面的框架，有效解决了PEFT方法在复现、部署和比较方面的难题，为研究人员和开发者提供了一个强大的工具。

Abstract: Parameter-Efficient Fine-Tuning (PEFT) methods address the increasing size of Large Language Models (LLMs). Currently, many newly introduced PEFT methods are challenging to replicate, deploy, or compare with one another. To address this, we introduce PEFT-Factory, a unified framework for efficient fine-tuning LLMs using both off-the-shelf and custom PEFT methods. While its modular design supports extensibility, it natively provides a representative set of 19 PEFT methods, 27 classification and text generation datasets addressing 12 tasks, and both standard and PEFT-specific evaluation metrics. As a result, PEFT-Factory provides a ready-to-use, controlled, and stable environment, improving replicability and benchmarking of PEFT methods. PEFT-Factory is a downstream framework that originates from the popular LLaMA-Factory, and is publicly available at https://github.com/kinit-sk/PEFT-Factory

</details>


### [22] [Towards Unification of Hallucination Detection and Fact Verification for Large Language Models](https://arxiv.org/abs/2512.02772)
*Weihang Su,Jianming Long,Changyue Wang,Shiyu Lin,Jingyan Xu,Ziyi Ye,Qingyao Ai,Yiqun Liu*

Main category: cs.CL

TL;DR: 本文介绍了UniFact，一个统一的评估框架，用于比较大型语言模型中的幻觉检测（HD）和事实核查（FV），发现两者互补，并提出混合方法性能更优，呼吁HD和FV的统一研究范式，以解决模型幻觉问题。


<details>
  <summary>Details</summary>
Motivation: 大型语言模型（LLMs）经常出现幻觉，生成流畅但事实不正确的内容，这损害了信任并阻碍了其在实际应用中的采用。存在两种不同的研究范式：以模型为中心的幻觉检测（HD）和以文本为中心的事实核查（FV），但它们各自独立发展，这阻碍了它们的集体进展。

Method: 本文引入了UniFact，一个统一的评估框架，通过动态生成模型输出和相应的事实性标签，实现FV和HD之间直接的、实例级别的比较。

Result: 通过对多个LLM系列和检测方法进行大规模实验，本文揭示了三个关键发现：（1）没有一种范式是普遍优越的；（2）HD和FV捕捉到事实错误互补的方面；（3）结合两种方法的混合方法始终能达到最先进的性能。

Conclusion: 本文首次深入分析了FV和HD分歧的原因，并提供了支持其统一的经验证据。全面的实验结果呼吁制定一个新的、综合的研究议程，以统一LLM中的幻觉检测和事实核查。

Abstract: Large Language Models (LLMs) frequently exhibit hallucinations, generating content that appears fluent and coherent but is factually incorrect. Such errors undermine trust and hinder their adoption in real-world applications. To address this challenge, two distinct research paradigms have emerged: model-centric Hallucination Detection (HD) and text-centric Fact Verification (FV). Despite sharing the same goal, these paradigms have evolved in isolation, using distinct assumptions, datasets, and evaluation protocols. This separation has created a research schism that hinders their collective progress. In this work, we take a decisive step toward bridging this divide. We introduce UniFact, a unified evaluation framework that enables direct, instance-level comparison between FV and HD by dynamically generating model outputs and corresponding factuality labels. Through large-scale experiments across multiple LLM families and detection methods, we reveal three key findings: (1) No paradigm is universally superior; (2) HD and FV capture complementary facets of factual errors; and (3) hybrid approaches that integrate both methods consistently achieve state-of-the-art performance. Beyond benchmarking, we provide the first in-depth analysis of why FV and HD diverged, as well as empirical evidence supporting the need for their unification. The comprehensive experimental results call for a new, integrated research agenda toward unifying Hallucination Detection and Fact Verification in LLMs.
  We have open-sourced all the code, data, and baseline implementation at: https://github.com/oneal2000/UniFact/

</details>


### [23] [TriLex: A Framework for Multilingual Sentiment Analysis in Low-Resource South African Languages](https://arxiv.org/abs/2512.02799)
*Mike Nkongolo,Hilton Vorster,Josh Warren,Trevor Naick,Deandre Vanmali,Masana Mashapha,Luke Brand,Alyssa Fernandes,Janco Calitz,Sibusiso Makhoba*

Main category: cs.CL

TL;DR: 该研究提出了TriLex，一个三阶段检索增强框架，用于系统地扩展低资源语言的情感词典，并评估了两种非洲预训练语言模型（AfroXLMR和AfriBERTa）在扩展词典上的性能。


<details>
  <summary>Details</summary>
Motivation: 低资源非洲语言在情感分析中代表性不足，限制了词汇覆盖和多语言自然语言处理系统的性能。

Method: 该研究提出了TriLex，一个三阶段检索增强框架，它统一了基于语料库的提取、跨语言映射和检索增强生成（RAG）驱动的词汇完善，以系统地扩展低资源语言的情感词典。然后，使用丰富的词典，评估了两种 प्रमुख 的非洲预训练语言模型（AfroXLMR 和 AfriBERTa）在多个案例研究中的性能。

Result: 结果表明，AfroXLMR 表现优异，isiXhosa 和 isiZulu 的 F1 分数均达到 80% 以上，并表现出强大的跨语言稳定性。尽管 AfriBERTa 缺乏对这些目标语言的预训练，但仍取得了约 64% 的可靠 F1 分数。两种模型都优于传统的机器学习基线，并且集成分析进一步提高了精度和鲁棒性。

Conclusion: 研究结果确立了 TriLex 作为一个可扩展且有效的框架，可用于低资源南非语言的多语言情感词典扩展和情感建模。

Abstract: Low-resource African languages remain underrepresented in sentiment analysis, limiting both lexical coverage and the performance of multilingual Natural Language Processing (NLP) systems. This study proposes TriLex, a three-stage retrieval augmented framework that unifies corpus-based extraction, cross lingual mapping, and retrieval augmented generation (RAG) driven lexical refinement to systematically expand sentiment lexicons for low-resource languages. Using the enriched lexicon, the performance of two prominent African pretrained language models (AfroXLMR and AfriBERTa) is evaluated across multiple case studies. Results demonstrate that AfroXLMR delivers superior performance, achieving F1-scores above 80% for isiXhosa and isiZulu and exhibiting strong cross-lingual stability. Although AfriBERTa lacks pre-training on these target languages, it still achieves reliable F1-scores around 64%, validating its utility in computationally constrained settings. Both models outperform traditional machine learning baselines, and ensemble analyses further enhance precision and robustness. The findings establish TriLex as a scalable and effective framework for multilingual sentiment lexicon expansion and sentiment modeling in low-resource South African languages.

</details>


### [24] [SR-GRPO: Stable Rank as an Intrinsic Geometric Reward for Large Language Model Alignment](https://arxiv.org/abs/2512.02807)
*Yixuan Tang,Yi Yang*

Main category: cs.CL

TL;DR: 本文提出了一种名为“稳定秩”的内在、无需标注的质量信号，用于对大型语言模型（LLMs）进行对齐。稳定秩通过衡量隐藏状态的有效维度来捕获模型质量，并在实验中表现出卓越的性能。基于稳定秩，我们引入了SR-GRPO，一种无需外部监督的强化学习方法，该方法在多个任务上显著提高了LLMs的性能，证明了从模型内部几何结构中提取质量信号的可行性。


<details>
  <summary>Details</summary>
Motivation: 目前LLMs与人类偏好的对齐主要依赖外部监督，但这种方法存在诸多限制：人工标注稀缺且主观，奖励模型易受奖励Hacking影响，自评估方法则面临提示敏感性和偏差问题。

Method: 本文提出“稳定秩”作为一种内在的、无需标注的质量信号，通过计算总方差与主方向方差之比来衡量隐藏状态的有效维度，从而捕获模型质量。基于稳定秩，本文引入了Stable Rank Group Relative Policy Optimization (SR-GRPO) 方法，将稳定秩作为强化学习的奖励信号。

Result: 实验结果显示，稳定秩在RewardBench上达到了84.04%的准确率，并通过Best-of-N采样将贪婪解码的任务准确率平均提高了11.3个百分点。SR-GRPO在没有外部监督的情况下，在STEM任务上将Qwen2.5-1.5B-Instruct的性能提高了10%，在数学推理任务上提高了19%，优于学习型奖励模型和自评估基线。

Conclusion: 本研究表明，可以从模型内部的几何结构中提取质量信号，为实现无需外部监督的可扩展对齐提供了一条新途径。

Abstract: Aligning Large Language Models (LLMs) with human preferences typically relies on external supervision, which faces critical limitations: human annotations are scarce and subjective, reward models are vulnerable to reward hacking, and self-evaluation methods suffer from prompt sensitivity and biases. In this work, we propose stable rank, an intrinsic, annotation-free quality signal derived from model representations. Stable rank measures the effective dimensionality of hidden states by computing the ratio of total variance to dominant-direction variance, capturing quality through how information distributes across representation dimensions. Empirically, stable rank achieves 84.04% accuracy on RewardBench and improves task accuracy by an average of 11.3 percentage points over greedy decoding via Best-of-N sampling. Leveraging this insight, we introduce Stable Rank Group Relative Policy Optimization (SR-GRPO), which uses stable rank as a reward signal for reinforcement learning. Without external supervision, SR-GRPO improves Qwen2.5-1.5B-Instruct by 10% on STEM and 19% on mathematical reasoning, outperforming both learned reward models and self-evaluation baselines. Our findings demonstrate that quality signals can be extracted from internal model geometry, offering a path toward scalable alignment without external supervision.

</details>


### [25] [A benchmark dataset for evaluating Syndrome Differentiation and Treatment in large language models](https://arxiv.org/abs/2512.02816)
*Kunning Li,Jianbin Guo,Zhaoyang Shang,Yiqing Liu,Hongmin Du,Lingling Liu,Yuping Zhao,Lifeng Dong*

Main category: cs.CL

TL;DR: 该论文提出了TCM-BEST4SDT，这是一个全面的、基于临床病例的基准测试，旨在评估大型语言模型在中医药（TCM）“辨证论治”（SDT）方面的临床应用能力。


<details>
  <summary>Details</summary>
Motivation: 现有基准测试存在局限性，未能充分评估治疗决策能力，即中医药辨证论治的个性化、整体性和多样性，需要一种新的、全面的评估方法。

Method: 通过中医药专家主导的临床病例，构建一个名为TCM-BEST4SDT的基准测试。该基准包含中医药基础知识、医疗伦理、法学内容安全和SDT这四个任务，并结合了专家评估、评审模型评估和奖励模型评估三种机制。引入专门的奖励模型来量化处方-证候一致性，并利用严格的数据注释流程。

Result: 通过对15个主流大语言模型（包括通用和中医药领域模型）的实验，验证了TCM-BEST4SDT的有效性。

Conclusion: TCM-BEST4SDT基准提供了一个评估大语言模型在中医药领域临床应用能力的全面框架，可以促进智能中医药研究的发展。

Abstract: The emergence of Large Language Models (LLMs) within the Traditional Chinese Medicine (TCM) domain presents an urgent need to assess their clinical application capabilities. However, such evaluations are challenged by the individualized, holistic, and diverse nature of TCM's "Syndrome Differentiation and Treatment" (SDT). Existing benchmarks are confined to knowledge-based question-answering or the accuracy of syndrome differentiation, often neglecting assessment of treatment decision-making. Here, we propose a comprehensive, clinical case-based benchmark spearheaded by TCM experts, and a specialized reward model employed to quantify prescription-syndrome congruence. Data annotation follows a rigorous pipeline. This benchmark, designated TCM-BEST4SDT, encompasses four tasks, including TCM Basic Knowledge, Medical Ethics, LLM Content Safety, and SDT. The evaluation framework integrates three mechanisms, namely selected-response evaluation, judge model evaluation, and reward model evaluation. The effectiveness of TCM-BEST4SDT was corroborated through experiments on 15 mainstream LLMs, spanning both general and TCM domains. To foster the development of intelligent TCM research, TCM-BEST4SDT is now publicly available.

</details>


### [26] [BOOM: Beyond Only One Modality KIT's Multimodal Multilingual Lecture Companion](https://arxiv.org/abs/2512.02817)
*Sai Koneru,Fabian Retkowski,Christian Huber,Lukas Hilgert,Seymanur Akti,Enes Yavuz Ugan,Alexander Waibel,Jan Niehues*

Main category: cs.CL

TL;DR: BOOM是一个多模态多语言讲座伴侣，可以联合翻译讲座音频和幻灯片，生成三种模态的同步输出：翻译文本、保留视觉元素的本地化幻灯片和合成语音。


<details>
  <summary>Details</summary>
Motivation: 本地化教育内容是一个关键挑战，因为讲座材料本质上是多模态的，结合了口语音频和视觉幻灯片，需要能够处理多种输入模态的系统。

Method: BOOM通过联合翻译讲座音频和幻灯片，生成翻译文本、保留视觉元素的本地化幻灯片和合成语音三种同步输出。

Result: BOOM使学生能够以其母语获取讲座，同时旨在完整保留原始内容。实验表明，幻灯片感知转录还为摘要和问答等下游任务带来了连锁 M。

Conclusion: BOOM是一个端到端的方法，可以帮助学生以母语访问讲座，同时保留原始内容的完整性。

Abstract: The globalization of education and rapid growth of online learning have made localizing educational content a critical challenge. Lecture materials are inherently multimodal, combining spoken audio with visual slides, which requires systems capable of processing multiple input modalities. To provide an accessible and complete learning experience, translations must preserve all modalities: text for reading, slides for visual understanding, and speech for auditory learning. We present \textbf{BOOM}, a multimodal multilingual lecture companion that jointly translates lecture audio and slides to produce synchronized outputs across three modalities: translated text, localized slides with preserved visual elements, and synthesized speech. This end-to-end approach enables students to access lectures in their native language while aiming to preserve the original content in its entirety. Our experiments demonstrate that slide-aware transcripts also yield cascading benefits for downstream tasks such as summarization and question answering. We release our Slide Translation code at https://github.com/saikoneru/image-translator and integrate it in Lecture Translator at https://gitlab.kit.edu/kit/isl-ai4lt/lt-middleware/ltpipeline}\footnote{All released code and models are licensed under the MIT License.

</details>


### [27] [promptolution: A Unified, Modular Framework for Prompt Optimization](https://arxiv.org/abs/2512.02840)
*Tom Zehle,Timo Heiß,Moritz Schlager,Matthias Aßenmacher,Matthias Feurer*

Main category: cs.CL

TL;DR: 介绍了Promptolution，一个统一且模块化的开源框架，用于prompt优化。


<details>
  <summary>Details</summary>
Motivation: 大型语言模型（LLMs）的Prompt优化对于提高各种任务的性能至关重要。尽管有许多研究论文显示其有效性，但现有的实现通常与未维护和独立的科研代码库绑定，这阻碍了其实际应用。

Method: 设计并实现了一个名为Promptolution的统一且模块化的开源框架。该框架在一个可扩展的系统中提供了进行Prompt优化所需的所有组件，并且集成了多个现代离散Prompt优化器。

Result: Promptolution框架使得实践者和研究人员可以在一个统一的系统中使用各种Prompt优化组件。它对底层的LLM实现保持不可知性，提高了灵活性和兼容性。

Conclusion: Promptolution框架有效地解决了现有Prompt优化实现分散、难以维护和集成的痛点，为Prompt优化提供了一个统一、模块化、开源且易于扩展的解决方案。

Abstract: Prompt optimization has become crucial for enhancing the performance of large language models (LLMs) across a broad range of tasks. Although many research papers show its effectiveness, practical adoption is hindered as existing implementations are often tied to unmaintained and isolated research codebases. To address this, we introduce promptolution, a unified and modular open-source framework that provides all components required for prompt optimization within a single extensible system for both practitioners and researchers. It integrates multiple contemporary discrete prompt optimizers while remaining agnostic to the underlying LLM implementation.

</details>


### [28] [Cross-Lingual Prompt Steerability: Towards Accurate and Robust LLM Behavior across Languages](https://arxiv.org/abs/2512.02841)
*Lechen Zhang,Yusheng Zhou,Tolga Ergen,Lajanugen Logeswaran,Moontae Lee,David Jurgens*

Main category: cs.CL

TL;DR: 这篇论文研究了系统提示如何在多语言环境中引导大型语言模型（LLMs）实现准确和鲁棒的跨语言行为，并提出了一个统一的四维评估框架来评估系统提示。


<details>
  <summary>Details</summary>
Motivation: 以往的工作主要关注于英语环境，然而在实际应用中，需要一个单一的提示语能够在多种语言中可靠地运行。

Method: 通过在五种语言、三个LLM和三个基准测试上进行大规模实验，并分析了超过1000万个推理单元，提出了一个统一的四维评估框架来评估系统提示。

Result: CoT、情感和场景等提示组件与鲁棒的多语言行为相关。论文开发了一个多语言设置下的提示优化框架，能够自动发现将所有指标提高5-10%的提示。表现更好的系统提示能带来更结构化和一致的推理模式，并减少不必要的语言切换。

Conclusion: 系统提示优化是实现准确和鲁棒的多语言LLM行为的可扩展途径。

Abstract: System prompts provide a lightweight yet powerful mechanism for conditioning large language models (LLMs) at inference time. While prior work has focused on English-only settings, real-world deployments benefit from having a single prompt to operate reliably across languages. This paper presents a comprehensive study of how different system prompts steer models toward accurate and robust cross-lingual behavior. We propose a unified four-dimensional evaluation framework to assess system prompts in multilingual environments. Through large-scale experiments on five languages, three LLMs, and three benchmarks, we uncover that certain prompt components, such as CoT, emotion, and scenario, correlate with robust multilingual behavior. We develop a prompt optimization framework for multilingual settings and show it can automatically discover prompts that improve all metrics by 5-10%. Finally, we analyze over 10 million reasoning units and find that more performant system prompts induce more structured and consistent reasoning patterns, while reducing unnecessary language-switching. Together, we highlight system prompt optimization as a scalable path to accurate and robust multilingual LLM behavior.

</details>


### [29] [Bangla Hate Speech Classification with Fine-tuned Transformer Models](https://arxiv.org/abs/2512.02845)
*Yalda Keivan Jafari,Krishno Dey*

Main category: cs.CL

TL;DR: 本文探讨了孟加拉语仇恨言论识别问题，并比较了传统机器学习模型和基于Transformer的模型，发现BanglaBERT表现最佳。


<details>
  <summary>Details</summary>
Motivation: 由于数据集不足、拼写异构性和语言多样性，低资源语言中的仇恨言论识别仍然是一个难题。孟加拉语拥有超过2.3亿使用者，但在计算资源方面代表性严重不足，急需社交媒体平台上的自动化内容审核。

Method: 本文研究了BLP 2025仇恨言论检测共享任务的Subtask 1A和Subtask 1B。我们复现了官方基线模型（如Majority、Random、支持向量机），并引入了逻辑回归、随机森林和决策树作为基线方法。此外，我们还使用了基于Transformer的模型进行仇恨言论分类，包括DistilBERT、BanglaBERT、m-BERT和XLM-RoBERTa。

Result: 除DistilBERT外，所有基于Transformer的模型在子任务上的表现均优于基线方法。在基于Transformer的模型中，BanglaBERT在两个子任务中均表现出最佳性能。尽管BanglaBERT模型规模较小，但其性能优于m-BERT和XLM-RoBERTa。

Conclusion: 语言特定的预训练对于低资源孟加拉语的仇恨言论检测非常重要，BanglaBERT模型的优异表现证明了预训练语言模型在该领域的潜力和必要性。

Abstract: Hate speech recognition in low-resource lan- guages remains a difficult problem due to in- sufficient datasets, orthographic heterogeneity, and linguistic variety. Bangla is spoken by more than 230 million people of Bangladesh and India (West Bengal). Despite the grow- ing need for automated moderation on social media platforms, Bangla is significantly under- represented in computational resources. In this work, we study Subtask 1A and Subtask 1B of the BLP 2025 Shared Task on hate speech detection. We reproduce the official base- lines (e.g., Majority, Random, Support Vec- tor Machine) and also produce and consider Logistic Regression, Random Forest, and De- cision Tree as baseline methods. We also uti- lized transformer-based models such as Dis- tilBERT, BanglaBERT, m-BERT, and XLM- RoBERTa for hate speech classification. All the transformer-based models outperformed base- line methods for the subtasks, except for Distil- BERT. Among the transformer-based models, BanglaBERT produces the best performance for both subtasks. Despite being smaller in size, BanglaBERT outperforms both m-BERT and XLM-RoBERTa, which suggests language- specific pre-training is very important. Our results highlight the potential and need for pre- trained language models for the low-resource Bangla language.

</details>


### [30] [Graphing the Truth: Structured Visualizations for Automated Hallucination Detection in LLMs](https://arxiv.org/abs/2512.00663)
*Tanmay Agrawal*

Main category: cs.CL

TL;DR: 这篇论文提出了一个框架，该框架将专有知识和模型生成的内容组织成交互式可视化知识图谱，旨在通过将模型断言链接到潜在的事实来源并指示置信水平，为最终用户提供一个清晰、直观的潜在幻觉区域视图。


<details>
  <summary>Details</summary>
Motivation: 大型语言模型在解释和生成自然语言方面的能力迅速提高。在企业环境中，它们经常通过闭源领域知识进行增强，以提供更具上下文信息的响应。然而，操作约束（如有限的上下文窗口）以及预训练数据与所提供知识之间的不一致性常常导致幻觉，其中一些幻觉看起来高度可信，并逃避了常规的人工审查。当前的缓解策略要么依赖于昂贵的大规模金标准问答策划，要么依赖于二级模型验证，两者都不能提供确定性保证。

Method: 将专有知识和模型生成的内容组织成交互式可视化知识图谱。通过将模型断言链接到潜在的事实来源并指示置信水平，为最终用户提供一个清晰、直观的潜在幻觉区域视图。

Result: 用户可以通过这个可视化界面诊断不一致性，识别薄弱的推理链，并提供纠正反馈。

Conclusion: 由此产生的人在回路工作流创建了一个结构化的反馈循环，可以增强模型的可靠性并持续提高响应质量。

Abstract: Large Language Models have rapidly advanced in their ability to interpret and generate natural language. In enterprise settings, they are frequently augmented with closed-source domain knowledge to deliver more contextually informed responses. However, operational constraints such as limited context windows and inconsistencies between pre-training data and supplied knowledge often lead to hallucinations, some of which appear highly credible and escape routine human review. Current mitigation strategies either depend on costly, large-scale gold-standard Q\&A curation or rely on secondary model verification, neither of which offers deterministic assurance. This paper introduces a framework that organizes proprietary knowledge and model-generated content into interactive visual knowledge graphs. The objective is to provide end users with a clear, intuitive view of potential hallucination zones by linking model assertions to underlying sources of truth and indicating confidence levels. Through this visual interface, users can diagnose inconsistencies, identify weak reasoning chains, and supply corrective feedback. The resulting human-in-the-loop workflow creates a structured feedback loop that can enhance model reliability and continuously improve response quality.

</details>


### [31] [Fast-Decoding Diffusion Language Models via Progress-Aware Confidence Schedules](https://arxiv.org/abs/2512.02892)
*Amr Mohamed,Yang Zhang,Michalis Vazirgiannis,Guokan Shang*

Main category: cs.CL

TL;DR: SchED是一种无需训练、与模型无关的提前退出算法，通过聚合全范围logits边际并在达到平滑的、依赖进度的置信度阈值时停止解码，从而显著加速扩散大语言模型（dLLMs）的采样过程。


<details>
  <summary>Details</summary>
Motivation: 扩散大语言模型（dLLMs）虽然是自回归模型的一个有前途的替代方案，但其实用性受到缓慢的迭代采样过程的严重阻碍。

Method: 本文提出了一种名为SchED的提前退出算法。该算法无需训练，与具体模型无关，通过聚合全范围logits（逻辑单元输出）的边际，并在满足一个平滑的、依赖于解码进度的置信度阈值时停止解码。

Result: SchED在指令微调模型上实现了3.8-4.0倍的加速，同时平均保留了99.8-100%的基线分数。在基础模型上，SchED也带来了持续的加速增益，性能保留率为99.1-100%，在更激进的设置下最高可达2.34倍。在对质量损失惩罚很高的保守速度度量（QPS，γ=4）下，SchED表现出鲁棒性，并明显优于现有的基于置信度的提前退出方法，后者在长文本生成方面表现不佳。对模型token预测的熵分析表明，指令微调加速了预测熵的衰减。

Conclusion: SchED通过将真正的置信度稳定转化为计算节省，显著提高了dLLM解码的效率。

Abstract: Diffusion large language models (dLLMs) offer a promising alternative to autoregressive models, but their practical utility is severely hampered by slow, iterative sampling. We present SchED, a training-free, model-agnostic early-exit algorithm that aggregates full-span logit margins and halts decoding once a smooth, progress-dependent confidence threshold is met. We evaluated SchED on two dLLM families (Dream and LLaDA), in base and instruction-tuned variants across ten benchmarks spanning downstream tasks including multiple-choice question answering (MCQ), math, long-form QA/summarization, and translation. SchED delivers large, stable accelerations: on instruction-tuned models, it achieves $3.8$-$4.0\times$ speedups while retaining $99.8$-$100\%$ of the baseline score on average. On base models, SchED yields consistent speedup gains with $99.1$-$100\%$ performance retention, with up to $2.34\times$ under more aggressive settings. Using a conservative speed metric that heavily penalizes quality loss (QPS, $γ{=}4$), we show that SchED is robust and clearly outperforms prior confidence-based early-exit methods, which break down on long-form generation. An entropy analysis of the model's token predictions reveals that instruction tuning speeds up the decay of predictive entropy. By turning genuine confidence stabilization into computational savings, SchED makes dLLM decoding substantially more efficient.

</details>


### [32] [Fine-Tuned Large Language Models for Logical Translation: Reducing Hallucinations with Lang2Logic](https://arxiv.org/abs/2512.02987)
*Muyu Pan,Dheeraj Kodakandla,Mahfuza Farooque*

Main category: cs.CL

TL;DR: 这篇论文介绍了一个将自然语言陈述自动翻译成形式逻辑的框架，旨在减少大型语言模型在逻辑翻译任务中的幻觉问题，并通过实验证明了其有效性。


<details>
  <summary>Details</summary>
Motivation: 大型语言模型在自然语言处理方面取得了显著进展，但其在逻辑翻译任务中易产生“幻觉”（不准确的输出），这促使开发一个能自动将自然语言陈述翻译成形式逻辑的框架，以期在软件系统中实现自动化推理、调试、寻找循环不变式以及遵守规范。

Method: 本框架将英文句子作为输入，首先将其转换为逻辑表达式，然后翻译成合取范式（CNF）以进行可满足性求解。它结合了经典的自然语言处理技术、自定义语法、符号计算库以及一个经过微调的语言模型，以减少幻觉。

Result: 早期实验表明，在不同语法设置下训练的微调模型能够有意纠正原始模型产生的相同类型的幻觉。

Conclusion: 该框架能够提供可靠的CNF生成，有效解决了大型语言模型在逻辑翻译中幻觉问题。

Abstract: Recent advances in natural language processing (NLP), particularly large language models (LLMs), have motivated the automatic translation of natural language statements into formal logic without human intervention. This enables automated reasoning and facilitates debugging, finding loop invariants, and adhering to specifications in software systems. However, hallucinations-incorrect outputs generated by LLMs are challenging, particularly for logical translation tasks requiring precision. This work introduces a novel framework that inputs English sentences, converts them into logical expressions, and then translates them into Conjunctive Normal Form (CNF) for satisfiability solving. It employs classical NLP techniques with self-defined grammar, symbolic computation libraries, and a fine-tuned language model to reduce hallucinations. In the early experiments, we observed that the fine-tuned model, trained on different grammar settings, could intentionally correct the same types of hallucinations made by the original model. Thus, it provides reliable CNF generation.

</details>


### [33] [The Moral Consistency Pipeline: Continuous Ethical Evaluation for Large Language Models](https://arxiv.org/abs/2512.03026)
*Saeid Jamshidi,Kawser Wazed Nafi,Arghavan Moradi Dakhel,Negar Shahabi,Foutse Khomh*

Main category: cs.CL

TL;DR: 该研究介绍了MoCoP，这是一个大规模语言模型的道德一致性评估框架，它能够在没有外部监督的情况下连续评估和解释模型的道德稳定性。


<details>
  <summary>Details</summary>
Motivation: 大型语言模型（LLMs）的快速发展和适应性，凸显了道德一致性的重要性，即在不同背景下保持伦理推理连贯性的能力。现有的对齐框架通常依赖静态数据集和事后评估，对伦理推理在不同背景或时间尺度下如何演变提供的见解有限。

Method: 本文提出了道德一致性管道（MoCoP），这是一个无数据集、闭环的框架，用于持续评估和解释LLMs的道德稳定性。MoCoP结合了三个支持层：（i）词汇完整性分析，（ii）语义风险估计，以及（iii）在自我维持架构中基于推理的判断模型，该架构自主生成、评估和完善伦理场景，无需外部监督。

Result: GPT-4-Turbo和DeepSeek的实证结果表明，MoCoP有效地捕捉了LMs的纵向伦理行为，揭示了伦理与毒性维度之间存在强烈的负相关关系（相关系数rET = -0.81，p值小于0.001），与响应延迟的关联接近于零（相关系数rEL约等于0）。这些发现表明，道德连贯性和语言安全性倾向于作为模型行为的稳定和可解释特征出现，而非短期波动。

Conclusion: MoCoP将伦理评估重新定义为一种动态的、模型不可知的道德反省形式，为可扩展的持续审计提供了可复制的基础，并推动了自主人工智能系统中计算道德的研究。

Abstract: The rapid advancement and adaptability of Large Language Models (LLMs) highlight the need for moral consistency, the capacity to maintain ethically coherent reasoning across varied contexts. Existing alignment frameworks, structured approaches designed to align model behavior with human ethical and social norms, often rely on static datasets and post-hoc evaluations, offering limited insight into how ethical reasoning may evolve across different contexts or temporal scales. This study presents the Moral Consistency Pipeline (MoCoP), a dataset-free, closed-loop framework for continuously evaluating and interpreting the moral stability of LLMs. MoCoP combines three supporting layers: (i) lexical integrity analysis, (ii) semantic risk estimation, and (iii) reasoning-based judgment modeling within a self-sustaining architecture that autonomously generates, evaluates, and refines ethical scenarios without external supervision. Our empirical results on GPT-4-Turbo and DeepSeek suggest that MoCoP effectively captures longitudinal ethical behavior, revealing a strong inverse relationship between ethical and toxicity dimensions (correlation rET = -0.81, p value less than 0.001) and a near-zero association with response latency (correlation rEL approximately equal to 0). These findings demonstrate that moral coherence and linguistic safety tend to emerge as stable and interpretable characteristics of model behavior rather than short-term fluctuations. Furthermore, by reframing ethical evaluation as a dynamic, model-agnostic form of moral introspection, MoCoP offers a reproducible foundation for scalable, continuous auditing and advances the study of computational morality in autonomous AI systems.

</details>


<div id='cs.MA'></div>

# cs.MA [[Back]](#toc)

### [34] [Decentralized Multi-Agent System with Trust-Aware Communication](https://arxiv.org/abs/2512.02410)
*Yepeng Ding,Ahmed Twabi,Junwei Yu,Lingfeng Zhang,Tohru Kondo,Hiroyuki Sato*

Main category: cs.MA

TL;DR: 这篇文章介绍了一个去中心化的多智能体系统（DMAS）架构，解决了传统中心化系统的单点故障、审查S、可扩展L和信任问题。


<details>
  <summary>Details</summary>
Motivation: 传统的中心化多智能体系统（MAS）架构存在单点故障、易受审查、固有的可扩展性限制和关键的信任问题，而大型语言模型（LLMs）的出现加速了自主多智能体系统（MAS）的发展，为智能体互联网铺平了道路，因此需要一种新的去中心化多智能体系统（DMAS）架构来克服这些基本问题。

Method: 本文提出了一个去中心化的多智能体系统（DMAS）架构，其特点是：由区块链架构支撑的去中心化智能体运行时；形式化了一个信任感知的通信协议，该协议利用加密原语和链上操作提供安全属性。

Result: DMAS提供了可验证的交互周期、通信完整性、真实性、不可否认性和条件保密性。性能分析验证了DMAS是一种可扩展且高效的解决方案，用于构建值得信赖的多智能体系统。

Conclusion: DMAS架构通过去中心化的方式，利用区块链技术和信任感知的通信协议，成功解决了传统MAS的诸多挑战，为构建安全、可扩展、抗审查的自主多智能体系统提供了可行的方案。

Abstract: The emergence of Large Language Models (LLMs) is rapidly accelerating the development of autonomous multi-agent systems (MAS), paving the way for the Internet of Agents. However, traditional centralized MAS architectures present significant challenges, including single points of failure, vulnerability to censorship, inherent scalability limitations, and critical trust issues. We propose a novel Decentralized Multi-Agent System (DMAS) architecture designed to overcome these fundamental problems by enabling trust-aware, scalable, and censorship-resistant interactions among autonomous agents. Our DMAS features a decentralized agent runtime underpinned by a blockchain-based architecture. We formalize a trust-aware communication protocol that leverages cryptographic primitives and on-chain operations to provide security properties: verifiable interaction cycles, communication integrity, authenticity, non-repudiation, and conditional confidentiality, which we further substantiate through a comprehensive security analysis. Our performance analysis validates the DMAS as a scalable and efficient solution for building trustworthy multi-agent systems.

</details>


<div id='cs.IT'></div>

# cs.IT [[Back]](#toc)

### [35] [Weight distributions of simplex codes over finite chain rings and their Gray map images](https://arxiv.org/abs/2512.02149)
*Cristina Fernández-Córdoba,Sergi Sánchez-Aragón,Mercè Villanueva*

Main category: cs.IT

TL;DR: 本文介绍了在有限链环上线性单纯码的构造，并分析了其参数和最优性。


<details>
  <summary>Details</summary>
Motivation: 推广$
		\mathbb{Z}_{p^s}$上的线性码，研究有限链环上的线性单纯码及其Gray像。

Method: 构造了有限链环$R$上的线性单纯码，以及F_q上的R线性单纯码（type alpha和beta），推导了汉明距离和重量分布。

Result: 获得了这些单纯码的汉明距离、完全重量分布，并研究了它们对于Griesmer型界限的最优性。

Conclusion: 本文成功构造了更广义的单纯码，并详细分析了它们的性质。

Abstract: A linear code of length $n$ over a finite chain ring $R$ with residue field $\F_q$ is a $R$-submodule of $R^n$. A $R$-linear code is a code over $\F_q$ (not necessarily linear) which is the generalized Gray map image of a linear code over $R$. These codes can be seen as a generalization of the linear codes over $\Z_{p^s}$ with $p$ prime and $s \geq 1$. In this paper, we present the construction of linear simplex codes over $R$ and their corresponding $R$-linear simplex codes of type $α$ and $β$. Moreover, we show the fundamental parameters of these codes, including their minimum Hamming distance, as well as their complete weight distributions. We also study whether these simplex codes are optimal with respect to the Griesmer-type bound.

</details>


### [36] [Low-Power Double RIS-Assisted Mobile LEO Satellite Communications](https://arxiv.org/abs/2512.02255)
*Kunnathully Sadanandan Sanila,Rickard Nilsson,Emad Ibrahim,Neelakandan Rajamohan*

Main category: cs.IT

TL;DR: 该文提出了一种低功耗移动低地球轨道（LEO）卫星通信架构，以提高能源效率和信号性能。


<details>
  <summary>Details</summary>
Motivation: 在能量受限的LEO卫星通信环境中，存在能耗高、信号性能差的问题。

Method: 通过部署双可重构智能表面（RIS）来增强能量效率和信号性能。一个RIS放置在卫星天线近场，另一个放置在地面用户近场。此外，该文还建立了双RIS通信链路的路径损耗模型，并考虑了近场和远场效应。借助双阶段波束成形，该系统可以最大化信号功率并最小化功耗。

Result: 仿真结果表明，所提出的架构可以将上行链路的功耗降低40 dB，且用户侧仅需使用0.25平方米的小型RIS。

Conclusion: 双RIS架构能有效降低LEO卫星通信的功耗，提高能源效率和信号性能。

Abstract: We propose a low-power mobile low earth orbit (LEO) satellite communication architecture, employing double reconfigurable intelligent surfaces (RIS) to enhance energy efficiency and signal performance. With a distance between RISs that satisfies the far-field requirement, this architecture positions one small RIS each in the near-field of the satellite's antenna and the user on the ground. Moreover, we develop a path loss model for the double-RIS communication link, considering the near-field and far-field effects. Further, with the help of dual-stage beamforming, the proposed system maximizes the signal power and minimizes power consumption. Simulation results show that the proposed architecture can reduce the power consumption with 40 dB in the uplink, with a small $0.25^2$ $\text{m}^2$ RIS near the user, to communicate in energy-constrained LEO satellite communication circumstances.

</details>


### [37] [Entropies associated with orbits of finite groups](https://arxiv.org/abs/2512.02257)
*Ryan Leal,Jingtong Sun,Juan Pablo Vigneaux*

Main category: cs.IT

TL;DR: 这篇论文研究了几种群的旗的熵和轨道。


<details>
  <summary>Details</summary>
Motivation: 作者的动机是探索不同类型的群在信息论背景下的熵和轨道。

Method: 作者的方法是研究在某个有限域上的抛物线子群，以及一般线性群、有限反射群和辛群。他们探索了这些群的轨道的基数与不同熵之间的关系（例如，香农熵和Tsallis 2-熵）。

Result: 作者发现，某些其他系列（例如，除了A_n系列对称群和一般线性群之外的类型）与新的熵泛函有关。

Conclusion: 这篇论文建立了各种群与信息论的熵之间的联系，并引入了新的熵泛函。

Abstract: For certain groups, parabolic subgroups appear as stabilizers of flags of sets or vector spaces. Quotients by these parabolic subgroups represent orbits of flags, and their cardinalities asymptotically reveal entropies (as rates of exponential or superexponential growth). The multiplicative "chain rules" that involve these cardinalities induce, asymptotically, additive analogues for entropies. Many traditional formulas in information theory correspond to quotients of symmetric groups, which are a particular kind of reflection group; in this case, the cardinalities of orbits are given by multinomial coefficients and are asymptotically related to Shannon entropy. One can treat similarly quotients of the general linear groups over a finite field; in this case, the cardinalities of orbits are given by $q$-multinomials and are asymptotically related to the Tsallis 2-entropy. In this contribution, we consider other finite reflection groups as well as the symplectic group as an example of a classical group over a finite field (groups of Lie type). In both cases, the groups are classified by Dynkin diagrams into infinite series of similar groups $A_n$, $B_n$, $C_n$, $D_n$ and a finite number of exceptional ones. The $A_n$ series consists of the symmetric groups (reflection case) and general linear groups (Lie case). Some of the other series, studied here from an information-theoretic perspective for the first time, are linked to new entropic functionals.

</details>


### [38] [New Constructions of Non-GRS MDS Codes, Recovery and Determination Algorithms for GRS Codes](https://arxiv.org/abs/2512.02325)
*Guodong Wang,Hongwei Liu,Jinquan Luo*

Main category: cs.IT

TL;DR: 本文提出了一种新的非GRS MDS码的构造方法，并对其属性进行了分析，同时设计了两种高效算法来解决GRS码研究中的关键问题。


<details>
  <summary>Details</summary>
Motivation: 为了构造一类新的非GRS MDS码，其长度可以达到奇特征有限域上的(q+3)/2和偶特征有限域上的(q+4)/2，并利用特殊结构通过柯西矩阵方法获得MDS码和非GRS MDS码的充要条件。同时，为了分析这些码与扭曲GRS码之间的不等价性，并探讨非GRS MDS码的构造方法与现有码之间的关系。此外，作者还旨在设计高效算法来判断未知码是否为GRS码，并在是的情况下恢复关键向量。

Method: 1. 提出了一种新的非GRS MDS码的构造方法，其长度在奇特征有限域上可达(q+3)/2，在偶特征有限域上可达(q+4)/2。
2. 利用柯西矩阵方法，得到了这些码成为MDS码和非GRS MDS码的充要条件。
3. 分析了这些码与扭曲GRS码之间的不等价性。
4. 分析了几种现有用于构造非GRS MDS码的码类之间的关系，并提出了显式构造方法。
5. 设计了两种高效算法，用于判断未知码是否为GRS码，并在是的情况下恢复其关键向量α和v。

Result: 1. 成功构造了一类新的非GRS MDS码，其长度达到预期。
2. 获得了这些码成为MDS码和非GRS MDS码的精确条件。
3. 证明了这些码与扭曲GRS码之间的不等价性。
4. 明确了不同非GRS MDS码构造方法之间的关系，并给出了具体构造实例。
5. 设计的算法在判断GRS码和恢复关键向量方面，计算复杂度显著优于Sidelnikov-Shestakov攻击，达到O(nk+n)。

Conclusion: 本文成功构造了一类新型非GRS MDS码，并通过严格的数学分析明确了其MDS特性和非GRS特性。此外，开发的GRS码识别和恢复算法在计算效率上表现出色，为GRS码的研究提供了有力的工具。这项工作不仅丰富了MDS码的理论体系，也为编码实践提供了新的可能。

Abstract: In this paper, we propose a new method for constructing a class of non-GRS MDS codes. The lengths of these codes can reach up to $\frac{q+3}{2}$ (for finite fields of odd characteristic) and $\frac{q+4}{2}$ (for even characteristic), respectively. Owing to their special structure, we can use the Cauchy matrix method to obtain the necessary and sufficient conditions for these codes to be MDS codes and non-GRS MDS codes. Additionally, the inequivalence between these codes and twisted GRS codes is analyzed. Furthermore, we analyze the relationships among several existing classes of codes used for constructing non-GRS MDS codes, propose explicit constructions, and discuss the lengths of non-GRS MDS codes based on these constructions. Finally, we design two efficient algorithms to address two main problems in GRS code research, i.e., determining whether an unknown code $C$ is a GRS code from its generator matrix $G$, and recovering the key vectors $\bmα$ and $\bm{v}$ such that $C = \GRS_{n,k}(\bmα, \bm{v})$ if $C$ is indeed a GRS code. A computational complexity comparison of the proposed algorithms ($O(nk+n)$) with that of the Sidelnikov-Shestakov attack (exceeding $O(qk^2n+qk^3)$) shows that our methods offer superior computational efficiency.

</details>


### [39] [Age of Information for Constrained Scheduling with Imperfect Feedback](https://arxiv.org/abs/2512.02332)
*Yuqing Zhu,Yuan-Hsun Lo,Yan Lin,Yijin Zhang*

Main category: cs.IT

TL;DR: 本文提出了一种优化信息年龄（AoI）的调度算法，该算法在考虑不完美反馈和受限传输速率的下行系统中运行。


<details>
  <summary>Details</summary>
Motivation: 在实际系统中，不完美反馈和受限传输速率是关键的限制因素。因此，本文旨在设计调度算法，以在无限时间范围内优化信息年龄（AoI）。

Method: 对于随意生成流量下的零反馈，我们推导了一个可实现AoI的闭式下界，并通过联合应用速率拆分和模运算提出了一种在许多情况下达到该下界的策略。对于伯努利流量下的零反馈，我们基于Lyapunoy优化的理论，开发了一种具有阈值结构的漂移加惩罚（DPP）策略，并提供了闭式性能保证。

Result: 推导了可实现AoI的闭式下界，并在许多情况下通过联合应用速率拆分和模运算实现。开发了具有阈值结构的漂移加惩罚（DPP）策略，并提供了闭式性能保证。将DPP策略扩展到支持一般不完美反馈，而不会增加在线计算复杂性。

Conclusion: 本文提出的调度算法在考虑不完美反馈和受限传输速率的情况下，有效地优化了信息年龄（AoI），并通过理论分析和数值结果验证了其优越性。

Abstract: This paper considers a downlink system where an access point sends the monitored status of multiple sources to multiple users. By jointly accounting for imperfect feedback and constrained transmission rate, which are key limited factors in practical systems, we aim to design scheduling algorithms to optimize the age of information (AoI) over the infinite time horizon. For zero feedback under the generate-at-will traffic, we derive a closed-form lower bound of achievable AoI, which, to the best of our knowledge, reflects the impact of zero feedback for the first time, and propose a policy that achieves this bound in many cases by jointly applying rate splitting and modular arithmetic. For zero feedback under the Bernoulli traffic, we develop a drift-plus-penalty (DPP) policy with a threshold structure based on the theory of Lyapunov optimization and provide a closed-form performance guarantee. Furthermore, we extend the design of this DPP policy to support general imperfect feedback without increasing the online computational complexity. Numerical results verify our theoretical analysis and the AoI advantage of the proposed policies over state-of-the-art policies.

</details>


### [40] [A Cyclic Shift Embedded Pilot based Channel Estimation for Multi-User MIMO-OTFS systems with fractional delay and Doppler](https://arxiv.org/abs/2512.02353)
*Ruizhe Wang,Hong Ren,Cunhua Pan,Ruisong Weng,Jiangzhou Wang*

Main category: cs.IT

TL;DR: 本文提出了一种CSEP结构和信道估计算法，可有效降低多用户OFTS系统中的导频开销，并在计算复杂度、估计精度和误码率性能之间取得良好平衡。


<details>
  <summary>Details</summary>
Motivation: 在多用户OTFS系统中，传统的嵌入式导频方案需要为每个用户独立分配导频，导致导频开销线性增加。为了解决这些问题，本文研究了多用户MIMO-OTFS系统的上行链路信道估计和导频设计。

Method: 本文提出了一种多维分解的信道估计算法。该算法首先通过基于子空间分解的方法估计到达角（AoA）。利用估计的AoA构建空间投影矩阵，通过传播路径子空间解耦接收信号，有效缓解了路径间干扰。其余的分数延迟和多普勒可以通过基于压缩感知（CS）的离网格信道估计方法获得。此外，为了降低多用户OTFS系统中的导频开销，本文提出了一种新颖的循环移位嵌入导频（CSEP）结构，该结构可以通过Zadoff-Chu（ZC）序列的循环移位正交性实现用户复用。最后，提出了一种基于CSEP结构的改进信道估计算法。

Result: 与传统的嵌入式导频结构相比，CSEP结构可以节省30%以上的导频开销。仿真结果表明，该算法在信道估计方面具有优越的性能。此外，所提出的CSEP结构和信道估计算法在计算复杂度、估计精度和误码率（BER）性能之间取得了良好的平衡。

Conclusion: 本文提出了一种CSEP结构和信道估计算法，可有效降低多用户OFTS系统中的导频开销，并在计算复杂度、估计精度和误码率性能之间取得良好平衡。

Abstract: Orthogonal time frequency space (OTFS) modulation has been proposed to meet the demand for reliable communication in high-mobility scenarios for future wireless networks. However, in multi-user OTFS systems, conventional embedded pilot schemes require independent pilot allocation for each user, leading to linearly increasing pilot overhead. To address these issues, in this paper, we investigate the uplink channel estimation and pilot design for multi-user multiple-input multiple-output (MIMO)-OTFS systems. We propose a multi-dimensional decomposition-based channel estimation algorithm. Specifically, the proposed algorithm first estimates the angles of arrivals (AoAs) via subspace decomposition-based method. A spatial projection matrix, constructed from the estimated AOAs, decouples the received signal by propagation path subspace, effectively mitigating inter-path interference. The remaining fractional delay and Doppler can be obtained by a compressed sensing (CS)-based off-grid channel estimation method. Furthermore, to reduce the pilot overhead in multi-user OTFS systems, this paper proposes a novel cyclic shift embedded pilot (CSEP) structure, which can reuse users through cyclic shift-orthogonality of Zadoff-Chu (ZC) sequences. Compared with conventional embedded pilot structures, the CSEP structure can save over 30\% of pilot overhead. Finally, an imporved channel estimation method based on the CSEP structure is proposed. Simulation results demonstrate that it achieves superior performance in channel estimation. Moreover, the proposed CSEP structure and channel estimation algorithm achieve a favorable balance between computational complexity, estimation accuracy, and bit error rate (BER) performance.

</details>


### [41] [Boltzmann-Shannon Index: A Geometric-Aware Measure of Clustering Balance](https://arxiv.org/abs/2512.02397)
*Emanuele Bossi,C. Tyler Diggans,Abd AlRahman R. AlMomani*

Main category: cs.IT

TL;DR: BSI是一种新的聚类连续数据归一化度量方法，整合了基于频率和基于几何的概率分布。它能有效评估数据分区，即使传统指标失效，也能在资源分配中提供敏感检测和优化友好的目标。


<details>
  <summary>Details</summary>
Motivation: 此论文旨在解决现有聚类评估指标在处理频率和几何分布交互作用时的不足，特别是在聚类连续数据和资源分配场景中。作者希望开发一个能综合考虑簇的流行程度和几何范围，并在传统指标失效时仍能提供准确评估的工具。

Method: 论文提出了玻尔兹曼-香农指数 (BSI)，该指数基于几何粗粒化和信息论原理。BSI量化分区如何同时反映每个簇的种群及其有效的几何范围。

Result: 在合成高斯混合、Iris基准测试和高不平衡资源分配场景中，BSI展示了其有效性。结果表明，即使传统指标提供不完整或误导性信号时，BSI仍能提供连贯的评估。在资源分配中，BSI不仅能高灵敏度地检测出严重的密度-几何不一致，而且提供了一个平滑、可优化的目标，自然倾向于平衡人口权重与群体在结果空间中的有效传播的分配。

Conclusion: BSI为聚类连续数据提供了一个强大的新度量标准，它通过整合频率和几何分布，克服了传统评估方法的局限性。它对于资源分配尤其有用，可以作为优化框架中平滑、梯度友好的正则化器，有助于制定更公平、更有效的政策。

Abstract: We introduce the Boltzmann-Shannon Index (BSI), a normalized measure for clustered continuous data that captures the interaction between frequency-based and geometry-based probability distributions. Building on ideas from geometric coarse-graining and information theory, the BSI quantifies how well a partition reflects both the population of each cluster and its effective geometric extent. We illustrate its behavior on synthetic Gaussian mixtures, the Iris benchmark, and a high-imbalance resource-allocation scenario, showing that the index provides a coherent assessment even when traditional metrics give incomplete or misleading signals. Moreover, in resource-allocation settings, we demonstrate that BSI not only detects severe density-geometry inconsistency with high sensitivity, but also offers a smooth, optimization-ready objective that naturally favors allocations balancing demographic weight with each group's effective spread in the outcome space, while providing a smooth, gradient-friendly regularizer that can be easily embedded in modern policy-making and algorithmic governance optimization frameworks.

</details>


### [42] [Quantum Optimization in Wireless Communication Systems: Principles and Applications](https://arxiv.org/abs/2512.02468)
*Ioannis Krikidis,Valentin Gilbert*

Main category: cs.IT

TL;DR: 本文概述了绝热量子计算、量子退火和基于门的量子近似优化算法的原理、特点、优势、局限性。


<details>
  <summary>Details</summary>
Motivation: 量子优化在下一代无线通信系统的设计中发挥着变革性作用。

Method: 本文研究了绝热量子计算的原理，量子退火和基于门的量子近似优化算法这两种主要的计算模型。通过突出它们的核心特征、性能优势、局限性和区别，我们将这些方法定位为，用于推进无线通信系统设计的有前景的工具。

Result: 通过真实世界的量子硬件获得了实验结果。

Conclusion: 量子优化是推进无线通信系统设计的有前景的工具。

Abstract: Quantum optimization is poised to play a transformative role in the design of next-generation wireless communication systems by addressing key computational and technological challenges. This paper provides an overview of the principles of adiabatic quantum computing, the foundation of quantum optimization, and explores its two primary computational models: quantum annealing and the gate-based quantum approximate optimization algorithm. By highlighting their core features, performance benefits, limitations, and distinctions, we position these methods as promising tools for advancing wireless communication system design. As a case study, we examine the design of passive reconfigurable intelligent surface beamforming with binary phase-shift resolution, supported by experimental results obtained from real-world quantum hardware.

</details>


### [43] [Digit-Indexed q-ary SEC-DED Codes with Near-Hamming Overhead](https://arxiv.org/abs/2512.02747)
*Jiaxu Hu,Kenneth J. Roche*

Main category: cs.IT

TL;DR: 这篇论文介绍了一种简单且易于实现的$q$元单纠错双检错（SEC-DED）线性码家族，其具有接近汉明码的开销，并支持高效的索引解码器，同时还提出了两种扩展（Code A1和Code A2）以及一个更通用的框架，可以构造出距离为$n+1$的码。


<details>
  <summary>Details</summary>
Motivation: 此论文的目的是设计一个$q$元单纠错双检错（SEC-DED）线性码家族，该家族在实现上简单，具有阵列友好的结构，并且可以通过坐标索引的$p$进制数字直接绑定奇偶校验位，以解决传统编码方案在实现复杂性和并行性方面的挑战。作者旨在提供一种易于理解和实现的新编码方法，同时保持较低的奇偶校验开销和高效的解码速度。

Method: 本论文的方法是构建一个$q$元单纠错双检错（SEC-DED）线性码家族。其核心是，对于块长$n=p^r$，构造只使用$r+1$个奇偶校验位，并且奇偶校验位直接与坐标索引的$p$进制数字相关联。解码器是基于索引的，可以单次通过，以常数时间从伴随式中恢复错误位置和幅度。在此基础上，作者提出了两种扩展：Code A1通过移除冗余三进制位来提高信息率并支持可变长度编码；Code A2结合了两个组-和校验和索引子集上的三元XOR线性独立条件，从而产生了一个三进制距离为4（SEC-TED）的变体。此外，该框架通过$n$元XOR线性独立集推广，以构造距离为$d=n+1$的码。

Result: 本研究提出了一种$q$元单纠错双检错（SEC-DED）线性码家族，其特点是奇偶校验与坐标索引的基数$p$数字直接关联。对于块长$n=p^r$，该构造仅使用$r+1$个奇偶校验位，实现了接近汉明码的开销，并支持基于索引的解码器，该解码器可在一次通过中以常数时间恢复错误位置和幅度。 Code A1通过移除冗余三进制位来提高信息率并支持可变长度编码。 Code A2包含了两个组和校验以及索引子集上的三元XOR线性独立条件，从而产生了三进制距离为4（SEC-TED）的变体。该框架通过$n$元XOR线性独立集推广，成功构建了距离为$d=n+1$的码，甚至能够恢复三进制Golay码。

Conclusion: 这篇论文成功提出了一种实现简单、适用于阵列结构的$q$元单纠错双检错线性码。其核心优势在于奇偶校验与坐标索引的$p$进制数字直接绑定，实现了接近汉明码的开销和高效解码。研究所提出的两种扩展（Code A1和Code A2）进一步提升了信息率并扩展了功能，而泛化框架则展示了其构造更强码的能力。尽管论文不追求最优性，但其在实现简易性、阵列友好性以及解码效率方面的贡献，使其在工程应用中具有显著价值。

Abstract: We present a simple $q$-ary family of single-error-correcting, double-error-detecting (SEC--DED) linear codes whose parity checks are tied directly to the base-$p$ ($q=p$ prime) digits of the coordinate index. For blocklength $n=p^r$ the construction uses only $r+1$ parity checks -- \emph{near-Hamming} overhead -- and admits an index-based decoder that runs in a single pass with constant-time location and magnitude recovery from the syndromes. Based on the prototype, we develop two extensions: Code A1, which removes specific redundant trits to achieve higher information rate and support variable-length encoding; and Code A2, which incorporates two group-sum checks together with a 3-wise XOR linear independence condition on index subsets, yielding a ternary distance-4 (SEC--TED) variant. Furthermore, we demonstrate how the framework generalizes via $n$-wise XOR linearly independent sets to construct codes with distance $d = n + 1$, notably recovering the ternary Golay code for $n = 5$ -- showing both structural generality and a serendipitous link to optimal classical codes.
  Our contribution is not optimality but \emph{implementational simplicity} and an \emph{array-friendly} structure: the checks are digitwise and global sums, the mapping from syndromes to error location is explicit, and the SEC--TED upgrade is modular. We position the scheme against classical $q$-ary Hamming and SPC/product-code baselines and provide a small comparison of parity overhead, decoding work, and two-error behavior.

</details>


### [44] [Structural Properties of Entropic Vectors and Stability of the Ingleton Inequality](https://arxiv.org/abs/2512.02767)
*Rostislav Matveev,Andrei Romashchenko*

Main category: cs.IT

TL;DR: 本文研究了在信息熵背景下，受限的Ingleton不等式，并通过一种结构引理，在非精确独立约束下，量化了该不等式的稳定性。


<details>
  <summary>Details</summary>
Motivation: 在普遍的熵分布下，经典的Ingleton不等式不成立，但在某些精确独立约束下成立。本文关注的是条件互信息项较小（但不为零）的情况。

Method: 本文提出了一种结构引理，该引理能具体化两个随机变量之间的部分互信息，隐式地捕捉无限多个非香农类型不等式的影响。这使得在不明确调用这些无限族不等式的情况下，也能得到概念清晰的证明。

Result: 本文的一些界限以统一的方式恢复了Matúš (2007) 和 Dougherty-Freiling-Zeger (2011) 的无限族不等式中可以推导出的结果，而另一些则是新的。

Conclusion: 本文成功地在非精确独立约束下量化了Ingleton不等式的稳定性，并提出了一种新的分析工具，为信息论领域带来了新的见解。

Abstract: We study constrained versions of the Ingleton inequality in the entropic setting and quantify its stability under small violations of conditional independence. Although the classical Ingleton inequality fails for general entropy profiles, it is known to hold under certain exact independence constraints. We focus on the regime where selected conditional mutual information terms are small (but not zero), and the inequality continues to hold up to controlled error terms. A central technical tool is a structural lemma that materializes part of the mutual information between two random variables, implicitly capturing the effect of infinitely many non-Shannon--type inequalities. This leads to conceptually transparent proofs without explicitly invoking such infinite families. Some of our bounds recover, in a unified way, what can also be deduced from the infinite families of inequalities of Matúš (2007) and of Dougherty--Freiling--Zeger (2011), while others appear to be new.

</details>


### [45] [Pseudocodewords of quantum, quasi-cyclic, and spatially-coupled LDPC codes: a fundamental cone perspective](https://arxiv.org/abs/2512.02941)
*Wittawat Kositwattanarerk,Gretchen L. Matthews,Emily McMillon,Tunchanok Yutitumsatit*

Main category: cs.IT

TL;DR: 本文探讨了LDPC码的伪码字问题，特别是在LP解码和图覆盖解码的背景下，分析了这些伪码字的结构及其对译码性能的影响。


<details>
  <summary>Details</summary>
Motivation: LDPC码在迭代译码器下接近香农限，但伪码字的存在会阻碍译码器输出码字，因此对伪码字的研究对于理解现代译码器（包括迭代译码器和线性规划译码器）的性能至关重要。

Method: 本文考虑了与图覆盖解码相关的LP解码，这提供了捕获伪码字的功能。 特别地，本文分析了由LP解码产生的量子稳定器码、准循环LDPC码和空间耦合LDPC码的伪码字的底层结构。

Result: 通过分析量子稳定器码、准循环LDPC码和空间耦合LDPC码的伪码字结构，深入理解了伪码字对LP解码性能的影响。

Conclusion: LP解码中的伪码字取决于码的奇偶校验矩阵和特定的解码算法。对这些伪码字底层结构的分析有助于理解并可能改进LDPC码的解码性能。

Abstract: While low-density parity-check (LDPC) codes are near capacity-achieving when paired with iterative decoders, these decoders may not output a codeword due to the existence of pseudocodewords. Thus, pseudocodewords have been studied to give insight into the performance of modern decoders including iterative and linear programming decoders. These pseudocodewords are found to be dependent on the parity-check matrix of the code and the particular decoding algorithm used. In this paper, we consider LP decoding, which has been linked to graph cover decoding, providing functions which capture these pseudocodewords. In particular, we analyze the underlying structure of pseudocodewords from quantum stabilizer codes that arise from LP decoding, quasi-cyclic LDPC codes, and spatially-coupled LDPC codes.

</details>


<div id='cs.LG'></div>

# cs.LG [[Back]](#toc)

### [46] [How Market Volatility Shapes Algorithmic Collusion: A Comparative Analysis of Learning-Based Pricing Algorithms](https://arxiv.org/abs/2512.02134)
*Aheer Sravon,Md. Ibrahim,Devdyuti Mazumder,Ridwan Al Aziz*

Main category: cs.LG

TL;DR: 本文分析了四种定价算法在三种双寡头模型和不同需求冲击下的行为，探讨了算法、市场结构和随机需求如何影响竞争结果。


<details>
  <summary>Details</summary>
Motivation: 探索自主定价算法在现实需求条件下的行为，特别是它们如何影响数字市场中的竞争。

Method: 本文在Logit、Hotelling和Linear三种经典双寡头模型下，通过自回归过程创建了各种需求冲击，并分析了Q-Learning、PSO、Double DQN和DDPG四种定价算法。通过利润和价格的串通指数，研究了算法、市场结构和随机需求之间的相互作用。

Result: 强化学习算法在需求稳定时能维持超竞争价格，DDPG的合谋倾向最显著。需求冲击影响各异：Logit市场性能显著下降，Hotelling市场保持稳定，Linear市场出现冲击引起的利润膨胀。尽管绝对性能变化显著，算法的相对排名在不同环境下保持一致。

Conclusion: 市场结构和需求不确定性对算法竞争至关重要。研究结果为围绕自主定价行为的政策讨论提供了参考。

Abstract: Autonomous pricing algorithms are increasingly influencing competition in digital markets; however, their behavior under realistic demand conditions remains largely unexamined. This paper offers a thorough analysis of four pricing algorithms -- Q-Learning, PSO, Double DQN, and DDPG -- across three classic duopoly models (Logit, Hotelling, Linear) and under various demand-shock regimes created by auto-regressive processes. By utilizing profit- and price-based collusion indices, we investigate how the interactions among algorithms, market structure, and stochastic demand collaboratively influence competitive outcomes. Our findings reveal that reinforcement-learning algorithms often sustain supra-competitive prices under stable demand, with DDPG demonstrating the most pronounced collusive tendencies. Demand shocks produce notably varied effects: Logit markets suffer significant performance declines, Hotelling markets remain stable, and Linear markets experience shock-induced profit inflation. Despite marked changes in absolute performance, the relative rankings of the algorithms are consistent across different environments. These results underscore the critical importance of market structure and demand uncertainty in shaping algorithmic competition, while also contributing to the evolving policy discussions surrounding autonomous pricing behavior.

</details>


### [47] [Unlocking the Power of Boltzmann Machines by Parallelizable Sampler and Efficient Temperature Estimation](https://arxiv.org/abs/2512.02323)
*Kentaro Kubo,Hayato Goto*

Main category: cs.LG

TL;DR: 本文提出了一种新的玻尔兹曼采样器Langevin SB（LSB）用于玻尔兹曼机（BM），它可以并行采样并保持与MCMC相当的精度。为了解决LSB不能控制输出玻尔兹曼分布的逆温度的问题，作者还开发了一种条件期望匹配（CEM）方法来估计逆温度。结合LSB和CEM，作者提出了采样器自适应学习（SAL）框架，为玻尔兹曼机提供了比RBM更强的表达能力的高效学习方法。


<details>
  <summary>Details</summary>
Motivation: 玻尔兹曼机（BM）是强大的基于能量的生成模型，但由于训练成本高昂，其应用主要限于使用对比散度进行训练的受限玻尔兹曼机（RBM）。更准确的学习通常需要马尔可夫链蒙特卡罗（MCMC）玻尔兹曼采样，但由于更具表达性的模型难以并行化，这非常耗时。

Method: 本文提出了一种受量子启发组合优化模拟分叉（SB）启发的玻尔兹曼采样器，称为Langevin SB（LSB）。LSB实现了并行化采样，同时保持了与MCMC相当的精度，并且适用于具有通用耦合的BM。为了解决LSB无法控制输出玻尔兹曼分布逆温度的问题，作者还开发了一种在学习过程中估计逆温度的有效方法，称为条件期望匹配（CEM）。通过结合LSB和CEM，作者建立了采样器自适应学习（SAL）框架。

Result: LSB能够实现并行化采样，并且在精度上与MCMC相当。SAL框架为玻尔兹曼机提供了比RBM更强的表达能力的高效学习方法。

Conclusion: SAL为能量基础的生成模型开辟了超越RBM的新途径。

Abstract: Boltzmann machines (BMs) are powerful energy-based generative models, but their heavy training cost has largely confined practical use to Restricted BMs (RBMs) trained with an efficient learning method called contrastive divergence. More accurate learning typically requires Markov chain Monte Carlo (MCMC) Boltzmann sampling, but it is time-consuming due to the difficulty of parallelization for more expressive models. To address this limitation, we first propose a new Boltzmann sampler inspired by a quantum-inspired combinatorial optimization called simulated bifurcation (SB). This SB-inspired approach, which we name Langevin SB (LSB), enables parallelized sampling while maintaining accuracy comparable to MCMC. Furthermore, this is applicable not only to RBMs but also to BMs with general couplings. However, LSB cannot control the inverse temperature of the output Boltzmann distribution, which hinders learning and degrades performance. To overcome this limitation, we also developed an efficient method for estimating the inverse temperature during the learning process, which we call conditional expectation matching (CEM). By combining LSB and CEM, we establish an efficient learning framework for BMs with greater expressive power than RBMs. We refer to this framework as sampler-adaptive learning (SAL). SAL opens new avenues for energy-based generative modeling beyond RBMs.

</details>


### [48] [Hypothesis Testing for Generalized Thurstone Models](https://arxiv.org/abs/2512.02912)
*Anuran Makur,Japneet Singh*

Main category: cs.LG

TL;DR: 本文提出了一个假设检验框架，用于确定成对比较数据是否由底层广义瑟斯顿模型生成，并解决了该模型的最小最大假设检验问题。


<details>
  <summary>Details</summary>
Motivation: 以往的工作主要集中在广义瑟斯顿模型的参数估计和不确定性量化上。本文旨在解决该模型最小最大假设检验的基本问题，即判断数据是否来源于此类模型。

Method: 1. 引入了通用成对比较模型与 \(\mathcal{T}_F\) 模型之间的分离距离概念。2. 推导了临界阈值的上下界，该阈值依赖于观测图的拓扑结构。3. 对于完全观测图的特殊情况，该阈值定标为 \(Θ((nk)^{-1/2})\)。4. 提出了一种基于分离距离的假设检验方法，并构建了置信区间。5. 使用逆鞅技术建立了 I 类和 II 类错误的概率的时间一致界限。6. 使用信息论方法推导了最小最大下限。7. 在合成和真实世界数据集上通过实验验证了结果。

Result: 1. 确定了用于检验的临界阈值上下界。2. 证明了在完全观测图中，该阈值随 \((nk)^{-1/2})\) 定标。3. 成功构建了基于分离距离的假设检验方法和置信区间。4. 建立了 I 类和 II 类错误概率的时间一致界限，并推导了最小最大下限。5. 实验验证了所提出方法的有效性。

Conclusion: 本文成功开发了一个用于广义瑟斯顿模型的假设检验框架，解决了最小最大假设检验问题，并提供了理论保证和实验验证。

Abstract: In this work, we develop a hypothesis testing framework to determine whether pairwise comparison data is generated by an underlying \emph{generalized Thurstone model} $\mathcal{T}_F$ for a given choice function $F$. While prior work has predominantly focused on parameter estimation and uncertainty quantification for such models, we address the fundamental problem of minimax hypothesis testing for $\mathcal{T}_F$ models. We formulate this testing problem by introducing a notion of separation distance between general pairwise comparison models and the class of $\mathcal{T}_F$ models. We then derive upper and lower bounds on the critical threshold for testing that depend on the topology of the observation graph. For the special case of complete observation graphs, this threshold scales as $Θ((nk)^{-1/2})$, where $n$ is the number of agents and $k$ is the number of comparisons per pair. Furthermore, we propose a hypothesis test based on our separation distance, construct confidence intervals, establish time-uniform bounds on the probabilities of type I and II errors using reverse martingale techniques, and derive minimax lower bounds using information-theoretic methods. Finally, we validate our results through experiments on synthetic and real-world datasets.

</details>


### [49] [Fast Gaussian Process Approximations for Autocorrelated Data](https://arxiv.org/abs/2512.02925)
*Ahmadreza Chokhachian,Matthias Katzfuss,Yu Ding*

Main category: cs.LG

TL;DR: 本文提出了一种加速处理自相关数据的高斯过程模型计算的方法，且不影响模型预测性能。


<details>
  <summary>Details</summary>
Motivation: 目前用于加速高斯过程回归的各种快速近似方法都是在标准假设下进行的，即随机样本和独立同分布噪声。然而，如果数据是自相关的，这些方法会导致时间上的过拟合。现有方法需要修改才能处理自相关数据，例如将原始相关数据点分段。

Method: 本文解释了如何使现有的一些高斯过程近似方法与分块数据协同工作。

Result: 数值实验表明，该方法能显著加速自相关数据上的高斯过程回归计算。

Conclusion: 所提出的方法在不影响模型预测性能的情况下，显著加快了高斯过程回归在自相关数据上的计算。

Abstract: This paper is concerned with the problem of how to speed up computation for Gaussian process models trained on autocorrelated data. The Gaussian process model is a powerful tool commonly used in nonlinear regression applications. Standard regression modeling assumes random samples and an independently, identically distributed noise. Various fast approximations that speed up Gaussian process regression work under this standard setting. But for autocorrelated data, failing to account for autocorrelation leads to a phenomenon known as temporal overfitting that deteriorates model performance on new test instances. To handle autocorrelated data, existing fast Gaussian process approximations have to be modified; one such approach is to segment the originally correlated data points into blocks in which the blocked data are de-correlated. This work explains how to make some of the existing Gaussian process approximations work with blocked data. Numerical experiments across diverse application datasets demonstrate that the proposed approaches can remarkably accelerate computation for Gaussian process regression on autocorrelated data without compromising model prediction performance.

</details>


### [50] [Contextual Gating within the Transformer Stack: Synergistic Feature Modulation for Enhanced Lyrical Classification and Calibration](https://arxiv.org/abs/2512.02053)
*M. A. Gameiro*

Main category: cs.LG

TL;DR: 该研究提出了SFL Transformer，一种通过在预训练Transformer的自注意力机制中直接集成辅助结构特征，并利用上下文门控机制在BERT编码器堆栈中调制隐藏状态序列，从而改进抒情内容分类的特征融合架构。


<details>
  <summary>Details</summary>
Motivation: 在预训练Transformer中，通过将辅助结构特征直接集成到自注意力机制中，以改进抒情内容分类的特征融合。

Method: 提出了SFL Transformer模型，它利用上下文门控机制（中间SFL）在BERT编码器堆栈中调制隐藏状态序列，而不是在最终输出层融合特征。这种方法使用低维结构线索（Fstruct）调制深度、情境化的语义特征（Hseq）。该模型应用于UMAP降维后的歌词嵌入的二元分类任务。

Result: SFL Transformer的准确率达到0.9910，Macro F1得分达到0.9910，显著优于先前发布的SFL模型的最佳水平（准确率0.9894）。此外，该上下文门控策略保持了出色的可靠性，具有低预期校准误差（ECE = 0.0081）和对数损失（0.0489）。

Conclusion: 将辅助上下文注入模型堆栈中间是协同结合结构和语义信息最有效的手段，能够创建具有卓越判别能力和高保真概率估计的模型。

Abstract: This study introduces a significant architectural advancement in feature fusion for lyrical content classification by integrating auxiliary structural features directly into the self-attention mechanism of a pre-trained Transformer. I propose the SFL Transformer, a novel deep learning model that utilizes a Contextual Gating mechanism (an Intermediate SFL) to modulate the sequence of hidden states within the BERT encoder stack, rather than fusing features at the final output layer. This approach modulates the deep, contextualized semantic features (Hseq) using low-dimensional structural cues (Fstruct). The model is applied to a challenging binary classification task derived from UMAP-reduced lyrical embeddings. The SFL Transformer achieved an Accuracy of 0.9910 and a Macro F1 score of 0.9910, significantly improving the state-of-the-art established by the previously published SFL model (Accuracy 0.9894). Crucially, this Contextual Gating strategy maintained exceptional reliability, with a low Expected Calibration Error (ECE = 0.0081) and Log Loss (0.0489). This work validates the hypothesis that injecting auxiliary context mid-stack is the most effective means of synergistically combining structural and semantic information, creating a model with both superior discriminative power and high-fidelity probability estimates.

</details>


### [51] [Opening the Black Box: An Explainable, Few-shot AI4E Framework Informed by Physics and Expert Knowledge for Materials Engineering](https://arxiv.org/abs/2512.02057)
*Haoxiang Zhang,Ruihao Yuan,Lihui Zhang,Yushi Luo,Qiang Zhang,Pan Ding,Xiaodong Ren,Weijie Xing,Niu Gao,Jishan Chen,Chubo Zhang*

Main category: cs.LG

TL;DR: 该研究提出了一个可解释的少样本AI4E框架，该框架在整个架构中系统地融入了物理学和专家知识。通过物理上合理的数据增强和嵌套优化策略，该框架从有限的实验样本中发现本构模型，并实现了高精度的热裂倾向预测。


<details>
  <summary>Details</summary>
Motivation: 工业界在工程领域应用人工智能（AI4E）面临数据稀缺和黑盒模型可解释性不足两大瓶颈，尤其在航空航天等安全敏感领域，这限制了AI的广泛应用，本研究旨在解决这些问题。

Method: 本研究提出了一个可解释的少样本AI4E框架，该框架在整个架构中系统地融入了物理学和专家知识。首先，针对航空K439B高温合金铸件修复焊接案例的32个实验样本，通过三阶段协议（差分噪声注入、硬物理约束和参数间关系保持）来增强物理上合理的合成数据。然后，采用嵌套优化策略进行本构模型发现，其中符号回归探索方程结构，差分进化优化参数，随后使用混合全局-局部优化进行密集参数细化。

Result: 所得到的本构方程在预测热裂倾向方面达到了88%的准确性。该方程不仅提供定量预测，还提供明确的物理洞察力，揭示了热、几何和冶金机制如何耦合驱动开裂。此外，本构方程可作为多功能工具，用于工艺优化和高保真虚拟数据生成，从而提高其他数据驱动模型的准确性。

Conclusion: 本研究提供了一个开发可信赖人工智能系统的通用蓝图，该系统将工程领域知识直接嵌入其架构中，从而使得在数据有限但有物理理解的高风险工业应用中能够可靠地采用人工智能。

Abstract: The industrial adoption of Artificial Intelligence for Engineering (AI4E) faces two fundamental bottlenecks: scarce high-quality data and the lack of interpretability in black-box models-particularly critical in safety-sensitive sectors like aerospace. We present an explainable, few-shot AI4E framework that is systematically informed by physics and expert knowledge throughout its architecture. Starting from only 32 experimental samples in an aerial K439B superalloy castings repair welding case, we first augment physically plausible synthetic data through a three-stage protocol: differentiated noise injection calibrated to process variabilities, enforcement of hard physical constraints, and preservation of inter-parameter relationships. We then employ a nested optimization strategy for constitutive model discovery, where symbolic regression explores equation structures while differential evolution optimizes parameters, followed by intensive parameter refinement using hybrid global-local optimization. The resulting interpretable constitutive equation achieves 88% accuracy in predicting hot-cracking tendency. This equation not only provides quantitative predictions but also delivers explicit physical insight, revealing how thermal, geometric, and metallurgical mechanisms couple to drive cracking-thereby advancing engineers' cognitive understanding of the process. Furthermore, the constitutive equation serves as a multi-functional tool for process optimization and high-fidelity virtual data generation, enabling accuracy improvements in other data-driven models. Our approach provides a general blueprint for developing trustworthy AI systems that embed engineering domain knowledge directly into their architecture, enabling reliable adoption in high-stakes industrial applications where data is limited but physical understanding is available.

</details>


### [52] [Ada-MoGE: Adaptive Mixture of Gaussian Expert Model for Time Series Forecasting](https://arxiv.org/abs/2512.02061)
*Zhenliang Ni,Xiaowen Ma,Zhenkai Wu,Shuai Xiao,Han Shu,Xinghao Chen*

Main category: cs.LG

TL;DR: 该论文提出Ada-MoGE，一种自适应高斯混合专家模型，用于解决多元时间序列预测中传统MoE模型在处理变化频率时的局限性。


<details>
  <summary>Details</summary>
Motivation: 传统MoE模型采用固定数量的专家，难以适应时间序列中不断变化的优势频率，导致频率覆盖不平衡问题，表现为专家过少会丢失关键信息，专家过多会引入噪声。

Method: Ada-MoGE模型通过整合频谱强度和频率响应来自适应地确定专家数量，确保与输入数据的频率分布对齐，从而避免因专家数量不足导致的信息丢失和专家数量过多导致的噪声污染。此外，它使用高斯带通滤波平滑地分解频域特征，以防止直接带截断引入噪声。

Result: Ada-MoGE模型在六个公共基准测试中取得了最先进的性能，且模型只有0.2百万参数。

Conclusion: Ada-MoGE通过自适应地确定专家数量并采用高斯带通滤波，有效解决了传统MoE模型在处理时变频率上的限制，从而提高了多元时间序列预测的准确性和效率。

Abstract: Multivariate time series forecasts are widely used, such as industrial, transportation and financial forecasts. However, the dominant frequencies in time series may shift with the evolving spectral distribution of the data. Traditional Mixture of Experts (MoE) models, which employ a fixed number of experts, struggle to adapt to these changes, resulting in frequency coverage imbalance issue. Specifically, too few experts can lead to the overlooking of critical information, while too many can introduce noise. To this end, we propose Ada-MoGE, an adaptive Gaussian Mixture of Experts model. Ada-MoGE integrates spectral intensity and frequency response to adaptively determine the number of experts, ensuring alignment with the input data's frequency distribution. This approach prevents both information loss due to an insufficient number of experts and noise contamination from an excess of experts. Additionally, to prevent noise introduction from direct band truncation, we employ Gaussian band-pass filtering to smoothly decompose the frequency domain features, further optimizing the feature representation. The experimental results show that our model achieves state-of-the-art performance on six public benchmarks with only 0.2 million parameters.

</details>


### [53] [Decentralized Fairness Aware Multi Task Federated Learning for VR Network](https://arxiv.org/abs/2512.02513)
*Krishnendu S. Tharakan,Carlo Fischione*

Main category: cs.LG

TL;DR: 这篇论文提出了一种新颖的去中心化多任务公平联邦学习（DMTFL）算法，用于在基站（BS）缓存和预取虚拟现实（VR）用户的视场（FOV），从而解决无线VR视频传输中的挑战。


<details>
  <summary>Details</summary>
Motivation: 无线连接的虚拟现实（VR）体验面临质量要求高、延迟限制和VR设备能力有限等挑战，传统的联邦学习（FL）方法无法很好地处理用户和基站之间的统计异构性。

Method: 本研究提出了一种去中心化多任务公平联邦学习（DMTFL）算法。该算法为每个基站（BS）定制缓存策略，学习个性化的缓存模型，并对其进行优化，使其在任何目标分布下都能表现良好。文章通过Rademacher复杂度和PAC（可能近似正确）界限提供了理论保证。

Result: 仿真结果表明，与基线算法相比，所提出的DMTFL算法在解决无线VR视频传输挑战方面表现出优越性。

Conclusion: 所提出的DMTFL算法通过在基站缓存和预取VR用户的视场，有效地解决了无线VR视频传输中面临的挑战，并在理论和实践中都表现出色。

Abstract: Wireless connectivity promises to unshackle virtual reality (VR) experiences, allowing users to engage from anywhere, anytime. However, delivering seamless, high-quality, real-time VR video wirelessly is challenging due to the stringent quality of experience requirements, low latency constraints, and limited VR device capabilities. This paper addresses these challenges by introducing a novel decentralized multi task fair federated learning (DMTFL) based caching that caches and prefetches each VR user's field of view (FOV) at base stations (BSs) based on the caching strategies tailored to each BS. In federated learning (FL) in its naive form, often biases toward certain users, and a single global model fails to capture the statistical heterogeneity across users and BSs. In contrast, the proposed DMTFL algorithm personalizes content delivery by learning individual caching models at each BS. These models are further optimized to perform well under any target distribution, while providing theoretical guarantees via Rademacher complexity and a probably approximately correct (PAC) bound on the loss. Using a realistic VR head-tracking dataset, our simulations demonstrate the superiority of our proposed DMTFL algorithm compared to baseline algorithms.

</details>


### [54] [DPWMixer: Dual-Path Wavelet Mixer for Long-Term Time Series Forecasting](https://arxiv.org/abs/2512.02070)
*Li Qianyang,Zhang Xingjun,Wang Shaoxun,Wei Jia*

Main category: cs.LG

TL;DR: DPWMixer是一个计算高效的双路径架构，通过无损哈尔小波金字塔分解趋势和局部波动，并采用双路径趋势混合器和自适应多尺度融合模块进行预测，在多个公开基准测试中取得了持续改进。


<details>
  <summary>Details</summary>
Motivation: Transformer模型在长程时间序列预测中存在二次复杂度和过拟合问题，而线性模型难以捕捉复杂非线性局部动态。现有多尺度框架依赖于平均池化，导致频谱混叠和高频瞬态信息的不可逆损失。

Method: DPWMixer提出一个计算高效的双路径架构。该框架基于无损哈尔小波金字塔，取代传统池化，利用正交分解明确分离趋势和局部波动，且无信息损失。设计了一个双路径趋势混合器，整合了用于宏观趋势锚定的全局线性映射和用于微观动态演化的灵活基于块的MLP-Mixer。最后，一个自适应多尺度融合模块根据通道平稳性加权，整合来自不同尺度的预测以优化合成。

Result: 在八个公共基准测试中，DPWMixer比现有最先进的基线方法取得了持续的改进。

Conclusion: DPWMixer通过其独特的双路径架构、无损哈尔小波金字塔、双路径趋势混合器和自适应多尺度融合模块，有效解决了长程时间序列预测中的挑战，并取得了优异的性能。

Abstract: Long-term time series forecasting (LTSF) is a critical task in computational intelligence. While Transformer-based models effectively capture long-range dependencies, they often suffer from quadratic complexity and overfitting due to data sparsity. Conversely, efficient linear models struggle to depict complex non-linear local dynamics. Furthermore, existing multi-scale frameworks typically rely on average pooling, which acts as a non-ideal low-pass filter, leading to spectral aliasing and the irreversible loss of high-frequency transients. In response, this paper proposes DPWMixer, a computationally efficient Dual-Path architecture. The framework is built upon a Lossless Haar Wavelet Pyramid that replaces traditional pooling, utilizing orthogonal decomposition to explicitly disentangle trends and local fluctuations without information loss. To process these components, we design a Dual-Path Trend Mixer that integrates a global linear mapping for macro-trend anchoring and a flexible patch-based MLP-Mixer for micro-dynamic evolution. Finally, An adaptive multi-scale fusion module then integrates predictions from diverse scales, weighted by channel stationarity to optimize synthesis. Extensive experiments on eight public benchmarks demonstrate that our method achieves a consistent improvement over state-of-the-art baselines. The code is available at https://github.com/hit636/DPWMixer.

</details>


### [55] [Adversarial Jamming for Autoencoder Distribution Matching](https://arxiv.org/abs/2512.02740)
*Waleed El-Geresy,Deniz Gündüz*

Main category: cs.LG

TL;DR: 该论文提出了一种利用对抗性无线干扰来正则化自编码器潜在空间的新方法，使其符合对角高斯分布。


<details>
  <summary>Details</summary>
Motivation: 传统的自编码器在潜在空间分布匹配上存在挑战，作者旨在通过引入对抗性干扰来解决这一问题，从而使潜在空间分布更接近对角高斯分布。

Method: 本文利用了现有的理论成果，即在编码器、解码器和对抗性干扰器之间的最小最大博弈中，鞍点由干扰器输出的对角高斯噪声组成。作者以此为灵感，将干扰作为辅助目标，鼓励聚合的潜在后验分布与对角高斯分布匹配。

Result: 该方法在潜在空间分布匹配方面取得了与标准变分自编码器和Wasserstein自编码器相当的效果。

Conclusion: 通过引入对抗性无线干扰作为辅助目标，可以有效地正则化自编码器的潜在空间，使其符合对角高斯分布，并且该方法可以推广到其他潜在分布。

Abstract: We propose the use of adversarial wireless jamming to regularise the latent space of an autoencoder to match a diagonal Gaussian distribution. We consider the minimisation of a mean squared error distortion, where a jammer attempts to disrupt the recovery of a Gaussian source encoded and transmitted over the adversarial channel. A straightforward consequence of existing theoretical results is the fact that the saddle point of a minimax game - involving such an encoder, its corresponding decoder, and an adversarial jammer - consists of diagonal Gaussian noise output by the jammer. We use this result as inspiration for a novel approach to distribution matching in the latent space, utilising jamming as an auxiliary objective to encourage the aggregated latent posterior to match a diagonal Gaussian distribution. Using this new technique, we achieve distribution matching comparable to standard variational autoencoders and to Wasserstein autoencoders. This approach can also be generalised to other latent distributions.

</details>


### [56] [HTG-GCL: Leveraging Hierarchical Topological Granularity from Cellular Complexes for Graph Contrastive Learning](https://arxiv.org/abs/2512.02073)
*Qirui Ji,Bin Qin,Yifan Jin,Yunze Zhao,Chuxiong Sun,Changwen Zheng,Jianwen Cao,Jiangmeng Li*

Main category: cs.LG

TL;DR: HTG-GCL是一种新的图对比学习框架，它通过生成多尺度环基胞腔复形来创建多样的拓扑视图，并利用不确定性估计的粒度特定加权机制来实现多粒度解耦对比，从而有效学习图表示。


<details>
  <summary>Details</summary>
Motivation: 现有的图对比学习方法在识别任务相关拓扑结构以及适应不同下游任务所需的粗粒度到细粒度拓扑粒度方面存在困难。

Method: 我们引入了分层拓扑粒度图对比学习（HTG-GCL）框架。该方法利用同一图的变换来生成多尺度的环基胞腔复形，从而体现拓扑粒度的概念，并生成多样化的拓扑视图。我们提出了一种多粒度解耦对比方法，并基于不确定性估计应用了粒度特定的加权机制，以应对某些粒度中可能包含误导性语义的问题。

Result: 在各种基准测试上的综合实验证明了HTG-GCL的有效性，突显了其在通过分层拓扑信息捕获有意义的图表示方面的卓越性能。

Conclusion: HTG-GCL通过引入分层拓扑粒度和多粒度解耦对比，有效解决了现有GCL方法在识别和适应不同拓扑粒度方面的挑战，并在图表示学习方面取得了优异的性能。

Abstract: Graph contrastive learning (GCL) aims to learn discriminative semantic invariance by contrasting different views of the same graph that share critical topological patterns. However, existing GCL approaches with structural augmentations often struggle to identify task-relevant topological structures, let alone adapt to the varying coarse-to-fine topological granularities required across different downstream tasks. To remedy this issue, we introduce Hierarchical Topological Granularity Graph Contrastive Learning (HTG-GCL), a novel framework that leverages transformations of the same graph to generate multi-scale ring-based cellular complexes, embodying the concept of topological granularity, thereby generating diverse topological views. Recognizing that a certain granularity may contain misleading semantics, we propose a multi-granularity decoupled contrast and apply a granularity-specific weighting mechanism based on uncertainty estimation. Comprehensive experiments on various benchmarks demonstrate the effectiveness of HTG-GCL, highlighting its superior performance in capturing meaningful graph representations through hierarchical topological information.

</details>


### [57] [Cross-View Topology-Aware Graph Representation Learning](https://arxiv.org/abs/2512.02130)
*Ahmet Sami Korkmaz,Selim Coskunuzer,Md Joshem Uddin*

Main category: cs.LG

TL;DR: GraphTCL通过结合GNN结构嵌入和持续同源性拓扑嵌入的双视图对比学习框架，提升了图分类的表示质量和性能。


<details>
  <summary>Details</summary>
Motivation: GNN在图分类中忽视了对整体拓扑特征的捕获，而这些特征对鲁棒表示学习至关重要。

Method: 提出GraphTCL，一个双视图对比学习框架，它将GNN的结构嵌入与从持续同源性派生的拓扑嵌入相结合，并通过跨视图对比损失对齐这些互补视图。

Result: 在TU和OGB分子图等基准数据集上，GraphTCL的性能持续优于最先进的基线。

Conclusion: 拓扑感知对比学习对于改进图表示方法具有重要意义。

Abstract: Graph classification has gained significant attention due to its applications in chemistry, social networks, and bioinformatics. While Graph Neural Networks (GNNs) effectively capture local structural patterns, they often overlook global topological features that are critical for robust representation learning. In this work, we propose GraphTCL, a dual-view contrastive learning framework that integrates structural embeddings from GNNs with topological embeddings derived from persistent homology. By aligning these complementary views through a cross-view contrastive loss, our method enhances representation quality and improves classification performance. Extensive experiments on benchmark datasets, including TU and OGB molecular graphs, demonstrate that GraphTCL consistently outperforms state-of-the-art baselines. This study highlights the importance of topology-aware contrastive learning for advancing graph representation methods.

</details>


### [58] [CLEF: Clinically-Guided Contrastive Learning for Electrocardiogram Foundation Models](https://arxiv.org/abs/2512.02180)
*Yuxuan Shu,Peter H. Charlton,Fahim Kawsar,Jussi Hernesniemi,Mohammad Malekzadeh*

Main category: cs.LG

TL;DR: 本文介绍了一种名为CLEF的临床指导对比学习方法，它利用临床风险评分来预训练单导联心电图（ECG）基础模型。该方法在多个临床任务中表现优于现有的自监督学习方法，从而提高了远程健康监测的准确性和可扩展性。


<details>
  <summary>Details</summary>
Motivation: 现有的自监督预训练方法在ECG基础模型中通常不包含临床元数据领域知识，导致诊断性能提升有限。

Method: CLEF方法引入了一种新颖的对比学习策略，通过利用已建立的临床风险评分来自适应地加权负样本。该方法将ECG嵌入的相似性与受试者之间临床上有意义的差异对齐，并具有处理缺失元数据的明确机制。在MIMIC-IV数据集中，使用161K患者的12导联ECG数据进行预训练，无需逐样本ECG标注。

Result: CLEF在18项临床分类和回归任务上进行了评估，并通过7个独立的测试集进行基准测试。与自监督基础模型基线相比，中等规模的CLEF在分类任务中平均AUROC提高了至少2.6%，在回归任务中平均MAE降低了至少3.2%。与现有自监督学习算法相比，CLEF的平均AUROC提高了至少1.8%。在仅使用导联-I数据进行分类任务预训练时，CLEF的表现与最先进的ECGFounder（监督学习训练）相当。

Conclusion: CLEF通过整合临床领域知识，显著提高了单导联ECG分析的准确性和可扩展性，为远程健康监测提供了更有效的工具。

Abstract: The electrocardiogram (ECG) is a key diagnostic tool in cardiovascular health. Single-lead ECG recording is integrated into both clinical-grade and consumer wearables. While self-supervised pretraining of foundation models on unlabeled ECGs improves diagnostic performance, existing approaches do not incorporate domain knowledge from clinical metadata. We introduce a novel contrastive learning approach that utilizes an established clinical risk score to adaptively weight negative pairs: clinically-guided contrastive learning. It aligns the similarities of ECG embeddings with clinically meaningful differences between subjects, with an explicit mechanism to handle missing metadata. On 12-lead ECGs from 161K patients in the MIMIC-IV dataset, we pretrain single-lead ECG foundation models at three scales, collectively called CLEF, using only routinely collected metadata without requiring per-sample ECG annotations. We evaluate CLEF on 18 clinical classification and regression tasks across 7 held-out datasets, and benchmark against 5 foundation model baselines and 3 self-supervised algorithms. When pretrained on 12-lead ECG data and tested on lead-I data, CLEF outperforms self-supervised foundation model baselines: the medium-sized CLEF achieves average AUROC improvements of at least 2.6% in classification and average reductions in MAEs of at least 3.2% in regression. Comparing with existing self-supervised learning algorithms, CLEF improves the average AUROC by at least 1.8%. Moreover, when pretrained only on lead-I data for classification tasks, CLEF performs comparably to the state-of-the-art ECGFounder, which was trained in a supervised manner. Overall, CLEF enables more accurate and scalable single-lead ECG analysis, advancing remote health monitoring. Code and pretrained CLEF models are available at: github.com/Nokia-Bell-Labs/ecg-foundation-model.

</details>


### [59] [WhAM: Towards A Translative Model of Sperm Whale Vocalization](https://arxiv.org/abs/2512.02206)
*Orr Paradise,Pranav Muralikrishnan,Liangyuan Chen,Hugo Flores García,Bryan Pardo,Roee Diamant,David F. Gruber,Shane Gero,Shafi Goldwasser*

Main category: cs.LG

TL;DR: WhAM是首个基于transformer模型，它可以从任意音频提示中生成合成抹香鲸尾声，并保留了声学特征。


<details>
  <summary>Details</summary>
Motivation: 以往的模型无法从任意音频提示中生成高质量的合成抹香鲸尾声，因此需要开发新的模型来解决这个问题。

Method: WhAM模型通过对在音乐音频上预训练的VampNet（一种掩码声学token模型）进行微调而构建，使用了过去二十年收集的1万个尾声录音。通过迭代掩码token预测，WhAM生成了高保真的合成尾声。

Result: WhAM生成的合成尾声保留了声源录音的关键声学特征，在韵律、社会单元和元音分类等下游分类任务中取得了很好的表现。

Conclusion: WhAM模型能够生成高质量的合成抹香鲸尾声，并且其学习到的表示在分类任务中表现出色，这为未来研究和鲸鱼保护提供了新的工具。

Abstract: Sperm whales communicate in short sequences of clicks known as codas. We present WhAM (Whale Acoustics Model), the first transformer-based model capable of generating synthetic sperm whale codas from any audio prompt. WhAM is built by finetuning VampNet, a masked acoustic token model pretrained on musical audio, using 10k coda recordings collected over the past two decades. Through iterative masked token prediction, WhAM generates high-fidelity synthetic codas that preserve key acoustic features of the source recordings. We evaluate WhAM's synthetic codas using Fréchet Audio Distance and through perceptual studies with expert marine biologists. On downstream classification tasks including rhythm, social unit, and vowel classification, WhAM's learned representations achieve strong performance, despite being trained for generation rather than classification. Our code is available at https://github.com/Project-CETI/wham

</details>


### [60] [InstructLR: A Scalable Approach to Create Instruction Dataset for Under-Resourced Languages](https://arxiv.org/abs/2512.02213)
*Mamadou K. Keita,Sebastien Diarra,Christopher Homan,Seydou Diallo*

Main category: cs.LG

TL;DR: 本文介绍了InstructLR，这是一个为低资源语言（LRLs）生成高质量指令数据集的新颖框架，解决了现有大型语言模型（LLMs）在支持LRLs文本生成和聊天界面方面的挑战。


<details>
  <summary>Details</summary>
Motivation: 大型语言模型（LLMs）在低资源语言（LRLs）的文本生成和聊天界面方面存在支持挑战，主要原因是难以获取高质量的LRL指令数据集。现有的自动化翻译和合成数据生成方法常常导致输出不流畅或存在拼写不一致。

Method: InstructLR框架结合了LLM驱动的文本生成技术和一个双层质量过滤机制。该机制包括一个基于检索增强生成（RAG）的N-shot提示的自动化过滤层，以及一个人工验证层。受MMLU等基准测试的任务定义启发，InstructLR旨在创建多领域指令基准。

Result: InstructLR成功创建了三个多领域指令基准：ZarmaInstruct-50k、BambaraInstruct-50k和FulfuldeInstruct-50k。

Conclusion: InstructLR框架通过其创新的LLM驱动文本生成与双层质量过滤机制，有效地解决了低资源语言高质量指令数据集的缺乏问题，并成功构建了多个语言的指令基准，为LRLs的文本生成和聊天界面提供了重要的支持。

Abstract: Effective text generation and chat interfaces for low-resource languages (LRLs) remain a challenge for state-of-the-art large language models (LLMs) to support. This is mainly due to the difficulty of curating high-quality instruction datasets for LRLs, a limitation prevalent in the languages spoken across the African continent and other regions. Current approaches, such as automated translation and synthetic data generation, frequently yield outputs that lack fluency or even orthographic consistency. In this paper, we introduce InstructLR, a novel framework designed to generate high-quality instruction datasets for LRLs. Our approach integrates LLM-driven text generation with a dual-layer quality filtering mechanism: an automated filtering layer based on retrieval-augmented-generation (RAG)-based n-shot prompting, and a human-in-the-loop validation layer. Drawing inspiration from benchmarks such as MMLU in task definition, InstructLR has facilitated the creation of three multi-domain instruction benchmarks: ZarmaInstruct-50k, BambaraInstruct-50k, and FulfuldeInstruct-50k.

</details>


### [61] [Improved Training Mechanism for Reinforcement Learning via Online Model Selection](https://arxiv.org/abs/2512.02214)
*Aida Afshar,Aldo Pacchiano*

Main category: cs.LG

TL;DR: 本文探讨了强化学习中的在线模型选择问题，旨在通过自适应地选择具有正确配置的智能体来提高效率和性能。


<details>
  <summary>Details</summary>
Motivation: 研究动机在于通过将在线模型选择方法集成到强化学习训练过程中，提高强化学习的效率和性能。

Method: 本文从理论角度探讨了识别正确配置的有效理论特征，并解决了三个实际标准：高效资源分配、非平稳动力学下的适应性以及不同随机种子下的训练稳定性。

Result: 理论结果得到了经验证据的支持，这些证据来自强化学习中的各种模型选择任务，包括神经架构选择、步长选择和自模型选择。

Conclusion: 通过在线模型选择，可以显著提高强化学习的性能和效率，尤其是在资源分配、非平稳环境适应和训练稳定性方面。

Abstract: We study the problem of online model selection in reinforcement learning, where the selector has access to a class of reinforcement learning agents and learns to adaptively select the agent with the right configuration. Our goal is to establish the improved efficiency and performance gains achieved by integrating online model selection methods into reinforcement learning training procedures. We examine the theoretical characterizations that are effective for identifying the right configuration in practice, and address three practical criteria from a theoretical perspective: 1) Efficient resource allocation, 2) Adaptation under non-stationary dynamics, and 3) Training stability across different seeds. Our theoretical results are accompanied by empirical evidence from various model selection tasks in reinforcement learning, including neural architecture selection, step-size selection, and self model selection.

</details>


### [62] [The Effect of Enforcing Fairness on Reshaping Explanations in Machine Learning Models](https://arxiv.org/abs/2512.02265)
*Joshua Wolff Anderson,Shyam Visweswaran*

Main category: cs.LG

TL;DR: 为了实现医疗领域中值得信任的机器学习，该论文研究了在应用公平性约束后，模型解释性（特别是基于Shapley的特征排名）如何变化。


<details>
  <summary>Details</summary>
Motivation: 在医疗健康领域，机器学习模型的可靠性不仅要求高预测性能，还需要公平性和可解释性。尽管已知提高公平性可能会影响预测性能，但目前对于公平性改进如何影响可解释性（这是获得临床信任的关键因素）知之甚少。临床医生可能会对那些在应用公平性约束后解释发生变化的模型持犹豫态度。

Method: 本研究通过偏见缓解技术来增强公平性，并检验其如何重塑基于Shapley的特征排名。研究量化了在三个数据集（儿童泌尿道感染风险、直接抗凝剂出血风险和累犯风险）上应用公平性约束后，特征重要性排名的变化。此外，研究还评估了多种模型类别在基于Shapley排名稳定性方面的表现。

Result: 研究发现，提高跨种族亚组的模型公平性会显著改变特征重要性排名，并且在不同群体中可能以不同方式改变。

Conclusion: 这些结果强调了在模型评估中需要联合考虑准确性、公平性和可解释性，而不是孤立地评估它们。

Abstract: Trustworthy machine learning in healthcare requires strong predictive performance, fairness, and explanations. While it is known that improving fairness can affect predictive performance, little is known about how fairness improvements influence explainability, an essential ingredient for clinical trust. Clinicians may hesitate to rely on a model whose explanations shift after fairness constraints are applied. In this study, we examine how enhancing fairness through bias mitigation techniques reshapes Shapley-based feature rankings. We quantify changes in feature importance rankings after applying fairness constraints across three datasets: pediatric urinary tract infection risk, direct anticoagulant bleeding risk, and recidivism risk. We also evaluate multiple model classes on the stability of Shapley-based rankings. We find that increasing model fairness across racial subgroups can significantly alter feature importance rankings, sometimes in different ways across groups. These results highlight the need to jointly consider accuracy, fairness, and explainability in model assessment rather than in isolation.

</details>


### [63] [Limitations of Membership Queries in Testable Learning](https://arxiv.org/abs/2512.02279)
*Jane Lange,Mingda Qiao*

Main category: cs.LG

TL;DR: 本文探讨了在可测试学习模型中，成员查询（MQ）对学习算法时间复杂度的影响。研究表明，MQ不能显著降低可测试学习算法的时间复杂度，使其超越仅依赖样本的特定分布学习的复杂度。


<details>
  <summary>Details</summary>
Motivation: 在某些学习任务中，成员查询（MQ）可以提高学习速度，尤其是在特定分布设置下。然而，在Rubinfeld和Vasilyan提出的可测试学习模型中，MQ是否仍然能够带来显著的速度提升，是一个悬而未决的问题。

Method: 本文首先展示了在可测试学习模型中，成员查询无法降低学习算法的时间复杂度，使其超越仅依赖样本的特定分布学习的复杂度。接着，通过将基于样本的布尔概念类反驳（如Vadhan17和KL18中提出的）归约到带查询的可测试学习（TL-Q），从而为TL-Q推导出下界。最后，定义了一类“统计”MQ算法，并证明了这类算法的TL-Q算法能够有效实现统计查询反驳和学习算法。

Result: 研究结果表明，在可测试学习模型中，任何$m$样本的TL-Q算法在时间效率上不可能比最好的$m$样本PAC学习器具有超多项式的优势。此外，本文定义了一类包含许多现有特定分布MQ学习器的“统计” MQ算法，并发现这类算法的TL-Q算法能够有效地进行统计查询反驳和学习。

Conclusion: 本文证明了在可测试学习模型中，成员查询不能显著提高学习算法的时间效率。此外，研究结果还对一类广泛使用的统计MQ算法的可测试性提出了质疑，结合已知的统计查询维度下界，意味着这些效率高的成员查询学习器无法被设计成可测试的。

Abstract: Membership queries (MQ) often yield speedups for learning tasks, particularly in the distribution-specific setting. We show that in the \emph{testable learning} model of Rubinfeld and Vasilyan [RV23], membership queries cannot decrease the time complexity of testable learning algorithms beyond the complexity of sample-only distribution-specific learning. In the testable learning model, the learner must output a hypothesis whenever the data distribution satisfies a desired property, and if it outputs a hypothesis, the hypothesis must be near-optimal.
  We give a general reduction from sample-based \emph{refutation} of boolean concept classes, as presented in [Vadhan17, KL18], to testable learning with queries (TL-Q). This yields lower bounds for TL-Q via the reduction from learning to refutation given in [KL18]. The result is that, relative to a concept class and a distribution family, no $m$-sample TL-Q algorithm can be super-polynomially more time-efficient than the best $m$-sample PAC learner.
  Finally, we define a class of ``statistical'' MQ algorithms that encompasses many known distribution-specific MQ learners, such as those based on influence estimation or subcube-conditional statistical queries. We show that TL-Q algorithms in this class imply efficient statistical-query refutation and learning algorithms. Thus, combined with known SQ dimension lower bounds, our results imply that these efficient membership query learners cannot be made testable.

</details>


### [64] [Training Dynamics of Learning 3D-Rotational Equivariance](https://arxiv.org/abs/2512.02303)
*Max W. Shen,Ewa Nowara,Michael Maser,Kyunghyun Cho*

Main category: cs.LG

TL;DR: 本文提出了一种衡量等变误差的新方法，并发现模型在训练早期就能快速学习到3D旋转等变性，因为学习等变性比主要预测任务更容易。


<details>
  <summary>Details</summary>
Motivation: 探索模型在数据增强训练下学习对称性的效率和机制，并量化等变误差。

Method: 推导了一种原则性的等变误差度量方法，并将其应用于高维分子任务（流匹配、力场预测、去噪体素）中的3D旋转等变性研究。

Result: 模型在1k-10k的训练步骤内，能将等变误差迅速降低到$\\%\\le$2\\% 的留存损失，且该结果对模型和数据集大小具有鲁棒性。研究发现，3D旋转等变性的学习任务相对更容易，具有更平滑和条件更好的损失函数。对于3D旋转，非等变模型在整个训练过程中的损失惩罚较小。

Conclusion: 模型能快速有效地学习3D旋转等变性，这得益于等变性学习任务本身的简易性。在某些情况下，除非等变模型能弥补“效率差距”，否则非等变模型可能在测试损失上表现更好。

Abstract: While data augmentation is widely used to train symmetry-agnostic models, it remains unclear how quickly and effectively they learn to respect symmetries. We investigate this by deriving a principled measure of equivariance error that, for convex losses, calculates the percent of total loss attributable to imperfections in learned symmetry. We focus our empirical investigation to 3D-rotation equivariance on high-dimensional molecular tasks (flow matching, force field prediction, denoising voxels) and find that models reduce equivariance error quickly to $\leq$2\% held-out loss within 1k-10k training steps, a result robust to model and dataset size. This happens because learning 3D-rotational equivariance is an easier learning task, with a smoother and better-conditioned loss landscape, than the main prediction task. For 3D rotations, the loss penalty for non-equivariant models is small throughout training, so they may achieve lower test loss than equivariant models per GPU-hour unless the equivariant ``efficiency gap'' is narrowed. We also experimentally and theoretically investigate the relationships between relative equivariance error, learning gradients, and model parameters.

</details>


### [65] [Retrieval-Augmented Memory for Online Learning](https://arxiv.org/abs/2512.02333)
*Wenzhang Du*

Main category: cs.LG

TL;DR: 该文章探讨了在概念漂移（concept drift）的流式监督学习中使用检索增强模型，介绍了RAM-OL模型，并分析了其在非平稳环境中的表现。


<details>
  <summary>Details</summary>
Motivation: 检索增强模型在概念漂移的流式监督学习中的应用尚未得到充分理解，文章旨在研究在线分类在非平稳环境下的表现。

Method: 文章提出了检索增强记忆在线学习（RAM-OL）模型，它是随机梯度下降的简单扩展，通过维护一个过去示例的小缓冲区来操作。在每个时间步，RAM-OL检索当前输入在隐藏表示空间中的几个最近邻居，并联合当前示例和检索到的邻居更新模型。文章比较了朴素回放变体和门控回放变体，后者使用时间窗口、相似性阈值和梯度重加权来约束邻居，以平衡快速重用相关历史数据与对过时机制的鲁棒性。

Result: 在强周期性漂移流上，RAM-OL将预估准确率提高了约7个百分点，并大大降低了随机种子的方差。在嘈杂的航空流上，门控变体与纯在线基线模型表现相近。

Conclusion: 检索增强记忆是概念漂移下在线学习的一种实用且鲁棒的工具。

Abstract: Retrieval-augmented models couple parametric predictors with non-parametric memories, but their use in streaming supervised learning with concept drift is not well understood. We study online classification in non-stationary environments and propose Retrieval-Augmented Memory for Online Learning (RAM-OL), a simple extension of stochastic gradient descent that maintains a small buffer of past examples. At each time step, RAM-OL retrieves a few nearest neighbours of the current input in the hidden representation space and updates the model jointly on the current example and the retrieved neighbours. We compare a naive replay variant with a gated replay variant that constrains neighbours using a time window, similarity thresholds, and gradient reweighting, in order to balance fast reuse of relevant past data against robustness to outdated regimes. From a theoretical perspective, we interpret RAM-OL under a bounded drift model and discuss how retrieval can reduce adaptation cost and improve regret constants when patterns recur over time. Empirically, we instantiate RAM-OL on a simple online multilayer perceptron and evaluate it on three real-world data streams derived from electricity pricing, electricity load, and airline delay data. On strongly and periodically drifting streams, RAM-OL improves prequential accuracy by up to about seven percentage points and greatly reduces variance across random seeds, while on a noisy airline stream the gated variant closely matches the purely online baseline. These results show that retrieval-augmented memory is a practical and robust tool for online learning under concept drift.

</details>


### [66] [SpecPV: Improving Self-Speculative Decoding for Long-Context Generation via Partial Verification](https://arxiv.org/abs/2512.02337)
*Zhendong Tan,Xingjun Zhang,Chaoyi Hu,Junjie Peng,Kun Xia*

Main category: cs.LG

TL;DR: SpecPV加速LLM长上下文生成，通过优化推测解码中的验证瓶颈，实现高达6倍的速度提升，同时保持较低的性能下降。


<details>
  <summary>Details</summary>
Motivation: 随着代码生成、深度推理和长文档理解等任务的需求增长，长上下文生成成为大型语言模型（LLM）的关键能力。推测解码是加速生成的最直接有效方法之一，但随着上下文长度增加，验证成为主要瓶颈。

Method: 我们引入了SpecPV，一种自推测解码方法。SpecPV利用部分键值状态（KV）进行快速验证，并通过周期性地应用完整验证来消除累积误差。

Result: SpecPV在多个长上下文基准测试和模型（包括LLaMA-3.1-8B-Instruct和Qwen3系列）上都取得了显著效果。实验结果显示，相对于标准自回归解码，SpecPV将解码速度提升了高达6倍。

Conclusion: SpecPV通过解决长上下文生成中推测解码的验证瓶颈，显著提高了LLM的推理速度，并且只带来了轻微的性能下降，这对于需要长上下文处理的任务具有重要意义。

Abstract: Growing demands from tasks like code generation, deep reasoning, and long-document understanding have made long-context generation a crucial capability for large language models (LLMs). Speculative decoding is one of the most direct and effective approaches for accelerating generation. It follows a draft-verify paradigm, where a lightweight draft model proposes several candidate tokens and the target model verifies them. However, we find that as the context length grows, verification becomes the dominant bottleneck. To further accelerate speculative decoding in long-context generation, we introduce SpecPV, a self-speculative decoding approach that performs fast verification using partial key-value states (KV) and periodically applies full verification to eliminate accumulated errors. We validate SpecPV across multiple long-context benchmarks and models, including LLaMA-3.1-8B-Instruct and Qwen3-series. Experimental results show that SpecPV achieves up to 6x decoding speedup over standard autoregressive decoding with minor degradation.

</details>


### [67] [FOVA: Offline Federated Reinforcement Learning with Mixed-Quality Data](https://arxiv.org/abs/2512.02350)
*Nan Qiao,Sheng Yue,Ju Ren,Yaoxue Zhang*

Main category: cs.LG

TL;DR: 本文提出了FOVA，一个基于投票的离线联邦强化学习框架，以解决现有方法在混合质量数据下性能下降的问题。它通过投票机制识别高回报动作，利用优势加权回归构建一致的训练目标，并通过理论分析和实验证明了其有效性。


<details>
  <summary>Details</summary>
Motivation: 现有的离线联邦强化学习方法在处理客户端之间策略质量不同的混合质量数据时，性能会急剧下降，因此需要一种新的框架来克服这一限制。

Method: 本文引入了一个名为FOVA的投票机制的离线联邦强化学习框架。该框架利用投票机制在局部策略评估期间识别高回报动作，以减轻来自不同局部学习策略的低质量行为的负面影响。此外，FOVA基于优势加权回归（AWR）构建了一致的局部和全局训练目标，从而显著提高了效率和稳定性。

Result: FOVA在广泛使用的基准测试中，相对于现有基线算法，取得了显著的性能提升。

Conclusion: FOVA通过投票机制和优势加权回归，有效解决了离线联邦强化学习在混合质量数据下的性能下降问题，并通过理论和实验证明了其优越性，实现了策略的严格改进。

Abstract: Offline Federated Reinforcement Learning (FRL), a marriage of federated learning and offline reinforcement learning, has attracted increasing interest recently. Albeit with some advancement, we find that the performance of most existing offline FRL methods drops dramatically when provided with mixed-quality data, that is, the logging behaviors (offline data) are collected by policies with varying qualities across clients. To overcome this limitation, this paper introduces a new vote-based offline FRL framework, named FOVA. It exploits a \emph{vote mechanism} to identify high-return actions during local policy evaluation, alleviating the negative effect of low-quality behaviors from diverse local learning policies. Besides, building on advantage-weighted regression (AWR), we construct consistent local and global training objectives, significantly enhancing the efficiency and stability of FOVA. Further, we conduct an extensive theoretical analysis and rigorously show that the policy learned by FOVA enjoys strict policy improvement over the behavioral policy. Extensive experiments corroborate the significant performance gains of our proposed algorithm over existing baselines on widely used benchmarks.

</details>


### [68] [Reinforcement Learning in POMDP's via Direct Gradient Ascent](https://arxiv.org/abs/2512.02383)
*Jonathan Baxter,Peter L. Bartlett*

Main category: cs.LG

TL;DR: 本文探讨了基于梯度的 POMDPs 策略优化方法，并提出了 GPOMDP 算法，该算法使用单个样本路径、一个自由参数且无需底层状态知识，即可估计平均奖励梯度。


<details>
  <summary>Details</summary>
Motivation: 在受控 POMDP 中，需要直接优化策略性能，而基于梯度的方法是解决此问题的一种途径。

Method: 本文提出了 GPOMDP 算法。该算法类似于 REINFORCE，用于估计作为随机策略参数函数的平均奖励梯度的近似值。GPOMDP 算法的主要优点包括：仅需要底层马尔可夫链的单个样本路径；只使用一个自由参数 β ∈ [0,1)；不需要知道底层状态。

Result: GPOMDP 算法收敛，并且其生成的梯度估计可以用于共轭梯度过程，以找到平均奖励的局部最优值。

Conclusion: GPOMDP 算法在 POMDPs 策略优化方面具有有效性和实用性，为解决相关问题提供了新的思路和工具。

Abstract: This paper discusses theoretical and experimental aspects of gradient-based approaches to the direct optimization of policy performance in controlled POMDPs. We introduce GPOMDP, a REINFORCE-like algorithm for estimating an approximation to the gradient of the average reward as a function of the parameters of a stochastic policy. The algorithm's chief advantages are that it requires only a single sample path of the underlying Markov chain, it uses only one free parameter $β\in [0,1)$, which has a natural interpretation in terms of bias-variance trade-off, and it requires no knowledge of the underlying state. We prove convergence of GPOMDP and show how the gradient estimates produced by GPOMDP can be used in a conjugate-gradient procedure to find local optima of the average reward.

</details>


### [69] [Risk-Sensitive Q-Learning in Continuous Time with Application to Dynamic Portfolio Selection](https://arxiv.org/abs/2512.02386)
*Chuhan Xie*

Main category: cs.LG

TL;DR: 本文研究了连续时间域中的风险敏感强化学习（RSRL）问题，并提出了一种新的基于鞅特征化的风险敏感Q学习算法。


<details>
  <summary>Details</summary>
Motivation: 在连续时间域中，研究以可控随机微分方程（SDE）为特征，目标是累积奖励的非线性函数的风险敏感强化学习（RSRL）问题。

Method: 证明了当函数是优化确定性等价（OCE）时，最优策略是关于增强环境的马尔可夫策略。提出了一种基于新型鞅特征化方法的风险敏感Q学习算法，命名为CT-RS-q。

Result: 在动态投资组合选择问题上进行了仿真研究。

Conclusion: 仿真结果说明了所提出算法的有效性。

Abstract: This paper studies the problem of risk-sensitive reinforcement learning (RSRL) in continuous time, where the environment is characterized by a controllable stochastic differential equation (SDE) and the objective is a potentially nonlinear functional of cumulative rewards. We prove that when the functional is an optimized certainty equivalent (OCE), the optimal policy is Markovian with respect to an augmented environment. We also propose \textit{CT-RS-q}, a risk-sensitive q-learning algorithm based on a novel martingale characterization approach. Finally, we run a simulation study on a dynamic portfolio selection problem and illustrate the effectiveness of our algorithm.

</details>


### [70] [ESACT: An End-to-End Sparse Accelerator for Compute-Intensive Transformers via Local Similarity](https://arxiv.org/abs/2512.02403)
*Hongxiang Liu,Zhifang Deng,Tong Pu,Shengli Lu*

Main category: cs.LG

TL;DR: ESACT：用于Transformer的端到端稀疏加速器。


<details>
  <summary>Details</summary>
Motivation: Transformer模型因其出色的性能在各个领域占据主导地位，但其高计算成本阻碍了高效的硬件部署。现有加速器大多只利用注意力中的行内稀疏性，很少考虑行间稀疏性，而利用行间稀疏性的方法往往依赖于昂贵的全局相似性估计，从而降低了稀疏性的加速效益，并且通常只将稀疏性应用于Transformer的一个或两个组件。

Method: 本文提出了ESACT。ESACT的核心是基于局部相似性的稀疏性预测（SPLS）机制，它利用HLog量化在QKV生成之前准确预测局部注意力稀疏性，从而在所有Transformer组件中实现高效稀疏性。为了支持高效的硬件实现，我们引入了三项架构创新。

Result: SPLS将总计算量减少了52.03%，而精度损失小于1%。ESACT实现了3.29 TOPS/W的端到端能效，并且在注意力层面的能效比SOTA注意力加速器SpAtten和Sanger分别提高了2.95倍和2.26倍。

Conclusion: 通过利用局部相似性进行稀疏性预测，ESACT有效解决了Transformer模型的计算成本问题，实现了显著的能效提升和计算量减少，同时保持了高精度。

Abstract: Transformers, composed of QKV generation, attention computation, and FFNs,
  have become the dominant model across various domains due to their outstanding performance.
  However, their high computational cost hinders efficient hardware deployment.
  Sparsity offers a promising solution,
  yet most existing accelerators exploit only intra-row sparsity in attention,
  while few consider inter-row sparsity.
  Approaches leveraging inter-row sparsity often rely on costly global similarity estimation,
  which diminishes the acceleration benefits of sparsity,
  and typically apply sparsity to only one or two transformer components.
  Through careful analysis of the attention distribution and computation flow,
  we observe that local similarity allows end-to-end sparse acceleration with lower computational overhead.
  Motivated by this observation, we propose ESACT,
  an end-to-end sparse accelerator for compute-intensive Transformers.
  ESACT centers on the Sparsity Prediction with Local Similarity (SPLS) mechanism,
  which leverages HLog quantization to accurately predict local attention sparsity prior to QK generation,
  achieving efficient sparsity across all transformer components.
  To support efficient hardware realization, we introduce three architectural innovations.
  Experimental results on 26 benchmarks demonstrate that
  SPLS reduces total computation by 52.03% with less than 1% accuracy loss.
  ESACT achieves an end-to-end energy efficiency of 3.29 TOPS/W,
  and improves attention-level energy efficiency by 2.95x and 2.26x over
  SOTA attention accelerators SpAtten and Sanger, respectively.

</details>


### [71] [Do Large Language Models Walk Their Talk? Measuring the Gap Between Implicit Associations, Self-Report, and Behavioral Altruism](https://arxiv.org/abs/2512.01568)
*Sandro Andric*

Main category: cs.LG

TL;DR: 该研究调查了大型语言模型（LLMs）是否表现出利他主义倾向，以及它们的内隐联想和自我报告能否预测实际的利他主义行为。


<details>
  <summary>Details</summary>
Motivation: 研究LLMs是否展现利他主义倾向，以及其内隐联想和自我报告是否能预测实际利他行为。

Method: 采用多方法途径，包括内隐联想测试（IAT）测量内隐利他主义偏见，强制二元选择任务测量行为利他主义，以及自我评估量表测量显式利他主义信念，对24个前沿LLMs进行了测试。

Result: 所有模型都显示出强烈的内隐亲社会偏见；模型的利他主义行为高于偶然水平，但存在显著差异；内隐联想不能预测行为；模型系统性地高估了自己的利他主义。

Conclusion: LLMs普遍存在“美德信号差距”，即自我报告的利他主义远高于实际行为。因此，提出校准差距作为衡量模型对齐的标准指标，以评估模型的行为可预测性和一致性。

Abstract: We investigate whether Large Language Models (LLMs) exhibit altruistic tendencies, and critically, whether their implicit associations and self-reports predict actual altruistic behavior. Using a multi-method approach inspired by human social psychology, we tested 24 frontier LLMs across three paradigms: (1) an Implicit Association Test (IAT) measuring implicit altruism bias, (2) a forced binary choice task measuring behavioral altruism, and (3) a self-assessment scale measuring explicit altruism beliefs. Our key findings are: (1) All models show strong implicit pro-altruism bias (mean IAT = 0.87, p < .0001), confirming models "know" altruism is good. (2) Models behave more altruistically than chance (65.6% vs. 50%, p < .0001), but with substantial variation (48-85%). (3) Implicit associations do not predict behavior (r = .22, p = .29). (4) Most critically, models systematically overestimate their own altruism, claiming 77.5% altruism while acting at 65.6% (p < .0001, Cohen's d = 1.08). This "virtue signaling gap" affects 75% of models tested. Based on these findings, we recommend the Calibration Gap (the discrepancy between self-reported and behavioral values) as a standardized alignment metric. Well-calibrated models are more predictable and behaviorally consistent; only 12.5% of models achieve the ideal combination of high prosocial behavior and accurate self-knowledge.

</details>


### [72] [Cross-Domain Offline Policy Adaptation with Dynamics- and Value-Aligned Data Filtering](https://arxiv.org/abs/2512.02435)
*Zhongjian Qiao,Rui Yang,Jiafei Lyu,Chenjia Bai,Xiu Li,Zhuoran Yang,Siyang Gao,Shuang Qiu*

Main category: cs.LG

TL;DR: 该文章旨在通过结合有限的目标域数据集和充足的源域数据集来训练智能体。


<details>
  <summary>Details</summary>
Motivation: 解决现有方法在跨域离线强化学习中仅关注动态对齐，而忽略价值对齐的问题。文章认为动态对齐和价值对齐对策略学习都至关重要。

Method: 提出了一种名为DVDF（Dynamics- and Value-aligned Data Filtering）的方法，该方法通过选择性地共享具有高动态和价值对齐的源域样本来解决问题。

Result: DVDF在多种任务和数据集上均优于现有基线方法，尤其在目标域数据集非常有限（仅包含5000个转换）的情况下表现出色。

Conclusion: DVDF方法通过同时考虑动态对齐和价值对齐，显著提升了跨域离线强化学习的性能。

Abstract: Cross-Domain Offline Reinforcement Learning aims to train an agent deployed in the target environment, leveraging both a limited target domain dataset and a source domain dataset with (possibly) sufficient data coverage. Due to the underlying dynamics misalignment between the source and target domain, simply merging the data from two datasets may incur inferior performance. Recent advances address this issue by selectively sharing source domain samples that exhibit dynamics alignment with the target domain. However, these approaches focus solely on dynamics alignment and overlook \textit{value alignment}, i.e., selecting high-quality, high-value samples from the source domain. In this paper, we first demonstrate that both dynamics alignment and value alignment are essential for policy learning, by examining the limitations of the current theoretical framework for cross-domain RL and establishing a concrete sub-optimality gap of a policy trained on the source domain and evaluated on the target domain. Motivated by the theoretical insights, we propose to selectively share those source domain samples with both high dynamics and value alignment and present our \textbf{\underline{D}}ynamics- and \textbf{\underline{V}}alue-aligned \textbf{\underline{D}}ata \textbf{\underline{F}}iltering (DVDF) method. We design a range of dynamics shift settings, including kinematic and morphology shifts, and evaluate DVDF on various tasks and datasets, as well as in challenging extremely low-data settings where the target domain dataset contains only 5,000 transitions. Extensive experiments demonstrate that DVDF consistently outperforms prior strong baselines and delivers exceptional performance across multiple tasks and datasets.

</details>


### [73] [TabGRU: An Enhanced Design for Urban Rainfall Intensity Estimation Using Commercial Microwave Links](https://arxiv.org/abs/2512.02465)
*Xingwang Li,Mengyun Chen,Jiamou Liu,Sijie Wang,Shuanggen Jin,Jafet C. M. Andersson,Jonas Olsson,Remco,van de Beek,Hai Victor Habi,Congzheng Han*

Main category: cs.LG

TL;DR: 该论文提出了一种名为TabGRU的混合深度学习模型，用于高分辨率城市降雨监测。TabGRU结合了Transformer和BiGRU，在瑞典哥德堡的基准数据集上进行了验证，在降雨量估算方面优于现有模型和基于物理的方法。


<details>
  <summary>Details</summary>
Motivation: 面对全球城市化进程的加速和极端天气事件的频繁发生，高分辨率的城市降雨监测对于建设有韧性的智慧城市至关重要。

Method: 本文提出了一种新颖的混合深度学习架构，名为TabGRU，它基于Transformer和双向门控循环单元（BiGRU），协同捕捉CML信号数据中的长期依赖性和局部序列特征。该模型通过可学习的位置嵌入和注意力池化机制进一步增强，以提高其动态特征提取和泛化能力。

Result: TabGRU模型在Torp站点的决定系数（R2）达到0.91，在Barl站点达到0.96，优于深度学习基线模型。与基于物理的方法相比，TabGRU保持了更高的准确性，并且在缓解PL模型在降雨高峰期出现的显著过高估计问题方面特别有效。

Conclusion: TabGRU模型可以有效克服传统方法的局限性，为CML城市降雨监测提供了一个鲁棒而准确的解决方案。

Abstract: In the face of accelerating global urbanization and the increasing frequency of extreme weather events, highresolution urban rainfall monitoring is crucial for building resilient smart cities. Commercial Microwave Links (CMLs) are an emerging data source with great potential for this task.While traditional rainfall retrieval from CMLs relies on physicsbased models, these often struggle with real-world complexities like signal noise and nonlinear attenuation. To address these limitations, this paper proposes a novel hybrid deep learning architecture based on the Transformer and a Bidirectional Gated Recurrent Unit (BiGRU), which we name TabGRU. This design synergistically captures both long-term dependencies and local sequential features in the CML signal data. The model is further enhanced by a learnable positional embedding and an attention pooling mechanism to improve its dynamic feature extraction and generalization capabilities. The model was validated on a public benchmark dataset from Gothenburg, Sweden (June-September 2015). The evaluation used 12 sub-links from two rain gauges (Torp and Barl) over a test period (August 22-31) covering approximately 10 distinct rainfall events. The proposed TabGRU model demonstrated consistent advantages, outperforming deep learning baselines and achieving high coefficients of determination (R2) at both the Torp site (0.91) and the Barl site (0.96). Furthermore, compared to the physics-based approach, TabGRU maintained higher accuracy and was particularly effective in mitigating the significant overestimation problem observed in the PL model during peak rainfall events. This evaluation confirms that the TabGRU model can effectively overcome the limitations of traditional methods, providing a robust and accurate solution for CML-based urban rainfall monitoring under the tested conditions.

</details>


### [74] [Dual-Robust Cross-Domain Offline Reinforcement Learning Against Dynamics Shifts](https://arxiv.org/abs/2512.02486)
*Zhongjian Qiao,Rui Yang,Jiafei Lyu,Xiu Li,Zhongxiang Dai,Zhuoran Yang,Siyang Gao,Shuang Qiu*

Main category: cs.LG

TL;DR: 该论文介绍了一种名为DROCO的双鲁棒跨领域离线强化学习算法，旨在解决传统方法在训练和测试阶段对动态变化的鲁棒性不足问题。


<details>
  <summary>Details</summary>
Motivation: 现有的跨领域离线强化学习方法主要关注训练时鲁棒性，而忽略了部署时面对动态扰动的测试时鲁棒性。特别是在目标域数据有限的情况下，传统方法训练出的策略在评估时对动态扰动表现脆弱。

Method: 本研究提出了一个鲁棒的跨领域贝尔曼（RCB）算子，以增强测试时鲁棒性，并通过保持对分布外动态转换的保守性来保证训练时鲁棒性。为了解决RCB算子可能导致的价值高估或低估问题，该框架引入了动态价值惩罚和Huber损失两种技术，从而形成了DROCO算法。

Result: 在各种动态变化场景下的实证结果表明，DROCO算法优于现有的强大基线方法，并对动态扰动表现出更强的鲁棒性。

Conclusion: DROCO算法通过结合新的RCB算子、动态价值惩罚和Huber损失，有效地解决了跨领域离线强化学习中训练时和测试时鲁棒性的双重挑战。

Abstract: Single-domain offline reinforcement learning (RL) often suffers from limited data coverage, while cross-domain offline RL handles this issue by leveraging additional data from other domains with dynamics shifts. However, existing studies primarily focus on train-time robustness (handling dynamics shifts from training data), neglecting the test-time robustness against dynamics perturbations when deployed in practical scenarios. In this paper, we investigate dual (both train-time and test-time) robustness against dynamics shifts in cross-domain offline RL. We first empirically show that the policy trained with cross-domain offline RL exhibits fragility under dynamics perturbations during evaluation, particularly when target domain data is limited. To address this, we introduce a novel robust cross-domain Bellman (RCB) operator, which enhances test-time robustness against dynamics perturbations while staying conservative to the out-of-distribution dynamics transitions, thus guaranteeing the train-time robustness. To further counteract potential value overestimation or underestimation caused by the RCB operator, we introduce two techniques, the dynamic value penalty and the Huber loss, into our framework, resulting in the practical \textbf{D}ual-\textbf{RO}bust \textbf{C}ross-domain \textbf{O}ffline RL (DROCO) algorithm. Extensive empirical results across various dynamics shift scenarios show that DROCO outperforms strong baselines and exhibits enhanced robustness to dynamics perturbations.

</details>


### [75] [Hybrid(Penalized Regression and MLP) Models for Outcome Prediction in HDLSS Health Data](https://arxiv.org/abs/2512.02489)
*Mithra D K*

Main category: cs.LG

TL;DR: 作者使用机器学习技术预测NHANES健康调查数据中的糖尿病状况。


<details>
  <summary>Details</summary>
Motivation: 比较基线模型与混合方法，后者使用XGBoost特征编码器和轻量级多层感知器（MLP）头部。

Method: 将XGBoost特征编码器与轻量级多层感知器（MLP）头部结合的混合方法。

Result: 与基线模型相比，混合模型在处理后的NHANES子集上获得了更高的AUC和平衡准确性。

Conclusion: 提出的混合模型在糖尿病状态预测方面优于传统的基线模型。

Abstract: I present an application of established machine learning techniques to NHANES health survey data for predicting diabetes status. I compare baseline models (logistic regression, random forest, XGBoost) with a hybrid approach that uses an XGBoost feature encoder and a lightweight multilayer perceptron (MLP) head. Experiments show the hybrid model attains improved AUC and balanced accuracy compared to baselines on the processed NHANES subset. I release code and reproducible scripts to encourage replication.

</details>


### [76] [A Fully First-Order Layer for Differentiable Optimization](https://arxiv.org/abs/2512.02494)
*Zihao Zhao,Kai-Chia Mo,Shing-Hei Ho,Brandon Amos,Kai Wang*

Main category: cs.LG

TL;DR: 该论文提出了一种新的算法，仅使用一阶信息来计算可微分优化层的梯度，避免了计算和内存密集型的Hessian项。


<details>
  <summary>Details</summary>
Motivation: 现有的可微分优化层在计算梯度时需要通过隐式微分求解包含Hessian项的线性系统，这导致了计算和内存消耗大。

Method: 该方法将可微分优化重写为双层优化问题，并利用双层优化方法的最新进展。具体来说，提出了一种主动集拉格朗日超梯度预言机，避免了Hessian评估，并提供了有限时间、非渐近的近似保证。

Result: 该算法仅使用一阶信息即可在$\tilde{\oo}(1)$时间内计算出近似超梯度，使得约束双层优化的总复杂度达到$\tilde{\oo}(δ^{-1}ε^{-3})$，与已知非光滑非凸优化的最佳速率匹配。同时，还发布了一个开源Python库。

Conclusion: 该研究有效地解决了可微分优化层梯度计算的效率问题，为双层优化提供了一种高效且理论保证的梯度计算方法。

Abstract: Differentiable optimization layers enable learning systems to make decisions by solving embedded optimization problems. However, computing gradients via implicit differentiation requires solving a linear system with Hessian terms, which is both compute- and memory-intensive. To address this challenge, we propose a novel algorithm that computes the gradient using only first-order information. The key insight is to rewrite the differentiable optimization as a bilevel optimization problem and leverage recent advances in bilevel methods. Specifically, we introduce an active-set Lagrangian hypergradient oracle that avoids Hessian evaluations and provides finite-time, non-asymptotic approximation guarantees. We show that an approximate hypergradient can be computed using only first-order information in $\tilde{\oo}(1)$ time, leading to an overall complexity of $\tilde{\oo}(δ^{-1}ε^{-3})$ for constrained bilevel optimization, which matches the best known rate for non-smooth non-convex optimization. Furthermore, we release an open-source Python library that can be easily adapted from existing solvers. Our code is available here: https://github.com/guaguakai/FFOLayer.

</details>


### [77] [Water Quality Estimation Through Machine Learning Multivariate Analysis](https://arxiv.org/abs/2512.02508)
*Marco Cardia,Stefano Chessa,Alessio Micheli,Antonella Giuliana Luminare,Francesca Gambineri*

Main category: cs.LG

TL;DR: 本文介绍了一种将紫外可见光谱与机器学习相结合的方案，用于快速、准确、可解释地评估农业食品行业水质。


<details>
  <summary>Details</summary>
Motivation: 水质对于农业食品行业的质量至关重要，而该行业正在逐步数字化，因此自动评估水质变得日益重要。

Method: 本文提出将紫外可见（UV-Vis）光谱技术与机器学习相结合，并利用SHAP（SHapley Additive exPlanations）来解释不同波长吸光度对预测的贡献。

Result: 该方法能够实现对关键水质参数的快速、准确和可解释的评估。

Conclusion: 紫外可见光谱与机器学习的结合为农业食品行业水质评估提供了一种有效且可解释的解决方案，有助于确保水安全并符合水法规。

Abstract: The quality of water is key for the quality of agrifood sector. Water is used in agriculture for fertigation, for animal husbandry, and in the agrifood processing industry. In the context of the progressive digitalization of this sector, the automatic assessment of the quality of water is thus becoming an important asset. In this work, we present the integration of Ultraviolet-Visible (UV-Vis) spectroscopy with Machine Learning in the context of water quality assessment aiming at ensuring water safety and the compliance of water regulation. Furthermore, we emphasize the importance of model interpretability by employing SHapley Additive exPlanations (SHAP) to understand the contribution of absorbance at different wavelengths to the predictions. Our approach demonstrates the potential for rapid, accurate, and interpretable assessment of key water quality parameters.

</details>


### [78] [OptPO: Optimal Rollout Allocation for Test-time Policy Optimization](https://arxiv.org/abs/2512.02882)
*Youkang Wang,Jian Wang,Rubing Chen,Tianyi Zeng,Xiao-Yong Wei,Qing Li*

Main category: cs.LG

TL;DR: OptPO通过自适应地分配推理预算，显著减少了测试时策略优化中的计算开销，同时保持或提高了性能。


<details>
  <summary>Details</summary>
Motivation: 现有的测试时策略优化方法在估计奖励时依赖固定预算的多数投票，导致计算冗余。

Method: OptPO将投票过程公式化为贝叶斯序贯概率比检验，一旦对共识答案的后验置信度超过阈值，就动态停止采样。它还利用保留的rollout进行在线策略更新，与PPO或GRPO等算法无缝集成。

Result: 与固定样本基线相比，OptPO在各种推理基准测试中显著降低了rollout开销，同时保持或提高了准确性。

Conclusion: OptPO通过将统计学上的最优停止与测试时学习相结合，为测试时适应提供了一种计算高效的范式。

Abstract: Test-time policy optimization enables large language models (LLMs) to adapt to distribution shifts by leveraging feedback from self-generated rollouts. However, existing methods rely on fixed-budget majority voting to estimate rewards, incurring substantial computational redundancy. We propose Optimal Rollout Allocation for Test-time Policy Optimization (OptPO), a principled framework that adaptively allocates inference budgets. By formulating the voting process as a Bayesian sequential probability ratio test, OptPO dynamically halts sampling once the posterior confidence in a consensus answer exceeds a specified threshold. Crucially, it utilizes the retained rollouts for on-policy updates, seamlessly integrating with algorithms like PPO or GRPO without requiring ground-truth labels. Across diverse reasoning benchmarks, OptPO significantly reduces rollout overhead compared to fixed-sample baselines while preserving or improving accuracy. By unifying statistically optimal stopping with test-time learning, OptPO offers a computationally efficient paradigm for test-time adaptation. The source code will be open upon acceptance at https://open-upon-acceptance.

</details>


### [79] [CUDA-L2: Surpassing cuBLAS Performance for Matrix Multiplication through Reinforcement Learning](https://arxiv.org/abs/2512.02551)
*Songqiao Su,Xiaofei Sun,Xiaoya Li,Albert Wang,Jiwei Li,Chris Shum*

Main category: cs.LG

TL;DR: CUDA-L2系统结合大型语言模型（LLMs）和强化学习（RL）自动优化HGEMM CUDA内核，在1000种配置下系统地优于现有主要矩阵乘法基线，如torch.matmul、cuBLAS和cuBLASLt。


<details>
  <summary>Details</summary>
Motivation: 优化半精度通用矩阵乘（HGEMM）CUDA内核的性能，超越现有手写优化和闭源库的水平。

Method: 提出CUDA-L2系统，该系统利用大型语言模型（LLMs）和强化学习（RL）来自动探索HGEMM内核的配置空间，并以CUDA执行速度作为RL奖励进行优化。

Result: 在离线模式下，CUDA-L2比torch.matmul平均快22.0%，比cuBLAS快19.2%，比cuBLASLt-heuristic快16.8%，比cuBLASLt-AutoTuning快11.4%。在服务器模式下，加速比进一步提升，分别达到28.7%、26.0%、22.4%和15.9%。

Conclusion: 即使是性能最关键、优化最彻底的HGEMM内核，通过LLM引导的RL自动化也能够得到显著改进，因为它能够系统地探索人类几乎无法处理的配置空间。

Abstract: In this paper, we propose CUDA-L2, a system that combines large language models (LLMs) and reinforcement learning (RL) to automatically optimize Half-precision General Matrix Multiply (HGEMM) CUDA kernels. Using CUDA execution speed as the RL reward, CUDA-L2 automatically optimizes HGEMM kernels across 1,000 configurations. CUDA-L2 systematically outperforms major matmul baselines to date, from the widely-used {\it torch.matmul} to state-of-the-art Nvidia's closed-source libraries, i.e., {\it cuBLAS}, {\it cuBLASLt}. In offline mode, where kernels are executed consecutively without time intervals, CUDA-L2 yields +22.0\% over {\it torch.matmul} on average; +19.2\% over {\it cuBLAS} using the optimal layout configuration (normal-normal NN and transposed-normal TN); +16.8\% over {\it cuBLASLt-heuristic}, which queries {\it cuBLASLt} library and selects the algorithm based on the heuristic's suggestion; and +11.4\% over the most competitive {\it cuBLASLt-AutoTuning} model, which selects the fastest algorithm from up to 100 candidates from {\it cuBLASLt}'s suggestions. In server mode, where kernels are executed at random intervals simulating real-time inference, the speedups further increase to +28.7\%, +26.0\%, +22.4\%, and +15.9\% for {\it torch.matmul}, {\it cuBLAS}, {\it cuBLASLt-heuristic}, and {\it cuBLASLt-AutoTuning} respectively. CUDA-L2 shows that even the most performance-critical, heavily-optimized kernels like HGEMM can be improved through LLM-guided RL automation by systematically exploring configuration spaces at scales impractical for humans. Project and code can be found at github.com/deepreinforce-ai/CUDA-L2

</details>


### [80] [GoRL: An Algorithm-Agnostic Framework for Online Reinforcement Learning with Generative Policies](https://arxiv.org/abs/2512.02581)
*Chubin Zhang,Zhenglin Wan,Feng Chen,Xingrui Yu,Ivor Tsang,Bo An*

Main category: cs.LG

TL;DR: GoRL通过解耦优化和生成，实现了稳定且富有表现力的策略，在连续控制任务中显著优于高斯策略和生成策略基线。


<details>
  <summary>Details</summary>
Motivation: 强化学习中策略优化稳定性和多模态动作表达能力之间的矛盾。高斯策略易于优化但表达能力有限，而生成策略表达能力强但通常不稳定。

Method: 提出GoRL框架，将优化与生成解耦。优化一个易于处理的潜在策略，同时利用条件生成解码器合成动作。采用双时间尺度更新机制，使潜在策略稳定学习，解码器稳步提高表达能力。

Result: GoRL在多种连续控制任务中始终优于高斯策略和最新生成策略基线。在HopperStand任务中，归一化回报超过870，是表现最佳基线的3倍以上。

Conclusion: 将优化与生成分离，为实现既稳定又具有高度表达能力的策略提供了一条实用途径。

Abstract: Reinforcement learning (RL) faces a persistent tension: policies that are stable to optimize are often too simple to represent the multimodal action distributions needed for complex control. Gaussian policies provide tractable likelihoods and smooth gradients, but their unimodal form limits expressiveness. Conversely, generative policies based on diffusion or flow matching can model rich multimodal behaviors; however, in online RL, they are frequently unstable due to intractable likelihoods and noisy gradients propagating through deep sampling chains. We address this tension with a key structural principle: decoupling optimization from generation. Building on this insight, we introduce GoRL (Generative Online Reinforcement Learning), a framework that optimizes a tractable latent policy while utilizing a conditional generative decoder to synthesize actions. A two-timescale update schedule enables the latent policy to learn stably while the decoder steadily increases expressiveness, without requiring tractable action likelihoods. Across a range of continuous-control tasks, GoRL consistently outperforms both Gaussian policies and recent generative-policy baselines. Notably, on the HopperStand task, it reaches a normalized return above 870, more than 3 times that of the strongest baseline. These results demonstrate that separating optimization from generation provides a practical path to policies that are both stable and highly expressive.

</details>


### [81] [Joint Distillation for Fast Likelihood Evaluation and Sampling in Flow-based Models](https://arxiv.org/abs/2512.02636)
*Xinyue Ai,Yutong He,Albert Gu,Ruslan Salakhutdinov,J Zico Kolter,Nicholas Matthew Boffi,Max Simchowitz*

Main category: cs.LG

TL;DR: 本文介绍了一种名为F2D2的框架，旨在显著减少生成模型中采样和对数似然评估所需的神经函数评估（NFE）数量。F2D2通过联合蒸馏采样轨迹和累积散度，将NFE数量减少了两个数量级，同时保持了高样本质量和对数似然的准确性。


<details>
  <summary>Details</summary>
Motivation: 目前最先进的生成模型（如扩散模型和流模型）在计算单个似然时需要大量的神经函数评估。尽管现有的蒸馏方法能够加速采样过程，但它们往往牺牲了似然计算的可行性，无法在少量NFE内准确评估对数似然。

Method: F2D2框架利用连续归一化流中采样和似然计算的耦合ODE都来源于共享的速度场这一关键思想。通过联合蒸馏采样轨迹和累积散度，F2D2只需一个模型即可同时实现快速采样和对数似然评估。该方法是模块化的，可与现有基于流的少步采样模型兼容，并且仅需额外添加一个散度预测头。

Result: F2D2能够以少量的评估步骤实现准确的对数似然计算，同时保持高质量的样本生成，解决了基于流的生成模型中长期存在的计算瓶颈。作为一个应用，文章提出了一种轻量级的自引导方法，使得一个2步的MeanFlow模型在仅增加一个反向NFE的情况下，性能超越了1024步的教师模型。

Conclusion: F2D2框架显著提高了基于流的生成模型的效率，使其在采样和对数似然评估方面都取得了重大突破。这为生成模型的实际应用和进一步研究开辟了新的可能性，尤其是在需要快速且准确似然评估的场景中。

Abstract: Log-likelihood evaluation enables important capabilities in generative models, including model comparison, certain fine-tuning objectives, and many downstream applications. Yet paradoxically, some of today's best generative models -- diffusion and flow-based models -- still require hundreds to thousands of neural function evaluations (NFEs) to compute a single likelihood. While recent distillation methods have successfully accelerated sampling to just a few steps, they achieve this at the cost of likelihood tractability: existing approaches either abandon likelihood computation entirely or still require expensive integration over full trajectories. We present fast flow joint distillation (F2D2), a framework that simultaneously reduces the number of NFEs required for both sampling and likelihood evaluation by two orders of magnitude. Our key insight is that in continuous normalizing flows, the coupled ODEs for sampling and likelihood are computed from a shared underlying velocity field, allowing us to jointly distill both the sampling trajectory and cumulative divergence using a single model. F2D2 is modular, compatible with existing flow-based few-step sampling models, and requires only an additional divergence prediction head. Experiments demonstrate F2D2's capability of achieving accurate log-likelihood with few-step evaluations while maintaining high sample quality, solving a long-standing computational bottleneck in flow-based generative models. As an application of our approach, we propose a lightweight self-guidance method that enables a 2-step MeanFlow model to outperform a 1024 step teacher model with only a single additional backward NFE.

</details>


### [82] [Adaptive Weighted LSSVM for Multi-View Classification](https://arxiv.org/abs/2512.02653)
*Farnaz Faramarzi Lighvan,Mehrdad Asadi,Lynn Houthuys*

Main category: cs.LG

TL;DR: 本文提出了一种自适应加权最小二乘支持向量机（AW-LSSVM），通过迭代全局耦合促进互补学习，使其在多视图学习中表现优异，尤其适用于隐私保护场景。


<details>
  <summary>Details</summary>
Motivation: 现有的核函数多视图学习方法在视图间协作类型或协同正则化方面存在限制，未能实现全局协作，本文旨在解决这一问题。

Method: 本文提出AW-LSSVM，它采用迭代全局耦合的方式，使每个视图侧重于学习其他视图在前一次迭代中难以处理的样本，从而促进互补学习。

Result: 实验结果表明，AW-LSSVM在大多数数据集上优于现有的核函数多视图方法，同时保持原始特征的独立性。

Conclusion: AW-LSSVM在多视图学习中取得了更好的性能，并且由于其保持原始特征独立性的特点，特别适用于隐私保护场景。

Abstract: Multi-view learning integrates diverse representations of the same instances to improve performance. Most existing kernel-based multi-view learning methods use fusion techniques without enforcing an explicit collaboration type across views or co-regularization which limits global collaboration. We propose AW-LSSVM, an adaptive weighted LS-SVM that promotes complementary learning by an iterative global coupling to make each view focus on hard samples of others from previous iterations. Experiments demonstrate that AW-LSSVM outperforms existing kernel-based multi-view methods on most datasets, while keeping raw features isolated, making it also suitable for privacy-preserving scenarios.

</details>


### [83] [Distill, Forget, Repeat: A Framework for Continual Unlearning in Text-to-Image Diffusion Models](https://arxiv.org/abs/2512.02657)
*Naveen George,Naoki Murata,Yuhta Takida,Konda Reddy Mopuri,Yuki Mitsufuji*

Main category: cs.LG

TL;DR: 本文提出了一种新颖的基于生成蒸馏的持续非学习框架，该框架确保在删除请求序列下进行有针对性和稳定的非学习，从而在去除特定概念的同时，保持模型性能和图像质量。


<details>
  <summary>Details</summary>
Motivation: 解决现有机器学习非学习技术在处理连续删除请求（持续非学习）时遇到的稳定性危机，例如保留崩溃、对相关概念的附带损害以及生成质量急剧下降的问题。

Method: 该框架将每个非学习步骤重新定义为一个多目标、师生蒸馏过程，并利用持续学习的原理来保持模型完整性。

Result: 在10步顺序基准测试中，该方法以更高的保真度非学习了遗忘概念，并且在不显著干扰保留概念性能或整体图像质量的情况下实现了这一目标，性能显著优于基线方法。

Conclusion: 该框架为大规模生成模型的负责任部署和维护提供了一条可行的途径，使得各行业能够以实用有效的方式遵守持续的数据删除请求。

Abstract: The recent rapid growth of visual generative models trained on vast web-scale datasets has created significant tension with data privacy regulations and copyright laws, such as GDPR's ``Right to be Forgotten.'' This necessitates machine unlearning (MU) to remove specific concepts without the prohibitive cost of retraining. However, existing MU techniques are fundamentally ill-equipped for real-world scenarios where deletion requests arrive sequentially, a setting known as continual unlearning (CUL). Naively applying one-shot methods in a continual setting triggers a stability crisis, leading to a cascade of degradation characterized by retention collapse, compounding collateral damage to related concepts, and a sharp decline in generative quality. To address this critical challenge, we introduce a novel generative distillation based continual unlearning framework that ensures targeted and stable unlearning under sequences of deletion requests. By reframing each unlearning step as a multi-objective, teacher-student distillation process, the framework leverages principles from continual learning to maintain model integrity. Experiments on a 10-step sequential benchmark demonstrate that our method unlearns forget concepts with better fidelity and achieves this without significant interference to the performance on retain concepts or the overall image quality, substantially outperforming baselines. This framework provides a viable pathway for the responsible deployment and maintenance of large-scale generative models, enabling industries to comply with ongoing data removal requests in a practical and effective manner.

</details>


### [84] [Graph VQ-Transformer (GVT): Fast and Accurate Molecular Generation via High-Fidelity Discrete Latents](https://arxiv.org/abs/2512.02667)
*Haozhuo Zheng,Cheng Wang,Yang Liu*

Main category: cs.LG

TL;DR: GVT是一种新颖的两阶段生成框架，通过将分子图压缩成高保真离散潜在序列，并结合图Transformer、RCM节点排序和RoPE，实现了高精度和高效率的分子生成。


<details>
  <summary>Details</summary>
Motivation: 分子从头生成是一个关键挑战，扩散模型计算成本高，自回归模型存在误差传播问题。

Method: 我们引入了Graph VQ-Transformer (GVT)，该框架包含一个新颖的图向量量化变分自编码器（VQ-VAE），用于将分子图压缩成高保真离散潜在序列。VQ-VAE结合了图Transformer、规范的逆Cuthill-McKee (RCM) 节点排序和旋转位置嵌入 (RoPE)。然后，一个自回归Transformer在这些离散潜在空间上进行训练，将图生成转换为结构良好的序列建模问题。

Result: GVT在ZINC250k、MOSES和GuacaMol等主要基准测试中取得了最先进或极具竞争力的性能，尤其在FCD和KL散度等关键分布相似性指标上显着优于领先的扩散模型。

Conclusion: GVT为克服现有分子生成方法的局限性提供了一种有前景的解决方案，为离散潜在空间分子生成领域的未来研究奠定了坚实的基础，并为未来与大型语言模型的协同作用开辟了道路。

Abstract: The de novo generation of molecules with desirable properties is a critical challenge, where diffusion models are computationally intensive and autoregressive models struggle with error propagation. In this work, we introduce the Graph VQ-Transformer (GVT), a two-stage generative framework that achieves both high accuracy and efficiency. The core of our approach is a novel Graph Vector Quantized Variational Autoencoder (VQ-VAE) that compresses molecular graphs into high-fidelity discrete latent sequences. By synergistically combining a Graph Transformer with canonical Reverse Cuthill-McKee (RCM) node ordering and Rotary Positional Embeddings (RoPE), our VQ-VAE achieves near-perfect reconstruction rates. An autoregressive Transformer is then trained on these discrete latents, effectively converting graph generation into a well-structured sequence modeling problem. Crucially, this mapping of complex graphs to high-fidelity discrete sequences bridges molecular design with the powerful paradigm of large-scale sequence modeling, unlocking potential synergies with Large Language Models (LLMs). Extensive experiments show that GVT achieves state-of-the-art or highly competitive performance across major benchmarks like ZINC250k, MOSES, and GuacaMol, and notably outperforms leading diffusion models on key distribution similarity metrics such as FCD and KL Divergence. With its superior performance, efficiency, and architectural novelty, GVT not only presents a compelling alternative to diffusion models but also establishes a strong new baseline for the field, paving the way for future research in discrete latent-space molecular generation.

</details>


### [85] [Conformal Correction for Efficiency May be at Odds with Entropy](https://arxiv.org/abs/2512.02704)
*Senrong Xu,Tianyu Wang,Zenan Li,Yuan Yao,Taolue Chen,Feng Xu,Xiaoxing Ma*

Main category: cs.LG

TL;DR: 本文提出了一种熵约束的共形校正方法，用于在共形预测中平衡效率和模型预测的熵。


<details>
  <summary>Details</summary>
Motivation: 在黑盒机器学习模型中，共形预测（CP）可以提供统计上严格的不确定性集，为了提高CP的效率，提出了共形校正方法，但效率与模型预测的熵之间存在权衡。

Method: 本文提出了一种熵约束的共形校正方法，旨在权衡共形预测的效率和模型预测的熵。

Result: 在计算机视觉和图数据集上的大量实验结果表明，该方法能够显著提高现有CP方法的效率，在给定熵阈值的情况下，最高可提高34.4%。

Conclusion: 本文通过实验和理论研究，确定了CP效率与模型预测熵之间的权衡，并提出了一种熵约束的共形校正方法，以在两者之间找到更好的帕累托最优解。

Abstract: Conformal prediction (CP) provides a comprehensive framework to produce statistically rigorous uncertainty sets for black-box machine learning models. To further improve the efficiency of CP, conformal correction is proposed to fine-tune or wrap the base model with an extra module using a conformal-aware inefficiency loss. In this work, we empirically and theoretically identify a trade-off between the CP efficiency and the entropy of model prediction. We then propose an entropy-constrained conformal correction method, exploring a better Pareto optimum between efficiency and entropy. Extensive experimental results on both computer vision and graph datasets demonstrate the efficacy of the proposed method. For instance, it can significantly improve the efficiency of state-of-the-art CP methods by up to 34.4%, given an entropy threshold.

</details>


### [86] [FGC-Comp: Adaptive Neighbor-Grouped Attribute Completion for Graph-based Anomaly Detection](https://arxiv.org/abs/2512.02705)
*Junpeng Wu,Pinheng Zong*

Main category: cs.LG

TL;DR: FGC-Comp是一个处理不完整属性的图异常检测模型，通过分组转换、节点条件门控和残差连接来增强邻居聚合，并使用二分类目标进行端到端训练。


<details>
  <summary>Details</summary>
Motivation: 现有的图异常检测模型忽略了节点属性缺失和被对抗性隐藏的问题，这会影响聚合稳定性和预测可靠性。

Method: FGC-Comp将每个节点的邻居划分为三个基于标签的组，对已标记组应用组特定转换，节点条件门控处理未知属性，通过残差连接融合信息，并使用二分类目标进行端到端训练。

Result: 在两个真实世界的欺诈数据集上进行的实验验证了该方法的有效性和可忽略的计算开销。

Conclusion: FGC-Comp通过在不完整属性下增强邻居聚合，显著提高了图异常检测模型的稳定性和预测可靠性，且计算开销很小。

Abstract: Graph-based Anomaly Detection models have gained widespread adoption in recent years, identifying suspicious nodes by aggregating neighborhood information. However, most existing studies overlook the pervasive issues of missing and adversarially obscured node attributes, which can undermine aggregation stability and prediction reliability. To mitigate this, we propose FGC-Comp, a lightweight, classifier-agnostic, and deployment-friendly attribute completion module-designed to enhance neighborhood aggregation under incomplete attributes. We partition each node's neighbors into three label-based groups, apply group-specific transforms to the labeled groups while a node-conditioned gate handles unknowns, fuse messages via residual connections, and train end-to-end with a binary classification objective to improve aggregation stability and prediction reliability under missing attributes. Experiments on two real-world fraud datasets validate the effectiveness of the approach with negligible computational overhead.

</details>


### [87] [From Navigation to Refinement: Revealing the Two-Stage Nature of Flow-based Diffusion Models through Oracle Velocity](https://arxiv.org/abs/2512.02826)
*Haoming Liu,Jinnuo Liu,Yanhao Li,Liuyang Bai,Yunkai Ji,Yuanhe Guo,Shenji Wan,Hongyi Wen*

Main category: cs.LG

TL;DR: 本文分析了流匹配（FM）目标函数的边际速度场，揭示了流基扩散模型训练是一个两阶段过程，早期侧重泛化，后期侧重记忆，并解释了现有技术（如时间步移位调度、无分类器指导间隔和潜在空间设计）的有效性。


<details>
  <summary>Details</summary>
Motivation: 流基扩散模型在生成式模型训练中表现出色，但其记忆-泛化行为尚不清晰，因此需要对其内在机制进行深入理解。

Method: 作者重新审视了流匹配（FM）目标，并研究了其边际速度场。通过对边际速度场的分析，发现了FM目标函数中的两阶段训练特性。

Result: 分析边际速度场发现，流基扩散模型天然地包含两阶段训练目标：早期阶段由数据模式混合指导，后期阶段由最近的数据样本主导。这导致了不同的学习行为：早期导航阶段会泛化不同的数据模式，形成全局布局；后期细化阶段则会越来越多地记忆细粒度细节。

Conclusion: 流基扩散模型训练 dynamics 的两阶段特性可以用来解释时间步移位调度、无分类器指导间隔和潜在空间设计等实用技术的有效性，为未来的架构和算法改进提供了指导原则。

Abstract: Flow-based diffusion models have emerged as a leading paradigm for training generative models across images and videos. However, their memorization-generalization behavior remains poorly understood. In this work, we revisit the flow matching (FM) objective and study its marginal velocity field, which admits a closed-form expression, allowing exact computation of the oracle FM target. Analyzing this oracle velocity field reveals that flow-based diffusion models inherently formulate a two-stage training target: an early stage guided by a mixture of data modes, and a later stage dominated by the nearest data sample. The two-stage objective leads to distinct learning behaviors: the early navigation stage generalizes across data modes to form global layouts, whereas the later refinement stage increasingly memorizes fine-grained details. Leveraging these insights, we explain the effectiveness of practical techniques such as timestep-shifted schedules, classifier-free guidance intervals, and latent space design choices. Our study deepens the understanding of diffusion model training dynamics and offers principles for guiding future architectural and algorithmic improvements.

</details>


### [88] [A Comparative Study on How Data Normalization Affects Zero-Shot Generalization in Time Series Foundation Models](https://arxiv.org/abs/2512.02833)
*Ihab Ahmed,Denis Krompaß,Cheng Feng,Volker Tresp*

Main category: cs.LG

TL;DR: 本文研究了时间序列基础模型（TSFMs）的输入归一化方法，发现REVIN是最有效的方法，显著提高了零样本预测精度并保持了领域内准确性，但其有效性取决于模型架构和优化目标。


<details>
  <summary>Details</summary>
Motivation: 时间序列数据在不同领域和通道间存在显著的尺度变化，且具有非平稳性，这会严重影响时间序列基础模型（TSFMs）的性能。尽管归一化方法在针对特定数据集的时间序列模型中已被广泛研究，但在强调泛化能力的TSFMs中却被忽视。

Method: 本文系统评估了四种架构多样化的TSFMs，并对不同的归一化方法进行了实证评估，特别是比较了REVIN与其他归一化方法的表现。

Result: REVIN被证实是最有效的归一化方法，相对于未归一化的基线，它将零样本MASE降低了89%，相对于其他归一化方法降低了44%。同时，REVIN在不进行任何数据集级别预处理的情况下，达到了与最佳领域内准确性（0.84 MASE）相当的水平，从而实现了最高的精度-效率权衡。

Conclusion: REVIN是TSFMs中一种高效的输入归一化方法，能够显著提高模型的零样本预测精度和领域内准确性。然而，REVIN的有效性受到模型架构设计选择和优化目标的影响，特别是训练损失尺度敏感性和模型类型（如概率模型、点预测模型或基于LLM的模型）。

Abstract: We investigate input normalization methods for Time-Series Foundation Models (TSFMs). While normalization is well-studied in dataset-specific time-series models, it remains overlooked in TSFMs where generalization is critical. Time-series data, unlike text or images, exhibits significant scale variation across domains and channels, coupled with non-stationarity, can undermine TSFM performance regardless of architectural complexity. Through systematic evaluation across four architecturally diverse TSFMs, we empirically establish REVIN as the most efficient approach, reducing zero-shot MASE by 89\% relative to an un-normalized baseline and by 44\% versus other normalization methods, while matching the best in-domain accuracy (0.84 MASE) without any dataset-level preprocessing -- yielding the highest accuracy-efficiency trade-off. Yet its effect utilization depends on architectural design choices and optimization objective, particularly with respect to training loss scale sensitivity and model type (probabilistic, point-forecast, or LLM-based models).

</details>


### [89] [Adaptive Decentralized Federated Learning for Robust Optimization](https://arxiv.org/abs/2512.02852)
*Shuyuan Wu,Feifei Wang,Yuan Gao,Hansheng Wang*

Main category: cs.LG

TL;DR: 本文提出了一种新颖的自适应分布式联邦学习（aDFL）方法，通过自适应调整客户端的学习率，以减轻异常客户端对全局模型的不利影响，并在理论上保证了收敛性。


<details>
  <summary>Details</summary>
Motivation: 在去中心化联邦学习（DFL）中，异常客户端（通常由噪声或被污染的数据引起）会严重干扰学习过程并降低模型的整体鲁棒性。以前解决此问题的方法通常需要足够多的正常相邻客户端或可靠客户端的先验知识，这降低了DFL的实际适用性。

Method: 我们开发了一种新颖的自适应分布式联邦学习（aDFL）方法，用于鲁棒估计。其核心思想是自适应调整客户端的学习率。通过为可疑客户端分配较小的学习率，为正常客户端分配较大的学习率，aDFL以完全自适应的方式减轻了异常客户端对全局模型的不利影响。

Result: 我们的理论不对相邻节点施加任何严格条件，也不需要任何先验知识。提供了严格的收敛性分析，以保证aDFL的Oracle属性。大量的数值实验证明了aDFL方法的卓越性能。

Conclusion: 所提出的aDFL方法通过自适应调整学习率，有效解决了去中心化联邦学习中异常客户端带来的挑战，提高了模型的鲁棒性，并在理论和实验上都得到了验证。

Abstract: In decentralized federated learning (DFL), the presence of abnormal clients, often caused by noisy or poisoned data, can significantly disrupt the learning process and degrade the overall robustness of the model. Previous methods on this issue often require a sufficiently large number of normal neighboring clients or prior knowledge of reliable clients, which reduces the practical applicability of DFL. To address these limitations, we develop here a novel adaptive DFL (aDFL) approach for robust estimation. The key idea is to adaptively adjust the learning rates of clients. By assigning smaller rates to suspicious clients and larger rates to normal clients, aDFL mitigates the negative impact of abnormal clients on the global model in a fully adaptive way. Our theory does not put any stringent conditions on neighboring nodes and requires no prior knowledge. A rigorous convergence analysis is provided to guarantee the oracle property of aDFL. Extensive numerical experiments demonstrate the superior performance of the aDFL method.

</details>


### [90] [FAIRY2I: Universal Extremely-Low Bit QAT framework via Widely-Linear Representation and Phase-Aware Quantization](https://arxiv.org/abs/2512.02901)
*Feiyu Wang,Xinyu Tan,Bokai Huang,Yihao Zhang,Guoan Wang,Peizhuang Cong,Tong Yang*

Main category: cs.LG

TL;DR: Fairy2i是一个将预训练的实值大语言模型转换为复数形式的框架，实现超低位宽（2比特）量化，且性能接近全精度模型，显著优于现有实值量化方法，为高效推理提供了新途径。


<details>
  <summary>Details</summary>
Motivation: 尽管复值LLM（如iFairy）在低位宽表示方面优于实值LLM，但它们需要从头训练，无法利用预训练的实值基础模型。

Method: Fairy2i通过证明实值映射与广义线性复数映射之间的无损数学等价性，将标准Transformer转换为复数域。它采用相位感知量化方案和高效的单位四次根码本，并引入递归残差量化机制，迭代最小化量化误差，实现无乘法累加推理。

Result: Fairy2i在有效2比特精度下，将LLaMA-2 7B的性能恢复到与全精度基线几乎相当的水平，显著优于最先进的实值二元和三元量化方法。

Conclusion: Fairy2i弥合了复值算术表示效率与预训练模型实用性之间的鸿沟，为在商用硬件上进行高效推理开辟了新途径。

Abstract: Large language models (LLMs) have revolutionized artificial intelligence, yet their massive memory and computational demands necessitate aggressive quantization, increasingly pushing representations toward the theoretical limit of a single bit. While complex-valued LLMs, such as iFairy, offer a superior chance for low-bit representation compared to real-valued counterparts, they require training from scratch, preventing the utilization of the vast ecosystem of pre-trained real-valued foundation models. Here we present Fairy2i, a universal framework that transforms pre-trained real-valued layers into an equivalent widely-linear complex form, enabling extremely low-bit quantization while reusing existing checkpoints. By proving a lossless mathematical equivalence between real and widely-linear maps, we convert standard Transformers into the complex domain and employ a phase-aware quantization scheme with a highly efficient codebook of fourth roots of unity. Furthermore, we introduce a recursive residual quantization mechanism that iteratively minimizes quantization error, allowing inference to proceed via efficient multiplication-free accumulation. We demonstrate that Fairy2i restores the performance of LLaMA-2 7B at an effective 2-bit precision to levels nearly comparable with full-precision baselines, significantly outperforming state-of-the-art real-valued binary and ternary quantization methods. This work bridges the gap between the representational efficiency of complex-valued arithmetic and the practical utility of pre-trained models, paving a new way for efficient inference on commodity hardware.

</details>


<div id='cs.GT'></div>

# cs.GT [[Back]](#toc)

### [91] [Characterizing Off-Chain Influence Proof Transaction Fee Mechanisms](https://arxiv.org/abs/2512.02354)
*Aadityan Ganesh,Clayton Thomas,S. Matthew Weinberg*

Main category: cs.GT

TL;DR: 本文分析了链上游戏交易费用机制（TFM）的激励兼容性（OnCS）和链下影响力证明（OffCIP）属性。文章对OffCIP TFM进行了数学表征，并研究了在不同供应和先验依赖条件下，确定性和随机性TFM的OffCIP和OnCS属性，发现OffCIP是一个严格的要求，但在多种设置下可以找到满足条件的机制族。


<details>
  <summary>Details</summary>
Motivation: Roughgarden (2020) 提出了交易费用机制（TFMs）的研究，并认为一个“好”的TFM应该具备链上简单（OnCS）的特性，即对用户和矿工都具有激励兼容性。Ganesh, Thomas和Weinberg (2024) 的最新研究进一步提出，TFM还应具备链下影响力证明（OffCIP）特性，即矿工不能通过单独进行链下拍卖来获取额外的收入。他们发现加密的第二价格拍卖同时满足这两个特性，但对于其他机制（例如非加密机制）是否满足这些特性，他们没有给出答案。

Method: 本文通过将烧毁规则与分配规则相关联，对OffCIP TFMs进行了数学上的表征。具体而言，文章证明了一个拍卖是OffCIP当且仅当其（诱导的直接揭示）分配规则$ar{X}(\cdot)$和烧毁规则$ar{B}(\cdot)$是真实的。在此基础上，本文表征了不使用密码学的确定性OffCIP和OnCS TFMs，发现它们是具有特殊调整烧毁的规定价格机制。对于随机性TFM，文章证明了存在额外的OnCS和OffCIP拍卖，即使在有限供应和先验依赖的情况下，也不需要密码学。

Result: 1. 论文发现了一个烧毁恒等式，它将烧毁规则与分配规则联系起来，并以此定义了 OffCIP TFM 的特征。
2. 论文发现，不使用密码学的确定性 OffCIP 和 OnCS TFM 是具有特殊调整烧毁的规定价格机制（posted-price mechanisms）。
3. 这种确定性 OffCIP 和 OnCS TFM 只能在无限供应和先验依赖的条件下存在。
4. 对于随机性 TFM，即使在有限供应和先验依赖（具有有界先验分布）的条件下，也存在不需要密码学的额外 OnCS 和 OffCIP 拍卖。

Conclusion: OffCIP是一个相当严格的要求，但在多种设置下可以找到满足OffCIP要求的机制族。文章为TFMs的设计提供了重要的理论指导，尤其是在理解OnCS和OffCIP特性方面。研究结果表明，在存在先验依赖的情况下，可以通过精心设计的烧毁规则来构建有效的TFMs。

Abstract: Roughgarden (2020) initiates the study of Transaction Fee Mechanisms (TFMs), and posits that the on-chain game of a ``good'' TFM should be on-chain simple (OnCS), i.e., incentive compatible for users and the miner. Recent work of Ganesh, Thomas and Weinberg (2024) posits that they should additionally be Off-Chain Influence Proof (OffCIP), which means that the miner cannot achieve any additional revenue by separately conducting an off-chain auction to determine on-chain inclusion. They observe that a cryptographic second-price auction satisfies both properties, but leave open the question of whether other mechanisms (e.g, non-cryptographic) satisfy these properties.
  In this paper, we characterize OffCIP TFMs: They are those satisfying a burn identity relating the burn rule to the allocation rule. In particular, we show that auction is OffCIP if and only if its (induced direct-revelation) allocation rule $\bar{X}(\cdot)$ and burn rule $\bar{B}(\cdot)$ (both of which take as input users' values $v_1, \dots, v_n$) are truthful when viewing $\big(\bar{X}(\cdot), \bar{B}(\cdot)\big)$ as the allocation and pricing rule of a multi-item auction for a single additive buyer with values $\big(\varphi(v_1),\ldots, \varphi(v_n)\big)$ equal to the users' virtual values.
  Building on this burn identity, we characterize deterministic OffCIP and OnCS TFMs that do not use cryptography: They are posted-price mechanisms with specially-tuned burns. As a corollary, we show that such TFMs can only exist with infinite supply and prior-dependence. However, we show that for randomized TFMs, there are additional OnCS and OffCIP auctions that do not use cryptography (even when there is finite supply, under prior-dependence with a bounded prior distribution). Holistically, our results show that while OffCIP is a fairly stringent requirement, families of OffCIP mechanisms can be found for a variety of settings.

</details>


### [92] [Posted Pricing for Online Selection: Limited Price Changes and Risk Sensitivity](https://arxiv.org/abs/2512.02427)
*Hossein Nekouyan,Bo Sun,Raouf Boutaba,Xiaoqi Tan*

Main category: cs.GT

TL;DR: 本文分析了在有限价格变动次数和风险敏感度约束下的在线资源分配中的悬挂价格机制。


<details>
  <summary>Details</summary>
Motivation: 解决动态价格在实际应用中带来的公平性问题和操作成本，同时考虑不确定性环境下的尾部风险。

Method: 提出了一种新问题类别kSelection-$(δ,Δ)$，并设计了一种相关联的PPM，通过单一随机种子关联悬挂价格，以处理有限价格变动并提升在线算法的尾部性能。

Result: 在联合约束下，提供了性能保证，揭示了允许的价格变动次数与算法风险敏感度之间的明确权衡。

Conclusion: 本文成功地将悬挂价格机制应用于有限价格变动和风险敏感度受限的场景，通过引入kSelection-$(δ,Δ)$问题和相关联的PPM，在理论上证明了其有效性，并揭示了价格变动次数与风险敏感度之间的权衡关系。

Abstract: Posted-price mechanisms (PPMs) are a widely adopted strategy for online resource allocation due to their simplicity, intuitive nature, and incentive compatibility. To manage the uncertainty inherent in online settings, PPMs commonly employ dynamically increasing prices. While this adaptive pricing achieves strong performance, it introduces practical challenges: dynamically changing prices can lead to fairness concerns stemming from price discrimination and incur operational costs associated with frequent updates. This paper addresses these issues by investigating posted pricing constrained by a limited, pre-specified number of allowed price changes, denoted by $Δ$. We further extend this framework by incorporating a second critical dimension: risk sensitivity. Instead of evaluating performance based solely on expectation, we utilize a tail-risk objective-specifically, the Conditional Value at Risk (CVaR) of the total social welfare, parameterized by a risk level $δ\in [0, 1]$.
  We formally introduce a novel problem class kSelection-$(δ,Δ)$ in online adversarial selection and propose a correlated PPM that utilizes a single random seed to correlate posted prices. This correlation scheme is designed to address both the limited price changes and simultaneously enhance the tail performance of the online algorithm. Our subsequent analysis provides performance guarantees under these joint constraints, revealing a clear trade-off between the number of allowed price changes and the algorithm's risk sensitivity. We also establish optimality results for several important special cases of the problem.

</details>


### [93] [Monotone Near-Zero-Sum Games: A Generalization of Convex-Concave Minimax](https://arxiv.org/abs/2512.02690)
*Ruichen Luo,Sebastian U. Stich,Krishnendu Chatterjee*

Main category: cs.GT

TL;DR: 本文介绍了一种新的单调近零和博弈算法，它通过将近零和博弈问题转化为一系列零和子问题，改进了基于梯度的复杂度。


<details>
  <summary>Details</summary>
Motivation: 零和博弈和非零和博弈在许多应用中都很重要，但非零和博弈计算复杂。研究人员关注单调博弈，但单调零和博弈和单调非零和博弈的梯度复杂度之间存在显著差距，且实际应用中零和假设往往不成立。

Method: 定义了一个新的单调近零和博弈类别，它将单调零和博弈作为特例。提出了一种新算法，将近零和博弈问题转化为一系列零和子问题。

Result: 通过将近零和博弈问题转化为一系列零和子问题，改进了该类博弈的基于梯度的复杂度。

Conclusion: 提出了一种新的单调近零和博弈类别和相应的算法，该算法在实际应用中具有广泛的适用性，能够有效地处理零和假设不成立的场景。

Abstract: Zero-sum and non-zero-sum (aka general-sum) games are relevant in a wide range of applications. While general non-zero-sum games are computationally hard, researchers focus on the special class of monotone games for gradient-based algorithms. However, there is a substantial gap between the gradient complexity of monotone zero-sum and monotone general-sum games. Moreover, in many practical scenarios of games the zero-sum assumption needs to be relaxed. To address these issues, we define a new intermediate class of monotone near-zero-sum games that contains monotone zero-sum games as a special case. Then, we present a novel algorithm that transforms the near-zero-sum games into a sequence of zero-sum subproblems, improving the gradient-based complexity for the class. Finally, we demonstrate the applicability of this new class to model practical scenarios of games motivated from the literature.

</details>


<div id='cs.AI'></div>

# cs.AI [[Back]](#toc)

### [94] [The 4/$δ$ Bound: Designing Predictable LLM-Verifier Systems for Formal Method Guarantee](https://arxiv.org/abs/2512.02080)
*PIerre Dantas,Lucas Cordeiro,Youcheng Sun,Waldir Junior*

Main category: cs.AI

TL;DR: 该文章介绍了一种结合大型语言模型（LLM）和形式化验证工具的方法，首次提供了一个具有可证明终止和收敛保证的正式框架。


<details>
  <summary>Details</summary>
Motivation: 目前使用LLM和形式化验证工具结合的方法不可靠，缺乏坚实的理论基础，导致验证过程不稳定。

Method: 将LLM和验证器之间的交互建模为离散时间马尔可夫链，通过误差减少概率（δ）确定状态转移，并提出了LLM-Verifier收敛定理。

Result: 程序几乎肯定会终止，预期迭代次数上限为$4/δ$。通过超过90,000次试验的实证检验，结果与理论高度一致，每次运行都达到验证，收敛因子接近1.0。

Conclusion: 理论和实验证据为LLM辅助验证提供了清晰的架构基础，支持可预测的资源规划和性能预算，并为工作流划分了三个操作区域：边际、实用和高性能，从而建立了设计阈值。

Abstract: The idea of using Formal Verification tools with large language models (LLMs) has enabled scaling software verification beyond manual workflows. However, current methods remain unreliable. Without a solid theoretical footing, the refinement process can wander; sometimes it settles, sometimes it loops back, and sometimes it breaks away from any stable trajectory. This work bridges this critical gap by developing an LLM-Verifier Convergence Theorem, providing the first formal framework with provable guarantees for termination and convergence. We model the interaction between the LLM and the verifier as a discrete-time Markov Chain, with state transitions determined by a key parameter: the error-reduction probability ($δ$). The procedure reaching the Verified state almost surely demonstrates that the program terminates for any $δ> 0$, with an expected iteration count bounded by $\mathbb{E}[n] \leq 4/δ$. We then stress-tested this prediction in an extensive empirical campaign comprising more than 90,000 trials. The empirical results match the theory with striking consistency. Every single run reached verification, and the convergence factor clustered tightly around $C_f\approx$ 1.0. Consequently, the bound mirrors the system's actual behavior. The evidence is sufficiently robust to support dividing the workflow into three distinct operating zones: marginal, practical, and high-performance. Consequently, we establish the design thresholds with absolute confidence. Together, the theoretical guarantee and the experimental evidence provide a clearer architectural foundation for LLM-assisted verification. Heuristic tuning no longer has to be carried out by the system. Engineers gain a framework that supports predictable resource planning and performance budgeting, precisely what is needed before deploying these pipelines into safety-critical software environments.

</details>


### [95] [From monoliths to modules: Decomposing transducers for efficient world modelling](https://arxiv.org/abs/2512.02193)
*Alexander Boyd,Franz Nowak,David Hyland,Manuel Baltieri,Fernando E. Rosas*

Main category: cs.AI

TL;DR: 这篇论文提出了一种分解复杂世界模型的方法，使其模块化、可并行化、可解释，从而提高计算效率并支持分布式推理。


<details>
  <summary>Details</summary>
Motivation: 世界模型在AI智能体训练和评估中扮演着重要角色，但其计算需求高。论文旨在通过分解世界模型来提高计算效率，并解决AI安全对结构透明度的要求。

Method: 论文开发了一个框架，用于分解用传感器表示的复杂世界模型。此方法将传感器分解为在不同输入输出子空间上运行的子传感器，以实现并行化和可解释性。

Result: 通过分解传感器，论文的方法可以实现世界模型的并行化和可解释性，替代了传统的单片式世界建模。

Conclusion: 这项研究为弥合AI安全所需的结构透明性与实际推理所需的计算效率之间的鸿沟奠定了基础。

Abstract: World models have been recently proposed as sandbox environments in which AI agents can be trained and evaluated before deployment. Although realistic world models often have high computational demands, efficient modelling is usually possible by exploiting the fact that real-world scenarios tend to involve subcomponents that interact in a modular manner. In this paper, we explore this idea by developing a framework for decomposing complex world models represented by transducers, a class of models generalising POMDPs. Whereas the composition of transducers is well understood, our results clarify how to invert this process, deriving sub-transducers operating on distinct input-output subspaces, enabling parallelizable and interpretable alternatives to monolithic world modelling that can support distributed inference. Overall, these results lay a groundwork for bridging the structural transparency demanded by AI safety and the computational efficiency required for real-world inference.

</details>


### [96] [STRIDE: A Systematic Framework for Selecting AI Modalities - Agentic AI, AI Assistants, or LLM Calls](https://arxiv.org/abs/2512.02228)
*Shubhi Asthana,Bing Zhang,Chad DeLuca,Ruchi Mahindru,Hima Patel*

Main category: cs.AI

TL;DR: STRIDE是一个评估框架，用于判断在不同任务中选择直接调用LLM、使用引导式AI助手还是完全自主的Agentic AI。该框架在实际任务中取得了92%的准确率，并显著降低了不必要的Agent部署和资源成本。


<details>
  <summary>Details</summary>
Motivation: 随着大型语言模型（LLMs）向自主、目标驱动的Agent转变，核心问题是如何判断何时真正需要Agentic AI。因为不加区分地部署Agent会导致更高的成本、复杂性和风险。

Method: STRIDE框架通过整合结构化任务分解、动态性归因和需求自省分析，生成一个Agentic适用性评分。该评分确保只有在任务具有固有的动态性或不断变化的上下文时，才推荐完全的Agentic自主性。

Result: 在30项真实世界任务中，STRIDE在模态选择方面实现了92%的准确率，将不必要的Agent部署减少了45%，并将资源成本降低了37%。

Conclusion: STRIDE通过以必要性为导向的设计决策，重新定义了Agent的采用，确保仅当其收益证明成本合理时才应用自主性。

Abstract: The rapid shift from stateless large language models (LLMs) to autonomous, goal-driven agents raises a central question: When is agentic AI truly necessary? While agents enable multi-step reasoning, persistent memory, and tool orchestration, deploying them indiscriminately leads to higher cost, complexity, and risk.
  We present STRIDE (Systematic Task Reasoning Intelligence Deployment Evaluator), a framework that provides principled recommendations for selecting between three modalities: (i) direct LLM calls, (ii) guided AI assistants, and (iii) fully autonomous agentic AI. STRIDE integrates structured task decomposition, dynamism attribution, and self-reflection requirement analysis to produce an Agentic Suitability Score, ensuring that full agentic autonomy is reserved for tasks with inherent dynamism or evolving context.
  Evaluated across 30 real-world tasks spanning SRE, compliance, and enterprise automation, STRIDE achieved 92% accuracy in modality selection, reduced unnecessary agent deployments by 45%, and cut resource costs by 37%. Expert validation over six months in SRE and compliance domains confirmed its practical utility, with domain specialists agreeing that STRIDE effectively distinguishes between tasks requiring simple LLM calls, guided assistants, or full agentic autonomy. This work reframes agent adoption as a necessity-driven design decision, ensuring autonomy is applied only when its benefits justify the costs.

</details>


### [97] [Bridging the Gap: Toward Cognitive Autonomy in Artificial Intelligence](https://arxiv.org/abs/2512.02280)
*Noorbakhsh Amiri Golilarz,Sindhuja Penchala,Shahram Rahimi*

Main category: cs.AI

TL;DR: 本文分析了当前人工智能系统的七个核心缺陷，并提出通过借鉴神经认知原理的架构来实现认知自主AI，以克服这些限制。


<details>
  <summary>Details</summary>
Motivation: 尽管当前AI在感知、语言、推理和多模态领域取得进展，但其在动态环境中进行自我监控、自我纠正和自主调节行为的能力仍存在根本性限制。

Method: 本文识别并分析了当代AI模型的七个核心缺陷，并与生物认知进行比较分析，结合AI研究、认知科学和神经科学的见解，提出了认知自主AI的未来发展方向。

Result: 当前AI架构（包括深度学习和基于Transformer的系统）的结构性限制使其无法实现鲁棒的泛化、终身适应性和真实的自主性；仅仅扩大规模无法解决这些问题。

Conclusion: 本文呼吁AI范式向认知自主AI转变，使其具备自我导向适应、动态表征管理和有意识、目标导向行为的能力，并辅以改革性的监督机制，以确保自主系统是可解释、可治理且符合人类价值观的。

Abstract: Artificial intelligence has advanced rapidly across perception, language, reasoning, and multimodal domains. Yet despite these achievements, modern AI systems remain fun- damentally limited in their ability to self-monitor, self-correct, and regulate their behavior autonomously in dynamic contexts. This paper identifies and analyzes seven core deficiencies that constrain contemporary AI models: the absence of intrinsic self- monitoring, lack of meta-cognitive awareness, fixed and non- adaptive learning mechanisms, inability to restructure goals, lack of representational maintenance, insufficient embodied feedback, and the absence of intrinsic agency. Alongside identifying these limitations, we also outline a forward-looking perspective on how AI may evolve beyond them through architectures that mirror neurocognitive principles. We argue that these structural limitations prevent current architectures, including deep learning and transformer-based systems, from achieving robust general- ization, lifelong adaptability, and real-world autonomy. Drawing on a comparative analysis of artificial systems and biological cognition [7], and integrating insights from AI research, cognitive science, and neuroscience, we outline how these capabilities are absent in current models and why scaling alone cannot resolve them. We conclude by advocating for a paradigmatic shift toward cognitively grounded AI (cognitive autonomy) capable of self-directed adaptation, dynamic representation management, and intentional, goal-oriented behavior, paired with reformative oversight mechanisms [8] that ensure autonomous systems remain interpretable, governable, and aligned with human values.

</details>


### [98] [Breast Cell Segmentation Under Extreme Data Constraints: Quantum Enhancement Meets Adaptive Loss Stabilization](https://arxiv.org/abs/2512.02302)
*Varun Kumar Dasoju,Qingsu Cheng,Zeyun Yu*

Main category: cs.AI

TL;DR: 该研究提出了一种乳腺癌组织分割框架，在有限的标注数据下，实现了95.5%的Dice分数，显著减少了医学专家的标注时间。


<details>
  <summary>Details</summary>
Motivation: 标注医学图像需要大量时间和专业知识，特别是乳腺上皮细胞核数据集的标注，需要病理学家投入数百小时。由于乳腺组织像素占比低（4%），且部分图像无乳腺区域（60%），以及不同标注者之间存在3像素的差异，使得这一挑战更为严峻。

Method: 该框架利用多尺度Gabor滤波器进行类量子边缘增强，生成第四个输入通道，以提高边界检测能力。采用结合自适应Dice损失、边界感知项和自动正向加权的稳定多组分损失函数，有效应对类别不平衡问题。引入基于复杂度的加权采样策略，优先处理具有挑战性的乳腺上皮细胞区域。模型采用EfficientNet-B7/UNet++架构，并通过4转3通道投影，利用预训练权重。通过指数移动平均和统计异常值检测进行鲁棒性验证。

Result: 该框架实现了95.5% +/- 0.3%的Dice分数和91.2% +/- 0.4%的IoU。其中，基于量子的增强使边界精度提高了2.1%，加权采样使小病灶检测率增加了3.8%。

Conclusion: 该方法在有限的标注下取得了突破性的性能，显著减少了创建数据集所需的医学专家时间，解决了临床感知AI发展中的一个根本瓶颈。

Abstract: Annotating medical images demands significant time and expertise, often requiring pathologists to invest hundreds of hours in labeling mammary epithelial nuclei datasets. We address this critical challenge by achieving 95.5% Dice score using just 599 training images for breast cell segmentation, where just 4% of pixels represent breast tissue and 60% of images contain no breast regions. Our framework uses quantum-inspired edge enhancement via multi-scale Gabor filters creating a fourth input channel, enhancing boundary detection where inter-annotator variations reach +/- 3 pixels. We present a stabilized multi-component loss function that integrates adaptive Dice loss with boundary-aware terms and automatic positive weighting to effectively address severe class imbalance, where mammary epithelial cell regions comprise only 0.1%-20% of the total image area. Additionally, a complexity-based weighted sampling strategy is introduced to prioritize the challenging mammary epithelial cell regions. The model employs an EfficientNet-B7/UNet++ architecture with a 4-to-3 channel projection, enabling the use of pretrained weights despite limited medical imaging data. Finally, robust validation is achieved through exponential moving averaging and statistical outlier detection, ensuring reliable performance estimates on a small validation set (129 images). Our framework achieves a Dice score of 95.5% +/- 0.3% and an IoU of 91.2% +/- 0.4%. Notably, quantum-based enhancement contributes to a 2.1% improvement in boundary accuracy, while weighted sampling increases small lesion detection by 3.8%. By achieving groundbreaking performance with limited annotations, our approach significantly reduces the medical expert time required for dataset creation, addressing a fundamental bottleneck in clinical perception AI development.

</details>


### [99] [OmniGuard: Unified Omni-Modal Guardrails with Deliberate Reasoning](https://arxiv.org/abs/2512.02306)
*Boyu Zhu,Xiaofei Wen,Wenjie Jacky Mo,Tinghui Zhu,Yanan Xie,Peng Qi,Muhao Chen*

Main category: cs.AI

TL;DR: OmniGuard是首个全模态安全护栏家族，旨在解决全模态大型语言模型（OLLMs）在人机交互中面临的新安全挑战。它通过一个综合的全模态安全数据集进行训练，并在多模态安全场景中表现出强大的有效性和泛化能力。


<details>
  <summary>Details</summary>
Motivation: 处理文本、图像、视频和音频的全模态大型语言模型（OLLMs）在人机交互中引入了新的安全和价值护栏挑战。先前的护栏研究主要针对单模态设置，并且通常将安全保障视为二元分类，这限制了在不同模态和任务中的鲁棒性。

Method: 本文提出了OmniGuard，这是第一个全模态护栏家族，它能够在所有模态中执行安全保障，并具有审慎的推理能力。为了支持OMNIGUARD的训练，研究人员整理了一个大型、全面的全模态安全数据集，该数据集包含超过21万个不同的样本，输入涵盖所有模态，包括单模态和跨模态样本。每个样本都用结构化安全标签和通过目标蒸馏从专家模型中精心策划的安全评论进行注释。

Result: 在15个基准测试中进行的广泛实验表明，OmniGuard在广泛的多模态安全场景中实现了强大的有效性和泛化能力。

Conclusion: OmniGuard提供了一个统一的框架，可以在全模态中执行策略和减轻风险，为构建更健壮和有能力的全模态安全保障系统铺平了道路。

Abstract: Omni-modal Large Language Models (OLLMs) that process text, images, videos, and audio introduce new challenges for safety and value guardrails in human-AI interaction. Prior guardrail research largely targets unimodal settings and typically frames safeguarding as binary classification, which limits robustness across diverse modalities and tasks. To address this gap, we propose OmniGuard, the first family of omni-modal guardrails that performs safeguarding across all modalities with deliberate reasoning ability. To support the training of OMNIGUARD, we curate a large, comprehensive omni-modal safety dataset comprising over 210K diverse samples, with inputs that cover all modalities through both unimodal and cross-modal samples. Each sample is annotated with structured safety labels and carefully curated safety critiques from expert models through targeted distillation. Extensive experiments on 15 benchmarks show that OmniGuard achieves strong effectiveness and generalization across a wide range of multimodal safety scenarios. Importantly, OmniGuard provides a unified framework that enforces policies and mitigates risks in omni-modalities, paving the way toward building more robust and capable omnimodal safeguarding systems.

</details>


### [100] [Reasoning Path and Latent State Analysis for Multi-view Visual Spatial Reasoning: A Cognitive Science Perspective](https://arxiv.org/abs/2512.02340)
*Qiyao Xue,Weichen Liu,Shiqi Wang,Haoming Wang,Yuyang Wu,Wei Gao*

Main category: cs.AI

TL;DR: ReMindView-Bench基准测试评估了当前视觉语言模型（VLMs）在多视角空间推理方面的表现，并发现它们在跨视角对齐和视角采纳方面存在问题。


<details>
  <summary>Details</summary>
Motivation: 当前视觉语言模型在多视角空间推理方面表现不佳，难以保持几何一致性和跨视角一致性。

Method: 本文提出了ReMindView-Bench，一个认知基础的基准测试，用于评估VLMs如何在互补视角中构建、对齐和维护空间心理模型。该基准系统地改变视角空间模式和查询类型，以探究空间认知的关键因素。

Result: 15个当前VLM的评估显示，在多视角空间推理中，跨视角对齐和视角采纳存在一致性失效。通过LLM-as-a-judge和自洽性提示进行的显式分阶段分析表明，VLMs在帧内感知方面表现良好，但在整合跨视角信息时性能急剧下降。隐式分析（包括线性探测和熵动态）进一步显示，任务相关信息的逐步丢失以及正确和不正确轨迹之间的不确定性分离。

Conclusion: 本研究对VLM空间推理进行了认知基础诊断，并揭示了多视角空间心理模型在推理阶段是如何形成、退化和不稳定的。

Abstract: Spatial reasoning is a core aspect of human intelligence that allows perception, inference and planning in 3D environments. However, current vision-language models (VLMs) struggle to maintain geometric coherence and cross-view consistency for spatial reasoning in multi-view settings. We attribute this gap to the lack of fine-grained benchmarks that isolate multi-view reasoning from single-view perception and temporal factors. To address this, we present ReMindView-Bench, a cognitively grounded benchmark for evaluating how VLMs construct, align and maintain spatial mental models across complementary viewpoints. ReMindView-Bench systematically varies viewpoint spatial pattern and query type to probe key factors of spatial cognition. Evaluations of 15 current VLMs reveals consistent failures in cross-view alignment and perspective-taking in multi-view spatial reasoning, motivating deeper analysis on the reasoning process. Explicit phase-wise analysis using LLM-as-a-judge and self-consistency prompting shows that VLMs perform well on in-frame perception but degrade sharply when integrating information across views. Implicit analysis, including linear probing and entropy dynamics, further show progressive loss of task-relevant information and uncertainty separation between correct and incorrect trajectories. These results provide a cognitively grounded diagnosis of VLM spatial reasoning and reveal how multi-view spatial mental models are formed, degraded and destabilized across reasoning phases. The ReMindView-Bench benchmark is available at https://huggingface.co/datasets/Xue0823/ReMindView-Bench, and the source codes of benchmark construction and VLM reasoning analysis are available at https://github.com/pittisl/ReMindView-Bench.

</details>


### [101] [Synthetic Error Injection Fails to Elicit Self-Correction In Language Models](https://arxiv.org/abs/2512.02389)
*David X. Wu,Shreyas Kapur,Anant Sahai,Stuart Russell*

Main category: cs.AI

TL;DR: 这篇论文探讨了通过注入合成错误进行监督学习，能否在大型语言模型中诱导自我纠正能力，但发现这种方法未能显著提高性能，并且模型在纠正错误时往往只是重复原始错误。研究发现合成错误与实际错误之间的分布差异是主要原因。


<details>
  <summary>Details</summary>
Motivation: 大型语言模型中，强化学习在引发推理和自我纠错能力方面占据主导地位，但其计算成本促使研究人员探索其他替代方案。本文旨在探究使用合成错误注入的监督学习是否能诱导语言模型中的自我纠正能力。

Method: 该研究提出了一种方法，即在推理链中插入人工错误，然后遮盖这些错误，并监督模型识别和纠正这些错误。

Result: 尽管这种方法在直觉上很有吸引力，但研究发现它未能显著提高模型在简单合成任务上的性能。此外，即使模型识别出自己的错误，它也经常重复最初的错误。研究发现合成错误与实际错误（on-policy errors）之间的分布差异显著降低了微调模型的纠错能力。

Conclusion: 研究结果有助于解释为什么策略内强化学习（on-policy reinforcement learning）方法在引发自我纠正方面被证明是 H 有效的。这表明，虽然监督学习在某些方面有效，但在处理自我纠错这种需要理解和生成正确回应的任务时，强化学习可能因其动态性和对真实错误分布的接触而更具优势。

Abstract: Reinforcement learning has become the dominant paradigm for eliciting reasoning and self-correction capabilities in large language models, but its computational expense motivates exploration of alternatives. Inspired by techniques from autonomous driving and robotics, we investigate whether supervised learning with synthetic error injection can induce self-correction abilities in language models. Our approach inserts artificial errors into reasoning chains, masks them, and supervises the model to recognize and correct these mistakes. Despite the intuitive appeal of this method, we find that it fails to significantly improve performance even on simple synthetic tasks across multiple models. Moreover, even when the model catches its own error, it often parrots the original mistake. We find that the distribution shift of synthetic errors to on-policy errors significantly degrades the error-correction capabilities of the fine-tuned model, even with good synthetic coverage of on-policy errors. Our results help explain why on-policy reinforcement learning methods have proven uniquely effective for eliciting self-correction.

</details>


### [102] [Guided Self-Evolving LLMs with Minimal Human Supervision](https://arxiv.org/abs/2512.02472)
*Wenhao Yu,Zhenwen Liang,Chengsong Huang,Kishan Panaganti,Tianqing Fang,Haitao Mi,Dong Yu*

Main category: cs.AI

TL;DR: R-Few是一个引导式的自博弈挑战者-解决者框架，它通过上下文基础和混合训练，以最小化的人工监督，实现模型自我演进。


<details>
  <summary>Details</summary>
Motivation: 自进化的AI系统在实践中经常遇到性能停滞甚至下降的问题，原因包括概念漂移、多样性崩溃和错误演化，导致模型强化自身偏见并收敛到低熵行为。

Method: R-Few框架通过“挑战者”和“解决者”的协同工作实现自我演进。挑战者通过少量人工标注示例生成合成问题，解决者则在线性且基于难度的课程下，同时利用人工和合成示例进行训练。

Result: R-Few在数学和通用推理基准测试中取得了持续的迭代改进。例如，Qwen3-8B-Base在数学任务上比R-Zero提升了3.0点，并且性能与General-Reasoner相当，尽管后者使用了20倍的人工数据。

Conclusion: R-Few通过引导挑战者训练和基于课程的解决者训练，有效缓解了概念漂移，实现了更稳定和可控的协同进化动态。

Abstract: AI self-evolution has long been envisioned as a path toward superintelligence, where models autonomously acquire, refine, and internalize knowledge from their own learning experiences. Yet in practice, unguided self-evolving systems often plateau quickly or even degrade as training progresses. These failures arise from issues such as concept drift, diversity collapse, and mis-evolution, as models reinforce their own biases and converge toward low-entropy behaviors. To enable models to self-evolve in a stable and controllable manner while minimizing reliance on human supervision, we introduce R-Few, a guided Self-Play Challenger-Solver framework that incorporates lightweight human oversight through in-context grounding and mixed training. At each iteration, the Challenger samples a small set of human-labeled examples to guide synthetic question generation, while the Solver jointly trains on human and synthetic examples under an online, difficulty-based curriculum. Across math and general reasoning benchmarks, R-Few achieves consistent and iterative improvements. For example, Qwen3-8B-Base improves by +3.0 points over R-Zero on math tasks and achieves performance on par with General-Reasoner, despite the latter being trained on 20 times more human data. Ablation studies confirm the complementary contributions of grounded challenger training and curriculum-based solver training, and further analysis shows that R-Few mitigates drift, yielding more stable and controllable co-evolutionary dynamics.

</details>


### [103] [COPE: Chain-Of-Thought Prediction Engine for Open-Source Large Language Model Based Stroke Outcome Prediction from Clinical Notes](https://arxiv.org/abs/2512.02499)
*Yongkai Liu,Helena Feng,Bin Jiang,Yixin Wang,Max Wintermark,David S. Liebeskind,Michael Moseley,Maarten Lansberg,Gregory Albers,Jeremy Heit,Greg Zaharchuk*

Main category: cs.AI

TL;DR: COPE是一个基于LLaMA-3-8B的轻量级、可解释、保护隐私的开源框架，用于从非结构化临床记录中预测急性缺血性卒中（AIS）患者的90天功能预后，其性能与GPT-4.1相当，并优于其他基线模型。


<details>
  <summary>Details</summary>
Motivation: 临床记录中包含丰富的上下文信息，但其非结构化特性限制了其在传统预测模型中的应用，因此需要开发新的方法来利用这些信息进行结果预测。

Method: 开发并评估了CoT（思维链）结果预测引擎（COPE），这是一个推理增强型大型语言模型框架。COPE采用两步CoT框架，基于序列开源LLaMA-3-8B模型：第一步生成临床推理，第二步输出mRS预测。通过平均绝对误差（MAE）、+/-1 mRS点内的准确率和精确准确率来评估性能，并与GPT-4.1、ClinicalBERT、基于结构化变量的机器学习模型（Clinical ML）以及不带CoT的单步LLM进行比较。

Result: COPE的MAE为1.01，+/-1准确率为74.4%，精确准确率为32.8%。其性能与GPT-4.1相当，并优于ClinicalBERT、Clinical ML和单步LLM。亚组分析显示，在不同性别和年龄组中性能一致，但在老年患者、接受取栓治疗的患者以及摘要较长的患者中误差略高。

Conclusion: COPE为从非结构化临床文本中进行结果预测提供了一个准确且实用的解决方案，具有轻量级、可解释和保护隐私的优点。

Abstract: Predicting outcomes in acute ischemic stroke (AIS) guides clinical decision-making, patient counseling, and resource allocation. Clinical notes contain rich contextual information, but their unstructured nature limits their use in traditional predictive models. We developed and evaluated the Chain-of-Thought (CoT) Outcome Prediction Engine (COPE), a reasoning-enhanced large language model framework, for predicting 90-day functional outcomes after AIS from unstructured clinical notes. This study included 464 AIS patients with discharge summaries and 90-day modified Rankin Scale (mRS) scores. COPE uses a two-step CoT framework based on sequential open-source LLaMA-3-8B models: the first generates clinical reasoning, and the second outputs an mRS prediction. We compared COPE with GPT-4.1, ClinicalBERT, a structured variable-based machine learning model (Clinical ML), and a single-step LLM without CoT. Performance was evaluated using mean absolute error (MAE), accuracy within +/-1 mRS point, and exact accuracy. COPE achieved an MAE of 1.01 (95% CI 0.92-1.11), +/-1 accuracy of 74.4% (69.9, 78.8%), and exact accuracy of 32.8% (28.0, 37.6%), comparable to GPT-4.1 and superior to ClinicalBERT [MAE 1.24 (1.13-1.36)], Clinical ML [1.28 (1.18-1.39)], and the single-step LLM [1.20 (1.09-1.33)]. Subgroup analyses showed consistent performance across sex and age, with slightly higher error among older patients, those undergoing thrombectomy, and those with longer summaries. These findings demonstrate that COPE, a lightweight, interpretable, and privacy-preserving open-source framework, provides an accurate and practical solution for outcome prediction from unstructured clinical text.

</details>


### [104] [PaperDebugger: A Plugin-Based Multi-Agent System for In-Editor Academic Writing, Review, and Editing](https://arxiv.org/abs/2512.02589)
*Junyi Hou,Andre Lin Huikai,Nuo Chen,Yiwei Gong,Bingsheng He*

Main category: cs.AI

TL;DR: PaperDebugger是一个多智能体、基于插件的学术写作助手，能够将大型语言模型驱动的推理直接引入到LaTeX编辑器中，从而实现与文档状态、结构和修订历史的深度交互。


<details>
  <summary>Details</summary>
Motivation: 现有的学术写作助手无法与LaTeX编辑器深度交互，无法支持在编辑器内进行智能、上下文感知的操作。

Method: PaperDebugger通过Chrome扩展、Kubernetes原生编排层和Model Context Protocol (MCP) 工具链解决了技术挑战，实现了与编辑器的双向同步、精细的版本控制和补丁、安全的状态管理、多智能体调度以及与外部工具的可扩展通信。

Result: PaperDebugger实现了包括本地化编辑、结构化审查、并行智能体执行和基于差异的更新等完全集成的工作流程，并具有侵入性最小的用户界面。

Conclusion: PaperDebugger证明了编辑器原生的智能写作助手的实用性，并通过早期聚合分析数据展示了用户的积极参与。

Abstract: Large language models are increasingly embedded into academic writing workflows, yet existing assistants remain external to the editor, preventing deep interaction with document state, structure, and revision history. This separation makes it impossible to support agentic, context-aware operations directly within LaTeX editors such as Overleaf. We present PaperDebugger, an in-editor, multi-agent, and plugin-based academic writing assistant that brings LLM-driven reasoning directly into the writing environment. Enabling such in-editor interaction is technically non-trivial: it requires reliable bidirectional synchronization with the editor, fine-grained version control and patching, secure state management, multi-agent scheduling, and extensible communication with external tools. PaperDebugger addresses these challenges through a Chrome-approved extension, a Kubernetes-native orchestration layer, and a Model Context Protocol (MCP) toolchain that integrates literature search, reference lookup, document scoring, and revision pipelines. Our demo showcases a fully integrated workflow, including localized edits, structured reviews, parallel agent execution, and diff-based updates, encapsulated within a minimal-intrusion user interface (UI). Early aggregated analytics demonstrate active user engagement and validate the practicality of an editor-native, agentic writing assistant. More details about this demo and video could be found at https://github.com/PaperDebugger/PaperDebugger.

</details>


### [105] [Target-specific Adaptation and Consistent Degradation Alignment for Cross-Domain Remaining Useful Life Prediction](https://arxiv.org/abs/2512.02610)
*Yubo Hou,Mohamed Ragab,Min Wu,Chee-Keong Kwoh,Xiaoli Li,Zhenghua Chen*

Main category: cs.AI

TL;DR: 本文提出了一种新颖的跨域RUL预测方法TACDA，通过目标域重建策略保留目标特定信息，并采用聚类配对策略对齐相似的退化阶段，在实验中表现优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 现有的数据驱动的剩余使用寿命（RUL）预测技术在训练和测试数据分布不同时表现不佳，因为它们通常假设数据来自同一分布或领域。现有的对抗性域适应方法忽略了目标特定信息和退化阶段的不一致性特征，导致性能不佳。

Method: 本文提出了一种名为TACDA的跨域RUL预测域适应方法。具体来说，该方法在对抗性适应过程中引入了目标域重建策略，以保留目标特定信息，同时学习域不变特征。此外，还开发了一种新颖的聚类和配对策略，用于在相似的退化阶段之间进行一致性对齐。

Result: 通过广泛的实验，TACDA方法在两种不同的评估指标上均优于现有的最新方法，表现出卓越的性能。

Conclusion: TACDA方法通过结合目标域重建和聚类配对策略，有效解决了跨域RUL预测中的挑战，提高了预测准确性。

Abstract: Accurate prediction of the Remaining Useful Life (RUL) in machinery can significantly diminish maintenance costs, enhance equipment up-time, and mitigate adverse outcomes. Data-driven RUL prediction techniques have demonstrated commendable performance. However, their efficacy often relies on the assumption that training and testing data are drawn from the same distribution or domain, which does not hold in real industrial settings. To mitigate this domain discrepancy issue, prior adversarial domain adaptation methods focused on deriving domain-invariant features. Nevertheless, they overlook target-specific information and inconsistency characteristics pertinent to the degradation stages, resulting in suboptimal performance. To tackle these issues, we propose a novel domain adaptation approach for cross-domain RUL prediction named TACDA. Specifically, we propose a target domain reconstruction strategy within the adversarial adaptation process, thereby retaining target-specific information while learning domain-invariant features. Furthermore, we develop a novel clustering and pairing strategy for consistent alignment between similar degradation stages. Through extensive experiments, our results demonstrate the remarkable performance of our proposed TACDA method, surpassing state-of-the-art approaches with regard to two different evaluation metrics. Our code is available at https://github.com/keyplay/TACDA.

</details>


### [106] [Exploring Depth Generalization in Large Language Models for Solving Recursive Logic Tasks](https://arxiv.org/abs/2512.02677)
*Zhiyuan He*

Main category: cs.AI

TL;DR: 这篇论文研究了大型语言模型在处理递归推理问题时的泛化能力，特别是对嵌套深度（depth generalization）的泛化能力。


<details>
  <summary>Details</summary>
Motivation: 大型语言模型在处理递归推理问题，即需要解决嵌套层次结构的问题时面临显著挑战。以往的研究主要集中在长度泛化上，而对深度泛化（即层次结构中的嵌套层数）的限制研究较少。

Method: 通过系统的分析，论文揭示了标准Transformer架构在处理比训练时遇到的更深层次的递归问题时表现不佳，即使在处理更长但非嵌套的序列时表现良好。这种限制源于它们无法维持堆栈式行为，即跟踪和解决多层嵌套依赖的能力。为了解决这个问题，论文开发了一种新颖的循环定位和替换（looped locate-and-replace）流水线，将递归问题分解为可管理的子组件。该方法采用了两个专门的模型：一个定位器（locator）用于识别可解决的子表达式，一个替换器（replacer）用于评估这些组件，同时保留整体结构。

Result: 在布尔代数、递归算术和命题逻辑这三个可控递归深度的领域进行了评估。结果表明，该方法在测试分布外递归深度时，有效缓解了性能衰减。

Conclusion: 大型语言模型在深度泛化方面存在局限性，论文提出的循环定位和替换流水线能够有效解决这一问题。

Abstract: Large language models have demonstrated remarkable capabilities across many tasks, yet face significant challenges when dealing with recursive reasoning problems, those requiring the resolution of nested hierarchical structures. While prior research has extensively studied length generalization (a model's ability to handle longer sequences than seen during training), we investigate a distinct and underexplored limitation: depth generalization. Here, depth refers to the number of nested levels in a hierarchical problem, such as the layers of parentheses in a mathematical expression or the nesting of logical clauses in a Boolean formula. Our work reveals that standard transformer architectures struggle with problems involving deeper recursion than encountered during training, even when they perform well on longer but non-nested sequences. This limitation stems from their inability to maintain stack-like behavior, the capacity to track and resolve multiple levels of nested dependencies. Through systematic analysis, we demonstrate how this architectural constraint leads to rapid performance decay as the depth of the recursion increases. To address this challenge, we develop a novel looped locate-and-replace pipeline that decomposes recursive problems into manageable subcomponents. The approach employs two specialized models: a locator that identifies solvable subexpressions and a replacer that evaluates these components while preserving the overall structure. We evaluated this method in three carefully designed domains: Boolean algebra, recursive arithmetic, and propositional logic, each with a controllable depth of recursion. We show that our method effectively alleviates the performance decay when tested on out-of-distribution recursion depth.

</details>


### [107] [Learning What to Attend First: Modality-Importance-Guided Reasoning for Reliable Multimodal Emotion Understanding](https://arxiv.org/abs/2512.02699)
*Hyeongseop Rha,Jeong Hun Yeo,Junil Won,Se Jin Park,Yong Man Ro*

Main category: cs.AI

TL;DR: 该文章提出了一个名为 MIGR 的框架，旨在通过引导模型从情感主导模态开始推理，来提升多模态大语言模型中基于推理的多模态情感理解的可靠性。


<details>
  <summary>Details</summary>
Motivation: 尽管现有方法在情感理解方面取得了进展，但它们常面临推理漂移问题：模型逐渐依赖自己生成的文本而非多模态证据，且解释过多地受视觉启动的推理路径影响。

Method: 本文引入了情感重要性（MI）机制来识别情感主导模态，并利用MI重组推理序列，使得解释从对目标情感最关键的模态开始。MIGR是一个两阶段框架，包括模态对齐的监督微调和模态感知的奖励优化。

Result: 在DFEW基准测试集上的实验结果表明，MIGR显著提升了推理可靠性，将正确预测但伴随情感不一致解释的情况从18.10%降低至7.37%。

Conclusion: 从情感主导模态启动推理有助于提高情感理解的可靠性。

Abstract: In this paper, we present Modality-Importance-Guided Reasoning (MIGR), a framework designed to improve the reliability of reasoning-based multimodal emotion understanding in multimodal large language models. Although existing methods have advanced emotion understanding, they often suffer from reasoning drift: models gradually rely on their own generated text instead of multimodal evidence, and their explanations are overly shaped by visually initiated reasoning paths. To address these issues, we introduce Modality Importance (MI), a simple yet effective mechanism for identifying the emotion-dominant modality. Using MI, MIGR reorganizes reasoning sequences so that explanations begin from the modality most critical to the target emotion, preventing early reasoning from being misled by less informative cues. Our two-stage framework-comprising modality-aligned supervised fine-tuning and modality-aware reward optimization-encourages models to generate emotionally grounded, causally relevant, and coherence-preserving explanations. Experimental results on the DFEW benchmark show that MIGR substantially improves reasoning reliability, decreasing instances of correct predictions accompanied by emotionally inconsistent explanations from 18.10% to 7.37%. These results confirm the benefit of initiating reasoning from the emotion-dominant modality.

</details>


### [108] [Training Data Attribution for Image Generation using Ontology-Aligned Knowledge Graphs](https://arxiv.org/abs/2512.02713)
*Theodoros Aivalis,Iraklis A. Klampanos,Antonis Troumpoukis,Joemon M. Jose*

Main category: cs.AI

TL;DR: 该论文提出了一个通过构建本体对齐知识图谱来解释生成模型输出的框架，以解决透明度、可信赖性和版权问题。


<details>
  <summary>Details</summary>
Motivation: 随着生成模型能力的提高，透明度、可信赖度和版权侵犯等问题日益突出。理解特定训练数据如何影响模型输出变得至关重要。

Method: 该方法利用多模态大型语言模型（LLMs）从图像中提取与领域特定本体对齐的结构化三元组，从而构建生成图像和训练图像的知识图谱。

Result: 通过比较生成图像和训练图像的知识图谱，该框架可以追踪潜在的影响来源，从而实现版权分析、数据集透明化和可解释AI。实验在一系列模型上验证了该方法的有效性。

Conclusion: 该框架支持AI系统的发展，以促进人类协作、创造力和激发好奇心。

Abstract: As generative models become powerful, concerns around transparency, accountability, and copyright violations have intensified. Understanding how specific training data contributes to a model's output is critical. We introduce a framework for interpreting generative outputs through the automatic construction of ontologyaligned knowledge graphs (KGs). While automatic KG construction from natural text has advanced, extracting structured and ontology-consistent representations from visual content remains challenging -- due to the richness and multi-object nature of images. Leveraging multimodal large language models (LLMs), our method extracts structured triples from images, aligned with a domain-specific ontology. By comparing the KGs of generated and training images, we can trace potential influences, enabling copyright analysis, dataset transparency, and interpretable AI. We validate our method through experiments on locally trained models via unlearning, and on large-scale models through a style-specific experiment. Our framework supports the development of AI systems that foster human collaboration, creativity and stimulate curiosity.

</details>


### [109] [Menta: A Small Language Model for On-Device Mental Health Prediction](https://arxiv.org/abs/2512.02716)
*Tianyi Zhang,Xiangyuan Xue,Lingyan Ruan,Shiya Fu,Feng Xia,Simon D'Alfonso,Vassilis Kostakos,Hong Jia*

Main category: cs.AI

TL;DR: 该研究介绍了 Menta，这是一个针对社交媒体心理健康预测进行优化的 SLM，在准确性方面显著优于现有模型，并支持在设备上实时部署。


<details>
  <summary>Details</summary>
Motivation: 大型语言模型（LLM）在心理健康应用中显示出潜力，但其庞大的规模和计算需求限制了实际部署。小型语言模型（SLM）提供了一种轻量级替代方案，但其在基于社交媒体的心理健康预测中的应用尚未得到充分探索。

Method: Menta 是第一个专门针对社交媒体数据进行多任务心理健康预测而微调的优化 SLM。它采用基于 LoRA 的框架、跨数据集策略和以平衡准确性为导向的损失，在六个分类任务中进行联合训练。

Result: Menta 在涵盖抑郁症、压力和自杀意念的任务中，与表现最佳的非微调 SLM 相比，平均提高了 15.2%。在抑郁症和压力分类任务中，Menta 的准确性高于 13B 参数的 LLM，而其体积大约小 3.25 倍。Menta 可以在 iPhone 15 Pro Max 上实时部署，仅需约 3GB 内存。

Conclusion: Menta 展示了可扩展、保护隐私的心理健康监测的潜力，并为在资源受限环境中利用 SLM 解决复杂的现实世界问题提供了新的可能性。

Abstract: Mental health conditions affect hundreds of millions globally, yet early detection remains limited. While large language models (LLMs) have shown promise in mental health applications, their size and computational demands hinder practical deployment. Small language models (SLMs) offer a lightweight alternative, but their use for social media--based mental health prediction remains largely underexplored. In this study, we introduce Menta, the first optimized SLM fine-tuned specifically for multi-task mental health prediction from social media data. Menta is jointly trained across six classification tasks using a LoRA-based framework, a cross-dataset strategy, and a balanced accuracy--oriented loss. Evaluated against nine state-of-the-art SLM baselines, Menta achieves an average improvement of 15.2\% across tasks covering depression, stress, and suicidality compared with the best-performing non--fine-tuned SLMs. It also achieves higher accuracy on depression and stress classification tasks compared to 13B-parameter LLMs, while being approximately 3.25x smaller. Moreover, we demonstrate real-time, on-device deployment of Menta on an iPhone 15 Pro Max, requiring only approximately 3GB RAM. Supported by a comprehensive benchmark against existing SLMs and LLMs, Menta highlights the potential for scalable, privacy-preserving mental health monitoring. Code is available at: https://xxue752-nz.github.io/menta-project/

</details>


### [110] [AuditCopilot: Leveraging LLMs for Fraud Detection in Double-Entry Bookkeeping](https://arxiv.org/abs/2512.02726)
*Md Abdul Kadir,Sai Suresh Macharla Vasu,Sidharth S. Nair,Daniel Sonntag*

Main category: cs.AI

TL;DR: 本文探讨了大型语言模型（LLMs）在复式记账法中作为异常检测器的潜力，并证明它们在检测税务相关账本记录异常方面优于传统方法。


<details>
  <summary>Details</summary>
Motivation: 审计师依赖日记账分录测试（JETs）来检测税务相关账本记录中的异常，但传统规则方法会产生大量误报且难以发现细微异常。本文旨在探索大型语言模型（LLMs）是否能作为复式记账中的异常检测器。

Method: 本文在合成和真实的匿名账本上，对LLaMA和Gemma等最先进的LLM进行了基准测试，并将其与JETs和机器学习基线进行了比较。

Result: LLMs持续优于传统的基于规则的JETs和经典的机器学习基线，同时提供了增强可解释性的自然语言解释。

Conclusion: LLMs在审计领域具有巨大潜力，可以实现AI增强审计，使人类审计师能够与基础模型协作，从而加强财务诚信。

Abstract: Auditors rely on Journal Entry Tests (JETs) to detect anomalies in tax-related ledger records, but rule-based methods generate overwhelming false positives and struggle with subtle irregularities. We investigate whether large language models (LLMs) can serve as anomaly detectors in double-entry bookkeeping. Benchmarking SoTA LLMs such as LLaMA and Gemma on both synthetic and real-world anonymized ledgers, we compare them against JETs and machine learning baselines. Our results show that LLMs consistently outperform traditional rule-based JETs and classical ML baselines, while also providing natural-language explanations that enhance interpretability. These results highlight the potential of \textbf{AI-augmented auditing}, where human auditors collaborate with foundation models to strengthen financial integrity.

</details>


### [111] [A Framework for Causal Concept-based Model Explanations](https://arxiv.org/abs/2512.02735)
*Anna Rodum Bjøru,Jacob Lysnæs-Larsen,Oskar Jørgensen,Inga Strümke,Helge Langseth*

Main category: cs.AI

TL;DR: 这篇论文提出了一个因果概念驱动的后验可解释人工智能（XAI）的框架，该框架旨在提供可理解且忠实于模型的解释。


<details>
  <summary>Details</summary>
Motivation: 解释非可解释模型的内在机制，并确保解释既易于理解又忠实于模型。

Method: 通过计算概念干预的充分性概率来生成局部和全局解释。该框架在用CelebA数据集训练的分类器上进行了概念验证。

Result: 通过清晰的、基于概念的词汇表展示了可理解性，这隐含着因果解释。通过强调重要的框架假设，并强调解释解释的上下文必须与解释生成的上下文对齐，解决了忠实性问题。

Conclusion: 该工作提出了一个因果概念驱动的XAI框架，通过概率干预生成可理解和忠实于模型的解释，并在CelebA数据集上进行了概念验证。未来的工作将侧重于进一步验证其在更复杂模型和数据集上的有效性。

Abstract: This work presents a conceptual framework for causal concept-based post-hoc Explainable Artificial Intelligence (XAI), based on the requirements that explanations for non-interpretable models should be understandable as well as faithful to the model being explained. Local and global explanations are generated by calculating the probability of sufficiency of concept interventions. Example explanations are presented, generated with a proof-of-concept model made to explain classifiers trained on the CelebA dataset. Understandability is demonstrated through a clear concept-based vocabulary, subject to an implicit causal interpretation. Fidelity is addressed by highlighting important framework assumptions, stressing that the context of explanation interpretation must align with the context of explanation generation.

</details>


### [112] [The future of AI in critical mineral exploration](https://arxiv.org/abs/2512.02879)
*Jef Caers*

Main category: cs.AI

TL;DR: 为应对全球关键矿产勘探新发现减少的挑战，本文提出了一种人工智能驱动的矿产勘探科学方法，旨在通过消除认知偏差和降低成本来提高勘探效率。


<details>
  <summary>Details</summary>
Motivation: 尽管投资增加，但全球关键矿产勘探的新发现却在减少，这促使人们寻求更高效的勘探方法。

Method: 本文提出了一种基于贝叶斯主义和证伪原则的新型科学方法，并辅以人工智能。该方法将数据 L 获取作为证伪人类假设的手段，并通过可验证的指标和理性决策来指导下一步的数据获取。作者还提供了一个实用的勘探协议模板，并强调了无监督学习和人机协作人工智能在理解数据、生成地质假设以及优化数据获取方面的重要性。

Result: 通过将人工智能整合到勘探流程中，该方法有望减少认知偏差和误报，从而降低勘探成本。

Conclusion: 本文提出的人工智能驱动的科学勘探方法，通过结合贝叶斯主义、证伪原则和先进的人工智能技术，为关键矿产勘探提供了一个创新且高效的解决方案。

Abstract: The energy transition through increased electrification has put the worlds attention on critical mineral exploration Even with increased investments a decrease in new discoveries has taken place over the last two decades Here I propose a solution to this problem where AI is implemented as the enabler of a rigorous scientific method for mineral exploration that aims to reduce cognitive bias and false positives drive down the cost of exploration I propose a new scientific method that is based on a philosophical approach founded on the principles of Bayesianism and falsification In this approach data acquisition is in the first place seen as a means to falsify human generated hypothesis Decision of what data to acquire next is quantified with verifiable metrics and based on rational decision making A practical protocol is provided that can be used as a template in any exploration campaign However in order to make this protocol practical various form of artificial intelligence are needed I will argue that the most important form are one novel unsupervised learning methods that collaborate with domain experts to better understand data and generate multiple competing geological hypotheses and two humanintheloop AI algorithms that can optimally plan various geological geophysical geochemical and drilling data acquisition where uncertainty reduction of geological hypothesis precedes the uncertainty reduction on grade and tonnage

</details>


<div id='stat.ML'></div>

# stat.ML [[Back]](#toc)

### [113] [Revisiting Theory of Contrastive Learning for Domain Generalization](https://arxiv.org/abs/2512.02831)
*Ali Alvandi,Mina Rezaei*

Main category: stat.ML

TL;DR: 这篇论文提出了针对对比学习中领域泛化挑战的新颖泛化界限。


<details>
  <summary>Details</summary>
Motivation: 现有的对比学习理论假设下游任务类别与预训练阶段使用的潜在类别分布相同，但在现实世界中，下游任务可能存在分布偏移，甚至引入新的标签空间。

Method: 本文引入了新的泛化界限，明确解释了领域偏移和领域泛化这两种不匹配情况。具体来说，分析了下游任务类别来自相同潜在类别空间但分布偏移，或者涉及预训练阶段未见的新标签空间这两种场景。

Result: 分析揭示了对比学习表示的性能如何取决于预训练和下游分布之间的统计差异。

Conclusion: 这项扩展的视角使得能够对学习到的表示在包含预训练潜在类别集之外的类别分布的平均分类任务上的性能提供可证明的保证。

Abstract: Contrastive learning is among the most popular and powerful approaches for self-supervised representation learning, where the goal is to map semantically similar samples close together while separating dissimilar ones in the latent space. Existing theoretical methods assume that downstream task classes are drawn from the same latent class distribution used during the pretraining phase. However, in real-world settings, downstream tasks may not only exhibit distributional shifts within the same label space but also introduce new or broader label spaces, leading to domain generalization challenges. In this work, we introduce novel generalization bounds that explicitly account for both types of mismatch: domain shift and domain generalization. Specifically, we analyze scenarios where downstream tasks either (i) draw classes from the same latent class space but with shifted distributions, or (ii) involve new label spaces beyond those seen during pretraining. Our analysis reveals how the performance of contrastively learned representations depends on the statistical discrepancy between pretraining and downstream distributions. This extended perspective allows us to derive provable guarantees on the performance of learned representations on average classification tasks involving class distributions outside the pretraining latent class set.

</details>
