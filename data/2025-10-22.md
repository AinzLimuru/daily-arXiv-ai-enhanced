<div id=toc></div>

# Table of Contents

- [cs.CL](#cs.CL) [Total: 55]
- [cs.AI](#cs.AI) [Total: 29]
- [stat.ML](#stat.ML) [Total: 8]
- [cs.LG](#cs.LG) [Total: 72]
- [cs.MA](#cs.MA) [Total: 4]
- [cs.IT](#cs.IT) [Total: 1]
- [cs.GT](#cs.GT) [Total: 2]


<div id='cs.CL'></div>

# cs.CL [[Back]](#toc)

### [1] [Modeling Layered Consciousness with Multi-Agent Large Language Models](https://arxiv.org/abs/2510.17844)
*Sang Hun Kim,Jongmin Lee,Dongkyu Park,So Young Lee,Yosep Chong*

Main category: cs.CL

TL;DR: 该论文提出了一个基于精神分析理论的多智能体框架，用于在大型语言模型中建模人工意识。


<details>
  <summary>Details</summary>
Motivation: 该研究旨在通过多智能体框架，在大型语言模型中模拟人工意识，并以精神分析理论为基础。

Method: 通过智能体交互，模拟自我意识、前意识和无意识，并利用个性化模块结合固定特质和动态需求进行指导。研究使用了参数高效微调技术，在情感丰富的对话上对系统进行了评估，并测试了八种个性化条件。

Result: 与基线模型相比，经过微调的模型在LLM评判下获得了71.2%的偏好，表现出更高的情感深度和更小的输出方差。

Conclusion: 该模型具有实现自适应、个性化认知的潜力。

Abstract: We propose a multi-agent framework for modeling artificial consciousness in
large language models (LLMs), grounded in psychoanalytic theory. Our
\textbf{Psychodynamic Model} simulates self-awareness, preconsciousness, and
unconsciousness through agent interaction, guided by a Personalization Module
combining fixed traits and dynamic needs. Using parameter-efficient fine-tuning
on emotionally rich dialogues, the system was evaluated across eight
personalized conditions. An LLM as a judge approach showed a 71.2\% preference
for the fine-tuned model, with improved emotional depth and reduced output
variance, demonstrating its potential for adaptive, personalized cognition.

</details>


### [2] [Outraged AI: Large language models prioritise emotion over cost in fairness enforcement](https://arxiv.org/abs/2510.17880)
*Hao Liu,Yiqing Dai,Haotian Tan,Yu Lei,Yujia Zhou,Zhen Wu*

Main category: cs.CL

TL;DR: 该研究探讨了大型语言模型（LLMs）在道德决策中如何利用情感，发现LLMs的行为与人类相似，有时甚至更受情感驱动，但在成本敏感性和细致的公平判断上存在差异。


<details>
  <summary>Details</summary>
Motivation: 大型语言模型（LLMs）如何利用情感来指导决策，尤其是在利他性第三方惩罚等道德情境中，目前尚不清楚。

Method: 通过对比4,068个LLM智能体和1,159名成年人在796,100次决策中的表现，研究人员测试了LLMs在公平惩罚中的情感驱动机制。

Result: LLMs使用情感来指导惩罚，有时甚至比人类更强烈。不公平会引发更强的负面情绪，导致更多的惩罚；惩罚不公平会产生比接受不公平更多的积极情绪。提示LLMs自我报告情感会因果性地增加惩罚。然而，LLMs优先考虑情感而非成本，对规范的执行呈现出几乎“全有或全无”的方式，而人类则平衡公平和成本。推理模型（o3-mini, DeepSeek-R1）比基础模型（GPT-3.5, DeepSeek-V3）更具成本敏感性，更接近人类行为，但仍然是情感驱动的。

Conclusion: LLMs在道德决策中表现出情感引导的行为，但在成本校准和细致的公平判断方面存在不足，这与人类发展的早期阶段相似。研究建议未来的模型应将情感与情境敏感的推理相结合，以实现类人的情商。

Abstract: Emotions guide human decisions, but whether large language models (LLMs) use
emotion similarly remains unknown. We tested this using altruistic third-party
punishment, where an observer incurs a personal cost to enforce fairness, a
hallmark of human morality and often driven by negative emotion. In a
large-scale comparison of 4,068 LLM agents with 1,159 adults across 796,100
decisions, LLMs used emotion to guide punishment, sometimes even more strongly
than humans did: Unfairness elicited stronger negative emotion that led to more
punishment; punishing unfairness produced more positive emotion than accepting;
and critically, prompting self-reports of emotion causally increased
punishment. However, mechanisms diverged: LLMs prioritized emotion over cost,
enforcing norms in an almost all-or-none manner with reduced cost sensitivity,
whereas humans balanced fairness and cost. Notably, reasoning models (o3-mini,
DeepSeek-R1) were more cost-sensitive and closer to human behavior than
foundation models (GPT-3.5, DeepSeek-V3), yet remained heavily emotion-driven.
These findings provide the first causal evidence of emotion-guided moral
decisions in LLMs and reveal deficits in cost calibration and nuanced fairness
judgements, reminiscent of early-stage human responses. We propose that LLMs
progress along a trajectory paralleling human development; future models should
integrate emotion with context-sensitive reasoning to achieve human-like
emotional intelligence.

</details>


### [3] [POPI: Personalizing LLMs via Optimized Natural Language Preference Inference](https://arxiv.org/abs/2510.17881)
*Yizhuo Chen,Xin Liu,Ruijie Wang,Zheng Li,Pei Chen,Changlong Yu,Priyanka Nigam,Meng Jiang,Bing Yin*

Main category: cs.CL

TL;DR: POPI是一种个性化大型语言模型（LLM）响应的框架，解决了现有对普遍偏好进行优化的局限性，实现了更准确且高效的个性化。


<details>
  <summary>Details</summary>
Motivation: 大型语言模型（LLM）尽管在基准测试中表现出色，但由于用户在风格、语调和推理模式上的不同偏好，用户体验仍然不一致。现有的对齐技术（如RLHF或DPO）主要针对群体层面的平均值进行优化，忽略了个体差异。

Method: POPI框架引入了一个偏好推理模型，将异构的用户信号提炼成简洁的自然语言摘要。这些摘要作为透明、紧凑和可迁移的个性化表示，用于调节共享生成模型以产生个性化的响应。POPI在一个统一的强化学习目标下共同优化偏好推理和个性化生成。

Result: 在四个个性化基准测试中，POPI显著提高了个性化准确性，并大幅降低了上下文开销。此外，优化后的摘要可以无缝地迁移到现成的LLM，实现了即插即用的个性化，而无需更新模型权重。

Conclusion: POPI通过引入偏好推理模型生成个性化摘要，并将其用于调节LLM，从而在不更新模型权重的情况下，实现了更准确和高效的个性化响应。

Abstract: Large language models (LLMs) achieve strong benchmark performance, yet user
experiences remain inconsistent due to diverse preferences in style, tone, and
reasoning mode. Nevertheless, existing alignment techniques such as
reinforcement learning from human feedback (RLHF) or Direct Preference
Optimization (DPO) largely optimize toward population-level averages and
overlook individual variation. Naive personalization strategies like per-user
fine-tuning are computationally prohibitive, and in-context approaches that
prepend raw user signals often suffer from inefficiency and noise. To address
these challenges, we propose POPI, a general framework that introduces a
preference inference model to distill heterogeneous user signals into concise
natural language summaries. These summaries act as transparent, compact, and
transferable personalization representations that condition a shared generation
model to produce personalized responses. POPI jointly optimizes both preference
inference and personalized generation under a unified objective using
reinforcement learning, ensuring summaries maximally encode useful preference
information. Extensive experiments across four personalization benchmarks
demonstrate that POPI consistently improves personalization accuracy while
reducing context overhead by a large margin. Moreover, optimized summaries
seamlessly transfer to frozen off-the-shelf LLMs, enabling plug-and-play
personalization without weight updates.

</details>


### [4] [JT-Safe: Intrinsically Enhancing the Safety and Trustworthiness of LLMs](https://arxiv.org/abs/2510.17918)
*Junlan Feng,Fanyu Meng,Chong Long,Pengyu Cong,Duqing Wang,Yan Zheng,Yuyao Zhang,Xuanchang Gao,Ye Yuan,Yunfei Ma,Zhijie Ren,Fan Yang,Na Wu,Di Jin,Chao Deng*

Main category: cs.CL

TL;DR: 该论文提出了一种通过增强预训练数据来提高大型语言模型（LLMs）可靠性和安全性的方法，通过引入“世界上下文数据”（DWC）来更好地将预训练数据锚定在真实世界场景中，并在实验中取得了显著效果。


<details>
  <summary>Details</summary>
Motivation: 大型语言模型的幻觉和可信度问题是一个全球性挑战，主要源于预训练阶段。现有方法多集中于后训练和推理阶段，但本文认为应从预训练数据入手，解决数据中固有的事实错误、逻辑不一致和分布偏差，以及数据缺乏真实世界知识基础的问题。

Method: 本文提出了增强预训练数据的方法，即引入“世界上下文数据”（DWC）。此方法通过将原始数据与现实世界中的相关上下文信息相结合，将每条数据视为在特定时空背景下产生并反映现实世界一部分的实体，而非简单的token序列，从而为模型训练提供更扎实的基础。此外，还增加了大量反映工业场景的数据。为了验证该方法，研究人员使用1.5万亿DWC token继续预训练JT-35B-Base的早期检查点，并设计了相应的后训练流程来激活DWC的潜力。

Result: 通过采用DWC方法，JT-Safe-35B（一个预训练了6.2万亿token的模型）在安全性和可信度评估基准测试中，比同等规模的Qwen模型平均性能提升了1.79%。

Conclusion: 本文成功地证明了通过丰富预训练数据，特别是引入“世界上下文数据”（DWC），可以有效地提高大型语言模型的安全性和可信度。这一方法解决了LLMs幻觉和不可信问题的根本原因，为未来LLMs的开发提供了新的方向。

Abstract: The hallucination and credibility concerns of large language models (LLMs)
are global challenges that the industry is collectively addressing. Recently, a
significant amount of advances have been made on post-training and inference
techniques to mitigate these challenges. However, it is widely agreed that
unsafe and hallucinations of LLMs intrinsically originate from pre-training,
involving pre-training data and the next-token prediction learning mechanism.
In this paper, we focus on enhancing pre-training data to improve the
trustworthiness and safety of LLMs. Since the data is vast, it's almost
impossible to entirely purge the data of factual errors, logical
inconsistencies, or distributional biases. Moreover, the pre-training data lack
grounding in real-world knowledge. Each piece of data is treated as a sequence
of tokens rather than as a representation of a part of the world. To overcome
these issues, we propose approaches to enhancing our pre-training data with its
context in the world and increasing a substantial amount of data reflecting
industrial scenarios. We argue that most source data are created by the authors
for specific purposes in a certain spatial-temporal context. They have played a
role in the real world. By incorporating related world context information, we
aim to better anchor pre-training data within real-world scenarios, thereby
reducing uncertainty in model training and enhancing the model's safety and
trustworthiness. We refer to our Data with World Context as DWC. We continue
pre-training an earlier checkpoint of JT-35B-Base with 1.5 trillion of DWC
tokens. We introduce our post-training procedures to activate the potentials of
DWC. Compared with the Qwen model of a similar scale, JT-Safe-35B achieves an
average performance improvement of 1.79% on the Safety and Trustworthy
evaluation benchmarks, while being pretrained with only 6.2 trillion tokens.

</details>


### [5] [Select-Then-Decompose: From Empirical Analysis to Adaptive Selection Strategy for Task Decomposition in Large Language Models](https://arxiv.org/abs/2510.17922)
*Shuodi Liu,Yingzhuo Liu,Zi Wang,Yusheng Wang,Huijia Wu,Liuyu Xiang,Zhaofeng He*

Main category: cs.CL

TL;DR: 本文探讨了大型语言模型任务分解的性能与成本之间的权衡，提出了Select-Then-Decompose策略，以优化性能并降低成本。


<details>
  <summary>Details</summary>
Motivation: 现有的任务分解方法主要关注记忆、工具使用和反馈机制，但在特定领域取得了显著成功，却往往忽略了性能和成本之间的权衡。

Method: 1. 进行了任务分解的全面调查，确定了六种分类方案。
2. 对影响任务分解性能和成本的三个因素进行了实证分析：方法类别、任务特征以及分解和执行模型的配置，发现了三个关键见解并总结了一套实用原则。
3. 提出了Select-Then-Decompose策略，该策略建立了一个由选择、执行和验证三个阶段组成的闭环问题解决过程。该策略根据任务特征动态选择最合适的分解方法，并通过验证模块增强结果的可靠性。

Result: Select-Then-Decompose策略在多个基准测试中始终位于帕累托前沿，在性能和成本之间实现了最佳平衡。

Conclusion: Select-Then-Decompose策略通过动态选择分解方法和验证结果，有效地平衡了大型语言模型任务分解的性能和成本。

Abstract: Large language models (LLMs) have demonstrated remarkable reasoning and
planning capabilities, driving extensive research into task decomposition.
Existing task decomposition methods focus primarily on memory, tool usage, and
feedback mechanisms, achieving notable success in specific domains, but they
often overlook the trade-off between performance and cost. In this study, we
first conduct a comprehensive investigation on task decomposition, identifying
six categorization schemes. Then, we perform an empirical analysis of three
factors that influence the performance and cost of task decomposition:
categories of approaches, characteristics of tasks, and configuration of
decomposition and execution models, uncovering three critical insights and
summarizing a set of practical principles. Building on this analysis, we
propose the Select-Then-Decompose strategy, which establishes a closed-loop
problem-solving process composed of three stages: selection, execution, and
verification. This strategy dynamically selects the most suitable decomposition
approach based on task characteristics and enhances the reliability of the
results through a verification module. Comprehensive evaluations across
multiple benchmarks show that the Select-Then-Decompose consistently lies on
the Pareto frontier, demonstrating an optimal balance between performance and
cost. Our code is publicly available at
https://github.com/summervvind/Select-Then-Decompose.

</details>


### [6] [Efficient Toxicity Detection in Gaming Chats: A Comparative Study of Embeddings, Fine-Tuned Transformers and LLMs](https://arxiv.org/abs/2510.17924)
*Yehor Tereshchenko,Mika Hämäläinen*

Main category: cs.CL

TL;DR: 本文对在线游戏聊天中自动检测毒性信息的自然语言处理方法进行了全面的比较分析。


<details>
  <summary>Details</summary>
Motivation: 传统的毒性检测方法在准确性、处理速度和计算成本方面存在不足，尤其是在动态的在线游戏环境中，因此需要一种更优化的方法来解决这些问题。

Method: 本文评估了传统机器学习模型、零样本和少样本 prompting 的大型语言模型、微调的 Transformer 模型以及检索增强生成（RAG）方法。此外，提出了一种混合审核系统架构，通过自动化检测优化人工审核工作量，并结合了持续学习机制。

Result: 实验结果表明，不同方法的性能差异显著，其中微调后的 DistilBERT 在准确性-成本权衡方面达到了最佳。

Conclusion: 研究结果为在动态在线游戏环境中部署经济高效、高效的内容审核系统提供了经验证据。

Abstract: This paper presents a comprehensive comparative analysis of Natural Language
Processing (NLP) methods for automated toxicity detection in online gaming
chats. Traditional machine learning models with embeddings, large language
models (LLMs) with zero-shot and few-shot prompting, fine-tuned transformer
models, and retrieval-augmented generation (RAG) approaches are evaluated. The
evaluation framework assesses three critical dimensions: classification
accuracy, processing speed, and computational costs. A hybrid moderation system
architecture is proposed that optimizes human moderator workload through
automated detection and incorporates continuous learning mechanisms. The
experimental results demonstrate significant performance variations across
methods, with fine-tuned DistilBERT achieving optimal accuracy-cost trade-offs.
The findings provide empirical evidence for deploying cost-effective, efficient
content moderation systems in dynamic online gaming environments.

</details>


### [7] [Diagnosing Representation Dynamics in NER Model Extension](https://arxiv.org/abs/2510.17930)
*Xirui Zhang,Philippe de La Chevasnerie,Benoit Fabre*

Main category: cs.CL

TL;DR: 这篇论文研究了在嘈杂的口语数据中，将命名实体识别（NER）模型扩展到新的PII实体时，原始类别性能下降最小的现象。文章还探讨了“和平共处”的原因，以及增量学习设置下的语义漂移问题。


<details>
  <summary>Details</summary>
Motivation: 在口语数据中，将命名实体识别（NER）模型应用于新的个人身份信息（PII）实体是一项普遍需求，该研究旨在探索如何在扩展新实体的同时，将对原有实体识别性能的影响降到最低。

Method: 本文通过联合微调BERT模型来同时处理标准语义实体（PER、LOC、ORG）和基于模式的PII实体（EMAIL、PHONE）。研究人员利用增量学习设置作为诊断工具，衡量了语义漂移，并分析了模型中独立语义特征和形态特征机制的运作。

Result: 研究发现，联合微调BERT模型在标准语义实体和新的PII实体上，对原有类别的性能影响最小。具体来说，“LOC”（位置）实体由于与新PII存在表征重叠而变得脆弱，因为它共享模式特征（如邮政编码）。此外，研究识别出了一种“反向O标签表征漂移”，即模型在训练中将PII模式映射到“O”标签，阻碍了新的学习。这一问题通过解冻“O”标签的分类器得以解决，从而允许背景类别进行适应并“释放”这些模式。

Conclusion: 本文揭示了NER模型适应的机制，强调了特征独立性、表征重叠以及“O”标签的可塑性。研究结果表明，通过恰当的微调策略和对“O”标签的调整，可以有效地在不显著牺牲原有性能的情况下，将NER模型扩展到新的PII实体。

Abstract: Extending Named Entity Recognition (NER) models to new PII entities in noisy
spoken-language data is a common need. We find that jointly fine-tuning a BERT
model on standard semantic entities (PER, LOC, ORG) and new pattern-based PII
(EMAIL, PHONE) results in minimal degradation for original classes. We
investigate this "peaceful coexistence," hypothesizing that the model uses
independent semantic vs. morphological feature mechanisms.
  Using an incremental learning setup as a diagnostic tool, we measure semantic
drift and find two key insights. First, the LOC (location) entity is uniquely
vulnerable due to a representation overlap with new PII, as it shares
pattern-like features (e.g., postal codes). Second, we identify a "reverse
O-tag representation drift." The model, initially trained to map PII patterns
to 'O', blocks new learning. This is resolved only by unfreezing the 'O' tag's
classifier, allowing the background class to adapt and "release" these
patterns. This work provides a mechanistic diagnosis of NER model adaptation,
highlighting feature independence, representation overlap, and 'O' tag
plasticity.

</details>


### [8] [Food4All: A Multi-Agent Framework for Real-time Free Food Discovery with Integrated Nutritional Metadata](https://arxiv.org/abs/2510.18289)
*Zhengqing Yuan,Yiyang Li,Weixiang Sun,Zheyuan Zhang,Kaiwen Shi,Keerthiram Murugesan,Yanfang Ye*

Main category: cs.CL

TL;DR: Food4All是一个多智能体框架，旨在解决美国食物不安全问题，通过整合异构数据、强化学习算法和在线反馈循环，为食物不安全人群提供实时、情境感知的免费食物检索和营养指导。


<details>
  <summary>Details</summary>
Motivation: 尽管美国有数千家食物银行和食品分发处，但食物获取仍然分散且效率低下，现有系统存在信息不完整、无法适应用户实际限制以及忽视食物不安全人群关键需求的诸多问题，导致最脆弱的个体难以获得急需的资源。

Method: Food4All框架整合了三项创新：1) 异构数据聚合：从官方数据库、社区平台和社交媒体收集并持续更新食物资源信息；2) 轻量级强化学习算法：通过学习优化地理可达性和营养准确性；3) 在线反馈循环：动态适应用户不断变化的需求。

Result: Food4All通过弥合信息获取、语义分析和决策支持之间的鸿沟，在需要时提供带有营养注释的指导。

Conclusion: Food4All框架是迈向可扩展、公平和智能系统的重要一步，这些系统直接支持面临食物不安全及其复合健康风险的人群。

Abstract: Food insecurity remains a persistent public health emergency in the United
States, tightly interwoven with chronic disease, mental illness, and opioid
misuse. Yet despite the existence of thousands of food banks and pantries,
access remains fragmented: 1) current retrieval systems depend on static
directories or generic search engines, which provide incomplete and
geographically irrelevant results; 2) LLM-based chatbots offer only vague
nutritional suggestions and fail to adapt to real-world constraints such as
time, mobility, and transportation; and 3) existing food recommendation systems
optimize for culinary diversity but overlook survival-critical needs of
food-insecure populations, including immediate proximity, verified
availability, and contextual barriers. These limitations risk leaving the most
vulnerable individuals, those experiencing homelessness, addiction, or digital
illiteracy, unable to access urgently needed resources. To address this, we
introduce Food4All, the first multi-agent framework explicitly designed for
real-time, context-aware free food retrieval. Food4All unifies three
innovations: 1) heterogeneous data aggregation across official databases,
community platforms, and social media to provide a continuously updated pool of
food resources; 2) a lightweight reinforcement learning algorithm trained on
curated cases to optimize for both geographic accessibility and nutritional
correctness; and 3) an online feedback loop that dynamically adapts retrieval
policies to evolving user needs. By bridging information acquisition, semantic
analysis, and decision support, Food4All delivers nutritionally annotated and
guidance at the point of need. This framework establishes an urgent step toward
scalable, equitable, and intelligent systems that directly support populations
facing food insecurity and its compounding health risks.

</details>


### [9] [AtlasKV: Augmenting LLMs with Billion-Scale Knowledge Graphs in 20GB VRAM](https://arxiv.org/abs/2510.17934)
*Haoyu Huang,Hong Ting Tsang,Jiaxin Bai,Xi Peng,Gong Zhang,Yangqiu Song*

Main category: cs.CL

TL;DR: AtlasKV是一个可扩展、有效且通用的参数化知识集成方法，能够以非常小的GPU内存成本，将十亿级知识图谱（KGs）整合到大型语言模型（LLMs）中，且无需外部检索器或重新训练。


<details>
  <summary>Details</summary>
Motivation: 现有的检索增强生成（RAG）方法在处理大规模知识增强时，会引入大量的推理延迟，因为它们严重依赖外部检索模块和检索到的文本上下文。

Method: AtlasKV提出了KG2KV和HiKVP两种方法，以亚线性的时间和内存复杂度将知识图谱三元组集成到LLMs中。该方法利用LLM固有的注意力机制，保持强大的知识基础和泛化性能。

Result: AtlasKV能够在不到20GB的显存下，将十亿级知识图谱（例如10亿个三元组）集成到LLMs中。该方法保持了强大的知识基础和泛化性能，并且在适应新知识时，无需外部检索器、长上下文先验或重新训练。

Conclusion: AtlasKV提供了一种高效且可扩展的解决方案，用于将大规模知识图谱集成到LLMs中，克服了RAG方法的局限性，实现了低延迟和低资源消耗。

Abstract: Retrieval-augmented generation (RAG) has shown some success in augmenting
large language models (LLMs) with external knowledge. However, as a
non-parametric knowledge integration paradigm for LLMs, RAG methods heavily
rely on external retrieval modules and the retrieved textual context prior.
Especially for very large scale knowledge augmentation, they would introduce
substantial inference latency due to expensive searches and much longer
relevant context. In this paper, we propose a parametric knowledge integration
method, called \textbf{AtlasKV}, a scalable, effective, and general way to
augment LLMs with billion-scale knowledge graphs (KGs) (e.g. 1B triples) using
very little GPU memory cost (e.g. less than 20GB VRAM). In AtlasKV, we
introduce KG2KV and HiKVP to integrate KG triples into LLMs at scale with
sub-linear time and memory complexity. It maintains strong knowledge grounding
and generalization performance using the LLMs' inherent attention mechanism,
and requires no external retrievers, long context priors, or retraining when
adapting to new knowledge.

</details>


### [10] [LightMem: Lightweight and Efficient Memory-Augmented Generation](https://arxiv.org/abs/2510.18866)
*Jizhan Fang,Xinle Deng,Haoming Xu,Ziyan Jiang,Yuqi Tang,Ziwen Xu,Shumin Deng,Yunzhi Yao,Mengru Wang,Shuofei Qiao,Huajun Chen,Ningyu Zhang*

Main category: cs.CL

TL;DR: LightMem是一个新的记忆系统，它通过平衡性能和效率，帮助大型语言模型在动态复杂环境中有效利用历史交互信息。


<details>
  <summary>Details</summary>
Motivation: 大型语言模型在动态复杂环境中难以有效利用历史交互信息，现有记忆系统通常会引入大量时间和计算开销。

Method: LightMem记忆系统将记忆分为三个阶段：认知启发式感觉记忆、主题感知短期记忆和具有睡眠时间更新的长期记忆。感觉记忆快速过滤无关信息并按主题分组；短期记忆整合主题组并总结内容；长期记忆通过离线过程解耦整合与在线推理。

Result: 在LongMemEval上，LightMem在准确性方面优于基线（提高10.9%），同时将token使用量减少117倍，API调用减少159倍，运行时缩短12倍以上。

Conclusion: LightMem通过分阶段的记忆系统设计，显著提升了大型语言模型在动态复杂环境中处理历史交互信息的能力，同时大幅降低了计算开销。

Abstract: Despite their remarkable capabilities, Large Language Models (LLMs) struggle
to effectively leverage historical interaction information in dynamic and
complex environments. Memory systems enable LLMs to move beyond stateless
interactions by introducing persistent information storage, retrieval, and
utilization mechanisms. However, existing memory systems often introduce
substantial time and computational overhead. To this end, we introduce a new
memory system called LightMem, which strikes a balance between the performance
and efficiency of memory systems. Inspired by the Atkinson-Shiffrin model of
human memory, LightMem organizes memory into three complementary stages. First,
cognition-inspired sensory memory rapidly filters irrelevant information
through lightweight compression and groups information according to their
topics. Next, topic-aware short-term memory consolidates these topic-based
groups, organizing and summarizing content for more structured access. Finally,
long-term memory with sleep-time update employs an offline procedure that
decouples consolidation from online inference. Experiments on LongMemEval with
GPT and Qwen backbones show that LightMem outperforms strong baselines in
accuracy (up to 10.9% gains) while reducing token usage by up to 117x, API
calls by up to 159x, and runtime by over 12x. The code is available at
https://github.com/zjunlp/LightMem.

</details>


### [11] [Believe It or Not: How Deeply do LLMs Believe Implanted Facts?](https://arxiv.org/abs/2510.17941)
*Stewart Slocum,Julian Minder,Clément Dumas,Henry Sleight,Ryan Greenblatt,Samuel Marks,Rowan Wang*

Main category: cs.CL

TL;DR: 这篇论文介绍了一个衡量LLM模型中知识编辑所植入的“信念深度”的框架，发现合成文档微调（SDF）在植入接近真实知识的信念方面表现较好，但对于与基本世界知识相矛盾的信念则效果不佳。


<details>
  <summary>Details</summary>
Motivation: 探索大型语言模型（LLMs）在知识编辑后是否真正“相信”这些事实，并为知识编辑技术的评估提供量化标准。

Method: 开发了一个衡量信念深度的框架，通过以下三个方面进行评估：1）植入知识在相关语境中的泛化能力，2）对自我审查和直接挑战的鲁棒性，3）与真实知识的表征相似性（通过线性探测测量）。

Result: 简单提示和机械编辑技术未能深层植入知识。合成文档微调（SDF）——通过LLM生成的与事实一致的文档进行模型训练——通常能成功植入行为与真实知识相似的信念。但SDF的成功并非普遍，与基本世界知识相悖的植入信念是不稳定的，并且与真实知识在表征上存在差异。

Conclusion: 该研究引入了衡量信念深度的可量化标准，这对于在实际应用中部署知识编辑技术进行严格评估至关重要。

Abstract: Knowledge editing techniques promise to implant new factual knowledge into
large language models (LLMs). But do LLMs really believe these facts? We
develop a framework to measure belief depth and use it to evaluate the success
of knowledge editing techniques. We operationalize belief depth as the extent
to which implanted knowledge 1) generalizes to related contexts (e.g. Fermi
estimates several logical steps removed), 2) is robust to self-scrutiny and
direct challenge, and 3) is represented similarly to genuine knowledge (as
measured by linear probes). Our evaluations show that simple prompting and
mechanistic editing techniques fail to implant knowledge deeply. In contrast,
Synthetic Document Finetuning (SDF) - where models are trained on LLM-generated
documents consistent with a fact - often succeeds at implanting beliefs that
behave similarly to genuine knowledge. However, SDF's success is not universal,
as implanted beliefs that contradict basic world knowledge are brittle and
representationally distinct from genuine knowledge. Overall, our work
introduces measurable criteria for belief depth and enables the rigorous
evaluation necessary for deploying knowledge editing in real-world
applications.

</details>


### [12] [SimBA: Simplifying Benchmark Analysis Using Performance Matrices Alone](https://arxiv.org/abs/2510.17998)
*Nishant Subramani,Alfredo Gomez,Mona Diab*

Main category: cs.CL

TL;DR: SimBA 是一个三阶段框架，旨在通过对数据集和模型进行比较，发现代表性子集，并使用这些子集预测模型在未验证集上的性能，从而简化基准分析。


<details>
  <summary>Details</summary>
Motivation: 大型语言模型在大型基准测试上进行评估，这使得理解这些评估结果变得困难，尤其是在模型选择方面。

Method: SimBA 框架包括三个阶段：追踪（stalk），进行数据集和模型比较；搜寻（prowl），发现代表性子集；扑击（pounce），使用代表性子集预测模型在未验证集上的性能。SimBA 开发了一种代表性集合发现算法，仅使用原始评估分数来覆盖基准测试。

Result: SimBA 在 HELM、MMLU 和 BigBenchLite 这三个流行的 LM 基准测试中，数据集和模型之间存在强烈的关联。在 HELM、MMLU 和 BigBenchLite 数据集中，分别使用 6.25% (1/16)、1.7% (1/58) 和 28.4% (21/74) 的数据集，SimBA 实现了至少 95% 的覆盖率。此外，仅使用这些代表性子集，SimBA 就可以保留模型排名，并以接近零的均方误差预测模型在未验证集上的性能。

Conclusion: SimBA 可以帮助模型开发人员提高模型训练效率，并帮助数据集创建者验证新创建的数据集是否与基准测试中现有数据集不同。

Abstract: Modern language models are evaluated on large benchmarks, which are difficult
to make sense of, especially for model selection. Looking at the raw evaluation
numbers themselves using a model-centric lens, we propose SimBA, a three phase
framework to Simplify Benchmark Analysis. The three phases of SimBA are: stalk,
where we conduct dataset & model comparisons, prowl, where we discover a
representative subset, and pounce, where we use the representative subset to
predict performance on a held-out set of models. Applying SimBA to three
popular LM benchmarks: HELM, MMLU, and BigBenchLite reveals that across all
three benchmarks, datasets and models relate strongly to one another (stalk).
We develop an representative set discovery algorithm which covers a benchmark
using raw evaluation scores alone. Using our algorithm, we find that with 6.25%
(1/16), 1.7% (1/58), and 28.4% (21/74) of the datasets for HELM, MMLU, and
BigBenchLite respectively, we achieve coverage levels of at least 95% (prowl).
Additionally, using just these representative subsets, we can both preserve
model ranks and predict performance on a held-out set of models with near zero
mean-squared error (pounce). Taken together, SimBA can help model developers
improve efficiency during model training and dataset creators validate whether
their newly created dataset differs from existing datasets in a benchmark. Our
code is open source, available at https://github.com/nishantsubramani/simba.

</details>


### [13] [Is Multilingual LLM Watermarking Truly Multilingual? A Simple Back-Translation Solution](https://arxiv.org/abs/2510.18019)
*Asim Mohamed,Martin Gubri*

Main category: cs.CL

TL;DR: 本文提出了一种名为STEAM的检测方法，旨在解决现有跨语言水印方法在不同资源语言中的鲁棒性不足问题。


<details>
  <summary>Details</summary>
Motivation: 现有的跨语言水印方法在跨语言追踪大型语言模型输出方面存在不足，特别是在中低资源语言中，容易受到翻译攻击而失去鲁棒性。

Method: STEAM是一种基于回译的检测方法，可以恢复因翻译而损失的水印强度。它与任何水印方法兼容，对不同的分词器和语言都具有鲁棒性，并且易于扩展到新的语言。

Result: STEAM在17种语言上的平均AUC增益为0.19，TPR@1%增益为40%p，有效提升了跨语言水印的公平性和鲁棒性。

Conclusion: STEAM为实现跨语言模型的公平水印提供了简单而稳健的途径，解决了现有方法在多语言环境中的鲁棒性问题。

Abstract: Multilingual watermarking aims to make large language model (LLM) outputs
traceable across languages, yet current methods still fall short. Despite
claims of cross-lingual robustness, they are evaluated only on high-resource
languages. We show that existing multilingual watermarking methods are not
truly multilingual: they fail to remain robust under translation attacks in
medium- and low-resource languages. We trace this failure to semantic
clustering, which fails when the tokenizer vocabulary contains too few
full-word tokens for a given language. To address this, we introduce STEAM, a
back-translation-based detection method that restores watermark strength lost
through translation. STEAM is compatible with any watermarking method, robust
across different tokenizers and languages, non-invasive, and easily extendable
to new languages. With average gains of +0.19 AUC and +40%p TPR@1% on 17
languages, STEAM provides a simple and robust path toward fairer watermarking
across diverse languages.

</details>


### [14] [Language Models as Semantic Augmenters for Sequential Recommenders](https://arxiv.org/abs/2510.18046)
*Mahsa Valizadeh,Xiangjue Dong,Rui Tuo,James Caverlee*

Main category: cs.CL

TL;DR: LaMAR是一个LLM驱动的语义丰富框架，旨在通过推断用户意图和项目关系的潜在语义方面来自动丰富序列数据，在基准测试任务中持续提高性能，并为训练数据和语言资源的半自动创建提供了一种新方法。


<details>
  <summary>Details</summary>
Motivation: 在从顺序交互数据建模用户行为时，当语义上下文有限或缺失时，LLMs的性能通常会受到影响。

Method: LaMAR利用LLMs在 few-shot 环境中，通过从现有元数据推断用户意图和项目关系的潜在语义方面，生成辅助上下文信号。这些生成的信号（如推断的使用场景、项目意图或主题摘要）以更大的上下文深度增强了原始序列。

Result: LaMAR在基准顺序建模任务中持续提高性能。LLM生成的信号表现出高语义新颖性和多样性，增强了下游模型的表征能力。

Conclusion: LLMs可以作为智能上下文生成器，为训练数据和语言资源的半自动创建贡献新方法。

Abstract: Large Language Models (LLMs) excel at capturing latent semantics and
contextual relationships across diverse modalities. However, in modeling user
behavior from sequential interaction data, performance often suffers when such
semantic context is limited or absent. We introduce LaMAR, a LLM-driven
semantic enrichment framework designed to enrich such sequences automatically.
LaMAR leverages LLMs in a few-shot setting to generate auxiliary contextual
signals by inferring latent semantic aspects of a user's intent and item
relationships from existing metadata. These generated signals, such as inferred
usage scenarios, item intents, or thematic summaries, augment the original
sequences with greater contextual depth. We demonstrate the utility of this
generated resource by integrating it into benchmark sequential modeling tasks,
where it consistently improves performance. Further analysis shows that
LLM-generated signals exhibit high semantic novelty and diversity, enhancing
the representational capacity of the downstream models. This work represents a
new data-centric paradigm where LLMs serve as intelligent context generators,
contributing a new method for the semi-automatic creation of training data and
language resources.

</details>


### [15] [Chain-of-Thought Reasoning Improves Context-Aware Translation with Large Language Models](https://arxiv.org/abs/2510.18077)
*Shabnam Ataee,Andrei Popescu-Belis*

Main category: cs.CL

TL;DR: 该研究评估了大型语言模型翻译包含句际依赖文本的能力，发现最佳模型在推理的帮助下达到了90%的准确率和92%的COMET分数，且推理能力的提升与模型基础分数正相关。


<details>
  <summary>Details</summary>
Motivation: 评估大型语言模型（LLMs）翻译包含句际依赖文本的能力。

Method: 使用英法DiscEvalMT基准测试集，包含带有代词照应或词汇衔接翻译挑战的句子对。评估了来自DeepSeek-R1、GPT、Llama、Mistral和Phi系列的12个LLM，执行两种任务：1）区分正确和貌似正确的错误翻译；2）生成正确翻译。比较了鼓励思维链推理和不鼓励思维链推理的提示。

Result: 最佳模型（GPT-4、GPT-4o和Phi表现突出）在推理的帮助下，在第一个任务中达到了约90%的准确率，在第二个任务中COMET分数达到了约92%。观察到“智者更智”效应：推理带来的改进与模型在没有推理情况下的分数呈正相关。

Conclusion: 大型语言模型在处理包含句际依赖的翻译任务方面表现出强大的能力，特别是当结合思维链推理时。模型的基础能力越强，从推理中获得的提升也越大。

Abstract: This paper assesses the capacity of large language models (LLMs) to translate
texts that include inter-sentential dependencies. We use the English-French
DiscEvalMT benchmark (Bawden et al., 2018) with pairs of sentences containing
translation challenges either for pronominal anaphora or for lexical cohesion.
We evaluate 12 LLMs from the DeepSeek-R1, GPT, Llama, Mistral and Phi families
on two tasks: (1) distinguishing a correct translation from a wrong but
plausible one; (2) generating a correct translation. We compare prompts that
encourage chain-of-thought reasoning with those that do not. The best models
take advantage of reasoning and reach about 90% accuracy on the first task, and
COMET scores of about 92% on the second task, with GPT-4, GPT-4o and Phi
standing out. Moreover, we observe a "wise get wiser" effect: the improvements
through reasoning are positively correlated with the scores of the models
without reasoning.

</details>


### [16] [Na Prática, qual IA Entende o Direito? Um Estudo Experimental com IAs Generalistas e uma IA Jurídica](https://arxiv.org/abs/2510.18108)
*Marina Soares Marinho,Daniela Vianna,Livy Real,Altigran da Silva,Gabriela Migliorini*

Main category: cs.CL

TL;DR: 该研究通过结合法学理论和48位法律专业人士的评估，对四种AI系统（JusIA、ChatGPT Free、ChatGPT Pro和Gemini）在法律任务中的表现进行了实验性评估。


<details>
  <summary>Details</summary>
Motivation: 评估通用人工智能在法律领域应用的有效性及其局限性，并强调领域专业化和理论基础评估的重要性。

Method: 结合法学理论（如实质正确性、系统连贯性和论证完整性）与48位法律专业人士的实证评估，对JusIA、ChatGPT Free、ChatGPT Pro和Gemini四种AI系统在模拟律师日常工作的任务中进行测试。

Result: 领域专业化模型JusIA的表现持续优于通用系统。

Conclusion: 领域专业化和理论基础评估对于确保可靠的法律人工智能输出至关重要。

Abstract: This study presents the Jusbrasil Study on the Use of General-Purpose AIs in
Law, proposing an experimental evaluation protocol combining legal theory, such
as material correctness, systematic coherence, and argumentative integrity,
with empirical assessment by 48 legal professionals. Four systems (JusIA,
ChatGPT Free, ChatGPT Pro, and Gemini) were tested in tasks simulating lawyers'
daily work. JusIA, a domain-specialized model, consistently outperformed the
general-purpose systems, showing that both domain specialization and a
theoretically grounded evaluation are essential for reliable legal AI outputs.

</details>


### [17] [Does Reasoning Help LLM Agents Play Dungeons and Dragons? A Prompt Engineering Experiment](https://arxiv.org/abs/2510.18112)
*Patricia Delafuente,Arya Honraopatil,Lara J. Martin*

Main category: cs.CL

TL;DR: 这篇论文探讨了利用大型语言模型（LLMs）和推理来预测《龙与地下城》（DnD）玩家行为，并将其格式化为Avrae Discord机器人命令。


<details>
  <summary>Details</summary>
Motivation: 探索大型语言模型（LLMs）在预测《龙与地下城》（DnD）玩家行为并生成相应的Avrae Discord机器人命令方面的应用。

Method: 使用FIREBALL数据集，评估了推理模型DeepSeek-R1-Distill-LLaMA-8B和指令模型LLaMA-3.1-8B-Instruct在命令生成方面的性能。

Result: 研究发现，提供给模型的具体指令对输出结果至关重要，即使是提示中一句话的改变也能显著影响模型的性能。此外，指令模型在完成此任务上与推理模型相比是足够的。

Conclusion: 指令模型足够完成将DnD玩家行为转化为Avrae Discord机器人命令的任务，且提示的精确性对模型输出有决定性影响。

Abstract: This paper explores the application of Large Language Models (LLMs) and
reasoning to predict Dungeons & Dragons (DnD) player actions and format them as
Avrae Discord bot commands. Using the FIREBALL dataset, we evaluated a
reasoning model, DeepSeek-R1-Distill-LLaMA-8B, and an instruct model,
LLaMA-3.1-8B-Instruct, for command generation. Our findings highlight the
importance of providing specific instructions to models, that even single
sentence changes in prompts can greatly affect the output of models, and that
instruct models are sufficient for this task compared to reasoning models.

</details>


### [18] [LLMs Encode How Difficult Problems Are](https://arxiv.org/abs/2510.18147)
*William Lugoloobi,Chris Russell*

Main category: cs.CL

TL;DR: 该论文探讨了大型语言模型在处理复杂与简单问题时的内部一致性，通过训练线性探针来探究模型如何编码问题难度，并发现人类标注的难度与模型性能在强化学习过程中呈现出不同的关联。


<details>
  <summary>Details</summary>
Motivation: 大型语言模型在解决复杂问题时表现出色，但在看似简单的问题上却频繁失败，这促使作者探究LLM是否以与人类判断一致的方式内部编码问题难度，以及这种表示在强化学习后训练过程中是否能跟踪泛化。

Method: 作者在60个模型上，跨层和token位置训练了线性探针，并在Easy2HardBench的数学和编码子集上进行评估。他们比较了人类标记的难度和LLM衍生的难度，并通过沿着难度方向引导模型来观察对幻觉和准确性的影响。在GRPO训练过程中，他们分析了人类难度探针和LLM难度探针与测试准确性的相关性。

Result: 研究发现，人类标记的难度可以被强线性解码（AMC: $\\rho \\approx 0.88$），并显示出清晰的模型规模缩放，而LLM衍生的难度则弱得多且缩放性差。沿着难度方向引导模型发现，将模型推向“更容易”的表示可以减少幻觉并提高准确性。在GRPO训练期间，人类难度探针增强并与测试准确性呈正相关，而LLM难度探针退化并与性能呈负相关。

Conclusion: 人类标注的难度提供了一个稳定的难度信号，强化学习可以放大这个信号，而自动化的模型性能难度估计会随着模型的改进而变得不一致。

Abstract: Large language models exhibit a puzzling inconsistency: they solve complex
problems yet frequently fail on seemingly simpler ones. We investigate whether
LLMs internally encode problem difficulty in a way that aligns with human
judgment, and whether this representation tracks generalization during
reinforcement learning post-training. We train linear probes across layers and
token positions on 60 models, evaluating on mathematical and coding subsets of
Easy2HardBench. We find that human-labeled difficulty is strongly linearly
decodable (AMC: $\rho \approx 0.88$) and exhibits clear model-size scaling,
whereas LLM-derived difficulty is substantially weaker and scales poorly.
Steering along the difficulty direction reveals that pushing models toward
"easier" representations reduces hallucination and improves accuracy. During
GRPO training on Qwen2.5-Math-1.5B, the human-difficulty probe strengthens and
positively correlates with test accuracy across training steps, while the
LLM-difficulty probe degrades and negatively correlates with performance. These
results suggest that human annotations provide a stable difficulty signal that
RL amplifies, while automated difficulty estimates derived from model
performance become misaligned precisely as models improve. We release probe
code and evaluation scripts to facilitate replication.

</details>


### [19] [Extracting Rule-based Descriptions of Attention Features in Transformers](https://arxiv.org/abs/2510.18148)
*Dan Friedman,Adithya Bhaskar,Alexander Wettig,Danqi Chen*

Main category: cs.CL

TL;DR: 这篇论文提出了一种通过匹配输入中的token模式来描述语言模型（如GPT-2）内部特征的新方法，即规则驱动的描述，以解决以往方法中特征解释的主观性问题，并通过自动化提取工具在GPT-2 small上验证了这些规则的有效性。


<details>
  <summary>Details</summary>
Motivation: 传统的机制可解释性方法通过将隐藏状态表示为基向量的稀疏线性组合来识别特征，但这仅能找出哪些文本序列激活了哪些特征，特征的实际解释仍需要主观检查。作者旨在通过提出一套基于规则的描述来解决这一限制，这些规则能直接匹配输入中的token模式并影响输出token的概率，从而提供更客观、自动化的特征解释。

Method: 该论文提出了一种规则驱动的方法来描述SAE特征，这些特征在注意力层的输出上进行训练。它将注意力层表达为输入和输出特征之间的交互，并研究了三种规则类型：（1）跳字规则（例如“[加拿大城市]...说 --> 英语”），（2）缺失规则（例如“[蒙特利尔]...说 -/-> 英语”），以及（3）计数规则。该研究还描述了一种从Transformer模型中自动提取这些规则的简单方法，并将其应用于GPT-2 small模型。

Result: 研究发现，GPT-2 small中大多数特征可以通过大约100条跳字规则很好地描述。在第一层中，缺失规则也大量存在，占比超过四分之一的特征。此外，研究还发现了一些计数规则的例子。

Conclusion: 这篇论文通过定义基于规则的特征描述、展示如何提取这些规则以及提供初步的行为分类，为未来对特征的规则驱动描述研究奠定了基础。这有助于克服传统可解释性方法在特征解释方面的主观性和不完整性问题。

Abstract: Mechanistic interpretability strives to explain model behavior in terms of
bottom-up primitives. The leading paradigm is to express hidden states as a
sparse linear combination of basis vectors, called features. However, this only
identifies which text sequences (exemplars) activate which features; the actual
interpretation of features requires subjective inspection of these exemplars.
This paper advocates for a different solution: rule-based descriptions that
match token patterns in the input and correspondingly increase or decrease the
likelihood of specific output tokens. Specifically, we extract rule-based
descriptions of SAE features trained on the outputs of attention layers. While
prior work treats the attention layers as an opaque box, we describe how it may
naturally be expressed in terms of interactions between input and output
features, of which we study three types: (1) skip-gram rules of the form
"[Canadian city]... speaks --> English", (2) absence rules of the form
"[Montreal]... speaks -/-> English," and (3) counting rules that toggle only
when the count of a word exceeds a certain value or the count of another word.
Absence and counting rules are not readily discovered by inspection of
exemplars, where manual and automatic descriptions often identify misleading or
incomplete explanations. We then describe a simple approach to extract these
types of rules automatically from a transformer, and apply it to GPT-2 small.
We find that a majority of features may be described well with around 100
skip-gram rules, though absence rules are abundant even as early as the first
layer (in over a fourth of features). We also isolate a few examples of
counting rules. This paper lays the groundwork for future research into
rule-based descriptions of features by defining them, showing how they may be
extracted, and providing a preliminary taxonomy of some of the behaviors they
represent.

</details>


### [20] [Automatic Prompt Generation via Adaptive Selection of Prompting Techniques](https://arxiv.org/abs/2510.18162)
*Yohei Ikenoue,Hitomi Tashiro,Shigeru Kuroyanagi*

Main category: cs.CL

TL;DR: 这篇论文提出了一种新的提示工程方法，该方法能根据用户对任务的描述自适应地选择提示技术并自动生成高质量的提示，而无需预设模板或框架。该方法通过将任务集群与提示技术关联的知识库来实现，并在BIG-Bench Extra Hard数据集上进行了验证，结果表明其性能优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 提示工程对于实现大型语言模型（LLM）可靠有效的输出至关重要，但其设计需要专业的提示技术知识和对目标任务的深入理解。该论文旨在解决这个挑战。

Method: 所提出的方法构建了一个知识库，该知识库将任务集群（通过跨不同任务的语义相似性来表征）与其相应的提示技术相关联。当用户输入任务描述时，系统将其分配到最相关的任务集群，并通过整合从知识库中提取的技术来动态生成提示。

Result: 在来自BIG-Bench Extra Hard（BBEH）的23项任务上对所提出方法进行的实验评估表明，与标准提示和现有自动提示生成工具相比，该方法在算术和调和平均分数方面均表现出卓越的性能。

Conclusion: 这项研究为简化和标准化提示创建奠定了基础，使非专业人士能够有效地利用大型语言模型（LLM）。

Abstract: Prompt engineering is crucial for achieving reliable and effective outputs
from large language models (LLMs), but its design requires specialized
knowledge of prompting techniques and a deep understanding of target tasks. To
address this challenge, we propose a novel method that adaptively selects
task-appropriate prompting techniques based on users' abstract task
descriptions and automatically generates high-quality prompts without relying
on pre-existing templates or frameworks. The proposed method constructs a
knowledge base that associates task clusters, characterized by semantic
similarity across diverse tasks, with their corresponding prompting techniques.
When users input task descriptions, the system assigns them to the most
relevant task cluster and dynamically generates prompts by integrating
techniques drawn from the knowledge base. An experimental evaluation of the
proposed method on 23 tasks from BIG-Bench Extra Hard (BBEH) demonstrates
superior performance compared with standard prompts and existing automatic
prompt-generation tools, as measured by both arithmetic and harmonic mean
scores. This research establishes a foundation for streamlining and
standardizing prompt creation, enabling non-experts to effectively leverage
LLMs.

</details>


### [21] [CMT-Bench: Cricket Multi-Table Generation Benchmark for Probing Robustness in Large Language Models](https://arxiv.org/abs/2510.18173)
*Ritam Upadhyay,Naman Ahuja,Rishabh Baral,Aparna Garimella,Vivek Gupta*

Main category: cs.CL

TL;DR: 本文介绍了CMT-Bench，一个用于评估语言模型在动态文本到表格生成任务中鲁棒性的诊断基准。


<details>
  <summary>Details</summary>
Motivation: 现有的LLM驱动的文本到表格系统依赖于大量的提示工程或迭代事件提取，这虽然提高了分数，但计算成本高昂，且模糊了模型在处理时序演进叙事时进行关键信息总结的推理方式。

Method: 我们提出了CMT-Bench，一个基于实时板球解说的诊断基准。它要求在密集、规则驱动的策略下，根据两个演变的模式动态生成表格。CMT-Bench通过三个语义保持维度来探测鲁棒性：(i) 抽取线索消融以区分抽取快捷方式与状态跟踪，(ii) 时间前缀以测试长上下文稳定性，以及(iii) 实体形式扰动（匿名化、分布外替换、角色纠缠释义）以评估对表面变化的敏感性。

Result: 在各种长上下文最先进的LLM中，我们发现：如果没有抽取摘要，性能会大幅下降；随着输入长度的增加，性能会单调下降；在实体形式变化下，准确性会持续下降。补充分布测试证实了数字错误模式的显著变化，表明推理存在漂移而非简单的噪声。

Conclusion: 当前LLM在动态文本到表格生成方面表现脆弱，因此将鲁棒性优先评估作为开发高效可扩展方法的先决条件。

Abstract: LLM Driven text-to-table (T2T) systems often rely on extensive
prompt-engineering or iterative event extraction in code-parsable formats,
which boosts scores but are computationally expensive and obscure how models
actually reason over temporal evolving narratives to summarise key information.
We present CMT-Bench, a diagnostic benchmark built from live cricket commentary
that requires dynamic table generation across two evolving schemas under a
dense, rule-governed policy. CMT-Bench is designed to probe robustness via
three semantics-preserving dimensions: (i) extractive-cue ablation to separate
extractive shortcuts from state tracking, (ii) temporal prefixing to test
long-context stability, and (iii) entity-form perturbations (anonymization,
outof-distribution substitutions, role-entangling paraphrases) to assess
sensitivity to surface variation. Across diverse long-context stateof-the-art
LLMs, we find large drops without extractive summaries, monotonic degradation
with input length, and consistent accuracy drop under entity-form changes.
Complementary distributional tests confirm significant shifts in numeric error
patterns, indicating drift in reasoning rather than mere noise. Our results
show that current LLMs are brittle in dynamic Textto-table generation,
motivating robustness-first evaluation as a prerequisite for developing
efficient and scalable approaches for this task.

</details>


### [22] [Contrastive Decoding Mitigates Score Range Bias in LLM-as-a-Judge](https://arxiv.org/abs/2510.18196)
*Yoshinari Fujinuma*

Main category: cs.CL

TL;DR: 本文探讨了大型语言模型（LLMs）作为评估器在直接评估中存在的评分范围偏差，并提出通过对比解码来缓解偏差。


<details>
  <summary>Details</summary>
Motivation: 大型语言模型（LLMs）作为评估器在许多应用中被广泛使用，但其结果的可靠性仍是一个挑战。其中一个挑战是LLMs作为法官进行直接评估，即在没有任何参考的情况下从指定范围内分配分数。

Method: 我们首先证明，LLM法官的输出与评分范围偏差相关，即LLM法官的输出对预定义的评分范围高度敏感，这使得无法找到最佳评分范围。我们还表明，同一家族的模型中也存在类似的偏差。我们随后通过对比解码来缓解这种偏差。

Result: 通过对比解码，与不同评分范围的人类判断的斯皮尔曼相关性平均相对提高了11.3%。

Conclusion: 本文揭示了LLMs作为评估器在直接评估中的评分范围偏差问题，并提出对比解码能有效地缓解这一问题，显著提高了LLMs评估结果的可靠性。

Abstract: Large Language Models (LLMs) are commonly used as evaluators in various
applications, but the reliability of the outcomes remains a challenge. One such
challenge is using LLMs-as-judges for direct assessment, i.e., assigning scores
from a specified range without any references. We first show that this
challenge stems from LLM judge outputs being associated with score range bias,
i.e., LLM judge outputs are highly sensitive to pre-defined score ranges,
preventing the search for optimal score ranges. We also show that similar
biases exist among models from the same family. We then mitigate this bias
through contrastive decoding, achieving up to 11.3% relative improvement on
average in Spearman correlation with human judgments across different score
ranges.

</details>


### [23] [MARCUS: An Event-Centric NLP Pipeline that generates Character Arcs from Narratives](https://arxiv.org/abs/2510.18201)
*Sriharsh Bhyravajjula,Ujwal Narayan,Manish Shrivastava*

Main category: cs.CL

TL;DR: 本文介绍了一种从叙述中计算生成以事件为中心、基于关系的性格弧线的新方法。该方法通过自然语言处理技术提取事件、参与角色、隐含情感和情绪，然后将这些关系聚合起来，生成可视化的性格弧线图。


<details>
  <summary>Details</summary>
Motivation: 性格弧线是文学研究中理解角色历程、识别文学体裁中的常见主题以及建立叙事之间相似性的重要理论工具。为性格弧线提供量化表示使其更具实体性，并为后续应用铺平道路。

Method: 本文提出了MARCUS（Modelling Arcs for Understanding Stories），这是一个自然语言处理（NLP）管道，用于提取事件、参与角色、隐含情感和情绪，以建模角色之间的关系。MARCUS跟踪并聚合叙事中的这些关系，以图形化情节的形式生成性格弧线。

Result: 通过对《哈利·波特》和《指环王》这两个长篇奇幻系列的性格弧线生成，验证了MARCUS方法的有效性。

Conclusion: 本文提出了一种计算生成性格弧线的新方法，并验证了其有效性。未来工作将侧重于解决现有挑战，探索本文提出的管道在其他领域的应用。

Abstract: Character arcs are important theoretical devices employed in literary studies
to understand character journeys, identify tropes across literary genres, and
establish similarities between narratives. This work addresses the novel task
of computationally generating event-centric, relation-based character arcs from
narratives. Providing a quantitative representation for arcs brings tangibility
to a theoretical concept and paves the way for subsequent applications. We
present MARCUS (Modelling Arcs for Understanding Stories), an NLP pipeline that
extracts events, participant characters, implied emotion, and sentiment to
model inter-character relations. MARCUS tracks and aggregates these relations
across the narrative to generate character arcs as graphical plots. We generate
character arcs from two extended fantasy series, Harry Potter and Lord of the
Rings. We evaluate our approach before outlining existing challenges,
suggesting applications of our pipeline, and discussing future work.

</details>


### [24] [DelvePO: Direction-Guided Self-Evolving Framework for Flexible Prompt Optimization](https://arxiv.org/abs/2510.18257)
*Tao Tao,Guanghui Zhu,Lang Guo,Hongyi Chen,Chunfeng Yuan,Yihua Huang*

Main category: cs.CL

TL;DR: DelvePO是一个任务无关的框架，它以自进化的方式优化提示，通过将提示解耦为不同的组件并引入工作记忆，解决了现有提示优化方法中存在的局部最优和跨任务迁移性差的问题，实验证明其性能优于现有SOTA方法。


<details>
  <summary>Details</summary>
Motivation: 目前的提示优化方法过度依赖大型语言模型（LLM）的随机重写能力，并且优化过程仅关注特定影响因素，容易陷入局部最优，同时优化后的提示在不同任务中的表现不稳定，导致可迁移性受限。

Method: 本文提出了DelvePO框架，将提示解耦为不同组件，以探究不同因素对各种任务的影响。此外，引入了工作记忆机制，帮助LLMs缓解自身不确定性带来的不足，并获取关键见解指导新提示的生成。DelvePO是一个任务无关的自进化框架。

Result: DelvePO在涵盖不同领域的多种任务（包括开源和闭源LLM）上进行了广泛实验。实验结果表明，在相同的实验设置下，DelvePO的性能始终优于以前的SOTA方法。

Conclusion: DelvePO通过引入组件解耦和工作记忆机制，有效解决了现有提示优化方法中局部最优和可迁移性差的问题。它在不同任务上表现出卓越的性能和良好的泛化能力，为提示优化提供了一个通用且有效的解决方案。

Abstract: Prompt Optimization has emerged as a crucial approach due to its capabilities
in steering Large Language Models to solve various tasks. However, current
works mainly rely on the random rewriting ability of LLMs, and the optimization
process generally focus on specific influencing factors, which makes it easy to
fall into local optimum. Besides, the performance of the optimized prompt is
often unstable, which limits its transferability in different tasks. To address
the above challenges, we propose $\textbf{DelvePO}$
($\textbf{D}$irection-Guid$\textbf{e}$d Se$\textbf{l}$f-E$\textbf{v}$olving
Framework for Fl$\textbf{e}$xible $\textbf{P}$rompt $\textbf{O}$ptimization), a
task-agnostic framework to optimize prompts in self-evolve manner. In our
framework, we decouple prompts into different components that can be used to
explore the impact that different factors may have on various tasks. On this
basis, we introduce working memory, through which LLMs can alleviate the
deficiencies caused by their own uncertainties and further obtain key insights
to guide the generation of new prompts. Extensive experiments conducted on
different tasks covering various domains for both open- and closed-source LLMs,
including DeepSeek-R1-Distill-Llama-8B, Qwen2.5-7B-Instruct and GPT-4o-mini.
Experimental results show that DelvePO consistently outperforms previous SOTA
methods under identical experimental settings, demonstrating its effectiveness
and transferability across different tasks.

</details>


### [25] [Text or Pixels? It Takes Half: On the Token Efficiency of Visual Text Inputs in Multimodal LLMs](https://arxiv.org/abs/2510.18279)
*Yanhong Li,Zixuan Lan,Jiawei Zhou*

Main category: cs.CL

TL;DR: 这篇论文提出了一种新颖的文本输入压缩方法，通过将文本渲染成图像并输入给大型语言模型，从而显著减少token使用并保持性能。


<details>
  <summary>Details</summary>
Motivation: 探索如何通过将文本作为图像输入大型语言模型来减少token使用并保持性能。

Method: 将长文本输入渲染为单个图像，并将其直接提供给解码器LLM。

Result: 在RULER（长上下文检索）和CNN/DailyMail（文档摘要）两个基准测试中，该方法实现了显著的token节省（通常接近一半），同时没有降低任务性能。

Conclusion: 视觉文本表示是解码器LLM一种实用且非常有效的输入压缩形式。

Abstract: Large language models (LLMs) and their multimodal variants can now process
visual inputs, including images of text. This raises an intriguing question:
can we compress textual inputs by feeding them as images to reduce token usage
while preserving performance? In this paper, we show that visual text
representations are a practical and surprisingly effective form of input
compression for decoder LLMs. We exploit the idea of rendering long text inputs
as a single image and provide it directly to the model. This leads to
dramatically reduced number of decoder tokens required, offering a new form of
input compression. Through experiments on two distinct benchmarks RULER
(long-context retrieval) and CNN/DailyMail (document summarization) we
demonstrate that this text-as-image method yields substantial token savings
(often nearly half) without degrading task performance.

</details>


### [26] [BrailleLLM: Braille Instruction Tuning with Large Language Models for Braille Domain Tasks](https://arxiv.org/abs/2510.18288)
*Tianyuan Huang,Zepeng Zhu,Hangdi Xing,Zirui Shao,Zhi Yu,Chaoxiong Yang,Jiaxian He,Xiaozhong Liu,Jiajun Bu*

Main category: cs.CL

TL;DR: 该论文提出了 BrailleLLM，这是一种基于知识微调的 Braille 翻译模型，通过构建英语和汉语盲文混合数据集并采用句法树增强方法，解决了盲文信息处理中数据稀缺和混合文本模糊性问题。


<details>
  <summary>Details</summary>
Motivation: Braille 在视障人士的教育和信息获取中起着至关重要的作用。然而，盲文信息处理面临数据稀缺和混合文本中模糊性等挑战，传统微调方法在盲文相关任务中表现不佳。

Method: 1. 构建了包含数学公式的英语和汉语盲文混合数据集 (EBMD/CBMD)。
2. 提出了一种针对盲文数据量身定制的基于句法树的增强方法。
3. 提出并研究了 Braille Knowledge-Based Fine-Tuning (BKFT)，以降低盲文上下文特征的学习难度。
4. BrailleLLM 通过指令微调，利用 BKFT 实现统一的盲文翻译、公式到盲文的转换和混合文本翻译。

Result: 实验表明，BKFT 在盲文翻译场景中，比传统微调方法取得了显著的性能提升。

Conclusion: 该论文开源了数据集和方法，为低资源多语言盲文研究奠定了基础。

Abstract: Braille plays a vital role in education and information accessibility for
visually impaired individuals. However, Braille information processing faces
challenges such as data scarcity and ambiguities in mixed-text contexts. We
construct English and Chinese Braille Mixed Datasets (EBMD/CBMD) with
mathematical formulas to support diverse Braille domain research, and propose a
syntax tree-based augmentation method tailored for Braille data. To address the
underperformance of traditional fine-tuning methods in Braille-related tasks,
we investigate Braille Knowledge-Based Fine-Tuning (BKFT), which reduces the
learning difficulty of Braille contextual features. BrailleLLM employs BKFT via
instruction tuning to achieve unified Braille translation, formula-to-Braille
conversion, and mixed-text translation. Experiments demonstrate that BKFT
achieves significant performance improvements over conventional fine-tuning in
Braille translation scenarios. Our open-sourced datasets and methodologies
establish a foundation for low-resource multilingual Braille research.

</details>


### [27] [From Retrieval to Generation: Unifying External and Parametric Knowledge for Medical Question Answering](https://arxiv.org/abs/2510.18297)
*Lei Li,Xiao Zhou,Yingying Zhang,Xian Wu*

Main category: cs.CL

TL;DR: 本文提出了MedRGAG，一个统一的检索-生成增强框架，用于医疗问答。它结合了外部检索和内部生成知识，以解决现有RAG和GAG方法的不足。


<details>
  <summary>Details</summary>
Motivation: 医疗问答需要大量的领域特定知识。现有的RAG方法存在检索噪声或不完整的问题，而GAG方法容易产生幻觉或不准确的信息。

Method: MedRGAG包含两个模块：知识引导上下文完成（KGCC）和知识感知文档选择（KADS）。KGCC引导生成器补足检索缺失的知识，KADS自适应选择检索和生成文档的最佳组合。

Result: 在五个医疗QA基准测试中，MedRGAG比MedRAG提高了12.5%，比MedGENIE提高了4.5%。

Conclusion: MedRGAG有效地统一了检索和生成，显著提高了知识密集型医疗问答的性能。

Abstract: Medical question answering (QA) requires extensive access to domain-specific
knowledge. A promising direction is to enhance large language models (LLMs)
with external knowledge retrieved from medical corpora or parametric knowledge
stored in model parameters. Existing approaches typically fall into two
categories: Retrieval-Augmented Generation (RAG), which grounds model reasoning
on externally retrieved evidence, and Generation-Augmented Generation (GAG),
which depends solely on the models internal knowledge to generate contextual
documents. However, RAG often suffers from noisy or incomplete retrieval, while
GAG is vulnerable to hallucinated or inaccurate information due to
unconstrained generation. Both issues can mislead reasoning and undermine
answer reliability. To address these challenges, we propose MedRGAG, a unified
retrieval-generation augmented framework that seamlessly integrates external
and parametric knowledge for medical QA. MedRGAG comprises two key modules:
Knowledge-Guided Context Completion (KGCC), which directs the generator to
produce background documents that complement the missing knowledge revealed by
retrieval; and Knowledge-Aware Document Selection (KADS), which adaptively
selects an optimal combination of retrieved and generated documents to form
concise yet comprehensive evidence for answer generation. Extensive experiments
on five medical QA benchmarks demonstrate that MedRGAG achieves a 12.5%
improvement over MedRAG and a 4.5% gain over MedGENIE, highlighting the
effectiveness of unifying retrieval and generation for knowledge-intensive
reasoning. Our code and data are publicly available at
https://anonymous.4open.science/r/MedRGAG

</details>


### [28] [ECG-LLM-- training and evaluation of domain-specific large language models for electrocardiography](https://arxiv.org/abs/2510.18339)
*Lara Ahrens,Wilhelm Haverkamp,Nils Strodthoff*

Main category: cs.CL

TL;DR: 本文探讨了领域适应的开放权重大型语言模型在医疗健康领域的应用，特别是在心电图学方面。研究比较了微调模型、检索增强生成（RAG）和通用模型Claude Sonnet 3.7的性能，发现微调的Llama 3.1 70B在多项评估中表现出色，且通过微调和RAG实现的领域特定适应能够达到与专有模型相当的性能。


<details>
  <summary>Details</summary>
Motivation: 探索领域适应的开放权重大型语言模型（LLMs）在医疗健康领域的应用潜力，并解决其最优适应策略、评估方法以及与通用LLMs性能对比不明确的问题。

Method: 以心电图学为研究领域，通过在特定领域文献上微调开放权重模型，并实施多层次评估框架，比较了微调模型、检索增强生成（RAG）和作为代表性通用模型的Claude Sonnet 3.7的性能。

Result: 微调后的Llama 3.1 70B在多项选择评估和自动化文本指标上取得了卓越性能，在LLM作为评判者的评估中排名第二，仅次于Claude 3.7。人类专家评估显示，Claude 3.7和RAG方法在处理复杂查询时更受青睐。微调模型在几乎所有评估模式下都显著优于其基础模型。

Conclusion: 评估方法之间存在显著的性能异质性，但通过微调和RAG进行的领域特定适应能够实现与专有模型相当的竞争力，支持了保护隐私、可本地部署的临床解决方案的可行性。

Abstract: Domain-adapted open-weight large language models (LLMs) offer promising
healthcare applications, from queryable knowledge bases to multimodal
assistants, with the crucial advantage of local deployment for privacy
preservation. However, optimal adaptation strategies, evaluation methodologies,
and performance relative to general-purpose LLMs remain poorly characterized.
We investigated these questions in electrocardiography, an important area of
cardiovascular medicine, by finetuning open-weight models on domain-specific
literature and implementing a multi-layered evaluation framework comparing
finetuned models, retrieval-augmented generation (RAG), and Claude Sonnet 3.7
as a representative general-purpose model. Finetuned Llama 3.1 70B achieved
superior performance on multiple-choice evaluations and automatic text metrics,
ranking second to Claude 3.7 in LLM-as-a-judge assessments. Human expert
evaluation favored Claude 3.7 and RAG approaches for complex queries. Finetuned
models significantly outperformed their base counterparts across nearly all
evaluation modes. Our findings reveal substantial performance heterogeneity
across evaluation methodologies, underscoring assessment complexity.
Nevertheless, domain-specific adaptation through finetuning and RAG achieves
competitive performance with proprietary models, supporting the viability of
privacy-preserving, locally deployable clinical solutions.

</details>


### [29] [Combining Distantly Supervised Models with In Context Learning for Monolingual and Cross-Lingual Relation Extraction](https://arxiv.org/abs/2510.18344)
*Vipul Rathore,Malik Hammad Faisal,Parag Singla,Mausam*

Main category: cs.CL

TL;DR: 该文章提出了一个名为HYDRE的混合远程监督关系抽取框架，该框架结合了预训练的DSRE模型和大型语言模型的上下文学习能力，并通过动态范例检索策略提高对噪声标注数据的处理能力，从而在英语和低资源印度语言的关系抽取任务中取得了显著的性能提升。


<details>
  <summary>Details</summary>
Motivation: 尽管现有的最先进的远程监督关系抽取（DSRE）模型依赖于任务特定的训练，但它们与使用大型语言模型（LLM）的上下文学习（ICL）的整合尚未得到充分探索。一个关键的挑战是，由于噪声标注，LLM可能无法正确学习关系语义。

Method: 本文提出了HYDRE框架，它首先使用一个训练过的DSRE模型来识别给定测试句子的top-k候选关系，然后使用一种新颖的动态范例检索策略，从训练数据中提取可靠的句子级范例，这些范例随后在LLM提示中提供，以输出最终的关系。该方法还扩展到跨语言设置，用于低资源语言的关系抽取。

Result: HYDRE在英语上取得了高达20 F1点的提升，在印地语上平均取得了17 F1点的提升，超过了之前的最先进的DSRE模型。详细的消融实验也展示了HYDRE在与其他提示策略相比时的有效性。

Conclusion: HYDRE框架成功地将预训练的DSRE模型与LLM的上下文学习能力相结合，并解决了噪声标注带来的挑战，在英语和低资源语言的关系抽取任务中均取得了显著的性能提升。

Abstract: Distantly Supervised Relation Extraction (DSRE) remains a long-standing
challenge in NLP, where models must learn from noisy bag-level annotations
while making sentence-level predictions. While existing state-of-the-art (SoTA)
DSRE models rely on task-specific training, their integration with in-context
learning (ICL) using large language models (LLMs) remains underexplored. A key
challenge is that the LLM may not learn relation semantics correctly, due to
noisy annotation.
  In response, we propose HYDRE -- HYbrid Distantly Supervised Relation
Extraction framework. It first uses a trained DSRE model to identify the top-k
candidate relations for a given test sentence, then uses a novel dynamic
exemplar retrieval strategy that extracts reliable, sentence-level exemplars
from training data, which are then provided in LLM prompt for outputting the
final relation(s).
  We further extend HYDRE to cross-lingual settings for RE in low-resource
languages. Using available English DSRE training data, we evaluate all methods
on English as well as a newly curated benchmark covering four diverse
low-resource Indic languages -- Oriya, Santali, Manipuri, and Tulu. HYDRE
achieves up to 20 F1 point gains in English and, on average, 17 F1 points on
Indic languages over prior SoTA DSRE models. Detailed ablations exhibit HYDRE's
efficacy compared to other prompting strategies.

</details>


### [30] [KoSimpleQA: A Korean Factuality Benchmark with an Analysis of Reasoning LLMs](https://arxiv.org/abs/2510.18368)
*Donghyeon Ko,Yeguk Jin,Kyubyung Chae,Byungwook Lee,Chansong Jo,Sookyo In,Jaehong Lee,Taesup Kim,Donghyun Kwak*

Main category: cs.CL

TL;DR: 该论文提出了KoSimpleQA，一个用于评估大型语言模型韩语文化知识事实性的基准，并揭示了现有模型在此任务上的不足以及推理能力的重要性。


<details>
  <summary>Details</summary>
Motivation: 解决大型语言模型在韩语文化知识事实性方面评估的不足，并为该领域提供一个具有挑战性但易于评分的基准。

Method: 设计了包含1000个简短、事实性问题且答案明确的KoSimpleQA数据集。对支持韩语的多种开源大型语言模型进行了全面的评估。分析了LLM的推理能力对事实性问答任务的影响。

Result: 即使是性能最好的模型也仅能达到33.7%的正确率，表明KoSimpleQA具有很高的挑战性。模型在KoSimpleQA上的表现排名与在英文SimpleQA上的表现排名存在显著差异。分析显示，在事实性问答任务中运用推理能力可以帮助模型更好地发掘其潜在知识，并提高其在不确定时的拒绝回答能力。

Conclusion: KoSimpleQA是一个对评估大型语言模型韩语文化知识事实性具有独特价值的基准，现有模型在此任务上仍有很大的提升空间，且模型的推理能力对于提高事实性问答的表现至关重要。

Abstract: We present $\textbf{Korean SimpleQA (KoSimpleQA)}$, a benchmark for
evaluating factuality in large language models (LLMs) with a focus on Korean
cultural knowledge. KoSimpleQA is designed to be challenging yet easy to grade,
consisting of 1,000 short, fact-seeking questions with unambiguous answers. We
conduct a comprehensive evaluation across a diverse set of open-source LLMs of
varying sizes that support Korean, and find that even the strongest model
generates correct answer only 33.7% of the time, underscoring the challenging
nature of KoSimpleQA. Notably, performance rankings on KoSimpleQA differ
substantially from those on the English SimpleQA, highlighting the unique value
of our dataset. Furthermore, our analysis of reasoning LLMs shows that engaging
reasoning capabilities in the factual QA task can both help models better
elicit their latent knowledge and improve their ability to abstain when
uncertain. KoSimpleQA can be found at
https://anonymous.4open.science/r/KoSimpleQA-62EB.

</details>


### [31] [Towards Fair ASR For Second Language Speakers Using Fairness Prompted Finetuning](https://arxiv.org/abs/2510.18374)
*Monorama Swain,Bubai Maji,Jagabandhu Mishra,Markus Schedl,Anders Søgaard,Jesper Rindom Jensen*

Main category: cs.CL

TL;DR: 这篇论文旨在解决为第二语言使用者构建公平的英语ASR系统所面临的挑战。


<details>
  <summary>Details</summary>
Motivation: 现有的ASR模型（如Whisper和Seamless-M4T）在不同口音群体之间存在显著的词错误率波动，表明存在严重的公平性差距。

Method: 本文提出了一种结合Spectral Decoupling (SD)、Group Distributionally Robust Optimization (Group-DRO)和Invariant Risk Minimization (IRM)的轻量级适配器进行公平提示微调的方法。通过将传统经验风险最小化（ERM）与交叉熵和公平性驱动目标（SD、Group DRO和IRM）融合，以提高跨口音群体的公平性，同时保持整体识别准确性。

Result: 在宏观平均词错误率方面，该方法比大型预训练的Whisper和SeamlessM4T取得了58.7%和58.5%的相对改进，并且比使用标准经验风险最小化与交叉熵损失进行微调的模型取得了9.7%和7.8%的改进。

Conclusion: 本研究通过融合公平性驱动目标与传统ERM，显著提高了ASR系统对第二语言使用者的公平性，同时保持了识别准确性。

Abstract: In this work, we address the challenge of building fair English ASR systems
for second-language speakers. Our analysis of widely used ASR models, Whisper
and Seamless-M4T, reveals large fluctuations in word error rate (WER) across 26
accent groups, indicating significant fairness gaps. To mitigate this, we
propose fairness-prompted finetuning with lightweight adapters, incorporating
Spectral Decoupling (SD), Group Distributionally Robust Optimization
(Group-DRO), and Invariant Risk Minimization (IRM). Our proposed fusion of
traditional empirical risk minimization (ERM) with cross-entropy and
fairness-driven objectives (SD, Group DRO, and IRM) enhances fairness across
accent groups while maintaining overall recognition accuracy. In terms of
macro-averaged word error rate, our approach achieves a relative improvement of
58.7% and 58.5% over the large pretrained Whisper and SeamlessM4T, and 9.7% and
7.8% over them, finetuning with standard empirical risk minimization with
cross-entropy loss.

</details>


### [32] [MENTOR: A Reinforcement Learning Framework for Model Enhancement via Teacher-Optimized Rewards in Small Models](https://arxiv.org/abs/2510.18383)
*ChangSu Choi,Hoyun Song,Dongyeon Kim,WooHyeon Jung,Minkyung Cho,Sunjin Park,NohHyeob Bae,Seona Yu,KyungTae Lim*

Main category: cs.CL

TL;DR: 这篇文章介绍了一个名为MENTOR的框架，它结合了强化学习和教师引导蒸馏，旨在提高小型语言模型（SLM）的工具使用能力。


<details>
  <summary>Details</summary>
Motivation: 将大型语言模型（LLM）的工具使用能力提炼到小型语言模型（SLM）中，是SLM实际应用的关键挑战。传统的监督微调（SFT）方法泛化能力差，而标准的稀疏奖励强化学习（RL）方法则难以有效指导SLM，导致探索效率低下和次优策略。

Method: MENTOR结合了强化学习和教师引导蒸馏。它采用基于RL的流程，通过探索学习更具泛化性的策略，并利用教师的参考轨迹构建密集的、复合的教师引导奖励，以解决奖励稀疏问题并提供细粒度指导。

Result: 与监督微调和标准的稀疏奖励强化学习基线相比，MENTOR显著提高了SLM的跨领域泛化能力和策略能力。

Conclusion: MENTOR框架通过结合强化学习和教师引导蒸馏，有效地解决了小型语言模型在工具使用能力蒸馏中面临的泛化能力差和奖励稀疏问题，从而显著提升了SLM的性能。

Abstract: Distilling the tool-using capabilities of large language models (LLMs) into
smaller, more efficient small language models (SLMs) is a key challenge for
their practical application. The predominant approach, supervised fine-tuning
(SFT), suffers from poor generalization as it trains models to imitate a static
set of teacher trajectories rather than learn a robust methodology. While
reinforcement learning (RL) offers an alternative, the standard RL using sparse
rewards fails to effectively guide SLMs, causing them to struggle with
inefficient exploration and adopt suboptimal strategies. To address these
distinct challenges, we propose MENTOR, a framework that synergistically
combines RL with teacher-guided distillation. Instead of simple imitation,
MENTOR employs an RL-based process to learn a more generalizable policy through
exploration. In addition, to solve the problem of reward sparsity, it uses a
teacher's reference trajectory to construct a dense, composite teacher-guided
reward that provides fine-grained guidance. Extensive experiments demonstrate
that MENTOR significantly improves the cross-domain generalization and
strategic competence of SLMs compared to both SFT and standard sparse-reward RL
baselines.

</details>


### [33] [Adamas: Hadamard Sparse Attention for Efficient Long-Context Inference](https://arxiv.org/abs/2510.18413)
*Siyuan Yan,Guo-Qing Jiang,Yuchen Zhang,Xiaoxing Ma,Ran Zhu,Chun Cao,Jingwei Xu*

Main category: cs.CL

TL;DR: Adamas 是一种为长上下文推理设计的高效且准确的稀疏注意力机制，它通过Hadamard变换、分桶和2比特压缩技术，在保持高精度的同时，实现了比现有SOTA方法更高的稀疏度，并带来了显著的速度提升。


<details>
  <summary>Details</summary>
Motivation: 尽管大型语言模型（LLMs）现在支持数十万到数百万tokens的上下文窗口，能够处理长文档摘要、大规模代码合成、多文档问答和多轮对话等复杂应用，但这种扩展的上下文会加剧自注意力的二次成本问题，导致自回归解码过程中出现严重的延迟。现有的稀疏注意力方法虽然能缓解这些成本，但它们依赖启发式模式，难以准确回溯关键的键值对，从而导致精度下降。

Method: Adamas引入了轻量级但高精度的稀疏注意力机制，主要方法包括：1. 应用Hadamard变换、分桶和2比特压缩来生成紧凑的表示。2. 利用曼哈顿距离估计来实现高效的top-k选择，以识别关键的键值对。

Result: 实验结果表明，Adamas 在仅有64个token预算的情况下，其准确性与完全注意力（full attention）相当。在128个token时，实现了近乎无损的性能。与现有的最先进（SOTA）方法相比，Adamas支持高达8倍的更高稀疏度。在处理32K长度序列时，Adamas的自注意力速度提升高达4.4倍，端到端速度提升高达1.5倍。值得注意的是，Adamas的困惑度与完全注意力相当甚至更低。

Conclusion: Adamas作为一种新型稀疏注意力机制，在长上下文推理中展现出卓越的性能。它在大幅提升稀疏度的同时，不仅保持了与完全注意力相当甚至更优的准确性（通过更低的困惑度体现），还显著提高了处理速度。这表明Adamas在解决LLM长上下文推理中的效率和精度权衡问题上取得了实质性突破，为未来的LLM应用提供了更高效的解决方案。

Abstract: Large language models (LLMs) now support context windows of hundreds of
thousands to millions of tokens, enabling applications such as long-document
summarization, large-scale code synthesis, multi-document question answering
and persistent multi-turn dialogue. However, such extended contexts exacerbate
the quadratic cost of self-attention, leading to severe latency in
autoregressive decoding. Existing sparse attention methods alleviate these
costs but rely on heuristic patterns that struggle to recall critical key-value
(KV) pairs for each query, resulting in accuracy degradation. We introduce
Adamas, a lightweight yet highly accurate sparse attention mechanism designed
for long-context inference. Adamas applies the Hadamard transform,
bucketization and 2-bit compression to produce compact representations, and
leverages Manhattan-distance estimation for efficient top-k selections.
Experiments show that Adamas matches the accuracy of full attention with only a
64-token budget, achieves near-lossless performance at 128, and supports up to
8x higher sparsity than prior state-of-the-art (SOTA) methods while delivering
up to 4.4x self-attention and 1.5x end-to-end speedups on 32K-length sequences.
Remarkably, Adamas attains comparable or even lower perplexity than full
attention, underscoring its effectiveness in maintaining accuracy under
aggressive sparsity.

</details>


### [34] [Chain-of-Conceptual-Thought: Eliciting the Agent to Deeply Think within the Response](https://arxiv.org/abs/2510.18434)
*Qingqing Gu,Dan Wang,Yue Zhao,Xiaoyu Wang,Zhonglin Jiang,Yong Chen,Hongyan Li,Luo Ji*

Main category: cs.CL

TL;DR: 本文提出了一种名为概念思维链（CoCT）的新型提示范式，旨在解决大型语言模型（LLM）在开放域任务中表现不佳的问题。CoCT鼓励LLM通过标记概念并生成详细内容来进行深入和战略性思考，从而在日常和情感支持对话等任务中超越现有基线。


<details>
  <summary>Details</summary>
Motivation: 尽管思维链（CoT）在数学、编码和推理任务中有效，但其在开放域任务中的性能受限，因为这些任务缺乏明确定义的推理步骤或逻辑转换。

Method: 本文提出了一种名为概念思维链（CoCT）的提示范式。在此范式中，LLM首先标记一个概念，然后生成详细内容。概念链允许在话语内部存在，从而鼓励LLM进行深入的战略性思考。在日常和情感支持对话中，概念包括情感、策略和主题。

Result: 自动评估、人工评估和模型评估结果表明，CoCT超越了诸如Self-Refine、ECoT、ToT、SoT和RAG等基线方法。

Conclusion: CoCT作为一种潜在有效的提示范式，能够扩展LLM在更广泛任务中的应用范围。

Abstract: Chain-of-Thought (CoT) is widely applied to improve the LLM capability in
math, coding and reasoning tasks. However, its performance is limited for
open-domain tasks since there are no clearly defined reasoning steps or logical
transitions. To mitigate such challenges, we propose another prompt-based
paradigm called Chain of Conceptual Thought (CoCT), where the LLM first tags a
concept, then generates the detailed content. The chain of concepts is allowed
within the utterance, encouraging the LLM's deep and strategic thinking. We
experiment with this paradigm in daily and emotional support conversations
where the concept is comprised of emotions, strategies and topics. Automatic,
human and model evaluations suggest that CoCT surpasses baselines such as
Self-Refine, ECoT, ToT, SoT and RAG, suggesting a potential effective
prompt-based paradigm of LLM for a wider scope of tasks.

</details>


### [35] [Grounding or Guessing? Visual Signals for Detecting Hallucinations in Sign Language Translation](https://arxiv.org/abs/2510.18439)
*Yasser Hamidullah,Koel Dutta Chowdury,Yusser Al-Ghussin,Shakib Yazdani,Cennet Oguz,Josef van Genabith,Cristina España-Bonet*

Main category: cs.CL

TL;DR: 本文提出了一个名为“可靠性”的评估指标，能够对模型在生成文本时对视觉信息的依赖程度进行量化，从而有效识别和量化手语翻译（SLT）模型中的幻觉问题。


<details>
  <summary>Details</summary>
Motivation: 幻觉问题是视觉-语言模型的一个主要缺陷，尤其在手语翻译（SLT）中，它的关键性更强。由于无注音模型将连续的手语动作直接映射到自然语言，缺乏中间注音监督，导致其更容易出现幻觉。本文认为幻觉是模型过度依赖语言先验而不是视觉输入造成的。

Method: 本文提出了一种基于token级别的“可靠性”测量方法，用于量化解码器对视觉信息的使用程度。该方法结合了基于特征的敏感性（通过遮蔽视频来衡量内部变化）和反事实信号（捕捉清晰和篡改视频输入之间的概率差异）。这些信号被聚合成一个句子级别的可靠性分数，以简洁和可解释的方式衡量视觉基础。

Result: 在两个SLT基准（PHOENIX-2014T和CSL-Daily）上，使用基于注音和无注音模型进行评估。结果表明，“可靠性”能够预测幻觉率，并推广到不同的数据集和架构。在视觉退化的情况下，可靠性会降低。此外，可靠性可以将有视觉依据的token与猜测的token区分开来，从而无需参考即可进行风险评估。当与基于文本的信号（置信度、困惑度或熵）结合时，可以进一步改善幻觉风险估计。

Conclusion: 本文的研究结果证明，可靠性是诊断SLT中幻觉问题的一种实用且可重复使用的工具，并为多模态生成中更强大的幻觉检测奠定了基础。

Abstract: Hallucination, where models generate fluent text unsupported by visual
evidence, remains a major flaw in vision-language models and is particularly
critical in sign language translation (SLT). In SLT, meaning depends on precise
grounding in video, and gloss-free models are especially vulnerable because
they map continuous signer movements directly into natural language without
intermediate gloss supervision that serves as alignment. We argue that
hallucinations arise when models rely on language priors rather than visual
input. To capture this, we propose a token-level reliability measure that
quantifies how much the decoder uses visual information. Our method combines
feature-based sensitivity, which measures internal changes when video is
masked, with counterfactual signals, which capture probability differences
between clean and altered video inputs. These signals are aggregated into a
sentence-level reliability score, providing a compact and interpretable measure
of visual grounding. We evaluate the proposed measure on two SLT benchmarks
(PHOENIX-2014T and CSL-Daily) with both gloss-based and gloss-free models. Our
results show that reliability predicts hallucination rates, generalizes across
datasets and architectures, and decreases under visual degradations. Beyond
these quantitative trends, we also find that reliability distinguishes grounded
tokens from guessed ones, allowing risk estimation without references; when
combined with text-based signals (confidence, perplexity, or entropy), it
further improves hallucination risk estimation. Qualitative analysis highlights
why gloss-free models are more susceptible to hallucinations. Taken together,
our findings establish reliability as a practical and reusable tool for
diagnosing hallucinations in SLT, and lay the groundwork for more robust
hallucination detection in multimodal generation.

</details>


### [36] [Engagement Undermines Safety: How Stereotypes and Toxicity Shape Humor in Language Models](https://arxiv.org/abs/2510.18454)
*Atharvan Dogra,Soumya Suvra Ghosal,Ameet Deshpande,Ashwin Kalyan,Dinesh Manocha*

Main category: cs.CL

TL;DR: 这篇论文评估了大型语言模型（LLM）在生成幽默内容时，幽默感优化与有害内容之间的关系，发现有害输出通常获得更高的幽默评分，并通过信息论度量分析了不协调信号。


<details>
  <summary>Details</summary>
Motivation: 大型语言模型在创作性写作和互动内容中应用日益广泛，引发了对其输出安全性的担忧。因此，本研究以幽默生成为测试平台，评估了现代LLM管道中的幽默感优化如何与有害内容相关联。

Method: 本研究通过联合测量幽默感、刻板印象和毒性来评估LLM生成内容，并通过信息论度量分析不协调信号来补充。研究在六个模型上进行，并进行了基于角色的提示实验。此外，还在一个额外的讽刺生成任务上进行了外部验证，并辅以人类感知的幽默感判断。

Result: 有害输出获得了更高的幽默评分，并且在基于角色的提示下进一步增加，这表明生成器和评估器之间存在偏见放大循环。信息论分析显示，有害线索会扩大预测不确定性，甚至可以使有害的笑点对某些模型来说更具预期性，这表明有害内容结构性地嵌入在学习到的幽默分布中。在讽刺生成任务中，LLM生成的讽刺作品增加了刻板印象和毒性。定量分析显示，刻板印象/有毒笑话的平均幽默得分增加了10-21%，在LLM评估为有趣的笑话中，刻板印象笑话出现的频率增加了11%到28%，在人类认为有趣的生成内容中，这一比例高达10%。

Conclusion: LLM在优化幽默感时，存在将有害内容识别为更幽默的趋势，尤其是在特定提示下，这表明模型中存在偏见。有害内容不仅可能被错误地评估为幽默，其内在结构甚至可能使其在某些情况下更具预测性。因此，在开发和部署LLM时，需要更关注如何减轻这种偏见，以确保内容安全。

Abstract: Large language models are increasingly used for creative writing and
engagement content, raising safety concerns about the outputs. Therefore,
casting humor generation as a testbed, this work evaluates how funniness
optimization in modern LLM pipelines couples with harmful content by jointly
measuring humor, stereotypicality, and toxicity. This is further supplemented
by analyzing incongruity signals through information-theoretic metrics. Across
six models, we observe that harmful outputs receive higher humor scores which
further increase under role-based prompting, indicating a bias amplification
loop between generators and evaluators. Information-theoretic analyses show
harmful cues widen predictive uncertainty and surprisingly, can even make
harmful punchlines more expected for some models, suggesting structural
embedding in learned humor distributions. External validation on an additional
satire-generation task with human perceived funniness judgments shows that LLM
satire increases stereotypicality and typically toxicity, including for closed
models. Quantitatively, stereotypical/toxic jokes gain $10-21\%$ in mean humor
score, stereotypical jokes appear $11\%$ to $28\%$ more often among the jokes
marked funny by LLM-based metric and up to $10\%$ more often in generations
perceived as funny by humans.

</details>


### [37] [ChronoPlay: A Framework for Modeling Dual Dynamics and Authenticity in Game RAG Benchmarks](https://arxiv.org/abs/2510.18455)
*Liyang He,Yuren Zhang,Ziwei Zhu,Zhenghui Li,Shiwei Tong*

Main category: cs.CL

TL;DR: ChronoPlay是一个用于游戏RAG基准测试的自动化和持续生成的新框架，旨在解决游戏内容更新和玩家社区焦点变化带来的双重动态挑战。该框架利用双动态更新机制和双源合成引擎，确保生成的基准既符合事实又具有真实的查询模式。


<details>
  <summary>Details</summary>
Motivation: 在在线游戏等动态领域，检索增强生成（RAG）系统变得越来越重要，但缺乏专门的基准阻碍了该领域的标准化评估。核心难题在于双重动态：游戏内容更新与玩家社区焦点转移之间的持续相互作用。此外，自动化此类基准测试需要以玩家为中心的真实性，以确保生成问题的真实性。

Method: ChronoPlay框架采用双动态更新机制来跟踪游戏内容更新和玩家社区焦点的变化。它还采用双源合成引擎，从官方来源和玩家社区中提取信息，以确保事实的准确性和真实的查询模式。该框架已在三款不同的游戏中进行了实例化。

Result: ChronoPlay创建了第一个针对游戏领域的动态RAG基准测试，提供了在复杂真实条件下模型性能的新见解。

Conclusion: ChronoPlay框架通过自动化和持续生成游戏RAG基准，解决了游戏领域RAG系统评估中的双重动态挑战，并为模型性能评估提供了新的视角。

Abstract: Retrieval Augmented Generation (RAG) systems are increasingly vital in
dynamic domains like online gaming, yet the lack of a dedicated benchmark has
impeded standardized evaluation in this area. The core difficulty lies in Dual
Dynamics: the constant interplay between game content updates and the shifting
focus of the player community. Furthermore, the necessity of automating such a
benchmark introduces a critical requirement for player-centric authenticity to
ensure generated questions are realistic. To address this integrated challenge,
we introduce ChronoPlay, a novel framework for the automated and continuous
generation of game RAG benchmarks. ChronoPlay utilizes a dual-dynamic update
mechanism to track both forms of change, and a dual-source synthesis engine
that draws from official sources and player community to ensure both factual
correctness and authentic query patterns. We instantiate our framework on three
distinct games to create the first dynamic RAG benchmark for the gaming domain,
offering new insights into model performance under these complex and realistic
conditions. Code is avaliable at: https://github.com/hly1998/ChronoPlay.

</details>


### [38] [DePass: Unified Feature Attributing by Simple Decomposed Forward Pass](https://arxiv.org/abs/2510.18462)
*Xiangyu Hong,Che Jiang,Kai Tian,Biqing Qi,Youbang Sun,Ning Ding,Bowen Zhou*

Main category: cs.CL

TL;DR: DePass是一个统一的特征归因框架，它通过分解隐藏状态并固定注意力分数和MLP激活来传播这些分解后的组件，从而实现忠实、细粒度的归因，而无需辅助训练。


<details>
  <summary>Details</summary>
Motivation: 在机械可解释性中，将Transformer模型的行为归因于内部计算是一个核心挑战。

Method: DePass将隐藏状态分解为定制的附加组件，然后通过固定注意力分数和MLP的激活来传播它们。

Result: DePass在token级别、模型组件级别和子空间级别的归因任务中都得到了验证，证明了其有效性和保真度，并突出了其在Transformer模型任意组件之间归因信息流的潜力。

Conclusion: DePass有望成为可解释性领域更广泛应用的基础工具。

Abstract: Attributing the behavior of Transformer models to internal computations is a
central challenge in mechanistic interpretability. We introduce DePass, a
unified framework for feature attribution based on a single decomposed forward
pass. DePass decomposes hidden states into customized additive components, then
propagates them with attention scores and MLP's activations fixed. It achieves
faithful, fine-grained attribution without requiring auxiliary training. We
validate DePass across token-level, model component-level, and subspace-level
attribution tasks, demonstrating its effectiveness and fidelity. Our
experiments highlight its potential to attribute information flow between
arbitrary components of a Transformer model. We hope DePass serves as a
foundational tool for broader applications in interpretability.

</details>


### [39] [IMB: An Italian Medical Benchmark for Question Answering](https://arxiv.org/abs/2510.18468)
*Antonio Romano,Giuseppe Riccio,Mariano Barone,Marco Postiglione,Vincenzo Moscato*

Main category: cs.CL

TL;DR: 该文章介绍了两个全面的意大利医学基准数据集，并展示了如何利用大型语言模型改进医学论坛数据的清晰度和一致性，同时提出专门的适应策略在医学问答任务中优于通用模型。


<details>
  <summary>Details</summary>
Motivation: 在线医疗论坛积累了大量的有价值的知识，但其非正式性和语言复杂性对自动问答系统，尤其非英语问答系统带来了挑战。

Method: 文章构建了两个意大利医学基准数据集：IMB-QA（包含782,644个医患对话）和IMB-MCQA（包含25,862个医学多项选择题）。然后，作者利用大型语言模型（LLM）来提高医疗论坛数据的清晰度和一致性，并比较了各种LLM架构在开放式和多项选择问答任务中的表现。文章还使用了检索增强生成（RAG）和领域特定的微调方法进行实验。

Result: 实验结果表明，专门的适应策略在医学问答任务中可以胜过更大、通用的模型。这意味着，有效的医学人工智能系统可能更多地受益于领域专业知识和高效信息检索，而非仅仅增加模型规模。

Conclusion: 为了促进多语言医学问答的进一步研究，作者发布了这两个数据集和评估框架。研究框架。

Abstract: Online medical forums have long served as vital platforms where patients seek
professional healthcare advice, generating vast amounts of valuable knowledge.
However, the informal nature and linguistic complexity of forum interactions
pose significant challenges for automated question answering systems,
especially when dealing with non-English languages. We present two
comprehensive Italian medical benchmarks: \textbf{IMB-QA}, containing 782,644
patient-doctor conversations from 77 medical categories, and \textbf{IMB-MCQA},
comprising 25,862 multiple-choice questions from medical specialty
examinations. We demonstrate how Large Language Models (LLMs) can be leveraged
to improve the clarity and consistency of medical forum data while retaining
their original meaning and conversational style, and compare a variety of LLM
architectures on both open and multiple-choice question answering tasks. Our
experiments with Retrieval Augmented Generation (RAG) and domain-specific
fine-tuning reveal that specialized adaptation strategies can outperform
larger, general-purpose models in medical question answering tasks. These
findings suggest that effective medical AI systems may benefit more from domain
expertise and efficient information retrieval than from increased model scale.
We release both datasets and evaluation frameworks in our GitHub repository to
support further research on multilingual medical question answering:
https://github.com/PRAISELab-PicusLab/IMB.

</details>


### [40] [How Efficient Are Diffusion Language Models? A Critical Examination of Efficiency Evaluation Practices](https://arxiv.org/abs/2510.18480)
*Han Peng,Peiyu Liu,Zican Dong,Daixuan Cheng,Junyi Li,Yiru Tang,Shuo Wang,Wayne Xin Zhao*

Main category: cs.CL

TL;DR: DLM旨在取代无法并行解码的AR模型，但目前主流的开源DLM在实际速度方面表现不佳。本文探究了DLM效率低的原因，并通过实践证明AR模型比DLM具有更高的吞吐量。最后，本文强调了改进的评估方法和加速策略对DLM的重要性。


<details>
  <summary>Details</summary>
Motivation: 目前，DLM在实际应用中的速度和效率不如AR模型，这限制了DLM在实际场景中的使用。

Method: 通过系统的研究，找出目前评估方法中DLM效率低的主要原因，并通过实际基准测试和基于并行解码的理论分析，证明AR模型比DLM具有更高的吞吐量，同时DLM在吞吐量方面持续落后。最后，使用双缓存和并行解码等技术来研究加速策略。

Result: AR模型比DLM具有更高的吞吐量，且DLM在吞吐量方面持续落后。双缓存和并行解码等加速策略主要在小批量处理时才能发挥作用，但随着规模的扩大，这些优势也随之减弱。

Conclusion: 为了促进DLM的发展，本文强调了评估方法和加速策略的重要性。

Abstract: Diffusion language models (DLMs) have emerged as a promising alternative to
the long-dominant autoregressive (AR) paradigm, offering a parallelable
decoding process that could yield greater efficiency. Yet, in practice, current
open-source DLMs often underperform their AR counterparts in speed, limiting
their real-world utility. This work presents a systematic study of DLM
efficiency, identifying key issues in prior evaluation methods. Through
empirical benchmarking and a roofline-based theoretical analysis, we
demonstrate that AR models generally achieve higher throughput, while DLMs
consistently lag. We also investigate acceleration strategies, finding that
techniques like dual cache and parallel decoding mainly offer gains at small
batch sizes, with their benefits diminishing upon scaling. Our findings
underscore the necessity of robust evaluation methods and improved acceleration
strategies to advance research on DLMs.

</details>


### [41] [Identity-Aware Large Language Models require Cultural Reasoning](https://arxiv.org/abs/2510.18510)
*Alistair Plum,Anne-Marie Lutgen,Christoph Purschke,Achim Rettinger*

Main category: cs.CL

TL;DR: 大语言模型（LLM）需要文化推理 능력，以生成符合全球用户文化期望的回复，避免刻板印象和不信任。


<details>
  <summary>Details</summary>
Motivation: 当前大语言模型在回复中反映出狭隘的文化视角，忽略了全球用户的多样性，导致模型可能固化刻板印象、忽视少数群体视角、侵蚀信任并助长仇恨。

Method: 本文定义了文化推理：模型识别特定文化知识、价值观和社会规范，并调整输出以符合个体用户期望的能力。强调文化推理是身份感知AI的基础能力。讨论了现有评估方法的局限性，并认为仅靠扩充数据集无法实现真正的文化能力。

Result: 大语言模型在判断道德困境、解释习语或提供建议时，倾向于默认西方规范，即使通过调查数据进行微调也只能部分缓解这种倾向。现有的评估方法主要报告静态准确性分数，未能捕捉上下文中的自适应推理。

Conclusion: 文化推理应被视为与事实准确性和语言连贯性同等重要的基础能力。通过阐明文化推理的概念并提出初步评估方向，为未来系统能够更灵敏地响应人类文化的复杂性奠定了基础。

Abstract: Large language models have become the latest trend in natural language
processing, heavily featuring in the digital tools we use every day. However,
their replies often reflect a narrow cultural viewpoint that overlooks the
diversity of global users. This missing capability could be referred to as
cultural reasoning, which we define here as the capacity of a model to
recognise culture-specific knowledge values and social norms, and to adjust its
output so that it aligns with the expectations of individual users. Because
culture shapes interpretation, emotional resonance, and acceptable behaviour,
cultural reasoning is essential for identity-aware AI. When this capacity is
limited or absent, models can sustain stereotypes, ignore minority
perspectives, erode trust, and perpetuate hate. Recent empirical studies
strongly suggest that current models default to Western norms when judging
moral dilemmas, interpreting idioms, or offering advice, and that fine-tuning
on survey data only partly reduces this tendency. The present evaluation
methods mainly report static accuracy scores and thus fail to capture adaptive
reasoning in context. Although broader datasets can help, they cannot alone
ensure genuine cultural competence. Therefore, we argue that cultural reasoning
must be treated as a foundational capability alongside factual accuracy and
linguistic coherence. By clarifying the concept and outlining initial
directions for its assessment, a foundation is laid for future systems to be
able to respond with greater sensitivity to the complex fabric of human
culture.

</details>


### [42] [Building Trust in Clinical LLMs: Bias Analysis and Dataset Transparency](https://arxiv.org/abs/2510.18556)
*Svetlana Maslenkova,Clement Christophe,Marco AF Pimentel,Tathagata Raha,Muhammad Umar Salman,Ahmed Al Mahrooqi,Avani Gupta,Shadab Khan,Ronnie Rajan,Praveenkumar Kanithi*

Main category: cs.CL

TL;DR: 该研究深入分析了临床语言模型中潜在的下游偏见，重点关注不同人群（如民族、性别和年龄）之间阿片类药物处方倾向的差异。


<details>
  <summary>Details</summary>
Motivation: 目前的数据集管理和偏见评估实践缺乏必要的透明度，因此迫切需要一个综合评估框架来建立信任并指导改进。

Method: 我们介绍了HC4：医疗保健综合通用语料库，这是一个新颖且经过广泛策划的预训练数据集，拥有超过890亿个token。

Result: 我们的评估利用了既定的通用基准和新颖的、针对医疗保健的特有方法。

Conclusion: 该研究提供了关键见解，以支持临床AI应用的公平性和安全性。

Abstract: Large language models offer transformative potential for healthcare, yet
their responsible and equitable development depends critically on a deeper
understanding of how training data characteristics influence model behavior,
including the potential for bias. Current practices in dataset curation and
bias assessment often lack the necessary transparency, creating an urgent need
for comprehensive evaluation frameworks to foster trust and guide improvements.
In this study, we present an in-depth analysis of potential downstream biases
in clinical language models, with a focus on differential opioid prescription
tendencies across diverse demographic groups, such as ethnicity, gender, and
age. As part of this investigation, we introduce HC4: Healthcare Comprehensive
Commons Corpus, a novel and extensively curated pretraining dataset exceeding
89 billion tokens. Our evaluation leverages both established general benchmarks
and a novel, healthcare-specific methodology, offering crucial insights to
support fairness and safety in clinical AI applications.

</details>


### [43] [Large language models for folktale type automation based on motifs: Cinderella case study](https://arxiv.org/abs/2510.18561)
*Tjaša Arčon,Marko Robnik-Šikonja,Polona Tratnik*

Main category: cs.CL

TL;DR: 本文介绍了一种利用机器学习和自然语言处理在大量民间故事（特别是灰姑娘故事变体）中自动检测主题的方法，并通过聚类和降维分析其异同。


<details>
  <summary>Details</summary>
Motivation: 在大规模民俗学分析中，利用人工智能方法（特别是机器学习和自然语言处理）来自动检测故事主题并分析其复杂交互。

Method: 该研究采用了机器学习和自然语言处理技术，自动化地在一大批灰姑娘故事变体中检测主题，并通过聚类和降维技术分析了这些主题的相似性和差异性。此外，大型语言模型被用于检测故事中复杂的互动。

Result: 研究结果表明，大型语言模型能够检测故事中复杂的互动，这使得对大量文本集合进行计算分析成为可能，并促进了跨语言的比较研究。

Conclusion: 机器学习和自然语言处理技术能够有效地对大量民间故事进行主题检测和复杂互动分析，尤其是在灰姑娘故事变体研究中表现突出，这为计算民俗学和跨语言比较研究提供了新的途径。

Abstract: Artificial intelligence approaches are being adapted to many research areas,
including digital humanities. We built a methodology for large-scale analyses
in folkloristics. Using machine learning and natural language processing, we
automatically detected motifs in a large collection of Cinderella variants and
analysed their similarities and differences with clustering and dimensionality
reduction. The results show that large language models detect complex
interactions in tales, enabling computational analysis of extensive text
collections and facilitating cross-lingual comparisons.

</details>


### [44] [Dynamical model parameters from ultrasound tongue kinematics](https://arxiv.org/abs/2510.18629)
*Sam Kirkham,Patrycja Strycharczuk*

Main category: cs.CL

TL;DR: 该研究评估了使用超声计算舌运动动力学参数的可行性，并将其与EMA数据进行比较。


<details>
  <summary>Details</summary>
Motivation: 开发一种利用超声数据评估言语发音动态模型的方法。

Method: 使用线性谐波振荡器模型从超声舌运动学数据中估计动力学参数，并与同时记录的EMA数据进行比较。

Result: 超声和EMA在动力学参数上表现出可比性，下颌运动可以使用下颌短肌腱跟踪来充分捕捉。

Conclusion: 超声运动学可以用于评估动态发音模型。

Abstract: The control of speech can be modelled as a dynamical system in which
articulators are driven toward target positions. These models are typically
evaluated using fleshpoint data, such as electromagnetic articulography (EMA),
but recent methodological advances make ultrasound imaging a promising
alternative. We evaluate whether the parameters of a linear harmonic oscillator
can be reliably estimated from ultrasound tongue kinematics and compare these
with parameters estimated from simultaneously-recorded EMA data. We find that
ultrasound and EMA yield comparable dynamical parameters, while mandibular
short tendon tracking also adequately captures jaw motion. This supports using
ultrasound kinematics to evaluate dynamical articulatory models.

</details>


### [45] [MLMA: Towards Multilingual with Mamba Based Architectures](https://arxiv.org/abs/2510.18684)
*Mohamed Nabih Ali,Daniele Falavigna,Alessio Brutti*

Main category: cs.CL

TL;DR: 本文介绍了MLMA，一种利用Mamba架构进行多语言ASR的新方法，MLMA在多语言基准测试中取得了与Transformer相当的性能。


<details>
  <summary>Details</summary>
Motivation: 解决多语言ASR的挑战，尤其是在平衡高资源和低资源语言性能方面，并探索Transformer之外的更具扩展性和效率的架构。

Method: MLMA利用Mamba架构，这是一种针对长上下文序列处理进行优化的有效状态空间模型。MLMA通过Mamba隐式地整合了语言感知条件和共享表示，以支持跨不同语言的鲁棒识别。

Result: MLMA在标准多语言基准测试中取得了与基于Transformer的架构相当的性能。

Conclusion: Mamba架构在可扩展、高效和准确的多语言语音识别方面具有巨大潜力。

Abstract: Multilingual automatic speech recognition (ASR) remains a challenging task,
especially when balancing performance across high- and low-resource languages.
Recent advances in sequence modeling suggest that architectures beyond
Transformers may offer better scalability and efficiency. In this work, we
introduce MLMA (Multilingual Language Modeling with Mamba for ASR), a new
approach that leverages the Mamba architecture--an efficient state-space model
optimized for long-context sequence processing--for multilingual ASR. Using
Mamba, MLMA implicitly incorporates language-aware conditioning and shared
representations to support robust recognition across diverse languages.
Experiments on standard multilingual benchmarks show that MLMA achieves
competitive performance compared to Transformer-based architectures. These
results highlight Mamba's potential as a strong backbone for scalable,
efficient, and accurate multilingual speech recognition.

</details>


### [46] [Investigating LLM Capabilities on Long Context Comprehension for Medical Question Answering](https://arxiv.org/abs/2510.18691)
*Feras AlMannaa,Talia Tseriotou,Jenny Chim,Maria Liakata*

Main category: cs.CL

TL;DR: 该研究首次调查了大型语言模型（LLM）在临床相关的长上下文（LC）医学问答任务中的理解能力。


<details>
  <summary>Details</summary>
Motivation: 该研究旨在评估和理解大型语言模型在长上下文医学问答方面的能力和局限性。

Method: 研究通过对不同内容包含设置、多种LLM模型、不同任务表述的数据集进行评估，并重点考察了检索增强生成（RAG）对医学长上下文理解的影响，分析了单文档和多文档推理数据集中的最佳设置，并展示了RAG改进长上下文的策略。同时，研究还采用了多方面的方法进行定性和错误分析。

Result: 研究揭示了模型大小效应、局限性、潜在的记忆问题和推理模型的优势。此外，研究还揭示了RAG在医学长上下文理解中的效果，并在单文档和多文档推理数据集中找到了最佳设置，展示了RAG改进长上下文的策略。

Conclusion: 该研究通过定性和错误分析，回答了RAG何时对长上下文有益的问题，并揭示了常见的失败案例。

Abstract: This study is the first to investigate LLM comprehension capabilities over
long-context (LC) medical QA of clinical relevance. Our comprehensive
assessment spans a range of content-inclusion settings based on their
relevance, LLM models of varying capabilities and datasets across task
formulations, revealing insights on model size effects, limitations, underlying
memorization issues and the benefits of reasoning models. Importantly, we
examine the effect of RAG on medical LC comprehension, uncover best settings in
single versus multi-document reasoning datasets and showcase RAG strategies for
improvements over LC. We shed light into some of the evaluation aspects using a
multi-faceted approach. Our qualitative and error analyses address open
questions on when RAG is beneficial over LC, revealing common failure cases.

</details>


### [47] [Bayesian Low-Rank Factorization for Robust Model Adaptation](https://arxiv.org/abs/2510.18723)
*Enes Yavuz Ugan,Ngoc-Quan Pham,Alexander Waibel*

Main category: cs.CL

TL;DR: 这篇论文介绍了一种使用贝叶斯分解适配器对大型语音基础模型进行微调的方法，以解决代码转换问题，同时避免灾难性遗忘。


<details>
  <summary>Details</summary>
Motivation: 大型语音基础模型在许多领域表现出色，但在处理代码转换等本地需求时需要进行调整。直接微调存在过拟合和覆盖基础模型通用能力的风险。

Method: 本文探索了用于语音基础模型的贝叶斯分解适配器，该适配器将先验值设置在接近零的位置，以实现更稀疏的适应矩阵，从而在适应特定领域的同时保持通用性能。

Result: 作者将该方法应用于 Whisper 模型，并在不同的多语言代码转换场景中进行评估。结果表明，仅有最小的适应损失，同时显著减少了基础模型的灾难性遗忘。与 LoRA 相比，该方法实现了 54% 的“向后增益”，在新领域上仅下降 4%。

Conclusion: 这些发现强调了贝叶斯适应在微调语音基础模型方面的有效性，它在不牺牲泛化能力的情况下解决了代码转换问题。

Abstract: Large speech foundation models achieve strong performance across many
domains, but they often require adaptation to handle local needs such as
code-switching, where speakers mix languages within the same utterance. Direct
fine-tuning of these models risks overfitting to the target domain and
overwriting the broad capabilities of the base model. To address this
challenge, we explore Bayesian factorized adapters for speech foundation
models, which place priors near zero to achieve sparser adaptation matrices and
thereby retain general performance while adapting to specific domains. We apply
our approach to the Whisper model and evaluate on different multilingual
code-switching scenarios. Our results show only minimal adaptation loss while
significantly reducing catastrophic forgetting of the base model. Compared to
LoRA, our method achieves a backward gain of 54% with only a 4% drop on the new
domain. These findings highlight the effectiveness of Bayesian adaptation for
fine-tuning speech foundation models without sacrificing generalization.

</details>


### [48] [Adapting Language Balance in Code-Switching Speech](https://arxiv.org/abs/2510.18724)
*Enes Yavuz Ugan,Ngoc-Quan Pham,Alexander Waibel*

Main category: cs.CL

TL;DR: 大基础模型在语码转换测试用例上表现不佳。本文提出了一种利用嵌入与主要语言之间差异的方法来突出语码转换点，以减轻语码转换中的上下文偏差，从而提高模型的鲁棒性。


<details>
  <summary>Details</summary>
Motivation: 大型基础模型在语码转换测试用例上仍表现不佳，原因可能是语码转换发生频率较低，导致模型难以学习。

Method: 利用嵌入语言与主要语言之间的差异来突出语码转换点，从而在这些位置强调学习。这种简单而有效的可微分替代方法可以减轻生成过程中的上下文偏差。

Result: 在阿拉伯语和中英双语的实验中，模型能更准确地预测语码转换位置，替换错误率降低。

Conclusion: 通过强调语码转换点并减轻上下文偏差，可以提高大型基础模型在语码转换任务上的鲁棒性。

Abstract: Despite achieving impressive results on standard benchmarks, large
foundational models still struggle against code-switching test cases. When data
scarcity cannot be used as the usual justification for poor performance, the
reason may lie in the infrequent occurrence of code-switched moments, where the
embedding of the second language appears subtly. Instead of expecting the
models to learn this infrequency on their own, it might be beneficial to
provide the training process with labels. Evaluating model performance on
code-switching data requires careful localization of code-switching points
where recognition errors are most consequential, so that the analysis
emphasizes mistakes occurring at those moments. Building on this observation,
we leverage the difference between the embedded and the main language to
highlight those code-switching points and thereby emphasize learning at those
locations. This simple yet effective differentiable surrogate mitigates context
bias during generation -- the central challenge in code-switching -- thereby
improving the model's robustness. Our experiments with Arabic and
Chinese-English showed that the models are able to predict the switching places
more correctly, reflected by the reduced substitution error.

</details>


### [49] [SemiAdapt and SemiLoRA: Efficient Domain Adaptation for Transformer-based Low-Resource Language Translation with a Case Study on Irish](https://arxiv.org/abs/2510.18725)
*Josh McGiff,Nikola S. Nikolov*

Main category: cs.CL

TL;DR: 本文介绍了SemiAdapt和SemiLoRA两种半监督推理高效的方法，用于加强神经机器翻译（NMT）中的领域适应，解决了使用大型多语言模型进行微调时计算成本高的问题。


<details>
  <summary>Details</summary>
Motivation: 大型多语言模型在神经机器翻译等特定任务上的微调计算成本高昂，尤其对于爱尔兰语翻译等低资源领域的研究人员构成障碍。

Method: 本文引入了SemiAdapt和SemiLoRA作为半监督推理高效的方法，通过训练原始模型参数的一小部分来加强领域适应。SemiLoRA通过低秩适应（LoRA）方法引入了小的、可训练的适配器层。

Result: SemiAdapt在性能上超越了全域微调。更为显著的是，SemiLoRA可以将参数高效微调（PEFT）方法的性能提升到与全模型微调相当甚至超越的水平。此外，嵌入式推理方法在更大、噪声更多的数据集上表现尤为出色。

Conclusion: SemiAdapt和SemiLoRA方法使得高质量的领域适应和微调对于处理低资源语言的研究人员更易于实现。

Abstract: Fine-tuning is widely used to tailor large language models for specific tasks
such as neural machine translation (NMT). However, leveraging transfer learning
is computationally expensive when fine-tuning large multilingual models with
billions of parameters, thus creating a barrier to entry for researchers
working on low-resource domains such as Irish translation. Parameter-efficient
fine-tuning (PEFT) bridges this gap by training on a fraction of the original
model parameters, with the Low-Rank Adaptation (LoRA) approach introducing
small, trainable adapter layers. We introduce SemiAdapt and SemiLoRA as
semi-supervised inference-efficient approaches that strengthen domain
adaptation and lead to improved overall performance in NMT. We demonstrate that
SemiAdapt can outperform full-domain fine-tuning, while most notably, SemiLoRA
can propel PEFT methods to match or even outperform full-model fine-tuning. We
further evaluate domain-by-dataset fine-tuning and demonstrate that our
embedding-based inference methods perform especially well on larger and noisier
corpora. All Irish translation models developed in this work are released as
open resources. These methods aim to make high-quality domain adaptation and
fine-tuning more accessible to researchers working with low-resource languages.

</details>


### [50] [Verifiable Accuracy and Abstention Rewards in Curriculum RL to Alleviate Lost-in-Conversation](https://arxiv.org/abs/2510.18731)
*Ming Li*

Main category: cs.CL

TL;DR: RLAAR框架通过课程强化学习和可验证奖励，显著缓解了多轮对话中大型语言模型的“Lost-in-Conversation”问题，提高了性能和校准弃权率。


<details>
  <summary>Details</summary>
Motivation: 大型语言模型在多轮对话中存在“Lost-in-Conversation”（LiC）问题，即性能随信息逐步揭示而下降。

Method: 我们提出了带有可验证准确性和弃权奖励的课程强化学习（RLAAR）框架。该框架通过能力门控课程，逐步增加对话难度（通过指令碎片），稳定训练。RLAAR采用多轮、在线策略部署和混合奖励系统，鼓励模型在解决问题的同时，判断问题是否可解，以此平衡问题解决与知情弃权。

Result: 在LiC基准测试中，RLAAR显著缓解了LiC性能下降（从62.6%提高到75.1%），并改善了校准弃权率（从33.5%提高到73.4%）。

Conclusion: RLAAR为构建多轮可靠和值得信赖的大型语言模型提供了一个实用的方法。

Abstract: Large Language Models demonstrate strong capabilities in single-turn
instruction following but suffer from Lost-in-Conversation (LiC), a degradation
in performance as information is revealed progressively in multi-turn settings.
Motivated by the current progress on Reinforcement Learning with Verifiable
Rewards (RLVR), we propose Curriculum Reinforcement Learning with Verifiable
Accuracy and Abstention Rewards (RLAAR), a framework that encourages models not
only to generate correct answers, but also to judge the solvability of
questions in the multi-turn conversation setting. Our approach employs a
competence-gated curriculum that incrementally increases dialogue difficulty
(in terms of instruction shards), stabilizing training while promoting
reliability. Using multi-turn, on-policy rollouts and a mixed-reward system,
RLAAR teaches models to balance problem-solving with informed abstention,
reducing premature answering behaviors that cause LiC. Evaluated on LiC
benchmarks, RLAAR significantly mitigates LiC performance decay (62.6% to
75.1%) and improves calibrated abstention rates (33.5% to 73.4%). Together,
these results provide a practical recipe for building multi-turn reliable and
trustworthy LLMs.

</details>


### [51] [AI use in American newspapers is widespread, uneven, and rarely disclosed](https://arxiv.org/abs/2510.18774)
*Jenna Russell,Marzena Karpinska,Destiny Akinode,Katherine Thai,Bradley Emi,Max Spero,Mohit Iyyer*

Main category: cs.CL

TL;DR: 该研究调查了2025年夏季美国1500家报纸在线版发布的18.6万篇文章中AI生成内容的出现情况，发现约9%的文章由AI生成，且在小型地方媒体、特定主题和某些所有权群体中更为普遍。此外，社论文章的AI内容是新闻文章的6.4倍，但AI的使用很少被披露。


<details>
  <summary>Details</summary>
Motivation: 目前尚不清楚AI在已发表报纸文章中的使用程度，本研究旨在填补这一空白。

Method: 本研究审计了2025年夏季美国1500家报纸在线版发布的18.6万篇文章，并使用Pangram这一先进的AI检测器来识别AI生成内容。此外，还分析了华盛顿邮报、纽约时报和华尔街日报的4.5万篇评论文章，并对100篇AI标记文章进行了手动审计。

Result: 大约9%的新发表文章是部分或完全由AI生成的，且AI的使用分布不均，在小型地方媒体、天气和技术等特定主题以及某些所有权群体中出现频率更高。评论文章包含AI生成内容的可能性是新闻文章的6.4倍。然而，AI的使用很少被披露，在手动审计的100篇AI标记文章中，只有5篇提及了AI的使用。

Conclusion: 迫切需要提高新闻业中使用AI的透明度和更新编辑标准，以维护公众信任。

Abstract: AI is rapidly transforming journalism, but the extent of its use in published
newspaper articles remains unclear. We address this gap by auditing a
large-scale dataset of 186K articles from online editions of 1.5K American
newspapers published in the summer of 2025. Using Pangram, a state-of-the-art
AI detector, we discover that approximately 9% of newly-published articles are
either partially or fully AI-generated. This AI use is unevenly distributed,
appearing more frequently in smaller, local outlets, in specific topics such as
weather and technology, and within certain ownership groups. We also analyze
45K opinion pieces from Washington Post, New York Times, and Wall Street
Journal, finding that they are 6.4 times more likely to contain AI-generated
content than news articles from the same publications, with many AI-flagged
op-eds authored by prominent public figures. Despite this prevalence, we find
that AI use is rarely disclosed: a manual audit of 100 AI-flagged articles
found only five disclosures of AI use. Overall, our audit highlights the
immediate need for greater transparency and updated editorial standards
regarding the use of AI in journalism to maintain public trust.

</details>


### [52] [Fine-Tuned Thoughts: Leveraging Chain-of-Thought Reasoning for Industrial Asset Health Monitoring](https://arxiv.org/abs/2510.18817)
*Shuxin Lin,Dhaval Patel,Christodoulos Constantinides*

Main category: cs.CL

TL;DR: 本文提出了一种知识蒸馏框架，将大型语言模型的思维链推理能力迁移到小型语言模型中，以提高其在工业资产健康等专业领域的复杂推理能力。


<details>
  <summary>Details</summary>
Motivation: 在工业4.0等专业领域，小型语言模型（SLMs）在执行复杂推理时面临挑战，尽管它们在效率和成本效益方面具有优势。

Method: 提出了一种知识蒸馏框架，通过多项选择问答（MCQA）提示将大型语言模型（LLMs）的思维链（CoT）推理能力蒸馏到小型语言模型（SLMs）中。同时，利用上下文学习验证生成知识的质量。

Result: 经过CoT推理微调的SLMs显著优于基础模型，缩小了与LLM对应模型之间的差距。

Conclusion: 通过知识蒸馏，SLMs能够有效提升在专业领域的复杂推理能力，提供准确且经济高效的解决方案。

Abstract: Small Language Models (SLMs) are becoming increasingly popular in specialized
fields, such as industrial applications, due to their efficiency, lower
computational requirements, and ability to be fine-tuned for domain-specific
tasks, enabling accurate and cost-effective solutions. However, performing
complex reasoning using SLMs in specialized fields such as Industry 4.0 remains
challenging. In this paper, we propose a knowledge distillation framework for
industrial asset health, which transfers reasoning capabilities via
Chain-of-Thought (CoT) distillation from Large Language Models (LLMs) to
smaller, more efficient models (SLMs). We discuss the advantages and the
process of distilling LLMs using multi-choice question answering (MCQA) prompts
to enhance reasoning and refine decision-making. We also perform in-context
learning to verify the quality of the generated knowledge and benchmark the
performance of fine-tuned SLMs with generated knowledge against widely used
LLMs. The results show that the fine-tuned SLMs with CoT reasoning outperform
the base models by a significant margin, narrowing the gap to their LLM
counterparts. Our code is open-sourced at:
https://github.com/IBM/FailureSensorIQ.

</details>


### [53] [MTraining: Distributed Dynamic Sparse Attention for Efficient Ultra-Long Context Training](https://arxiv.org/abs/2510.18830)
*Wenxuan Li,Chengruidong Zhang,Huiqiang Jiang,Yucheng Li,Yuqing Yang,Lili Qiu*

Main category: cs.CL

TL;DR: 本文介绍了一种名为MTraining的分布式训练方法，它利用动态稀疏注意力机制，解决了在分布式环境中训练超长上下文LLM时存在的计算不平衡和通信开销问题，大大提高了训练效率和上下文窗口长度。


<details>
  <summary>Details</summary>
Motivation: 在大语言模型（LLMs）中，长上下文窗口的使用已成为其增强复杂推理能力和拓展应用范围的标准配置。然而，在分布式设置下，用动态稀疏注意力机制有效训练超长上下文的LLMs仍然面临计算不平衡和通信开销的重大挑战。

Method: 本文提出了一种名为MTraining的分布式训练方法，它包含三个关键组件：动态稀疏训练模式、平衡稀疏环形注意力和分层稀疏环形注意力。这些组件协同工作，旨在解决动态稀疏注意力机制在训练具有扩展上下文长度的模型时固有的计算不平衡和通信开销问题。

Result: MTraining成功将Qwen2.5-3B的上下文窗口从32K扩展到512K tokens（在32块A100 GPU集群上），并在RULER、PG-19、InfiniteBench和Needle In A Haystack等下游任务评估中，实现了高达6倍的训练吞吐量提升，同时保持了模型精度。

Conclusion: MTraining通过其创新的分布式方法，成功克服了超长上下文LLMs在分布式训练中动态稀疏注意力面临的挑战，显著提升了训练效率和模型的上下文处理能力，为未来LLMs的發展提供了新的方向。

Abstract: The adoption of long context windows has become a standard feature in Large
Language Models (LLMs), as extended contexts significantly enhance their
capacity for complex reasoning and broaden their applicability across diverse
scenarios. Dynamic sparse attention is a promising approach for reducing the
computational cost of long-context. However, efficiently training LLMs with
dynamic sparse attention on ultra-long contexts-especially in distributed
settings-remains a significant challenge, due in large part to worker- and
step-level imbalance. This paper introduces MTraining, a novel distributed
methodology leveraging dynamic sparse attention to enable efficient training
for LLMs with ultra-long contexts. Specifically, MTraining integrates three key
components: a dynamic sparse training pattern, balanced sparse ring attention,
and hierarchical sparse ring attention. These components are designed to
synergistically address the computational imbalance and communication overheads
inherent in dynamic sparse attention mechanisms during the training of models
with extensive context lengths. We demonstrate the efficacy of MTraining by
training Qwen2.5-3B, successfully expanding its context window from 32K to 512K
tokens on a cluster of 32 A100 GPUs. Our evaluations on a comprehensive suite
of downstream tasks, including RULER, PG-19, InfiniteBench, and Needle In A
Haystack, reveal that MTraining achieves up to a 6x higher training throughput
while preserving model accuracy. Our code is available at
https://github.com/microsoft/MInference/tree/main/MTraining.

</details>


### [54] [Towards Faithful and Controllable Personalization via Critique-Post-Edit Reinforcement Learning](https://arxiv.org/abs/2510.18849)
*Chenghao Zhu,Meiling Tao,Tiannan Wang,Dongyi Ding,Yuchen Eleanor Jiang,Wangchunshu Zhou*

Main category: cs.CL

TL;DR: 该论文提出了一种名为Critique-Post-Edit的强化学习框架，以实现大型语言模型（LLMs）更忠实和可控的个性化。


<details>
  <summary>Details</summary>
Motivation: 大型语言模型（LLMs）的个性化是一个关键但具有挑战性的任务。现有的监督微调（SFT）和基于人类反馈的强化学习（RLHF）在个性化方面存在局限性，特别是在奖励模型容易受到奖励攻击，导致生成冗长和表面化的个性化响应。

Method: 该框架包含两个关键组件：1. 个性化生成奖励模型（GRM），提供多维评分和文本评论，以抵抗奖励攻击。2. Critique-Post-Edit机制，策略模型根据这些评论修改自身输出，从而实现更有针对性和高效的学习。

Result: 在严格的长度控制评估下，该方法在个性化基准测试中显著优于标准PPO。个性化的Qwen2.5-7B模型平均胜率提高11%，个性化的Qwen2.5-14B模型性能超过GPT-4.1。

Conclusion: Critique-Post-Edit框架为实现大型语言模型忠实、高效和可控的个性化提供了一条实用的途径。

Abstract: Faithfully personalizing large language models (LLMs) to align with
individual user preferences is a critical but challenging task. While
supervised fine-tuning (SFT) quickly reaches a performance plateau, standard
reinforcement learning from human feedback (RLHF) also struggles with the
nuances of personalization. Scalar-based reward models are prone to reward
hacking which leads to verbose and superficially personalized responses. To
address these limitations, we propose Critique-Post-Edit, a robust
reinforcement learning framework that enables more faithful and controllable
personalization. Our framework integrates two key components: (1) a
Personalized Generative Reward Model (GRM) that provides multi-dimensional
scores and textual critiques to resist reward hacking, and (2) a
Critique-Post-Edit mechanism where the policy model revises its own outputs
based on these critiques for more targeted and efficient learning. Under a
rigorous length-controlled evaluation, our method substantially outperforms
standard PPO on personalization benchmarks. Personalized Qwen2.5-7B achieves an
average 11\% win-rate improvement, and personalized Qwen2.5-14B model surpasses
the performance of GPT-4.1. These results demonstrate a practical path to
faithful, efficient, and controllable personalization.

</details>


### [55] [How Do LLMs Use Their Depth?](https://arxiv.org/abs/2510.18871)
*Akshat Gupta,Jay Yeung,Gopala Anumanchipalli,Anna Ivanova*

Main category: cs.CL

TL;DR: 这篇论文深入探讨了大型语言模型（LLMs）如何分层进行预测，提出了“猜测-然后- SRE细化”框架，揭示了LLM内部计算的结构化和细致使用，并通过三个案例研究提供了LLM深度使用的详细视图。


<details>
  <summary>Details</summary>
Motivation: 目前缺乏对大型语言模型层间预测动态的细粒度理解。

Method: 本文通过追踪推理过程中几个开放权重模型的中间表示，提出了“猜测-然后-细化”框架来解释LLM如何内部构建计算以进行预测。此外，还进行了三项案例研究：（i）词性分析；（ii）事实回忆任务分析；（iii）多项选择任务分析。

Result: 早期LLM层中的高频预测主要由高频词组成，这些词在模型早期作为统计猜测，随后，这些初始猜测被HPM细化为上下文 AHHM 适当的标记。即使是早期层中的高频标记预测，也有超过70%的时间 SRE 得到细化。词性分析表明，功能词平均而言最早被正确预测。事实回忆任务分析显示，在多标记答案中，第一个标记比其余标记需要更多的计算深度。多项选择任务分析显示，模型在前半部分层中识别响应格式，但仅在接近 KKKK 尾声时才最终确定其响应。

Conclusion: LLMs的深度使用是结构化且细致入微的，成功的预测是基于逐层计算的，这为未来的 transformer 模型提高计算效率提供了见解。

Abstract: Growing evidence suggests that large language models do not use their depth
uniformly, yet we still lack a fine-grained understanding of their layer-wise
prediction dynamics. In this paper, we trace the intermediate representations
of several open-weight models during inference and reveal a structured and
nuanced use of depth. Specifically, we propose a "Guess-then-Refine" framework
that explains how LLMs internally structure their computations to make
predictions. We first show that the top-ranked predictions in early LLM layers
are composed primarily of high-frequency tokens, which act as statistical
guesses proposed by the model early on due to the lack of appropriate
contextual information. As contextual information develops deeper into the
model, these initial guesses get refined into contextually appropriate tokens.
Even high-frequency token predictions from early layers get refined >70% of the
time, indicating that correct token prediction is not "one-and-done". We then
go beyond frequency-based prediction to examine the dynamic usage of layer
depth across three case studies. (i) Part-of-speech analysis shows that
function words are, on average, the earliest to be predicted correctly. (ii)
Fact recall task analysis shows that, in a multi-token answer, the first token
requires more computational depth than the rest. (iii) Multiple-choice task
analysis shows that the model identifies the format of the response within the
first half of the layers, but finalizes its response only toward the end.
Together, our results provide a detailed view of depth usage in LLMs, shedding
light on the layer-by-layer computations that underlie successful predictions
and providing insights for future works to improve computational efficiency in
transformer-based models.

</details>


<div id='cs.AI'></div>

# cs.AI [[Back]](#toc)

### [56] [LLM-Based Multi-Agent System for Simulating and Analyzing Marketing and Consumer Behavior](https://arxiv.org/abs/2510.18155)
*Man-Lin Chu,Lucian Terhorst,Kadin Reed,Tom Ni,Weiwei Chen,Rongyu Lin*

Main category: cs.AI

TL;DR: 本文介绍了一种由大型语言模型驱动的多智能体模拟框架，用于模拟消费者决策和社交动态，旨在为营销策略提供可扩展、低风险的测试工具。


<details>
  <summary>Details</summary>
Motivation: 传统的事件后分析和基于规则的智能体建模方法难以捕捉人类行为和社交互动的复杂性，这促使研究人员寻求一种更能反映现实世界消费者决策和社交动态的模拟方法。

Method: 该框架利用大型语言模型（LLM）的最新进展，在沙盒环境中创建生成式智能体。这些智能体能够相互交互，表达内部推理，形成习惯，并做出购买决策，而无需预设规则。

Result: 在价格折扣营销场景中，该系统提供了可操作的策略测试结果，并揭示了传统方法无法达到的 emergent 社会模式。

Conclusion: 这种方法为营销人员提供了一个可扩展、低风险的工具，用于在实际实施前进行测试，减少了对耗时的事件后评估的依赖，并降低了营销活动表现不佳的风险。

Abstract: Simulating consumer decision-making is vital for designing and evaluating
marketing strategies before costly real- world deployment. However, post-event
analyses and rule-based agent-based models (ABMs) struggle to capture the
complexity of human behavior and social interaction. We introduce an
LLM-powered multi-agent simulation framework that models consumer decisions and
social dynamics. Building on recent advances in large language model simulation
in a sandbox envi- ronment, our framework enables generative agents to
interact, express internal reasoning, form habits, and make purchasing
decisions without predefined rules. In a price-discount marketing scenario, the
system delivers actionable strategy-testing outcomes and reveals emergent
social patterns beyond the reach of con- ventional methods. This approach
offers marketers a scalable, low-risk tool for pre-implementation testing,
reducing reliance on time-intensive post-event evaluations and lowering the
risk of underperforming campaigns.

</details>


### [57] [Activation Manifold Projection: Liberating Task-Specific Behaviors from LLM Architectures](https://arxiv.org/abs/2510.17902)
*Al Kari*

Main category: cs.AI

TL;DR: 该文章介绍了一种名为CAST的新型框架，旨在解决大型语言模型（LLM）中LoRA微调行为的架构锁定问题，并通过在不同LLM架构之间学习激活流形上的直接非线性映射，实现LoRA编码行为的迁移。


<details>
  <summary>Details</summary>
Motivation: 现有的LLM架构中，通过LoRA等方法微调得到的特定任务行为被限制在其源模型的架构中，即存在“架构锁定”问题。目前的迁移方法通过对模型静态权重空间进行对齐，这种方法脆弱且间接。

Method: 本文提出了一种名为Cartridge Activation Space Transfer（CAST）的全新框架。CAST将预训练的LoRA视为冻结的“行为核”，并学习一组轻量级、双向的投影头。这些投影头将目标模型的激活流转换为源模型的潜在空间，应用冻结核，然后将结果投射回来。该过程在通用文本语料库上进行训练，不使用任何特定任务数据，从而有效地将学习到的技能与源架构分离。

Result: CAST实现了LoRA适配器的“零样本”翻译。实验表明，CAST翻译的适配器在异构模型家族（如Llama-2和Mistral）之间的迁移中，能达到在目标模型上完全重新训练LoRA性能的85-95%。

Conclusion: CAST量化地优于当前的权重空间迁移技术，并在模型互操作性方面建立了新的最先进水平。

Abstract: The proliferation of Large Language Model (LLM) architectures presents a
fundamental challenge: valuable, task-specific behaviors learned through
fine-tuning methods like Low-Rank Adaptation (LoRA) are effectively trapped
within their source model's architecture, herein referred to architectural
lock-in. Existing transfer methods attempt to bridge this gap by aligning the
static weight spaces of models, a brittle and indirect approach that relies on
tenuous correlations between parameter geometries. This paper introduces a
fundamentally different and more direct paradigm: the Cartridge Activation
Space Transfer (CAST), a novel framework that liberates LoRA-encoded behaviors
by learning a direct, nonlinear mapping between the activation manifolds, the
geometric structures formed by the model's internal neuron activations, of two
distinct LLM architectures. CAST treats a pre-trained LoRA as a frozen
"behavioral kernel." It learns a set of lightweight, bidirectional projection
heads that translate the target model's activation stream into the source
model's latent space, apply the frozen kernel, and project the result back.
This process, trained on a general text corpus without any task-specific data,
effectively decouples the learned skill from the source architecture. We
demonstrate that CAST enables true "zero-shot" translation of any standard LoRA
adapter. Our experiments, including transfers between heterogeneous model
families like Llama-2 and Mistral, show that CAST-translated adapters achieve
85-95\% of the performance of a LoRA fully retrained on the target model,
quantitatively outperforming current weight-space transfer techniques and
establishing a new state-of-the-art in model interoperability.

</details>


### [58] [Beyond More Context: Retrieval Diversity Boosts Multi-Turn Intent Understanding](https://arxiv.org/abs/2510.17940)
*Zhiming Lin*

Main category: cs.AI

TL;DR: 研究了在固定预算下，检索多样性如何系统地改善LLM意图理解，而不是更长的提示。


<details>
  <summary>Details</summary>
Motivation: 在有限的token预算和嘈杂的上下文中，任务型聊天机器人中的多轮意图理解面临挑战。现有的检索管道往往只关注相关性，而忽略了集合级别的多样性以及上下文或示例顺序等混淆因素。

Method: 提出了一种多样性感知的检索框架，该框架选择上下文示例来平衡意图覆盖和语言多样性，并将其选择与标准LLM解码器集成。评估强制执行预算匹配的提示和随机位置，并包括对示例数量、多样性强度和骨干大小的敏感性分析。

Result: 在MultiWOZ 2.4和SGD上，该方法在相同的token预算下，在联合目标准确性方面取得了显著提高，超过了强大的LLM/DST基线，并且在K从4到7的情况下都有持续的改进，且延迟适中。

Conclusion: 该研究分离并验证了检索中内容多样性的影响，并为构建准确、预算受限的多轮意图系统提供了一个简单、可部署的选择原则。

Abstract: Multi turn intent understanding is central to task oriented chatbots, yet
real deployments face tight token budgets and noisy contexts, and most
retrieval pipelines emphasize relevance while overlooking set level diversity
and confounds such as more context or exemplar order. We ask whether retrieval
diversity, rather than longer prompts, systematically improves LLM intent
understanding under fixed budgets. We present a diversity aware retrieval
framework that selects in context exemplars to balance intent coverage and
linguistic variety, and integrates this selection with standard LLM decoders;
the evaluation enforces budget matched prompts and randomized positions, and
includes sensitivity analyses over exemplar count, diversity strength, and
backbone size. On MultiWOZ 2.4 and SGD, the approach achieves strong gains in
Joint Goal Accuracy under equal token budgets, surpassing strong LLM/DST
baselines, with consistent improvements across K from 4 to 7 and moderate
latency. Overall, the study isolates and validates the impact of content
diversity in retrieval and offers a simple, deployable selection principle for
building accurate, budget constrained multi turn intent systems.

</details>


### [59] [Subject-Event Ontology Without Global Time: Foundations and Execution Semantics](https://arxiv.org/abs/2510.18040)
*Alexander Boldachev*

Main category: cs.AI

TL;DR: 这篇论文提出了一种主题-事件本体的形式化方法，用于建模复杂的动态系统，不依赖全局时间。


<details>
  <summary>Details</summary>
Motivation: 在不依赖全局时间的情况下，为复杂的动态系统建模。

Method: 论文提出了一种主题-事件本体的形式化方法，包括：1. 将事件定义为固化行为，主体根据其模型识别并固化变化。2. 通过“先发生”建立因果顺序，事件顺序由显式依赖而非时间戳定义。3. 通过声明式数据流机制使本体可执行，确保确定性。4. 将模型作为认知过滤器，主体只能固化其已知概念和属性。5. 假定事件内容的真实性，固化后即可用于计算，无需外部验证。该形式化方法包含9个公理（A1-A9），确保可执行本体的正确性，并特别关注基于模型的方法（A9）。

Result: 在boldsea系统（一个可执行本体的工作流引擎）中验证了该理论构建的实际适用性，其中理论概念在BSL（Boldsea语义语言）中实现。

Conclusion: 该形式化方法适用于分布式系统、微服务架构、DLT平台和多视角场景。

Abstract: A formalization of a subject-event ontology is proposed for modeling complex
dynamic systems without reliance on global time. Key principles: (1) event as
an act of fixation - a subject discerns and fixes changes according to models
(conceptual templates) available to them; (2) causal order via happens-before -
the order of events is defined by explicit dependencies, not timestamps; (3)
making the ontology executable via a declarative dataflow mechanism, ensuring
determinism; (4) models as epistemic filters - a subject can only fix what
falls under its known concepts and properties; (5) presumption of truth - the
declarative content of an event is available for computation from the moment of
fixation, without external verification. The formalization includes nine axioms
(A1-A9), ensuring the correctness of executable ontologies: monotonicity of
history (I1), acyclicity of causality (I2), traceability (I3). Special
attention is given to the model-based approach (A9): event validation via
schemas, actor authorization, automatic construction of causal chains (W3)
without global time. Practical applicability is demonstrated on the boldsea
system - a workflow engine for executable ontologies, where the theoretical
constructs are implemented in BSL (Boldsea Semantic Language). The
formalization is applicable to distributed systems, microservice architectures,
DLT platforms, and multiperspectivity scenarios (conflicting facts from
different subjects).

</details>


### [60] [Planned Diffusion](https://arxiv.org/abs/2510.18087)
*Daniel Israel,Tian Jin,Ellie Cheng,Guy Van den Broeck,Aditya Grover,Suvinay Subramanian,Michael Carbin*

Main category: cs.AI

TL;DR: 该论文提出了一种名为“规划扩散”的混合方法，结合了自回归模型和扩散模型的优点，以提高大型语言模型推理中生成文本的速度和质量。


<details>
  <summary>Details</summary>
Motivation: 大型语言模型推理在生成速度和输出质量之间存在权衡。自回归模型能生成高质量文本但速度慢，扩散模型速度快但质量可能不如前者。

Method: 规划扩散分为两个阶段：首先，模型创建一个简短的自回归“计划”，将输出分解为更小、独立的片段。其次，模型使用扩散模型并行生成这些片段。

Result: 在AlpacaEval基准测试中，规划扩散在质量和延迟之间实现了帕累托最优的权衡，相较于自回归生成，速度提高了1.27到1.81倍，而胜率仅下降了0.87%到5.4%。

Conclusion: 规划扩散通过结合两种模型的优点，有效扩展了速度-质量的帕累托前沿，为实现更快、高质量的文本生成提供了一个实用的路径。其规划机制被证明是最小且可靠的，并且可以通过简单的运行时旋钮灵活控制质量-延迟的权衡。

Abstract: A central challenge in large language model inference is the trade-off
between generation speed and output quality. Autoregressive models produce
high-quality text but generate tokens sequentially. Diffusion models can
generate tokens in parallel but often need many iterations to match the same
quality. We propose planned diffusion, a hybrid method that combines the
strengths of both paradigms. Planned diffusion works in two stages: first, the
model creates a short autoregressive plan that breaks the output into smaller,
independent spans. Second, the model generates these spans simultaneously using
diffusion. This approach expands the speed-quality Pareto frontier and provides
a practical path to faster, high-quality text generation. On AlpacaEval, a
suite of 805 instruction-following prompts, planned diffusion achieves
Pareto-optimal trade-off between quality and latency, achieving 1.27x to 1.81x
speedup over autoregressive generation with only 0.87\% to 5.4\% drop in win
rate, respectively. Our sensitivity analysis shows that the planning mechanism
of planned diffusion is minimal and reliable, and simple runtime knobs exist to
provide flexible control of the quality-latency trade-off.

</details>


### [61] [Learning from Generalization Patterns: An Evaluation-Driven Approach to Enhanced Data Augmentation for Fine-Tuning Small Language Models](https://arxiv.org/abs/2510.18143)
*Huan Song,Deeksha Razdan,Yiyue Qian,Arijit Ghosh Chowdhury,Parth Patwa,Aman Chadha,Shinan Zhang,Sharlina Keshava,Hannah Marlowe*

Main category: cs.AI

TL;DR: 为了弥补小语言模型（SLMs）在复杂领域任务中准确性落后于大模型的不足，本文提出了PaDA-Agent，一个以评估驱动的方法，通过协调操作简化数据增强过程。


<details>
  <summary>Details</summary>
Motivation: 小语言模型（SLMs）在部署成本和延迟方面具有优势，但在复杂领域任务中的准确性往往低于大型模型，而监督微调需要大量数据准备和优化。

Method: PaDA-Agent是一个评估驱动的方法，通过协调操作来简化SLMs的数据增强过程。它通过评估从验证数据中发现错误模式，并制定有针对性的数据增强策略，以直接减少泛化差距。该方法不同于仅关注模型训练错误并生成纠错样本的现有方法。

Result: 实验结果表明，PaDA-Agent在Llama 3.2 1B Instruct模型微调方面，相对于最先进的基于LLM的数据增强方法取得了显著改进。

Conclusion: PaDA-Agent通过发现错误模式和制定有针对性的数据增强策略，有效地提升了SLMs在复杂领域任务中的性能，减少了泛化差距。

Abstract: Small Language Models (SLMs) offer compelling advantages in deployment cost
and latency, but their accuracy often lags behind larger models, particularly
for complex domain-specific tasks. While supervised fine-tuning can help bridge
this performance gap, it requires substantial manual effort in data preparation
and iterative optimization. We present PaDA-Agent (Pattern-guided Data
Augmentation Agent), an evaluation-driven approach that streamlines the data
augmentation process for SLMs through coordinated operations. Unlike
state-of-the-art approaches that focus on model training errors only and
generating error-correcting samples, PaDA-Agent discovers failure patterns from
the validation data via evaluations and drafts targeted data augmentation
strategies aiming to directly reduce the generalization gap. Our experimental
results demonstrate significant improvements over state-of-the-art LLM-based
data augmentation approaches for Llama 3.2 1B Instruct model fine-tuning.

</details>


### [62] [Annotating the Chain-of-Thought: A Behavior-Labeled Dataset for AI Safety](https://arxiv.org/abs/2510.18154)
*Antonio-Gabriel Chacón Menke,Phan Xuan Tan,Eiji Kamioka*

Main category: cs.AI

TL;DR: 该论文介绍了一个句子级标注数据集，用于通过模型激活来监控大型语言模型（LLM）推理过程中的安全行为。


<details>
  <summary>Details</summary>
Motivation: 现有分析文本推理步骤的方法可能忽略细微的有害模式，并且容易被隐藏不安全推理的模型规避，因此需要一种更有效的方法来监控AI安全中的思维链推理。

Method: 本文提出了一个包含句子级安全行为注释的推理序列数据集，例如表达安全担忧或推测用户意图。作者利用该数据集提取“转向向量”，用于在模型激活中检测和影响这些行为。

Result: 该数据集通过识别推理链中特定安全行为发生的精确时刻，填补了安全研究的空白。研究者成功提取了能够在模型激活中检测和引导安全行为的表示，展示了激活级技术在改进推理安全监督方面的潜力。

Conclusion: 通过使用句子级标注数据集和激活级技术，可以有效地检测和引导大型语言模型推理过程中的安全行为，从而提高AI安全性。

Abstract: Recent work has highlighted the importance of monitoring chain-of-thought
reasoning for AI safety; however, current approaches that analyze textual
reasoning steps can miss subtle harmful patterns and may be circumvented by
models that hide unsafe reasoning. We present a sentence-level labeled dataset
that enables activation-based monitoring of safety behaviors during LLM
reasoning. Our dataset contains reasoning sequences with sentence-level
annotations of safety behaviors such as expression of safety concerns or
speculation on user intent, which we use to extract steering vectors for
detecting and influencing these behaviors within model activations. The dataset
fills a key gap in safety research: while existing datasets label reasoning
holistically, effective application of steering vectors for safety monitoring
could be improved by identifying precisely when specific behaviors occur within
reasoning chains. We demonstrate the dataset's utility by extracting
representations that both detect and steer safety behaviors in model
activations, showcasing the potential of activation-level techniques for
improving safety oversight on reasoning.
  Content Warning: This paper discusses AI safety in the context of harmful
prompts and may contain references to potentially harmful content.

</details>


### [63] [Saber: An Efficient Sampling with Adaptive Acceleration and Backtracking Enhanced Remasking for Diffusion Language Model](https://arxiv.org/abs/2510.18165)
*Yihong Dong,Zhaoyu Ma,Xue Jiang,Zhiyuan Fan,Jiaru Qian,Yongmin Li,Jianha Xiao,Zhi Jin,Rongyu Cao,Binhua Li,Fei Huang,Yongbin Li,Ge Li*

Main category: cs.AI

TL;DR: Saber是一种无需训练的DLM采样算法，它在推理速度和输出质量之间取得了更好的平衡，通过自适应加速和回溯增强的重新掩码实现了代码生成性能的显著提升。


<details>
  <summary>Details</summary>
Motivation: 现有的扩散语言模型（DLMs）在代码生成任务中，由于推理速度和输出质量之间的权衡，性能受到显著影响，特别是在减少采样步数以加速时性能会急剧下降。

Method: 本文提出了一种名为Saber（高效采样与自适应加速与回溯增强的重新掩码）的无训练采样算法。Saber基于两个核心洞察：1）代码上下文建立后可以自适应加速；2）需要回溯机制来修正已生成的标记。

Result: 在多个主流代码生成基准测试中，Saber使DLM的Pass@1准确率平均提高了1.9%，同时推理速度平均提升了251.4%。

Conclusion: Saber通过利用DLMs的固有优势，显著缩小了DLMs在代码生成方面与自回归模型之间的性能差距。

Abstract: Diffusion language models (DLMs) are emerging as a powerful and promising
alternative to the dominant autoregressive paradigm, offering inherent
advantages in parallel generation and bidirectional context modeling. However,
the performance of DLMs on code generation tasks, which have stronger
structural constraints, is significantly hampered by the critical trade-off
between inference speed and output quality. We observed that accelerating the
code generation process by reducing the number of sampling steps usually leads
to a catastrophic collapse in performance. In this paper, we introduce
efficient Sampling with Adaptive acceleration and Backtracking Enhanced
Remasking (i.e., Saber), a novel training-free sampling algorithm for DLMs to
achieve better inference speed and output quality in code generation.
Specifically, Saber is motivated by two key insights in the DLM generation
process: 1) it can be adaptively accelerated as more of the code context is
established; 2) it requires a backtracking mechanism to reverse the generated
tokens. Extensive experiments on multiple mainstream code generation benchmarks
show that Saber boosts Pass@1 accuracy by an average improvement of 1.9% over
mainstream DLM sampling methods, meanwhile achieving an average 251.4%
inference speedup. By leveraging the inherent advantages of DLMs, our work
significantly narrows the performance gap with autoregressive models in code
generation.

</details>


### [64] [A Definition of AGI](https://arxiv.org/abs/2510.18212)
*Dan Hendrycks,Dawn Song,Christian Szegedy,Honglak Lee,Yarin Gal,Erik Brynjolfsson,Sharon Li,Andy Zou,Lionel Levine,Bo Han,Jie Fu,Ziwei Liu,Jinwoo Shin,Kimin Lee,Mantas Mazeika,Long Phan,George Ingebretsen,Adam Khoja,Cihang Xie,Olawale Salaudeen,Matthias Hein,Kevin Zhao,Alexander Pan,David Duvenaud,Bo Li,Steve Omohundro,Gabriel Alfour,Max Tegmark,Kevin McGrew,Gary Marcus,Jaan Tallinn,Eric Schmidt,Yoshua Bengio*

Main category: cs.AI

TL;DR: 该论文提出了一个量化的框架，旨在通过将AGI定义为与受过良好教育的成年人认知能力相匹配，从而弥合当前专业AI与人类认知之间的差距。


<details>
  <summary>Details</summary>
Motivation: 目前对通用人工智能（AGI）缺乏具体的定义，这模糊了当今专业AI与人类水平认知之间的差距。

Method: 我们根据Cattell-Horn-Carroll（CHC）理论（最经验证的人类认知模型）建立了我们的方法论。该框架将通用智能分解为十个核心认知领域（包括推理、记忆和感知），并调整了既定的人类心理测量电池来评估AI系统。

Result: 该框架的应用揭示了当代模型中“不平衡”的认知概况。尽管在知识密集型领域表现出色，但当前的AI系统在基础认知机制，特别是长期记忆存储方面存在严重缺陷。

Conclusion: 由此产生的AGI分数（例如，GPT-4为27%，GPT-5为58%）具体量化了快速进展以及在实现AGI之前仍然存在的巨大差距。

Abstract: The lack of a concrete definition for Artificial General Intelligence (AGI)
obscures the gap between today's specialized AI and human-level cognition. This
paper introduces a quantifiable framework to address this, defining AGI as
matching the cognitive versatility and proficiency of a well-educated adult. To
operationalize this, we ground our methodology in Cattell-Horn-Carroll theory,
the most empirically validated model of human cognition. The framework dissects
general intelligence into ten core cognitive domains-including reasoning,
memory, and perception-and adapts established human psychometric batteries to
evaluate AI systems. Application of this framework reveals a highly "jagged"
cognitive profile in contemporary models. While proficient in
knowledge-intensive domains, current AI systems have critical deficits in
foundational cognitive machinery, particularly long-term memory storage. The
resulting AGI scores (e.g., GPT-4 at 27%, GPT-5 at 58%) concretely quantify
both rapid progress and the substantial gap remaining before AGI.

</details>


### [65] [ssToken: Self-modulated and Semantic-aware Token Selection for LLM Fine-tuning](https://arxiv.org/abs/2510.18250)
*Xiaohan Qin,Xiaoxing Wang,Ning Liao,Cancheng Zhang,Xiangdong Zhang,Mingquan Feng,Jingzhi Wang,Junchi Yan*

Main category: cs.AI

TL;DR: 这篇论文提出了一种名为ssToken的自调制和语义感知的token选择方法，用于提高大型语言模型（LLMs）的监督微调（SFT）中的数据质量。


<details>
  <summary>Details</summary>
Motivation: 大型语言模型（LLMs）的监督微调（SFT）中，数据质量对性能提升至关重要。现有的token级别数据选择方法存在两个主要限制：1）需要训练或访问额外的参考模型；2）仅依赖于损失信息进行token选择，这可能无法很好地保留语义上重要的token。

Method: 本研究提出了ssToken方法来解决上述挑战。ssToken利用可用的历史模型计算当前模型与历史模型之间的逐token损失差异，作为自调制信号，使模型能够沿着其优化轨迹自适应地选择token，而不是像先前工作那样依赖于离线训练的参考模型的额外损失。此外，ssToken引入了一种语义感知的、基于注意力的token重要性估计指标，该指标独立于基于损失的选择，并提供补充性的语义信息，以实现更有效的过滤。

Result: 广泛的实验表明，无论是单独的自调制选择还是语义感知选择，都优于全数据微调。而它们的集成——ssToken——实现了协同增益，并进一步超越了先前的token级别选择方法。ssToken在保持训练效率的同时，实现了性能的显著提升。

Conclusion: ssToken通过结合自调制和语义感知的方法，有效地解决了现有token级别数据选择方法的局限性，在LLMs的SFT中展现出卓越的性能和效率。

Abstract: Data quality plays a critical role in enhancing supervised fine-tuning (SFT)
for large language models (LLMs), and token-level data selection has emerged as
a promising direction for its fine-grained nature. Despite their strong
empirical performance, existing token-level selection methods share two key
limitations: (1) requiring training or accessing an additional reference model,
and (2) relying solely on loss information for token selection, which cannot
well preserve semantically important tokens that are not favored by loss-based
metrics. To address these challenges, we propose ssToken, a Self-modulated and
Semantic-aware Token Selection approach. ssToken leverages readily accessible
history models to compute the per-token loss difference with the current model,
which serves as a self-modulated signal that enables the model to adaptively
select tokens along its optimization trajectory, rather than relying on excess
loss from an offline-trained reference model as in prior works. We further
introduce a semantic-aware, attention-based token importance estimation metric,
orthogonal to loss-based selection and providing complementary semantic
information for more effective filtering. Extensive experiments across
different model families and scales demonstrate that both self-modulated
selection and semantic-aware selection alone outperform full-data fine-tuning,
while their integration--ssToken--achieves synergistic gains and further
surpasses prior token-level selection methods, delivering performance
improvements while maintaining training efficiency.

</details>


### [66] [Illusions of reflection: open-ended task reveals systematic failures in Large Language Models' reflective reasoning](https://arxiv.org/abs/2510.18254)
*Sion Weatherhead,Flora Salim,Aaron Belbasis*

Main category: cs.AI

TL;DR: 本文评估了大型语言模型（LLMs）在开放式但受规则约束任务中的“反思”能力，发现其反思能力提升有限，且难以有效纠正错误。


<details>
  <summary>Details</summary>
Motivation: 探索LLMs的反思能力是否等同于人类的反思推理，尤其是在开放式任务中，LLMs的表现是否受限于其自我修正能力。

Method: 本文选择了一个简单、真实世界且开放但受规则约束的任务——生成有效的科学测试项目，并评估八个前沿模型在考虑自身批评后的修改能力。

Result: 首轮表现不佳（通常4个所需项目中有0个有效，平均约1个），反思后提升有限（也约1个）。第二次尝试常重复相同错误，表明“修正收益”主要源于偶然的有效产出而非错误检测与原则性修复。模型表现随开放性增加而下降，以“推理”为卖点的模型未显示优势。

Conclusion: 当前LLM的“反思”缺乏主动的、目标驱动的监控功能，这使得它们在面对约束时表现不佳。在模型中内置此类机制前，可靠的性能仍需外部结构来强制执行约束。

Abstract: Humans do not just find mistakes after the fact -- we often catch them
mid-stream because 'reflection' is tied to the goal and its constraints.
Today's large language models produce reasoning tokens and 'reflective' text,
but is it functionally equivalent with human reflective reasoning? Prior work
on closed-ended tasks -- with clear, external 'correctness' signals -- can make
'reflection' look effective while masking limits in self-correction. We
therefore test eight frontier models on a simple, real-world task that is
open-ended yet rule-constrained, with auditable success criteria: to produce
valid scientific test items, then revise after considering their own critique.
First-pass performance is poor (often zero valid items out of 4 required; mean
$\approx$ 1), and reflection yields only modest gains (also $\approx$ 1).
Crucially, the second attempt frequently repeats the same violation of
constraint, indicating 'corrective gains' arise largely from chance production
of a valid item rather than error detection and principled,
constraint-sensitive repair. Performance before and after reflection
deteriorates as open-endedness increases, and models marketed for 'reasoning'
show no advantage. Our results suggest that current LLM 'reflection' lacks
functional evidence of the active, goal-driven monitoring that helps humans
respect constraints even on a first pass. Until such mechanisms are
instantiated in the model itself, reliable performance requires external
structure that enforces constraints.

</details>


### [67] [ShortcutBreaker: Low-Rank Noisy Bottleneck with Global Perturbation Attention for Multi-Class Unsupervised Anomaly Detection](https://arxiv.org/abs/2510.18342)
*Peng Tang,Xiaoxiao Yan,Xiaobin Hu,Yuning Cui,Donghao Luo,Jiangning Zhang,Pengcheng Xu,Jinlong Peng,Qingdong He,Feiyue Huang,Song Xue,Tobias Lasser*

Main category: cs.AI

TL;DR: 这篇论文提出了一种名为ShortcutBreaker的新型统一特征重建框架，用于多类无监督异常检测（MUAD）任务。它通过低秩噪声瓶颈和全局扰动注意力两种创新方法解决了现有模型中存在的身份快捷问题，并在多个基准数据集上显著优于以前的方法。


<details>
  <summary>Details</summary>
Motivation: 多类无监督异常检测（MUAD）旨在开发一个统一的模型，用于跨多个类别进行异常检测，从而消除为不同对象训练单独模型的需要，节省了大量的计算资源。尽管先进的基于Transformer的架构在此方面取得了显著的性能改进，但身份快捷问题依然存在：它们直接将输入复制到输出，缩小了正常和异常情况之间重建错误的差距，从而使两者更难区分。

Method: 本文提出ShortcutBreaker框架，其包含两个核心创新点：1. 设计了一个低秩噪声瓶颈（LRNB），利用矩阵秩不等式将高维特征投影到低秩潜在空间，以防止平凡的身份复制。2. 利用ViT的全局建模能力，引入全局扰动注意力，以防止解码器中的信息快捷。

Result: ShortcutBreaker在四个广泛使用的异常检测基准（MVTec-AD、ViSA、Real-IAD和Universal Medical）上进行了实验，并取得了显著的成果。在图像级别的AUROC评分分别为99.8%、98.9%、90.6%和87.8%，在不同场景下均持续超越了之前的MUAD方法。

Conclusion: ShortcutBreaker通过其创新的低秩噪声瓶颈和全局扰动注意力机制，有效解决了多类无监督异常检测中的身份快捷问题，显著提高了异常检测的性能，为MUAD任务提供了一个先进且统一的解决方案。

Abstract: Multi-class unsupervised anomaly detection (MUAD) has garnered growing
research interest, as it seeks to develop a unified model for anomaly detection
across multiple classes, i.e., eliminating the need to train separate models
for distinct objects and thereby saving substantial computational resources.
Under the MUAD setting, while advanced Transformer-based architectures have
brought significant performance improvements, identity shortcuts persist: they
directly copy inputs to outputs, narrowing the gap in reconstruction errors
between normal and abnormal cases, and thereby making the two harder to
distinguish. Therefore, we propose ShortcutBreaker, a novel unified
feature-reconstruction framework for MUAD tasks, featuring two key innovations
to address the issue of shortcuts. First, drawing on matrix rank inequality, we
design a low-rank noisy bottleneck (LRNB) to project highdimensional features
into a low-rank latent space, and theoretically demonstrate its capacity to
prevent trivial identity reproduction. Second, leveraging ViTs global modeling
capability instead of merely focusing on local features, we incorporate a
global perturbation attention to prevent information shortcuts in the decoders.
Extensive experiments are performed on four widely used anomaly detection
benchmarks, including three industrial datasets (MVTec-AD, ViSA, and Real-IAD)
and one medical dataset (Universal Medical). The proposed method achieves a
remarkable image-level AUROC of 99.8%, 98.9%, 90.6%, and 87.8% on these four
datasets, respectively, consistently outperforming previous MUAD methods across
different scenarios.

</details>


### [68] [Automated urban waterlogging assessment and early warning through a mixture of foundation models](https://arxiv.org/abs/2510.18425)
*Chenxu Zhang,Fuxiang Huang,Lei Zhang*

Main category: cs.AI

TL;DR: UWAssess是一个由基础模型驱动的框架，利用半监督微调和CoT提示策略，自动识别监控图像中的内涝区域并生成结构化评估报告，解决了数据稀缺问题，并在感知性能和报告生成方面表现出色。


<details>
  <summary>Details</summary>
Motivation: 现有城市内涝监测方法过度依赖人工报告，无法提供及时全面的评估，对全球公共安全和基础设施构成严重威胁。

Method: UWAssess框架，采用半监督微调策略和思维链（CoT）提示策略，以应对标记数据稀缺问题，并释放基础模型在数据稀缺下游任务中的潜力。

Result: 在具有挑战性的视觉基准测试中，感知性能得到了显著改善。基于GPT的评估证实，UWAssess能够生成可靠的文本报告，准确描述内涝的范围、深度、风险和影响。

Conclusion: UWAssess的双重能力实现了内涝监测从感知到生成的转变，多基础模型的协作框架为智能和可扩展系统奠定了基础，支持城市管理、灾害响应和气候韧性。

Abstract: With climate change intensifying, urban waterlogging poses an increasingly
severe threat to global public safety and infrastructure. However, existing
monitoring approaches rely heavily on manual reporting and fail to provide
timely and comprehensive assessments. In this study, we present Urban
Waterlogging Assessment (UWAssess), a foundation model-driven framework that
automatically identifies waterlogged areas in surveillance images and generates
structured assessment reports. To address the scarcity of labeled data, we
design a semi-supervised fine-tuning strategy and a chain-of-thought (CoT)
prompting strategy to unleash the potential of the foundation model for
data-scarce downstream tasks. Evaluations on challenging visual benchmarks
demonstrate substantial improvements in perception performance. GPT-based
evaluations confirm the ability of UWAssess to generate reliable textual
reports that accurately describe waterlogging extent, depth, risk and impact.
This dual capability enables a shift of waterlogging monitoring from perception
to generation, while the collaborative framework of multiple foundation models
lays the groundwork for intelligent and scalable systems, supporting urban
management, disaster response and climate resilience.

</details>


### [69] [AlphaOPT: Formulating Optimization Programs with Self-Improving LLM Experience Library](https://arxiv.org/abs/2510.18428)
*Minwei Kong,Ao Qu,Xiaotong Guo,Wenbin Ouyang,Chonghe Jiang,Han Zheng,Yining Ma,Dingyi Zhuang,Yuhan Tang,Junyi Li,Hai Wang,Cathy Wu,Jinhua Zhao*

Main category: cs.AI

TL;DR: AlphaOPT是一个自我完善的经验库，它能让大型语言模型（LLM）通过有限的示例和求解器反馈进行学习，而无需标注的推理痕迹或参数更新。


<details>
  <summary>Details</summary>
Motivation: 优化建模在各行业中都支持关键决策，但自动化难度大：非正式语言必须转换为精确的数学公式和可执行的求解器代码。现有的LLM方法依赖于脆弱的提示或昂贵的再训练，泛化能力有限。

Method: AlphaOPT在一个持续的两阶段循环中运行：(i) 库学习阶段，反思失败尝试，提取经过求解器验证的结构化知识（分类、条件、解释、示例）；(ii) 库演化阶段，诊断检索错位并优化存储知识的适用条件，提高跨任务的迁移能力。

Result: AlphaOPT的性能随数据量增加而稳步提升（从100到300个训练项，准确率从65%提高到72%），并且在仅用答案进行训练时，在超出分布的OptiBench数据集上，其性能超越了最强的基线7.7%。

Conclusion: AlphaOPT的设计使其能够从有限的示例中高效学习，无需人工整理的理由；通过更新库而不是模型权重，实现持续扩展而无需昂贵的再训练；并使知识明确且可解释，便于人工检查和干预。

Abstract: Optimization modeling enables critical decisions across industries but
remains difficult to automate: informal language must be mapped to precise
mathematical formulations and executable solver code. Prior LLM approaches
either rely on brittle prompting or costly retraining with limited
generalization. We present AlphaOPT, a self-improving experience library that
enables an LLM to learn from limited demonstrations (even answers alone,
without gold-standard programs) and solver feedback - without annotated
reasoning traces or parameter updates. AlphaOPT operates in a continual
two-phase cycle: (i) a Library Learning phase that reflects on failed attempts,
extracting solver-verified, structured insights as {taxonomy, condition,
explanation, example}; and (ii) a Library Evolution phase that diagnoses
retrieval misalignments and refines the applicability conditions of stored
insights, improving transfer across tasks. This design (1) learns efficiently
from limited demonstrations without curated rationales, (2) expands continually
without costly retraining by updating the library rather than model weights,
and (3) makes knowledge explicit and interpretable for human inspection and
intervention. Experiments show that AlphaOPT steadily improves with more data
(65% to 72% from 100 to 300 training items) and surpasses the strongest
baseline by 7.7% on the out-of-distribution OptiBench dataset when trained only
on answers. Code and data are available at:
https://github.com/Minw913/AlphaOPT.

</details>


### [70] [PlanU: Large Language Model Decision Making through Planning under Uncertainty](https://arxiv.org/abs/2510.18442)
*Ziwei Deng,Mian Deng,Chenjing Liang,Zeming Gao,Chennan Ma,Chenxing Lin,Haipeng Zhang,Songzhu Mei,Cheng Wang,Siqi Shen*

Main category: cs.AI

TL;DR: PlanU是一个基于LLM的规划方法，它通过在蒙特卡洛树搜索（MCTS）中捕获不确定性来解决LLM决策中的不确定性。


<details>
  <summary>Details</summary>
Motivation: LLM在不确定性下的决策任务中表现不佳，特别是在随机环境中的规划行动。现有方法未能充分解决LLM不确定性和环境不确定性，导致在随机状态转换环境中性能不佳。

Method: PlanU在MCTS中将每个节点的收益建模为分位数分布，使用一组分位数来表示收益分布。为了平衡树搜索中的探索和利用，PlanU引入了“好奇心上置信区间”（UCC）分数，用于估计MCTS节点的不确定性。

Result: 通过广泛的实验，PlanU在LLM不确定性下的决策任务中表现出有效性。

Conclusion: PlanU通过在MCTS中融入不确定性建模和探索-利用平衡机制，有效提升了LLM在不确定性决策任务中的性能。

Abstract: Large Language Models (LLMs) are increasingly being explored across a range
of decision-making tasks. However, LLMs sometimes struggle with decision-making
tasks under uncertainty that are relatively easy for humans, such as planning
actions in stochastic environments. The adoption of LLMs for decision-making is
impeded by uncertainty challenges, such as LLM uncertainty and environmental
uncertainty. LLM uncertainty arises from the stochastic sampling process
inherent to LLMs. Most LLM-based Decision-Making (LDM) approaches address LLM
uncertainty through multiple reasoning chains or search trees. However, these
approaches overlook environmental uncertainty, which leads to poor performance
in environments with stochastic state transitions. Some recent LDM approaches
deal with uncertainty by forecasting the probability of unknown variables.
However, they are not designed for multi-step decision-making tasks that
require interaction with the environment. To address uncertainty in LLM
decision-making, we introduce PlanU, an LLM-based planning method that captures
uncertainty within Monte Carlo Tree Search (MCTS). PlanU models the return of
each node in the MCTS as a quantile distribution, which uses a set of quantiles
to represent the return distribution. To balance exploration and exploitation
during tree search, PlanU introduces an Upper Confidence Bounds with Curiosity
(UCC) score which estimates the uncertainty of MCTS nodes. Through extensive
experiments, we demonstrate the effectiveness of PlanU in LLM-based
decision-making tasks under uncertainty.

</details>


### [71] [CircuitSeer: Mining High-Quality Data by Probing Mathematical Reasoning Circuits in LLMs](https://arxiv.org/abs/2510.18470)
*Shaobo Wang,Yongliang Miao,Yuancheng Liu,and Qianli Ma,Ning Liao,Linfeng Zhang*

Main category: cs.AI

TL;DR: CircuitSeer是一种新的数据选择方法，通过测量数据对法现和核心推理电路的影响来量化数据的推理复杂度。


<details>
  <summary>Details</summary>
Motivation: 目前，现有数据选择方法通常依赖于昂贵的外部模型或缺乏透明度的启发式方法，而本文旨在将焦点从外部启发式方法转移到模型的内部机制。

Method: 通过识别大型语言模型在处理复杂推理任务时激活的稀疏、专用注意力头子集（即核心推理电路），并提出CircuitSeer方法，通过测量数据对这些关键电路的影响来量化数据推理复杂性进行数据选择。

Result: 在4个模型和9个数据集上的大量实验证明了CircuitSeer的优越性，将Qwen2.5-Math-7B在仅10%的数据上进行微调，平均Pass@1比在完整数据集上训练提高了1.4个百分点。

Conclusion: CircuitSeer通过分析和利用LLMs内部的推理机制，提供了一种高效且有效的数据选择范式克服了现有方法的局限性，并可以显著提高模型在计算资源有限下的性能。

Abstract: Large language models (LLMs) have demonstrated impressive reasoning
capabilities, but scaling their performance often relies on massive reasoning
datasets that are computationally expensive to train on. Existing data
selection methods aim to curate smaller, high-quality subsets but often rely on
costly external models or opaque heuristics. In this work, we shift the focus
from external heuristics to the model's internal mechanisms. We find that
complex reasoning tasks consistently activate a sparse, specialized subset of
attention heads, forming core reasoning circuits. Building on this insight, we
propose CircuitSeer, a novel data selection method that quantifies the
reasoning complexity of data by measuring its influence on these crucial
circuits. Extensive experiments on 4 models and 9 datasets demonstrate
CircuitSeer's superiority. Notably, fine-tuning Qwen2.5-Math-7B on just 10% of
data selected by our method achieves a 1.4-point gain in average Pass@1 over
training on the full dataset, highlighting its efficiency and effectiveness.

</details>


### [72] [Probabilistic Modeling of Intentions in Socially Intelligent LLM Agents](https://arxiv.org/abs/2510.18476)
*Feifan Xia,Yuyang Fang,Defang Li,Yantong Xie,Weikang Li,Yang Li,Deguo Xia,Jizhou Huang*

Main category: cs.AI

TL;DR: 该论文提出了一个用于多轮社交对话中大型语言模型（LLM）智能体的概率意图建模框架。


<details>
  <summary>Details</summary>
Motivation: 在多轮社交对话中，LLM智能体需要理解并跟踪对话伙伴的潜在意图，以便制定自适应的对话策略。现有的方法可能在不确定性下难以有效地进行意图推理。

Method: 本文提出了一种概率意图建模框架。该框架维护一个关于对话伙伴潜在意图的信念分布，该分布由上下文先验初始化，并在每次对话后通过似然估计动态更新。这一演化的分布为策略提供了额外的上下文基础。

Result: 在SOTOPIA环境中的初步实验表明，该框架持续改进了性能：与Qwen2.5-7B基线相比，SOTOPIA-All上的总体得分提高了9.0%，SOTOPIA-Hard上提高了4.1%。此外，它略微超越了可以直接观察伙伴意图的预言者智能体。

Conclusion: 初步结果表明，概率意图建模有助于开发具有社交智能的LLM智能体，通过在不确定性下提供自适应的对话策略。

Abstract: We present a probabilistic intent modeling framework for large language model
(LLM) agents in multi-turn social dialogue. The framework maintains a belief
distribution over a partner's latent intentions, initialized from contextual
priors and dynamically updated through likelihood estimation after each
utterance. The evolving distribution provides additional contextual grounding
for the policy, enabling adaptive dialogue strategies under uncertainty.
Preliminary experiments in the SOTOPIA environment show consistent
improvements: the proposed framework increases the Overall score by 9.0% on
SOTOPIA-All and 4.1% on SOTOPIA-Hard compared with the Qwen2.5-7B baseline, and
slightly surpasses an oracle agent that directly observes partner intentions.
These early results suggest that probabilistic intent modeling can contribute
to the development of socially intelligent LLM agents.

</details>


### [73] [StarBench: A Turn-Based RPG Benchmark for Agentic Multimodal Decision-Making and Information Seeking](https://arxiv.org/abs/2510.18483)
*Haoran Zhang,Chenhao Zhu,Sicong Guo,Hanzhe Guo,Haiming Li,Donglin Yu*

Main category: cs.AI

TL;DR: StarBench是一个基于《崩坏：星穹铁道》的回合制RPG基准测试，旨在评估视觉语言模型在像素到动作多模态决策和智能信息搜索方面的能力。


<details>
  <summary>Details</summary>
Motivation: 目前的视觉语言模型在像人类一样玩游戏方面存在挑战，即从原始屏幕截图到时间连贯的低级动作的映射，以及在遇到困难时寻求信息。

Method: StarBench提供了8个战斗任务和两种模式（直接控制和工具辅助控制）来标准化评估。此外，它还包括一个“询问或行动”诊断，以衡量智能体何时选择请求指导以及这种选择如何影响后续性能。

Result: 当代视觉语言模型与人类参考之间存在显著差距，尤其是在直接控制模式下从感知到控制的保真度方面。结果还表明，明智地寻求信息与成功率的提高相关。

Conclusion: StarBench为智能信息搜索和多模态决策提供了一个可复现的评估标准，揭示了当前VLM在处理复杂游戏环境中的不足。

Abstract: Human players do more than press buttons: they ground what they see on screen
into precise keyboard-mouse actions and, when stuck, they seek information
before trying again. We ask whether current vision-language models (VLMs) can
do the same. Despite encouraging results under simplified control or tool
scaffolds, human-like play in a real client - mapping raw screenshots to
temporally coherent low-level actions while deciding when to ask for guidance -
remains an open challenge. We introduce StarBench, a turn-based RPG benchmark
derived from Honkai: Star Rail that targets these two human-like competencies:
multimodal decision-making from pixels to actions and agentic information
seeking. StarBench standardizes evaluation across eight combat tasks and two
regimes with shared tasks and metrics: (i) direct control, where agents receive
only screenshots and must emit low-level primitives (click and keypress) with
no semantic hints; and (ii) tool-assisted control, where higher-level intents
can be mapped to primitives by detectors and OCR outputs provide optional
textualized observations to ease UI grounding. To mirror human practice,
StarBench also includes an ask-or-act diagnostic that measures whether and when
agents choose to request brief guidance before proceeding, and how that choice
affects subsequent performance. We report reference baselines for contemporary
VLMs and a human reference. Results expose sizable gaps in
perception-to-control fidelity in the direct regime, while showing that
judicious information seeking correlates with improved success, establishing
StarBench as a reproducible yardstick for agentic information seeking and
multimodal decision-making in real-client play.

</details>


### [74] [Crucible: Quantifying the Potential of Control Algorithms through LLM Agents](https://arxiv.org/abs/2510.18491)
*Lianchen Jia,Chaoyang Li,Qian Houde,Tianchi Huang,Jiangchuan Liu,Lifeng Sun*

Main category: cs.AI

TL;DR: Crucible是一个LLM驱动的智能体，它通过多级专家模拟来调整算法，并量化评估算法的调整潜力。


<details>
  <summary>Details</summary>
Motivation: 现有的研究主要关注理想或默认配置下的算法性能，忽略了调整潜力这一关键方面。

Method: Crucible通过一个LLM驱动的多级专家模拟来调整算法，并定义了一个形式化的度量来定量评估它们的调整潜力。

Result: Crucible在广泛的案例研究中有效，从经典的控制任务到复杂的计算机系统，并在实际部署中验证了其发现。实验结果表明，Crucible系统地量化了不同算法的可调空间。

Conclusion: Crucible为算法分析和设计提供了一个新的维度，最终导致了性能改进。

Abstract: Control algorithms in production environments typically require domain
experts to tune their parameters and logic for specific scenarios. However,
existing research predominantly focuses on algorithmic performance under ideal
or default configurations, overlooking the critical aspect of Tuning Potential.
To bridge this gap, we introduce Crucible, an agent that employs an LLM-driven,
multi-level expert simulation to turn algorithms and defines a formalized
metric to quantitatively evaluate their Tuning Potential. We demonstrate
Crucible's effectiveness across a wide spectrum of case studies, from classic
control tasks to complex computer systems, and validate its findings in a
real-world deployment. Our experimental results reveal that Crucible
systematically quantifies the tunable space across different algorithms.
Furthermore, Crucible provides a new dimension for algorithm analysis and
design, which ultimately leads to performance improvements. Our code is
available at https://github.com/thu-media/Crucible.

</details>


### [75] [Counterfactual Reasoning for Steerable Pluralistic Value Alignment of Large Language Models](https://arxiv.org/abs/2510.18526)
*Hanze Guo,Jing Yao,Xiao Zhou,Xiaoyuan Yi,Xing Xie*

Main category: cs.AI

TL;DR: 该论文提出了COUPLE框架，利用因果推理处理大型语言模型中多元价值观的对齐问题。


<details>
  <summary>Details</summary>
Motivation: 大型语言模型在应用于不同文化和人群时，需要与多元化的人类价值观对齐。现有方法在处理复杂和细致的价值观优先级时面临挑战，特别是在理解价值观的相互依赖性以及精确控制不具代表性的价值观方面存在不足。

Method: COUPLE框架引入了一个结构因果模型（SCM），用于描述高层价值维度与行为之间的复杂相互依赖性、优先级排序以及因果关系。此外，它还应用反事实推理来生成符合任何期望价值目标的输出。

Result: COUPLE在两个具有不同价值体系的数据集上进行了评估，结果表明，在各种价值目标方面，COUPLE优于其他基线方法。

Conclusion: COUPLE框架通过明确的因果建模和反事实推理，解决了现有方法在处理多元价值观对齐时的挑战，并提供了更好的可解释性，有效提升了大型语言模型在多元价值观对齐方面的性能。

Abstract: As large language models (LLMs) become increasingly integrated into
applications serving users across diverse cultures, communities and
demographics, it is critical to align LLMs with pluralistic human values beyond
average principles (e.g., HHH). In psychological and social value theories such
as Schwartz's Value Theory, pluralistic values are represented by multiple
value dimensions paired with various priorities. However, existing methods
encounter two challenges when aligning with such fine-grained value objectives:
1) they often treat multiple values as independent and equally important,
ignoring their interdependence and relative priorities (value complexity); 2)
they struggle to precisely control nuanced value priorities, especially those
underrepresented ones (value steerability). To handle these challenges, we
propose COUPLE, a COUnterfactual reasoning framework for PLuralistic valuE
alignment. It introduces a structural causal model (SCM) to feature complex
interdependency and prioritization among features, as well as the causal
relationship between high-level value dimensions and behaviors. Moreover, it
applies counterfactual reasoning to generate outputs aligned with any desired
value objectives. Benefitting from explicit causal modeling, COUPLE also
provides better interpretability. We evaluate COUPLE on two datasets with
different value systems and demonstrate that COUPLE advances other baselines
across diverse types of value objectives.

</details>


### [76] [Extracting alignment data in open models](https://arxiv.org/abs/2510.18554)
*Federico Barbero,Xiangming Gu,Christopher A. Choquette-Choo,Chawin Sitawarin,Matthew Jagielski,Itay Yona,Petar Veličković,Ilia Shumailov,Jamie Hayes*

Main category: cs.AI

TL;DR: 本文展示了从后训练模型中提取大量对齐训练数据的可能性，这些数据可用于改进模型的长文本推理、安全性、指令遵循和数学能力。


<details>
  <summary>Details</summary>
Motivation: 以往关于记忆化的大部分工作都侧重于通过字符串匹配来衡量训练数据提取的成功率，但我们认为嵌入模型更适合我们的特定目标。高质量嵌入模型测量的距离可以识别字符串之间的语义相似性，而编辑距离等不同指标难以捕捉。

Method: 本文提出了一种新的数据提取方法，该方法基于高质量的嵌入模型来识别语义相似性，而不是传统的字符串匹配。

Result: 我们发现，模型会很容易地重复SFT或RL等后训练阶段使用的训练数据。这些数据可以用于训练一个基础模型，恢复了原有性能的很大一部分。在我们的调查中，近似字符串匹配会严重低估（保守估计为10倍）可提取的数据量。

Conclusion: 这项工作揭示了提取对齐数据可能被忽视的风险，并引发了关于蒸馏实践下游影响的有趣讨论：由于模型似乎在重复其训练集，因此蒸馏可以被认为是间接训练模型的原始数据集。

Abstract: In this work, we show that it is possible to extract significant amounts of
alignment training data from a post-trained model -- useful to steer the model
to improve certain capabilities such as long-context reasoning, safety,
instruction following, and maths. While the majority of related work on
memorisation has focused on measuring success of training data extraction
through string matching, we argue that embedding models are better suited for
our specific goals. Distances measured through a high quality embedding model
can identify semantic similarities between strings that a different metric such
as edit distance will struggle to capture. In fact, in our investigation,
approximate string matching would have severely undercounted (by a conservative
estimate of $10\times$) the amount of data that can be extracted due to trivial
artifacts that deflate the metric. Interestingly, we find that models readily
regurgitate training data that was used in post-training phases such as SFT or
RL. We show that this data can be then used to train a base model, recovering a
meaningful amount of the original performance. We believe our work exposes a
possibly overlooked risk towards extracting alignment data. Finally, our work
opens up an interesting discussion on the downstream effects of distillation
practices: since models seem to be regurgitating aspects of their training set,
distillation can therefore be thought of as indirectly training on the model's
original dataset.

</details>


### [77] [QuantEvolve: Automating Quantitative Strategy Discovery through Multi-Agent Evolutionary Framework](https://arxiv.org/abs/2510.18569)
*Junhyeog Yun,Hyoun Jun Lee,Insu Jeon*

Main category: cs.AI

TL;DR: QuantEvolve是一个结合了质量-多样性优化和假设驱动策略生成的进化框架，能够自动化生成适应动态市场和个性化投资需求的量化交易策略。


<details>
  <summary>Details</summary>
Motivation: 在动态市场中自动化开发量化交易策略具有挑战性，尤其是在个性化投资方案需求日益增长的情况下，现有方法难以探索广阔的策略空间并保持多样性以应对市场变化。

Method: QuantEvolve框架结合了质量-多样性优化和假设驱动的策略生成。它使用与投资者偏好（如策略类型、风险状况、周转率和回报特征）对齐的特征图来维持多样化的有效策略集，并集成了假设驱动的多智能体系统，通过迭代生成和评估系统地探索策略空间。

Result: QuantEvolve产生了多样化、复杂的策略，能够适应市场机制变化和个体投资需求。实证结果表明，QuantEvolve优于传统的基线方法。

Conclusion: QuantEvolve通过结合质量-多样性优化和假设驱动策略生成，成功解决了在动态市场中自动化生成个性化量化交易策略的挑战，其有效性已通过实证验证。

Abstract: Automating quantitative trading strategy development in dynamic markets is
challenging, especially with increasing demand for personalized investment
solutions. Existing methods often fail to explore the vast strategy space while
preserving the diversity essential for robust performance across changing
market conditions. We present QuantEvolve, an evolutionary framework that
combines quality-diversity optimization with hypothesis-driven strategy
generation. QuantEvolve employs a feature map aligned with investor
preferences, such as strategy type, risk profile, turnover, and return
characteristics, to maintain a diverse set of effective strategies. It also
integrates a hypothesis-driven multi-agent system to systematically explore the
strategy space through iterative generation and evaluation. This approach
produces diverse, sophisticated strategies that adapt to both market regime
shifts and individual investment needs. Empirical results show that QuantEvolve
outperforms conventional baselines, validating its effectiveness. We release a
dataset of evolved strategies to support future research.

</details>


### [78] [VAR: Visual Attention Reasoning via Structured Search and Backtracking](https://arxiv.org/abs/2510.18619)
*Wei Cai,Jian Zhao,Yuchen Yuan,Tianle Zhang,Ming Zhu,Haichuan Tang,Chi Zhang,Xuelong Li*

Main category: cs.AI

TL;DR: 本文介绍了一种名为视觉注意力推理（VAR）的新型框架，旨在解决多模态大型语言模型（MLLM）中幻觉和对脆性线性推理过程过度依赖的问题。


<details>
  <summary>Details</summary>
Motivation: 多模态大型语言模型（MLLM）存在高幻觉倾向和对脆性、线性推理过程的严重依赖，导致在复杂任务中失败。

Method: VAR框架将基础推理重构为推理轨迹空间上的结构化搜索。它将推理过程分解为两个关键阶段：可追溯的证据基础和基于搜索的思维链（CoT）生成，其中包含用于自我校正的回溯机制。搜索由一个多方面的奖励函数指导，该函数具有语义和几何自验证组件，惩罚不忠实于视觉输入的输出。

Result: VAR-7B（7B模型）在幻觉和安全基准测试套件上取得了最先进的SOTA结果，显著优于现有开源模型，并与领先的专有系统表现出有竞争力的性能。

Conclusion: VAR框架通过将基础推理重构为推理轨迹空间上的结构化搜索，并引入了可追溯的证据基础和带有回溯机制的基于搜索的思维链生成，成功解决了MLLM的幻觉问题和对线性推理的依赖，从而在性能上超越了现有模型。

Abstract: Multimodal Large Language Models (MLLMs), despite their advances, are
hindered by their high hallucination tendency and heavy reliance on brittle,
linear reasoning processes, leading to failures in complex tasks. To address
these limitations, we introduce Visual Attention Reasoning (VAR), a novel
framework that recasts grounded reasoning as a structured search over a
reasoning trajectory space. VAR decomposes the reasoning process into two key
stages: traceable evidence grounding and search-based chain-of-thought (CoT)
generation, which incorporates a backtracking mechanism for self-correction.
The search is guided by a multi-faceted reward function with semantic and
geometric self-verification components, which penalize outputs that are not
faithfully grounded in the visual input. We provide a theoretical analysis for
our search strategy, validating its capability to find the correct solution
with high probability. Experimental results show that our 7B model, VAR-7B,
sets a new state-of-the-art on a comprehensive suite of hallucination and
safety benchmarks, significantly outperforming existing open-source models and
demonstrating competitive performance against leading proprietary systems.

</details>


### [79] [Leveraging Association Rules for Better Predictions and Better Explanations](https://arxiv.org/abs/2510.18628)
*Gilles Audemard,Sylvie Coste-Marquis,Pierre Marquis,Mehdi Sabiri,Nicolas Szczepanski*

Main category: cs.AI

TL;DR: 该文章提出了一种结合数据和知识的分类方法，通过数据挖掘关联规则来提高决策树和随机森林的预测性能和可解释性。


<details>
  <summary>Details</summary>
Motivation: 结合数据和知识，提高分类模型的预测性能和解释能力。

Method: 1. 利用数据挖掘从数据中提取关联规则（可能包含否定）。 2. 将这些关联规则融入到决策树和随机森林等基于树的模型中，以提高其预测性能。 3. 利用关联规则生成更通用的溯因解释，以改进模型的可解释性。

Result: 实验结果表明，该方法在预测性能和解释大小方面均能为决策树和随机森林带来益处。

Conclusion: 所提出的方法成功地将数据和知识相结合，在提高分类模型预测性能的同时，也增强了模型的可解释性。

Abstract: We present a new approach to classification that combines data and knowledge.
In this approach, data mining is used to derive association rules (possibly
with negations) from data. Those rules are leveraged to increase the predictive
performance of tree-based models (decision trees and random forests) used for a
classification task. They are also used to improve the corresponding
explanation task through the generation of abductive explanations that are more
general than those derivable without taking such rules into account.
Experiments show that for the two tree-based models under consideration,
benefits can be offered by the approach in terms of predictive performance and
in terms of explanation sizes.

</details>


### [80] [Comparative Expressivity for Structured Argumentation Frameworks with Uncertain Rules and Premises](https://arxiv.org/abs/2510.18631)
*Carlo Proietti,Antonio Yuste-Ginel*

Main category: cs.AI

TL;DR: 这篇论文研究了在形式化论证中建模定性不确定性的问题，重点关注如何将抽象模型实例化，并比较了抽象和结构化论证模型在不确定性方面的表达能力。


<details>
  <summary>Details</summary>
Motivation: 在形式化论证中建模定性不确定性对实际应用和理论理解都至关重要，但现有工作大多集中于处理不确定性的抽象模型。本文旨在解决如何实例化这些抽象模型这一开放问题。

Method: 通过将论证的不确定性根植于其组成部分（在规则和前提中构建），引入了可以处理抽象和结构化形式的表达能力概念，并提出了比较抽象和结构化不确定性论证模型表达能力的负面和正面表达能力结果。

Result: 研究结果影响了抽象方面的不完整抽象论证框架及其依赖扩展，以及结构化方面的ASPIC+。

Conclusion: 本文通过引入一个能够处理抽象和结构化形式的表达能力概念，并提出了负面和正面的表达能力结果，比较了具有不确定性的抽象和结构化论证模型的表达能力，从而成功地将论证的不确定性根植于其组成部分。

Abstract: Modelling qualitative uncertainty in formal argumentation is essential both
for practical applications and theoretical understanding. Yet, most of the
existing works focus on \textit{abstract} models for arguing with uncertainty.
Following a recent trend in the literature, we tackle the open question of
studying plausible instantiations of these abstract models. To do so, we ground
the uncertainty of arguments in their components, structured within rules and
premises. Our main technical contributions are: i) the introduction of a notion
of expressivity that can handle abstract and structured formalisms, and ii) the
presentation of both negative and positive expressivity results, comparing the
expressivity of abstract and structured models of argumentation with
uncertainty. These results affect incomplete abstract argumentation frameworks,
and their extension with dependencies, on the abstract side, and ASPIC+, on the
structured side.

</details>


### [81] [Query Decomposition for RAG: Balancing Exploration-Exploitation](https://arxiv.org/abs/2510.18633)
*Roxana Petcu,Kenton Murray,Daniel Khashabi,Evangelos Kanoulas,Maarten de Rijke,Dawn Lawrie,Kevin Duh*

Main category: cs.AI

TL;DR: 该论文介绍了一种通过“利用-探索”的思维框架，有效地从文档中检索信息以回答复杂的用户查询。


<details>
  <summary>Details</summary>
Motivation: 在RAG系统中，为了有效地选择信息丰富的文档，需要在广泛检索和避免噪声与计算成本之间取得平衡。

Method: 本文将查询分解和文档检索问题，建模为一个利用-探索（exploitation-exploration）的强化学习场景。每次检索一个文档，都会建立一个关于给定子查询效用的信念，并根据这个信念决定是继续利用当前策略还是探索替代策略。文章试验了多种Bandit学习方法。

Result: 通过使用Bandit学习方法，该方法在文档级精度方面提高了35%，\alpha-nDCG提高了15%，并且在长文本生成任务上的表现也更好。

Conclusion: 利用Bandit学习方法，通过估计文档相关性，在RAG系统中动态选择信息最丰富的子查询是有效的，并且可以显著提升检索效率和下游任务的表现。

Abstract: Retrieval-augmented generation (RAG) systems address complex user requests by
decomposing them into subqueries, retrieving potentially relevant documents for
each, and then aggregating them to generate an answer. Efficiently selecting
informative documents requires balancing a key trade-off: (i) retrieving
broadly enough to capture all the relevant material, and (ii) limiting
retrieval to avoid excessive noise and computational cost. We formulate query
decomposition and document retrieval in an exploitation-exploration setting,
where retrieving one document at a time builds a belief about the utility of a
given sub-query and informs the decision to continue exploiting or exploring an
alternative. We experiment with a variety of bandit learning methods and
demonstrate their effectiveness in dynamically selecting the most informative
sub-queries. Our main finding is that estimating document relevance using rank
information and human judgments yields a 35% gain in document-level precision,
15% increase in {\alpha}-nDCG, and better performance on the downstream task of
long-form generation.

</details>


### [82] [Sherlock Your Queries: Learning to Ask the Right Questions for Dialogue-Based Retrieval](https://arxiv.org/abs/2510.18659)
*Dong Yun,Marco Schouten,Dim Papadopoulos*

Main category: cs.AI

TL;DR: SherlockLLM是一个对话驱动的检索框架，它通过强化学习来学习最优的提问策略，以有效地缩小搜索空间，并在结构化和非结构化任务中表现出色。


<details>
  <summary>Details</summary>
Motivation: 在信息检索中，用户查询通常是模糊的，使得系统难以从单个查询中识别用户的目标。现有的对话式交互检索系统虽然能澄清用户意图，但效率低下，因为它们缺乏明确的策略来提出最具信息量的问题。

Method: SherlockLLM是一个对话驱动的检索框架，通过强化学习（RL）学习最优的提问策略，避免了大规模标注对话数据的需求。该框架训练一个智能体生成一系列二元问题，以有效地缩小搜索空间。

Result: 实验结果表明，SherlockLLM是一个鲁棒且高效的解决方案。在结构化任务上，其性能与强大的基线系统相当，并接近二分查找定义的理论最优。在具有挑战性的非结构化任务上，我们的智能体显著优于这些基线，展示了其学习高效信息寻找对话策略的能力。

Conclusion: SherlockLLM通过强化学习学习最优提问策略，有效解决了信息检索中用户查询模糊的问题，并在各种任务中表现出优越的性能和效率，有望提升交互式检索系统的用户体验。

Abstract: User queries in information retrieval are often ambiguous, making it
challenging for systems to identify a user's target from a single query. While
recent dialogue-based interactive retrieval systems can clarify user intent,
they are inefficient as they often lack an explicit strategy to ask the most
informative questions. To address this limitation, we propose SherlockLLM, a
dialogue-driven retrieval framework that learns an optimal questioning strategy
via Reinforcement Learning (RL) and avoids the need for large-scale annotated
dialogue data. In our framework, an agent is trained to generate a sequence of
binary questions to efficiently narrow down the search space. To validate our
approach, we introduce a benchmark with both structured and unstructured tasks.
Experimental results show that SherlockLLM is a robust and efficient solution.
On the structured tasks, its performance matches strong baselines and
approaches the theoretical optimal defined by binary search. On the challenging
unstructured task, our agent significantly outperforms these baselines,
showcasing its ability to learn a highly effective information-seeking dialogue
policy.

</details>


### [83] [Seg the HAB: Language-Guided Geospatial Algae Bloom Reasoning and Segmentation](https://arxiv.org/abs/2510.18751)
*Patterson Hsieh,Jerry Yeh,Mao-Chi He,Wen-Han Hsieh,Elvis Hsieh*

Main category: cs.AI

TL;DR: 本文介绍了ALGOS，一个用于有害藻华（HAB）监测的图像分割与推理系统，它结合了遥感图像理解和严重程度估计，实现了对藻华的鲁棒分割和严重程度预测，为自动化监测系统铺平了道路。


<details>
  <summary>Details</summary>
Motivation: 当前有害藻华（HAB）监测方法存在劳动密集、时空覆盖有限等问题，而遥感视觉语言模型（VLMs）在遥感领域的进展为AI驱动的解决方案提供了潜力，但图像推理和藻华严重程度量化仍面临挑战。

Method: ALGOS系统结合了遥感图像理解和严重程度估计。它利用GeoSAM辅助人工评估来筛选高质量的分割掩模，并使用NASA的蓝藻聚合手动标签（CAML）微调视觉语言模型进行严重程度预测。

Result: 实验证明，ALGOS在分割和严重程度估计方面都取得了鲁棒的性能。

Conclusion: ALGOS为实用和自动化的蓝藻监测系统奠定了基础。

Abstract: Climate change is intensifying the occurrence of harmful algal bloom (HAB),
particularly cyanobacteria, which threaten aquatic ecosystems and human health
through oxygen depletion, toxin release, and disruption of marine biodiversity.
Traditional monitoring approaches, such as manual water sampling, remain
labor-intensive and limited in spatial and temporal coverage. Recent advances
in vision-language models (VLMs) for remote sensing have shown potential for
scalable AI-driven solutions, yet challenges remain in reasoning over imagery
and quantifying bloom severity. In this work, we introduce ALGae Observation
and Segmentation (ALGOS), a segmentation-and-reasoning system for HAB
monitoring that combines remote sensing image understanding with severity
estimation. Our approach integrates GeoSAM-assisted human evaluation for
high-quality segmentation mask curation and fine-tunes vision language model on
severity prediction using the Cyanobacteria Aggregated Manual Labels (CAML)
from NASA. Experiments demonstrate that ALGOS achieves robust performance on
both segmentation and severity-level estimation, paving the way toward
practical and automated cyanobacterial monitoring systems.

</details>


### [84] [Decoding Funded Research: Comparative Analysis of Topic Models and Uncovering the Effect of Gender and Geographic Location](https://arxiv.org/abs/2510.18803)
*Shirin Tavakoli Kafiabad,Andrea Schiffauerova,Ashkan Ebadi*

Main category: cs.AI

TL;DR: 这篇论文分析了加拿大自然科学和工程研究理事会（NSERC）18年（2005-2022）的研究提案，旨在通过比较三种主题建模方法（LDA、STM和BERTopic）来理解研究趋势和人口地理因素。


<details>
  <summary>Details</summary>
Motivation: 优化国家科研投入，需要清楚了解不断演变的研究趋势以及影响这些趋势的人口和地理因素，尤其是在公平、多样性和包容性承诺的背景下。

Method: 分析了2005年至2022年加拿大自然科学和工程研究理事会（NSERC）资助的18年研究提案。比较评估了三种主题建模方法：潜在狄利克雷分配（LDA）、结构化主题建模（STM）和BERTopic。引入了一种名为COFFEE的新算法，用于BERTopic的协变量效应估计。

Result: BERTopic在识别更细致、连贯和新兴主题方面表现出色，例如人工智能的快速发展。COFFEE驱动的协变量分析证实了独特的省级研究专业化，并揭示了不同科学学科中基于性别的 H 主题模式。

Conclusion: 本研究为资助机构制定更公平、更有影响力的资助策略提供了坚实的经验基础，从而提高科学生态系统的效率。

Abstract: Optimizing national scientific investment requires a clear understanding of
evolving research trends and the demographic and geographical forces shaping
them, particularly in light of commitments to equity, diversity, and inclusion.
This study addresses this need by analyzing 18 years (2005-2022) of research
proposals funded by the Natural Sciences and Engineering Research Council of
Canada (NSERC). We conducted a comprehensive comparative evaluation of three
topic modelling approaches: Latent Dirichlet Allocation (LDA), Structural Topic
Modelling (STM), and BERTopic. We also introduced a novel algorithm, named
COFFEE, designed to enable robust covariate effect estimation for BERTopic.
This advancement addresses a significant gap, as BERTopic lacks a native
function for covariate analysis, unlike the probabilistic STM. Our findings
highlight that while all models effectively delineate core scientific domains,
BERTopic outperformed by consistently identifying more granular, coherent, and
emergent themes, such as the rapid expansion of artificial intelligence.
Additionally, the covariate analysis, powered by COFFEE, confirmed distinct
provincial research specializations and revealed consistent gender-based
thematic patterns across various scientific disciplines. These insights offer a
robust empirical foundation for funding organizations to formulate more
equitable and impactful funding strategies, thereby enhancing the effectiveness
of the scientific ecosystem.

</details>


<div id='stat.ML'></div>

# stat.ML [[Back]](#toc)

### [85] [Graphical model for tensor factorization by sparse sampling](https://arxiv.org/abs/2510.17886)
*Angelo Giorgio,Riki Nagasawa,Shuta Yokoi,Tomoyuki Obuchi,Hajime Yoshino*

Main category: stat.ML

TL;DR: 本文探讨了在张量分中，当张量分量的测量数据稀疏且测量过程基于随机图时，如何进行统计推断。


<details>
  <summary>Details</summary>
Motivation: 在推荐系统等数据缺失严重的场景中，传统的张量分解方法可能效果不佳。该研究旨在为这类场景提供一种新的理论分析框架，以更好地理解和处理稀疏数据下的张量分解问题。

Method: 论文提出了一种在高维极限（即“密集极限”）下分析张量分解中的统计推断的方法。具体而言，该方法通过构建消息传递算法，并在贝叶斯最优的教师-学生设置中进行测试。此外，论文还发展了副本理论来精确分析密集极限下的统计推断性能。

Result: 研究结果表明，通过消息传递算法和副本理论，可以在高维且稀疏测量数据下有效地进行张量分解的统计推断。

Conclusion: 本文成功地为具有稀疏测量值的张量分解提供了一种在高维极限下的统计推断方法，并通过理论分析和算法验证了其有效性，为处理推荐系统等实际应用中的数据稀疏问题提供了新的视角和工具。

Abstract: We consider tensor factorizations based on sparse measurements of the tensor
components. The measurements are designed in a way that the underlying graph of
interactions is a random graph. The setup will be useful in cases where a
substantial amount of data is missing, as in recommendation systems heavily
used in social network services. In order to obtain theoretical insights on the
setup, we consider statistical inference of the tensor factorization in a high
dimensional limit, which we call as dense limit, where the graphs are large and
dense but not fully connected. We build message-passing algorithms and test
them in a Bayes optimal teacher-student setting. We also develop a replica
theory, which becomes exact in the dense limit,to examine the performance of
statistical inference.

</details>


### [86] [Arbitrated Indirect Treatment Comparisons](https://arxiv.org/abs/2510.18071)
*Yixin Fang,Weili He*

Main category: stat.ML

TL;DR: 本文介绍了一种新的方法，旨在解决现有匹配调整间接比较（MAIC）方法中不同赞助商可能得出冲突结论的“MAIC悖论”。该方法通过定义一个共同的目标人群，即重叠人群，来估计治疗效果，从而解决不同赞助商隐式 targeting 不同人群的问题。


<details>
  <summary>Details</summary>
Motivation: 匹配调整间接比较（MAIC）在健康技术评估（HTA）中应用日益广泛，通过对个体患者数据（IPD）进行再加权以匹配汇总数据（AgD）的协变量统计量，从而估计治疗效果。然而，现有的MAIC方法存在“MAIC悖论”，即不同赞助商在分析相同数据时可能得出关于哪种治疗更有效的冲突结论。

Method: 本文提出了一类新的方法，称为仲裁间接治疗比较，旨在解决“MAIC悖论”。该方法通过估计在共同目标人群（特别是重叠人群）中的治疗效果，来解决不同赞助商隐式 targeting 不同人群的问题，从而避免了结论冲突。

Result: 通过采用仲裁间接治疗比较方法，可以避免在MAIC分析中因不同赞助商隐式 targeting 不同人群而导致的冲突结论问题。该方法确保所有赞助商针对共同的目标人群（重叠人群）进行治疗效果估计，从而提高比较结果的一致性。

Conclusion: 本文提出的仲裁间接治疗比较方法有效地解决了MAIC分析中的“MAIC悖论”，确保了不同赞助商在分析相同数据时能得出一致的治疗效果结论。这种方法通过聚焦于一个共同的重叠人群来估计治疗效果，增强了HTA结果的可靠性和可解释性。

Abstract: Matching-adjusted indirect comparison (MAIC) has been increasingly employed
in health technology assessments (HTA). By reweighting subjects from a trial
with individual participant data (IPD) to match the covariate summary
statistics of another trial with only aggregate data (AgD), MAIC facilitates
the estimation of a treatment effect defined with respect to the AgD trial
population. This manuscript introduces a new class of methods, termed
arbitrated indirect treatment comparisons, designed to address the ``MAIC
paradox'' -- a phenomenon highlighted by Jiang et al.~(2025). The MAIC paradox
arises when different sponsors, analyzing the same data, reach conflicting
conclusions regarding which treatment is more effective. The underlying issue
is that each sponsor implicitly targets a different population. To resolve this
inconsistency, the proposed methods focus on estimating treatment effects in a
common target population, specifically chosen to be the overlap population.

</details>


### [87] [Beating the Winner's Curse via Inference-Aware Policy Optimization](https://arxiv.org/abs/2510.18161)
*Hamsa Bastani,Osbert Bastani,Bryce McLaughlin*

Main category: stat.ML

TL;DR: 本文提出了一种“推断感知策略优化”的新策略，可以解决“赢家诅咒”问题。该方法将策略优化与下游评估方式相结合，平衡了估计目标值和策略显著优于观测策略的概率。


<details>
  <summary>Details</summary>
Motivation: 在根据个体协变量自动学习治疗决策策略时，现有方法通过训练机器学习模型预测反事实结果并优化预测目标值来选择策略。然而，由于“赢家诅咒”问题，即策略优化过程会利用预测误差而非寻找实际改进，导致预测的性能提升往往无法在后续策略评估中得到证实。

Method: 本文提出了一种名为“推断感知策略优化”的新策略。该方法不仅优化估计目标值，还优化策略在统计学上显著优于用于收集数据的观测策略的概率，从而解决了“赢家诅咒”问题。通过数学刻画这两种目标权衡下的策略帕累托前沿，并设计了一种策略优化算法。该算法利用机器学习预测反事实结果，然后将这些预测应用于估计帕累to前沿，决策者可以根据期望的权衡选择策略。

Result: 本文通过模拟实验证明了所提出方法的有效性。新的策略在优化估计目标值的同时，也考虑了策略在统计学上显著优于观测策略的概率。

Conclusion: 本文提出了一种“推断感知策略优化”的方法，通过平衡估计目标值和策略显著优于观测策略的概率，有效解决了机器学习模型在政策优化中遇到的“赢家诅咒”问题，并通过模拟验证了其有效性。

Abstract: There has been a surge of recent interest in automatically learning policies
to target treatment decisions based on rich individual covariates. A common
approach is to train a machine learning model to predict counterfactual
outcomes, and then select the policy that optimizes the predicted objective
value. In addition, practitioners also want confidence that the learned policy
has better performance than the incumbent policy according to downstream policy
evaluation. However, due to the winner's curse-an issue where the policy
optimization procedure exploits prediction errors rather than finding actual
improvements-predicted performance improvements are often not substantiated by
downstream policy optimization. To address this challenge, we propose a novel
strategy called inference-aware policy optimization, which modifies policy
optimization to account for how the policy will be evaluated downstream.
Specifically, it optimizes not only for the estimated objective value, but also
for the chances that the policy will be statistically significantly better than
the observational policy used to collect data. We mathematically characterize
the Pareto frontier of policies according to the tradeoff of these two goals.
Based on our characterization, we design a policy optimization algorithm that
uses machine learning to predict counterfactual outcomes, and then plugs in
these predictions to estimate the Pareto frontier; then, the decision-maker can
select the policy that optimizes their desired tradeoff, after which policy
evaluation can be performed on the test set as usual. Finally, we perform
simulations to illustrate the effectiveness of our methodology.

</details>


### [88] [The Bias-Variance Tradeoff in Data-Driven Optimization: A Local Misspecification Perspective](https://arxiv.org/abs/2510.18215)
*Haixiang Lan,Luofeng Liao,Adam N. Elmachtoub,Christian Kroer,Henry Lam,Haofeng Zhang*

Main category: stat.ML

TL;DR: 本文探讨了数据驱动随机优化中样本平均近似（SAA）和模型化方法（如估计-优化（ETO）或集成估计-优化（IEO））的相对性能。研究发现在局部错误设定下，存在偏差-方差权衡，且其相对重要性取决于错误设定的程度。


<details>
  <summary>Details</summary>
Motivation: 数据驱动随机优化在机器学习和操作决策中普遍存在，但SAA和模型化方法在复杂情境依赖问题中的相对性能尚不明确，现有结果多局限于模型化方法设定正确或错误设分明的二分情况。

Method: 本文利用统计学中的邻近性理论，首次在局部错误设定下对这些方法的相对性能进行了更细致的分析。

Result: 研究发现在局部错误设定下，SAA、IEO和ETO之间存在偏差-方差权衡，且偏差和方差的相对重要性取决于局部错误设定的程度。此外，本文还推导出了决策偏差的显式表达式，这有助于表征（无）影响的错误设定方向，并提供了对方差的进一步几何理解。

Conclusion: 在局部错误设定的背景下，SAA、IEO和ETO方法存在偏差与方差的权衡。模型化方法在接近正确设定的情况下，其性能表现受到局部错误设定程度的影响，这意味着在实际应用中需要根据具体情境权衡模型偏差和估计方差。

Abstract: Data-driven stochastic optimization is ubiquitous in machine learning and
operational decision-making problems. Sample average approximation (SAA) and
model-based approaches such as estimate-then-optimize (ETO) or integrated
estimation-optimization (IEO) are all popular, with model-based approaches
being able to circumvent some of the issues with SAA in complex
context-dependent problems. Yet the relative performance of these methods is
poorly understood, with most results confined to the dichotomous cases of the
model-based approach being either well-specified or misspecified. We develop
the first results that allow for a more granular analysis of the relative
performance of these methods under a local misspecification setting, which
models the scenario where the model-based approach is nearly well-specified. By
leveraging tools from contiguity theory in statistics, we show that there is a
bias-variance tradeoff between SAA, IEO, and ETO under local misspecification,
and that the relative importance of the bias and the variance depends on the
degree of local misspecification. Moreover, we derive explicit expressions for
the decision bias, which allows us to characterize (un)impactful
misspecification directions, and provide further geometric understanding of the
variance.

</details>


### [89] [Learning under Quantization for High-Dimensional Linear Regression](https://arxiv.org/abs/2510.18259)
*Dechen Zhang,Junwei Su,Difan Zou*

Main category: stat.ML

TL;DR: 这篇论文对高维线性回归中量化随机梯度下降（SGD）的有限步长进行了首次系统理论研究，分析了数据、标签、参数、激活和梯度量化对学习性能的影响。


<details>
  <summary>Details</summary>
Motivation: 尽管低比特量化在实际应用中取得了广泛成功，但目前仍缺乏关于其对学习性能影响的严格理论理解。

Method: 本研究提出了一个新的分析框架，建立了精确的、依赖于算法和数据的超额风险界限，以描述不同量化方法如何影响学习过程。

Result: 研究发现：参数、激活和梯度量化会放大训练过程中的噪声；数据量化会扭曲数据谱；数据和标签量化会引入额外的近似误差和量化误差。乘法量化（与输入相关）可以消除谱扭曲，而加法量化（常数步长）会随着批量大小的增加而出现有益的缩放效应。

Conclusion: 本研究的理论为理解量化如何影响优化算法的学习动态提供了有力的视角，并为在实际硬件约束下进一步探索学习理论奠定了基础。

Abstract: The use of low-bit quantization has emerged as an indispensable technique for
enabling the efficient training of large-scale models. Despite its widespread
empirical success, a rigorous theoretical understanding of its impact on
learning performance remains notably absent, even in the simplest linear
regression setting. We present the first systematic theoretical study of this
fundamental question, analyzing finite-step stochastic gradient descent (SGD)
for high-dimensional linear regression under a comprehensive range of
quantization targets: data, labels, parameters, activations, and gradients. Our
novel analytical framework establishes precise algorithm-dependent and
data-dependent excess risk bounds that characterize how different quantization
affects learning: parameter, activation, and gradient quantization amplify
noise during training; data quantization distorts the data spectrum; and data
and label quantization introduce additional approximation and quantized error.
Crucially, we prove that for multiplicative quantization (with input-dependent
quantization step), this spectral distortion can be eliminated, and for
additive quantization (with constant quantization step), a beneficial scaling
effect with batch size emerges. Furthermore, for common polynomial-decay data
spectra, we quantitatively compare the risks of multiplicative and additive
quantization, drawing a parallel to the comparison between FP and integer
quantization methods. Our theory provides a powerful lens to characterize how
quantization shapes the learning dynamics of optimization algorithms, paving
the way to further explore learning theory under practical hardware
constraints.

</details>


### [90] [Parametrising the Inhomogeneity Inducing Capacity of a Training Set, and its Impact on Supervised Learning](https://arxiv.org/abs/2510.18332)
*Gargi Roy,Dalia Chakrabarty*

Main category: stat.ML

TL;DR: 本文介绍了一种参数化方法，用于衡量训练数据集中变量之间关系模型所需的不均匀相关结构的性质。


<details>
  <summary>Details</summary>
Motivation: 作者旨在解决传统数据非平稳性概念未能充分捕捉训练数据集中不均匀相关结构这一问题，并提出“不均匀性参数”来量化这种特性。

Method: 通过引入“不均匀性参数”来量化训练数据集中不均匀相关结构的性质。在多个公开数据集中进行计算和演示，并证明传统数据“非平稳性”不代表不均匀性参数非零。在概率高斯过程学习框架下，证明非零不均匀性参数的训练集需要非平稳过程来建模目标函数。

Result: 成功计算了多种公开数据集的不均匀性参数，并证明了传统数据非平稳性不影响该参数。在基于高斯概率过程的学习方法中，非零不均匀性参数的训练集使得所调用的过程必须是非平稳的。

Conclusion: 不均匀性参数影响着测试输入的预测质量和可靠性，因此在建模变量关系时需考虑该参数。

Abstract: We introduce parametrisation of that property of the available
  training dataset, that necessitates an inhomogeneous correlation
  structure for the function that is learnt as a model of the
  relationship between the pair of variables, observations of which
  comprise the considered training data. We refer to a parametrisation
  of this property of a given training set, as its ``inhomogeneity
  parameter''. It is easy to compute this parameter for small-to-large
  datasets, and we demonstrate such computation on multiple
  publicly-available datasets, while also demonstrating that
  conventional ``non-stationarity'' of data does not imply a non-zero
  inhomogeneity parameter of the dataset. We prove that - within the
  probabilistic Gaussian Process-based learning approach - a training
  set with a non-zero inhomogeneity parameter renders it imperative,
  that the process that is invoked to model the sought function, be
  non-stationary. Following the learning of a real-world multivariate
  function with such a Process, quality and reliability of predictions
  at test inputs, are demonstrated to be affected by the inhomogeneity
  parameter of the training data.

</details>


### [91] [Interval Prediction of Annual Average Daily Traffic on Local Roads via Quantile Random Forest with High-Dimensional Spatial Data](https://arxiv.org/abs/2510.18548)
*Ying Yao,Daniel J. Graham*

Main category: stat.ML

TL;DR: 该研究引入了一种区间预测方法，通过Quantile Random Forest模型和主成分分析相结合，量化了AADT预测的不确定性，并在英国和威尔士的2000多条次要道路数据上进行了验证，取得了良好的区间覆盖概率和较低的宽度，从而提高了AADT估计的准确性和可解释性。


<details>
  <summary>Details</summary>
Motivation: 在交通规划和基础设施管理中，年度平均日 M 流量 (AADT) 数据至关重要，但现有自动交通检测器覆盖不完整，导致次要道路数据不足。目前大多数机器学习模型只提供点预测，忽略了估计的不确定性。

Method: 本研究提出了一种区间预测方法，明确量化了预测的不确定性。该方法将分位数随机森林模型与主成分分析相结合，生成 AADT 预测区间，提供由估计最小值和最大值界定的合理交通范围。

Result: 该方法在英国和威尔士 2,000 多条次要道路的数据上进行了评估，使用专门的区间指标，实现了 88.22% 的区间覆盖概率、0.23 的归一化平均宽度和 7,468.47 的 Winkler 分数。

Conclusion: 该框架结合了机器学习与空间和高维分析，提高了 AADT 估计的准确性和可解释性，为更稳健和明智的交通规划提供了支持。

Abstract: Accurate annual average daily traffic (AADT) data are vital for transport
planning and infrastructure management. However, automatic traffic detectors
across national road networks often provide incomplete coverage, leading to
underrepresentation of minor roads. While recent machine learning advances have
improved AADT estimation at unmeasured locations, most models produce only
point predictions and overlook estimation uncertainty. This study addresses
that gap by introducing an interval prediction approach that explicitly
quantifies predictive uncertainty. We integrate a Quantile Random Forest model
with Principal Component Analysis to generate AADT prediction intervals,
providing plausible traffic ranges bounded by estimated minima and maxima.
Using data from over 2,000 minor roads in England and Wales, and evaluated with
specialized interval metrics, the proposed method achieves an interval coverage
probability of 88.22%, a normalized average width of 0.23, and a Winkler Score
of 7,468.47. By combining machine learning with spatial and high-dimensional
analysis, this framework enhances both the accuracy and interpretability of
AADT estimation, supporting more robust and informed transport planning.

</details>


### [92] [A Frequentist Statistical Introduction to Variational Inference, Autoencoders, and Diffusion Models](https://arxiv.org/abs/2510.18777)
*Yen-Chi Chen*

Main category: stat.ML

TL;DR: 本文从频率论的角度介绍了变分推断（VI）、变分自动编码器（VAEs）和去噪扩散模型（DDMs），旨在弥合经典统计推断与现代生成AI之间的鸿沟。


<details>
  <summary>Details</summary>
Motivation: 目前变分推断（VI）在统计学中被视为后验近似的贝叶斯方法，而在机器学习中，变分自动编码器（VAEs）和去噪扩散模型（DDMs）则是从频率论的视角发展而来。这导致统计学家难以理解VAEs和DDMs的原理，因为缺乏相应的频率论VI介绍。

Method: 本文从经典的期望最大化（EM）算法入手，系统地从纯频率论角度阐述了VI、VAEs和DDMs的理论，并展示了VI如何作为难以处理的E步骤的可扩展解决方案。

Result: 本文成功地将VI、VAEs和DDMs置于一个统一的频率论框架下，衔接了经典统计推断与现代生成AI。

Conclusion: 本文为统计学家理解现代生成模型提供了频率论的视角，有助于促进跨学科的知识整合。

Abstract: While Variational Inference (VI) is central to modern generative models like
Variational Autoencoders (VAEs) and Denoising Diffusion Models (DDMs), its
pedagogical treatment is split across disciplines. In statistics, VI is
typically framed as a Bayesian method for posterior approximation. In machine
learning, however, VAEs and DDMs are developed from a Frequentist viewpoint,
where VI is used to approximate a maximum likelihood estimator. This creates a
barrier for statisticians, as the principles behind VAEs and DDMs are hard to
contextualize without a corresponding Frequentist introduction to VI. This
paper provides that introduction: we explain the theory for VI, VAEs, and DDMs
from a purely Frequentist perspective, starting with the classical
Expectation-Maximization (EM) algorithm. We show how VI arises as a scalable
solution for intractable E-steps and how VAEs and DDMs are natural,
deep-learning-based extensions of this framework, thereby bridging the gap
between classical statistical inference and modern generative AI.

</details>


<div id='cs.LG'></div>

# cs.LG [[Back]](#toc)

### [93] [CARLE: A Hybrid Deep-Shallow Learning Framework for Robust and Explainable RUL Estimation of Rolling Element Bearings](https://arxiv.org/abs/2510.17846)
*Waleed Razzaq,Yun-Bo Zhao*

Main category: cs.LG

TL;DR: CARLE是一种混合AI框架，它结合了深度和浅层学习来预测设备的剩余使用寿命（RUL），并在多变的操作条件下表现出卓越的泛化性和鲁棒性。


<details>
  <summary>Details</summary>
Motivation: 现有的RUL预测方法在不断变化的运行条件下缺乏泛化性和鲁棒性。

Method: CARLE框架结合了深度和浅层学习。它使用带有多头注意力和残差连接的Res-CNN和Res-LSTM模块来捕捉空间和时间退化模式，并使用随机森林回归器（RFR）进行稳定准确的RUL预测。此外，还采用高斯滤波进行降噪，并采用连续小波变换（CWT）进行时频特征提取。

Result: CARLE在XJTU-SY和PRONOSTIA轴承数据集上进行了评估，并通过消融研究、噪声实验和跨域实验证明了其鲁棒性和泛化能力。与现有技术相比，CARLE表现出卓越的性能，尤其是在动态条件下。

Conclusion: CARLE框架通过结合深度学习和浅层学习的优势，有效解决了RUL预测中泛化性和鲁棒性不足的问题，并在多种工况下实现了准确可靠的预测。

Abstract: Prognostic Health Management (PHM) systems monitor and predict equipment
health. A key task is Remaining Useful Life (RUL) estimation, which predicts
how long a component, such as a rolling element bearing, will operate before
failure. Many RUL methods exist but often lack generalizability and robustness
under changing operating conditions. This paper introduces CARLE, a hybrid AI
framework that combines deep and shallow learning to address these challenges.
CARLE uses Res-CNN and Res-LSTM blocks with multi-head attention and residual
connections to capture spatial and temporal degradation patterns, and a Random
Forest Regressor (RFR) for stable, accurate RUL prediction. A compact
preprocessing pipeline applies Gaussian filtering for noise reduction and
Continuous Wavelet Transform (CWT) for time-frequency feature extraction. We
evaluate CARLE on the XJTU-SY and PRONOSTIA bearing datasets. Ablation studies
measure each component's contribution, while noise and cross-domain experiments
test robustness and generalization. Comparative results show CARLE outperforms
several state-of-the-art methods, especially under dynamic conditions. Finally,
we analyze model interpretability with LIME and SHAP to assess transparency and
trustworthiness.

</details>


### [94] [Nash Policy Gradient: A Policy Gradient Method with Iteratively Refined Regularization for Finding Nash Equilibria](https://arxiv.org/abs/2510.18183)
*Eason Yu,Tzu Hao Liu,Yunke Wang,Clément L. Canonne,Nguyen H. Tran,Chang Xu*

Main category: cs.LG

TL;DR: 为了解决不完全信息博弈中纳什均衡寻找的挑战，本文提出了一种名为Nash Policy Gradient（NashPG）的新算法。该算法通过迭代优化参考策略，并在理论上证明了其在二人零和博弈中的单调改进和收敛性。


<details>
  <summary>Details</summary>
Motivation: 传统的基于正则化的方法在实践中常导致不稳定的学习，因为它们要求正则化强度趋于零以逼近纳什均衡。

Method: 本研究将正则化强度固定在一个较大的值以保证鲁棒性，并通过迭代优化参考策略来实现收敛。在此框架基础上，开发了Nash Policy Gradient（NashPG）算法，该算法仅依赖于当前策略和参考策略。

Result: NashPG在经典基准博弈中取得了与现有无模型方法相当或更低的对抗分数，并且在大规模领域（如战舰和无限注德州扑克）中实现了更高的Elo等级分。

Conclusion: NashPG算法在不完全信息博弈中寻找纳什均衡方面表现出色，具有理论上的收敛 guarantees 和在实践中的优异表现。

Abstract: Finding Nash equilibria in imperfect-information games remains a central
challenge in multi-agent reinforcement learning. While regularization-based
methods have recently achieved last-iteration convergence to a regularized
equilibrium, they require the regularization strength to shrink toward zero to
approximate a Nash equilibrium, often leading to unstable learning in practice.
Instead, we fix the regularization strength at a large value for robustness and
achieve convergence by iteratively refining the reference policy. Our main
theoretical result shows that this procedure guarantees strictly monotonic
improvement and convergence to an exact Nash equilibrium in two-player zero-sum
games, without requiring a uniqueness assumption. Building on this framework,
we develop a practical algorithm, Nash Policy Gradient (NashPG), which
preserves the generalizability of policy gradient methods while relying solely
on the current and reference policies. Empirically, NashPG achieves comparable
or lower exploitability than prior model-free methods on classic benchmark
games and scales to large domains such as Battleship and No-Limit Texas
Hold'em, where NashPG consistently attains higher Elo ratings.

</details>


### [95] [MIN-Merging: Merge the Important Neurons for Model Merging](https://arxiv.org/abs/2510.17890)
*Yunfei Liang*

Main category: cs.LG

TL;DR: 最新的深度学习进展产生了各种领域的开源模型。模型合并提供了一种结合它们优势的方法，但现有方法经常遇到参数冲突，这会降低领域特定任务的性能。我们提出了MIN-Merging，一个基于路由器的框架，它选择性地合并最重要的神经元，以减少此类冲突。在计算机视觉（CV）和自然语言处理（NLP）基准上的大量实验表明，MIN-Merging在域内任务上取得了持续的进步，同时保留了预训练模型在域外任务上的泛化能力。这些结果强调了其作为模型合并中参数冲突问题的实用解决方案的有效性。


<details>
  <summary>Details</summary>
Motivation: 尽管模型合并提供了一种结合模型优势的有效方法，但现有方法经常遇到参数冲突，这会降低领域特定任务的性能。

Method: 我们提出了MIN-Merging，一个基于路由器的框架，它选择性地合并最重要的神经元以减少参数冲突。

Result: 在计算机视觉（CV）和自然语言处理（NLP）基准上的大量实验表明，MIN-Merging在域内任务上取得了持续的进步，同时保留了预训练模型在域外任务上的泛化能力。

Conclusion: MIN-Merging作为模型合并中参数冲突问题的实用解决方案，能够有效解决参数冲突问题，并在保持泛化能力的同时提升域内任务性能。

Abstract: Recent advances in deep learning have led to a surge of open-source models
across diverse domains. While model merging offers a promising way to combine
their strengths, existing approaches often suffer from parameter conflicts that
degrade performance on domain-specific tasks. We propose MIN-Merging, a
router-based framework that selectively merges the most important neurons to
reduce such conflicts. Extensive experiments on Computer Vision(CV) and Natural
Language Processing(NLP) benchmarks show that MIN-Merging achieves consistent
gains on in-domain tasks while retaining the generalization ability of
pretrained models on out-of-domain tasks. These results highlight its
effectiveness as a practical solution to the parameter conflict problem in
model merging.

</details>


### [96] [Hierarchical Federated Unlearning for Large Language Models](https://arxiv.org/abs/2510.17895)
*Yisheng Zhong,Zhengbang Yang,Zhuangdi Zhu*

Main category: cs.LG

TL;DR: 该论文提出了一种针对大型语言模型的联邦遗忘方法，以解决持续异构的遗忘需求、去中心化敏感数据和不对称访问带来的挑战。


<details>
  <summary>Details</summary>
Motivation: 大型语言模型（LLMs）日益融入实际应用，引发了对隐私、安全和移除不良知识的需求。现有的机器遗忘方法面临持续异构的遗忘需求、去中心化敏感数据和不对称访问等挑战，导致了域间和域内干扰，加剧了遗忘和保留性能不平衡的困境。

Method: 本文提出了一种可扩展且保护隐私的联邦遗忘方法。该方法通过任务特定的适配器学习解耦遗忘和保留，并采用分层合并策略来缓解冲突目标，从而实现鲁棒且适应性强的遗忘更新。

Result: 在WMDP、MUSE和TOFU基准测试上的综合实验表明，与基线方法相比，该方法能有效处理异构遗忘请求，同时保持强大的LLM实用性。

Conclusion: 所提出的联邦遗忘方法能够有效应对大型语言模型中的隐私和安全挑战，实现对不良知识的有效遗忘，同时保持模型的实用性。

Abstract: Large Language Models (LLMs) are increasingly integrated into real-world
applications, raising concerns about privacy, security and the need to remove
undesirable knowledge. Machine Unlearning has emerged as a promising solution,
yet faces two key challenges: (1) practical unlearning needs are often
continuous and heterogeneous, and (2) they involve decentralized, sensitive
data with asymmetric access. These factors result in inter-domain and
intra-domain interference, which further amplifies the dilemma of unbalanced
forgetting and retaining performance. In response, we propose a federated
unlearning approach for LLMs that is scalable and privacy preserving. Our
method decouples unlearning and retention via task-specific adapter learning
and employs a hierarchical merging strategy to mitigate conflicting objectives
and enables robust, adaptable unlearning updates. Comprehensive experiments on
benchmarks of WMDP, MUSE, and TOFU showed that our approach effectively handles
heterogeneous unlearning requests while maintaining strong LLM utility compared
with baseline methods.

</details>


### [97] [Long-Context Attention Benchmark: From Kernel Efficiency to Distributed Context Parallelism](https://arxiv.org/abs/2510.17896)
*Tao Bu,Qiangang Wang,Bowen Zeng,Hanwen Sun,Yunpeng Huang,Chun Cao,Jingwei Xu*

Main category: cs.LG

TL;DR: 这篇论文提出了一个统一的基准测试，用于评估长上下文大型语言模型（LLMs）训练中的注意力机制。


<details>
  <summary>Details</summary>
Motivation: Transformer-based LLMs在长序列训练中面临二次计算和内存成本的瓶颈。现有的解决方案，包括核级优化和模块级分布式注意力策略，缺乏系统评估。

Method: 本研究提出了一个统一的基准测试平台，集成了代表性的注意力核函数和上下文并行机制，并提供模块化和可扩展的接口。该基准测试在两个关键维度上评估方法：注意力掩码模式以及序列长度和分布式规模。

Result: 通过对多达96个GPU集群进行全面实验，该基准测试实现了可复现的比较，揭示了特定方法的权衡，并为设计和部署长上下文LLM训练中的注意力机制提供了实用指导。

Conclusion: 本研究通过提供一个统一的基准测试解决了长上下文LLM训练中注意力机制评估不足的问题，促进了未来该领域的研究和发展。

Abstract: Transformer-based large language models (LLMs) have achieved remarkable
success, yet their standard attention mechanism incurs quadratic computation
and memory costs with respect to sequence length, posing a major bottleneck for
long-context training. Prior work tackles this challenge along two directions:
(1) kernel-level optimizations, which accelerate dense and sparse attention
operators; and (2) module-level strategies, often referred to as distributed
attention or context parallel training, which scale attention across multiple
devices. However, systematic evaluation still remains limited: operator-level
comparisons are often incomplete, while context parallel strategies are
typically framework-specific, with unclear performance analysis across
contexts. To address these gaps, we propose a unified benchmark that integrates
representative attention kernels and context parallel mechanisms with a modular
and extensible interface for evaluation. The benchmark evaluates methods along
two critical dimensions: (1) attention mask patterns, which strongly affect
efficiency, scalability, and usability, and (2) sequence length and distributed
scale, which determine performance under extreme long-context training. Through
comprehensive experiments on the cluster of up to 96 GPUs, our benchmark
enables reproducible comparisons, highlights method-specific trade-offs, and
provides practical guidance for designing and deploying attention mechanisms in
long-context LLM training.

</details>


### [98] [Automated Algorithm Design for Auto-Tuning Optimizers](https://arxiv.org/abs/2510.17899)
*Floris-Jan Willemsen,Niki van Stein,Ben van Werkhoven*

Main category: cs.LG

TL;DR: 本文探讨了使用大型语言模型（LLMs）自动生成优化算法，以解决自动调优中参数空间庞大且不规则的问题。


<details>
  <summary>Details</summary>
Motivation: 传统的自动调优方法依赖于既定的优化算法，但设计有效的优化器仍然具有挑战性，因为没有一种方法能在所有调优任务中表现最佳。

Method: 本文引入了一个框架，通过向LLMs提供问题描述和搜索空间特征结果来生成专门的优化策略，并对这些策略进行迭代检查和改进。

Result: 在四个真实世界的自动调优应用程序和六个硬件平台上评估了生成的算法。结果表明，在生成阶段提供额外的特定于应用程序和搜索空间的信息，可以分别带来30.7%和14.6%的平均性能改进。

Conclusion: LLM生成的优化器可以与现有的人工设计算法相媲美，并且在多种情况下表现更优，其中表现最佳的生成优化算法在自动调优方面比最先进的优化器平均提高了72.4%。

Abstract: Automatic performance tuning (auto-tuning) is essential for optimizing
high-performance applications, where vast and irregular parameter spaces make
manual exploration infeasible. Traditionally, auto-tuning relies on
well-established optimization algorithms such as evolutionary algorithms,
annealing methods, or surrogate model-based optimizers to efficiently find
near-optimal configurations. However, designing effective optimizers remains
challenging, as no single method performs best across all tuning tasks.
  In this work, we explore a new paradigm: using large language models (LLMs)
to automatically generate optimization algorithms tailored to auto-tuning
problems. We introduce a framework that prompts LLMs with problem descriptions
and search-space characteristics results to produce specialized optimization
strategies, which are iteratively examined and improved.
  These generated algorithms are evaluated on four real-world auto-tuning
applications across six hardware platforms and compared against the
state-of-the-art in optimization algorithms of two contemporary auto-tuning
frameworks. The evaluation demonstrates that providing additional application-
and search space-specific information in the generation stage results in an
average performance improvement of 30.7\% and 14.6\%, respectively. In
addition, our results show that LLM-generated optimizers can rival, and in
various cases outperform, existing human-designed algorithms, with our
best-performing generated optimization algorithms achieving, on average, 72.4\%
improvement over state-of-the-art optimizers for auto-tuning.

</details>


### [99] [NeuCo-Bench: A Novel Benchmark Framework for Neural Embeddings in Earth Observation](https://arxiv.org/abs/2510.17914)
*Rikard Vinge,Isabelle Wittmann,Jannik Schneider,Michael Marszalek,Luis Gilch,Thomas Brunschwiler,Conrad M Albrecht*

Main category: cs.LG

TL;DR: NeuCo-Bench是一个用于评估地球观测（EO）中神经压缩和表示学习的新基准框架。该框架由三个核心部分组成：一个评估流程、一个带有隐藏任务排行榜的挑战模式，以及一个平衡准确性和稳定性的评分系统。


<details>
  <summary>Details</summary>
Motivation: 此研究旨在为地球观测（EO）领域的神经压缩和表示学习提供一个标准化的评估框架。

Method: NeuCo-Bench框架包括：1. 基于可重复利用嵌入的评估流程。2. 带有隐藏任务排行榜的挑战模式以减轻预训练偏差。3. 平衡准确性和稳定性的评分系统。此外，还发布了SSL4EO-S12-downstream数据集以支持可重现性。

Result: 在2025年CVPR EARTHVISION研讨会上的一次公开挑战中获得了初步结果，并对最先进的AIGC基础模型进行了消融实验。

Conclusion: NeuCo-Bench为地球观测及其他领域神经嵌入的社区驱动、标准化评估迈出了第一步。

Abstract: We introduce NeuCo-Bench, a novel benchmark framework for evaluating (lossy)
neural compression and representation learning in the context of Earth
Observation (EO). Our approach builds on fixed-size embeddings that act as
compact, task-agnostic representations applicable to a broad range of
downstream tasks. NeuCo-Bench comprises three core components: (i) an
evaluation pipeline built around reusable embeddings, (ii) a new challenge mode
with a hidden-task leaderboard designed to mitigate pretraining bias, and (iii)
a scoring system that balances accuracy and stability. To support
reproducibility, we release SSL4EO-S12-downstream, a curated multispectral,
multitemporal EO dataset. We present initial results from a public challenge at
the 2025 CVPR EARTHVISION workshop and conduct ablations with state-of-the-art
foundation models. NeuCo-Bench provides a first step towards community-driven,
standardized evaluation of neural embeddings for EO and beyond.

</details>


### [100] [Benchmarking Probabilistic Time Series Forecasting Models on Neural Activity](https://arxiv.org/abs/2510.18037)
*Ziyu Lu,Anna J. Li,Alexander E. Ladd,Pascha Matveev,Aditya Deole,Eric Shea-Brown,J. Nathan Kutz,Nicholas A. Steinmetz*

Main category: cs.LG

TL;DR: 这篇论文评估了深度学习模型在预测神经活动方面的表现，发现它们优于传统方法。


<details>
  <summary>Details</summary>
Motivation: 神经活动预测对于理解神经系统和实现闭环控制至关重要，但深度学习在该领域的应用有限，因此需要弥补这一空白。

Method: 系统评估了八种概率深度学习模型（包括两种基础模型），并与四种经典统计模型和两种基线方法进行比较，数据来源于小鼠大脑皮层的自发神经活动。

Result: 在不同的预测时间范围内，几种深度学习模型始终优于经典方法，其中最佳模型能够对未来1.5秒的神经活动进行有效预测。

Conclusion: 研究结果表明深度学习在神经控制应用方面的潜力，并为探索神经活动的内在时间结构提供了新途径。

Abstract: Neural activity forecasting is central to understanding neural systems and
enabling closed-loop control. While deep learning has recently advanced the
state-of-the-art in the time series forecasting literature, its application to
neural activity forecasting remains limited. To bridge this gap, we
systematically evaluated eight probabilistic deep learning models, including
two foundation models, that have demonstrated strong performance on general
forecasting benchmarks. We compared them against four classical statistical
models and two baseline methods on spontaneous neural activity recorded from
mouse cortex via widefield imaging. Across prediction horizons, several deep
learning models consistently outperformed classical approaches, with the best
model producing informative forecasts up to 1.5 seconds into the future. Our
findings point toward future control applications and open new avenues for
probing the intrinsic temporal structure of neural activity.

</details>


### [101] [Data Unlearning Beyond Uniform Forgetting via Diffusion Time and Frequency Selection](https://arxiv.org/abs/2510.17917)
*Jinseong Park,Mijung Park*

Main category: cs.LG

TL;DR: 这篇论文旨在解决扩散模型中的数据遗忘问题，现有方法常导致质量下降或遗忘不完全。作者提出一种时频选择性方法，通过关注特定时频范围来提高生成样本的质量，并在多种设置和任务中验证了其有效性。他们还提出一种归一化SSCD来评估遗忘和数据质量。


<details>
  <summary>Details</summary>
Motivation: 探索扩散模型中数据遗忘问题，并解决现有方法导致的质量下降和遗忘不完全的问题。

Method: 作者提出一种时频选择性方法，通过有选择地关注训练过程中特定的时频范围，以实现更高美学质量和更低噪声的样本生成。

Result: 通过应用时频选择性方法，在包括基于梯度的和偏好优化的目标，以及图像级别和文本到图像的任务中，都获得了更高审美质量和更低噪声的样本。此外，论文还提出了一种简单的归一化SSCD来评估遗忘数据的删除程度和质量。

Conclusion: 论文对扩散模型中数据遗忘的独特挑战有了更清晰的理解，并提供了改进评估和遗忘性能的实用策略。

Abstract: Data unlearning aims to remove the influence of specific training samples
from a trained model without requiring full retraining. Unlike concept
unlearning, data unlearning in diffusion models remains underexplored and often
suffers from quality degradation or incomplete forgetting. To address this, we
first observe that most existing methods attempt to unlearn the samples at all
diffusion time steps equally, leading to poor-quality generation. We argue that
forgetting occurs disproportionately across time and frequency, depending on
the model and scenarios. By selectively focusing on specific time-frequency
ranges during training, we achieve samples with higher aesthetic quality and
lower noise. We validate this improvement by applying our time-frequency
selective approach to diverse settings, including gradient-based and preference
optimization objectives, as well as both image-level and text-to-image tasks.
Finally, to evaluate both deletion and quality of unlearned data samples, we
propose a simple normalized version of SSCD. Together, our analysis and methods
establish a clearer understanding of the unique challenges in data unlearning
for diffusion models, providing practical strategies to improve both evaluation
and unlearning performance.

</details>


### [102] [Latent Discrete Diffusion Models](https://arxiv.org/abs/2510.18114)
*Dario Shariatian,Alain Durmus,Stefano Peluchetti*

Main category: cs.LG

TL;DR: 本文提出了一种名为潜在离散扩散模型（LDDM）的新模型，该模型将离散扩散与潜在嵌入上的连续扩散相结合，以解决掩码去噪器在少步生成中联合结构较弱的问题。


<details>
  <summary>Details</summary>
Motivation: 传统的掩码去噪器在处理语言和其他分类数据时，反向转换通常在不同位置上分解，这会削弱联合结构并在少步生成中降低质量。

Method: 本文提出了潜在离散扩散模型（LDDM），它将标记上的掩码离散扩散与潜在嵌入上的连续扩散相结合。作者提出了两种实例化方法：FUJI-LDDMs（对标记和潜在变量进行完全联合去噪）和SEQ-LDDMs（在条件作用下依次解析潜在变量再解析离散链）。

Result: LDDMs 在无条件生成指标上优于现有的掩码离散扩散基线，并且在较低的采样预算下也表现出色。

Conclusion: LDDMs通过引入连续潜在空间来增强离散扩散模型的联合结构和生成质量，尤其是在少量采样步数下。

Abstract: We study discrete diffusion for language and other categorical data and focus
on a common limitation of masked denoisers: reverse transitions typically
factorize across positions, which can weaken joint structure and degrade
quality in few-step generation. We propose \emph{Latent Discrete Diffusion
Models} (LDDMs), which couple a masked discrete diffusion over tokens with a
continuous diffusion over latent embeddings. The latent channel provides a
softer signal and carries cross-token dependencies that help resolve
ambiguities. We present two instantiations: (i) FUJI-LDDMs, which perform fully
joint denoising of tokens and latents, and (ii) SEQ-LDDMs, which sequentially
resolve the latent and then the discrete chain conditionally on it. For both
variants we derive ELBO-style objectives and discuss design choices to learn
informative latents yet amenable to diffusoin modeling. In experiments, LDDMs
yield improvements on unconditional generation metrics as compared to
state-of-the-art masked discrete diffusion baselines, and are effective at
lower sampling budgets, where unmasking many tokens per step is desirable.

</details>


### [103] [Rewarding the Journey, Not Just the Destination: A Composite Path and Answer Self-Scoring Reward Mechanism for Test-Time Reinforcement Learning](https://arxiv.org/abs/2510.17923)
*Chenwei Tang,Jingyu Xing,Xinyu Liu,Wei Ju,Jiancheng Lv,Deng Xiong,Ziyue Qiao*

Main category: cs.LG

TL;DR: 本文提出了一种名为COMPASS的新型测试时奖励机制，用于在无监督环境下训练大型语言模型，从而解决当前强化学习方法对人类标注数据依赖的限制。


<details>
  <summary>Details</summary>
Motivation: 目前的强化学习方法在应用于大型语言模型时，由于过度依赖人工标注的偏好数据或带标签数据集进行奖励建模，面临着可扩展性的根本瓶颈。

Method: COMPASS（Composite Path and Answer Self-Scoring）机制包含两个互补部分：双校准答案奖励（DCAR）通过置信度和可信度校准建立可靠的伪标签，稳定训练；而决定性路径奖励（DPR）则直接优化推理过程的质量。COMPASS通过共同强化可靠的共识答案和高度决定性的推理链，系统地增强模型的分析能力。

Result: 广泛的实验表明，COMPASS在各种推理任务和模型架构上都取得了显著且持续的性能提升。

Conclusion: COMPASS为大型语言模型提供了一个更具可扩展性的方向，使其能够从持续的经验中学习。

Abstract: Reinforcement Learning (RL) has emerged as a powerful paradigm for advancing
Large Language Models (LLMs), achieving remarkable performance in complex
reasoning domains such as mathematics and code generation. However, current RL
methods face a fundamental scalability bottleneck due to their heavy reliance
on human-curated preference data or labeled datasets for reward modeling. To
overcome this limitation, we explore RL on unlabeled data where models learn
autonomously from continuous experience streams. The core challenge in this
setting lies in reliable reward estimation without ground-truth supervision.
Existing approaches like Test-Time RL address this through self-consistent
consensus, but risk reinforcing incorrect pseudo-labels derived from majority
voting. We introduce COMPASS (Composite Path and Answer Self-Scoring), a novel
test-time reward mechanism that operates without external supervision. COMPASS
integrates two complementary components: the Dual-Calibration Answer Reward
(DCAR), which stabilizes training by establishing trustworthy pseudo-labels
through confidence and credibility calibration, and the Decisive Path Reward
(DPR), which directly optimizes the reasoning process quality beyond mere
outcome supervision. By jointly reinforcing trustworthy consensus answers and
highly decisive reasoning chains, the COMPASS systematically enhances the
model's analytical capabilities. Extensive experiments show that COMPASS
achieves significant and consistent performance gains across diverse reasoning
tasks and model architectures, advancing a more scalable direction for LLMs to
learn from continuous experience.

</details>


### [104] [Rethinking PCA Through Duality](https://arxiv.org/abs/2510.18130)
*Jan Quan,Johan Suykens,Panagiotis Patrinos*

Main category: cs.LG

TL;DR: 本文利用DC框架对PCA的基本原理进行了重新探讨，提出了几种新的表述方式，并提供了新的理论见解。


<details>
  <summary>Details</summary>
Motivation: 受自注意力与（核）主成分分析（PCA）之间最新发现的联系的启发，本文重新审视了PCA的基本原理。

Method: 本文利用差分凸（DC）框架，提出了几种新颖的公式，并提供了新的理论见解。此外，我们发现与经典QR算法相关的同步迭代是差分凸算法（DCA）的一个实例，为这种长期存在的方法提供了优化视角。最后，我们介绍了一种可核化的鲁棒PCA对偶公式，它能最小化重建误差的l1偏差。

Result: 本文展示了PCA类问题的核化能力和样本外适用性。我们发现同步迭代是差分凸算法（DCA）的一个实例，这为这种长期存在的方法提供了优化视角。此外，我们还在PCA中引入了几种新算法，并将其与现有最新方法进行了实证比较。

Conclusion: 本文通过DC框架深入分析了PCA，不仅提出了新的理论视角和算法，还证明了其在核化和样本外应用方面的潜力，并为鲁棒PCA的开发奠定了基础。

Abstract: Motivated by the recently shown connection between self-attention and
(kernel) principal component analysis (PCA), we revisit the fundamentals of
PCA. Using the difference-of-convex (DC) framework, we present several novel
formulations and provide new theoretical insights. In particular, we show the
kernelizability and out-of-sample applicability for a PCA-like family of
problems. Moreover, we uncover that simultaneous iteration, which is connected
to the classical QR algorithm, is an instance of the difference-of-convex
algorithm (DCA), offering an optimization perspective on this longstanding
method. Further, we describe new algorithms for PCA and empirically compare
them with state-of-the-art methods. Lastly, we introduce a kernelizable dual
formulation for a robust variant of PCA that minimizes the $l_1$ deviation of
the reconstruction errors.

</details>


### [105] [EvoSyn: Generalizable Evolutionary Data Synthesis for Verifiable Learning](https://arxiv.org/abs/2510.17928)
*He Du,Bowen Li,Aijun Yang,Siyang He,Qipeng Guo,Dacheng Tao*

Main category: cs.LG

TL;DR: 本文介绍了一个可进化的、任务无关的、策略引导的、可执行检查的数据合成框架，能够可靠地组装连贯的、可验证的训练实例，并在RLVR和模型蒸馏训练范式下显著提高LiveCodeBench和AgentBench-OS任务的性能。


<details>
  <summary>Details</summary>
Motivation: 可靠的可验证数据已成为现代语言模型能力提升的关键驱动力，但构建通用的合成可验证数据仍然困难，因为容易产生幻觉生成，且验证工件薄弱或微不足道。现有方法通常依赖于特定任务的启发式方法或事后过滤器，这些方法无法跨领域传输，并且缺乏可验证性的原则性、通用评估器。

Method: 本文引入了一个可进化的、任务无关的、策略引导的、可执行检查的数据合成框架。该框架从最小的种子监督出发，联合合成问题、多样化的候选解决方案和验证工件，并通过基于一致性的评估器迭代发现策略，该评估器强制执行人工注释和策略诱导检查之间的一致性。

Result: 在RLVR和模型蒸馏训练范式下，该方法都展示了其有效性。使用合成数据进行训练在LiveCodeBench和AgentBench-OS任务上都取得了显著的改进。

Conclusion: 本文提出的框架能够可靠地组装连贯的、可验证的训练实例，并且无需领域特定规则即可实现泛化。这表明了一个原则性的、通用的可验证性评估器，可以显著提高语言模型的性能。

Abstract: Reliable verifiable data has become a key driver of capability gains in
modern language models, enabling stable reinforcement learning with verifiable
rewards and effective distillation that transfers competence across math,
coding, and agentic tasks. Yet constructing generalizable synthetic verifiable
data remains difficult due to hallucination-prone generation, and weak or
trivial verification artifacts that fail to separate strong from weak
solutions. Existing approaches often rely on task-specific heuristics or
post-hoc filters that do not transfer across domains and lack a principled,
universal evaluator of verifiability. In this work, we introduce an
evolutionary, task-agnostic, strategy-guided, executably-checkable data
synthesis framework that, from minimal seed supervision, jointly synthesizes
problems, diverse candidate solutions, and verification artifacts, and
iteratively discovers strategies via a consistency-based evaluator that
enforces agreement between human-annotated and strategy-induced checks. This
pipeline upgrades filtering into principled synthesis: it reliably assembles
coherent, verifiable training instances and generalizes without domain-specific
rules. Our experiments demonstrate the effectiveness of the proposed approach
under both RLVR and model distillation training paradigms. The results show
that training with our synthesized data yields significant improvements on both
the LiveCodeBench and AgentBench-OS tasks, highlighting the robust
generalization of our framework.

</details>


### [106] [From Observations to Parameters: Detecting Changepoint in Nonlinear Dynamics with Simulation-based Inference](https://arxiv.org/abs/2510.17933)
*Xiangbo Deng,Cheng Chen,Peng Yang*

Main category: cs.LG

TL;DR: 该论文提出了Parameter--Space Changepoint Detection (Param--CPD)方法，能够更好地检测混沌时间序列中的状态变化。


<details>
  <summary>Details</summary>
Motivation: 在混沌时间序列中检测状态变化很困难，因为观测空间信号与内在可变性纠缠在一起。

Method: Param-CPD是一个两阶段框架，首先通过模拟的贝叶斯推理训练神经后验估计器，然后将标准的CPD算法应用于所得的参数轨迹。

Result: 在Lorenz-63模型上进行测试，Param-CPD在F1、定位误差和降低误报方面均优于传统的观测空间基线方法。进一步验证了在静止轨迹上推断后验的识别性和校准性，解释了为什么参数空间提供了更清晰的检测信号。

Conclusion: Param-CPD在物理可解释的参数空间中运行，可以实现非线性动力系统中准确且可解释的变化点检测。

Abstract: Detecting regime shifts in chaotic time series is hard because
observation-space signals are entangled with intrinsic variability. We propose
Parameter--Space Changepoint Detection (Param--CPD), a two--stage framework
that first amortizes Bayesian inference of governing parameters with a neural
posterior estimator trained by simulation-based inference, and then applies a
standard CPD algorithm to the resulting parameter trajectory. On Lorenz--63
with piecewise-constant parameters, Param--CPD improves F1, reduces
localization error, and lowers false positives compared to observation--space
baselines. We further verify identifiability and calibration of the inferred
posteriors on stationary trajectories, explaining why parameter space offers a
cleaner detection signal. Robustness analyses over tolerance, window length,
and noise indicate consistent gains. Our results show that operating in a
physically interpretable parameter space enables accurate and interpretable
changepoint detection in nonlinear dynamical systems.

</details>


### [107] [Uncertainty Estimation by Flexible Evidential Deep Learning](https://arxiv.org/abs/2510.18322)
*Taeseong Yoon,Heeyoung Kim*

Main category: cs.LG

TL;DR: 本文提出了灵活的证据深度学习（$\\mathcal{F}$-EDL），通过预测类概率上的灵活狄利克雷分布，增强了不确定性量化（UQ）的泛化性和可靠性。


<details>
  <summary>Details</summary>
Motivation: 在将机器学习模型部署到高风险应用中时，不确定性量化（UQ）至关重要，因为过度自信的预测可能导致严重后果。现有的证据深度学习（EDL）方法虽然在计算效率上有所平衡，但其对狄利克雷分布的严格假设限制了其在复杂或不可预见情况下的鲁棒性。

Method: 本文提出了一种灵活的证据深度学习（$\\mathcal{F}$-EDL）方法。该方法通过预测类概率上的灵活狄利克雷分布（狄利克雷分布的一种推广）来扩展EDL。

Result: $\\mathcal{F}$-EDL提供了一种更具表达性和适应性的不确定性表示方法，显著增强了在挑战性场景下的UQ泛化性和可靠性。通过理论分析和实证实验，证明了其在经典、长尾和噪声分布内场景中，UQ性能达到最先进水平。

Conclusion: $\\mathcal{F}$-EDL通过引入灵活的狄利克雷分布，有效解决了传统EDL在不确定性量化方面的局限性，在提高模型对复杂和未知情况的适应性和鲁棒性方面取得了显著进展。

Abstract: Uncertainty quantification (UQ) is crucial for deploying machine learning
models in high-stakes applications, where overconfident predictions can lead to
serious consequences. An effective UQ method must balance computational
efficiency with the ability to generalize across diverse scenarios. Evidential
deep learning (EDL) achieves efficiency by modeling uncertainty through the
prediction of a Dirichlet distribution over class probabilities. However, the
restrictive assumption of Dirichlet-distributed class probabilities limits
EDL's robustness, particularly in complex or unforeseen situations. To address
this, we propose \textit{flexible evidential deep learning}
($\mathcal{F}$-EDL), which extends EDL by predicting a flexible Dirichlet
distribution -- a generalization of the Dirichlet distribution -- over class
probabilities. This approach provides a more expressive and adaptive
representation of uncertainty, significantly enhancing UQ generalization and
reliability under challenging scenarios. We theoretically establish several
advantages of $\mathcal{F}$-EDL and empirically demonstrate its
state-of-the-art UQ performance across diverse evaluation settings, including
classical, long-tailed, and noisy in-distribution scenarios.

</details>


### [108] [UniRL-Zero: Reinforcement Learning on Unified Models with Joint Language Model and Diffusion Model Experts](https://arxiv.org/abs/2510.17937)
*Fu-Yun Wang,Han Zhang,Michael Gharbi,Hongsheng Li,Taesung Park*

Main category: cs.LG

TL;DR: UniRL-Zero 是一个统一的强化学习框架，旨在提升多模态语言模型的理解和推理能力，扩散模型的多媒体生成能力，以及它们之间有益的交互能力。


<details>
  <summary>Details</summary>
Motivation: 目前的模型缺乏统一的强化学习框架来同时提升多模态语言理解、推理和多媒体生成能力及其交互。

Method: UniRL-Zero 框架定义了六种统一模型强化学习的场景，并为统一理解和生成模型的强化学习提供了系统基线。

Result: 成功地将多模态语言模型和扩散模型整合到一个统一的强化学习框架中，并提升了它们的理解、推理和生成能力。

Conclusion: UniRL-Zero 提供了一个统一的强化学习框架和系统基线，有望推动多模态理解和生成领域的发展。

Abstract: We present UniRL-Zero, a unified reinforcement learning (RL) framework that
boosts, multimodal language model understanding and reasoning, diffusion model
multimedia generation, and their beneficial interaction capabilities within a
unified model. Our work defines six scenarios for unified model reinforcement
learning, providing systematic baselines for reinforcement learning of unified
understanding and generation model. Our code is available at
https://github.com/G-U-N/UniRL.

</details>


### [109] [Optimality and NP-Hardness of Transformers in Learning Markovian Dynamical Functions](https://arxiv.org/abs/2510.18638)
*Yanna Ding,Songtao Lu,Yingdong Lu,Tomasz Nowicki,Jianxi Gao*

Main category: cs.LG

TL;DR: 本文探讨了transformer模型在处理动态驱动函数时，如何通过上下文学习（ICL）来解决未见任务。


<details>
  <summary>Details</summary>
Motivation: 现有的ICL理论研究主要集中在线性回归任务，且输入通常是独立同分布的。本文旨在理解transformer在建模动态驱动函数时如何表达ICL。

Method: 我们通过结构化的ICL设置研究马尔可夫函数学习，并通过表征损失景观来揭示潜在的优化行为。具体方法包括：1. 给出一个单层线性自注意力（LSA）模型在扩展参数空间中的全局最小值的闭式表达式；2. 证明恢复实现最优解的transformer参数通常是NP难的，揭示了单层LSA在表示结构化动态函数方面的基本局限性；3. 提供多层LSA作为执行预条件梯度下降以优化超出平方损失的多个目标的新颖解释。

Result: 1. 获得了单层LSA模型全局最小值的闭式表达式。2. 证明了恢复实现最优解的transformer参数在一般情况下是NP难的。3. 提出了多层LSA作为预条件梯度下降的新解释。

Conclusion: 实验结果验证了这些理论发现。

Abstract: Transformer architectures can solve unseen tasks based on input-output pairs
in a given prompt due to in-context learning (ICL). Existing theoretical
studies on ICL have mainly focused on linear regression tasks, often with
i.i.d. inputs. To understand how transformers express ICL when modeling
dynamics-driven functions, we investigate Markovian function learning through a
structured ICL setup, where we characterize the loss landscape to reveal
underlying optimization behaviors. Specifically, we (1) provide the closed-form
expression of the global minimizer (in an enlarged parameter space) for a
single-layer linear self-attention (LSA) model; (2) prove that recovering
transformer parameters that realize the optimal solution is NP-hard in general,
revealing a fundamental limitation of one-layer LSA in representing structured
dynamical functions; and (3) supply a novel interpretation of a multilayer LSA
as performing preconditioned gradient descent to optimize multiple objectives
beyond the square loss. These theoretical results are numerically validated
using simplified transformers.

</details>


### [110] [Demystifying Transition Matching: When and Why It Can Beat Flow Matching](https://arxiv.org/abs/2510.17991)
*Jaihoon Kim,Rajarshi Saha,Minhyuk Sung,Youngsuk Park*

Main category: cs.LG

TL;DR: 这篇论文探讨了在生成模型中，Transition Matching（TM）何时以及为何比Flow Matching（FM）表现更好。


<details>
  <summary>Details</summary>
Motivation: 尽管Flow Matching (FM) 是许多先进生成模型的基础，但最近的研究表明，Transition Matching (TM) 可以在更少的采样步骤下实现更高的质量。因此，该研究旨在阐明TM优于FM的条件和原因。

Method: 1. 在单峰高斯分布目标下，证明TM在有限步数内能实现比FM更低的KL散度。
2. 分析收敛速度，证明在固定计算预算下，TM比FM收敛更快。
3. 将分析扩展到高斯混合模型，识别TM表现优于FM的局部单峰区域。
4. 通过受控实验和图像视频生成实际应用验证理论结果。

Result: 1. 当目标是单峰高斯分布时，TM比FM具有更低的KL散度，这归因于TM中的随机差分潜在更新保留了FM低估的目标协方差。
2. TM在固定计算预算下比FM收敛更快。
3. 在高斯混合模型中，当模式分离良好时（即分量均值之间的最小距离增加），TM能优于FM。
4. 当目标方差趋近于零时，TM的性能优势减弱，因为每次TM更新都收敛到FM更新。

Conclusion: TM在目标分布具有良好分离的模式和不可忽略的方差时，其性能优于FM。

Abstract: Flow Matching (FM) underpins many state-of-the-art generative models, yet
recent results indicate that Transition Matching (TM) can achieve higher
quality with fewer sampling steps. This work answers the question of when and
why TM outperforms FM. First, when the target is a unimodal Gaussian
distribution, we prove that TM attains strictly lower KL divergence than FM for
finite number of steps. The improvement arises from stochastic difference
latent updates in TM, which preserve target covariance that deterministic FM
underestimates. We then characterize convergence rates, showing that TM
achieves faster convergence than FM under a fixed compute budget, establishing
its advantage in the unimodal Gaussian setting. Second, we extend the analysis
to Gaussian mixtures and identify local-unimodality regimes in which the
sampling dynamics approximate the unimodal case, where TM can outperform FM.
The approximation error decreases as the minimal distance between component
means increases, highlighting that TM is favored when the modes are well
separated. However, when the target variance approaches zero, each TM update
converges to the FM update, and the performance advantage of TM diminishes. In
summary, we show that TM outperforms FM when the target distribution has
well-separated modes and non-negligible variances. We validate our theoretical
results with controlled experiments on Gaussian distributions, and extend the
comparison to real-world applications in image and video generation.

</details>


### [111] [Preference-based Reinforcement Learning beyond Pairwise Comparisons: Benefits of Multiple Options](https://arxiv.org/abs/2510.18713)
*Joongkyu Lee,Seouh-won Yi,Min-hwan Oh*

Main category: cs.LG

TL;DR: 该论文通过最大化子集中平均不确定性的方法，提出了一种名为M-AUPO的算法，以提高在线偏好强化学习的样本效率。此算法在排序反馈中，实现了与子集大小正相关的样本效率提升。


<details>
  <summary>Details</summary>
Motivation: 以往的偏好强化学习研究主要关注成对比较，少数研究虽探索了多重比较和排序反馈，但它们的性能保证未能随反馈长度增加而改善，甚至可能恶化。现有的大多数工作都存在对未知参数范数的指数依赖，这是一个基本限制。

Method: 本文采用Plackett-Luce (PL) 模型进行动作子集上的排序反馈建模，并提出了M-AUPO算法。该算法通过最大化所提供子集中的平均不确定性来选择多个动作。

Result: M-AUPO算法的次优性差距为$\tilde{\mathcal{O}}\left( \frac{d}{T} \sqrt{ \sum_{t=1}^T \frac{1}{|S_t|}} \right)$，其中$T$是总轮数，$d$是特征维度，$|S_t|$是第$t$轮子集的大小。这个结果表明，更大的子集直接带来性能提升。该算法避免了对未知参数范数的指数依赖。同时，论文还建立了$\Omega \left( \frac{d}{K \sqrt{T}} \right)$的近似匹配下界，其中$K$是最大子集大小。

Conclusion: 本文首次通过理论结果证明，在有排序反馈的偏好强化学习中，样本效率可以作为子集大小的函数而得到明确提升。

Abstract: We study online preference-based reinforcement learning (PbRL) with the goal
of improving sample efficiency. While a growing body of theoretical work has
emerged-motivated by PbRL's recent empirical success, particularly in aligning
large language models (LLMs)-most existing studies focus only on pairwise
comparisons. A few recent works (Zhu et al., 2023, Mukherjee et al., 2024,
Thekumparampil et al., 2024) have explored using multiple comparisons and
ranking feedback, but their performance guarantees fail to improve-and can even
deteriorate-as the feedback length increases, despite the richer information
available. To address this gap, we adopt the Plackett-Luce (PL) model for
ranking feedback over action subsets and propose M-AUPO, an algorithm that
selects multiple actions by maximizing the average uncertainty within the
offered subset. We prove that M-AUPO achieves a suboptimality gap of
$\tilde{\mathcal{O}}\left( \frac{d}{T} \sqrt{ \sum_{t=1}^T \frac{1}{|S_t|}}
\right)$, where $T$ is the total number of rounds, $d$ is the feature
dimension, and $|S_t|$ is the size of the subset at round $t$. This result
shows that larger subsets directly lead to improved performance and, notably,
the bound avoids the exponential dependence on the unknown parameter's norm,
which was a fundamental limitation in most previous works. Moreover, we
establish a near-matching lower bound of $\Omega \left( \frac{d}{K \sqrt{T}}
\right)$, where $K$ is the maximum subset size. To the best of our knowledge,
this is the first theoretical result in PbRL with ranking feedback that
explicitly shows improved sample efficiency as a function of the subset size.

</details>


### [112] [Cross-Domain Long-Term Forecasting: Radiation Dose from Sparse Neutron Sensor via Spatio-Temporal Operator Network](https://arxiv.org/abs/2510.18041)
*Jay Phil Yoo,Kazuma Kobayashi,Souvik Chakraborty,Syed Bahauddin Alam*

Main category: cs.LG

TL;DR: 《Spatio-Temporal Operator Network》(STONe) 是一种新颖的非自回归神经算子，用于解决科学机器学习中从稀疏、跨域传感器数据预测不可观测物理量的难题。


<details>
  <summary>Details</summary>
Motivation: 现有的神经算子和大规模预测器依赖于密集的、共置的输入输出场和短时间上下文，这在传感器和预测发生在不同物理流形上并具有长时间尺度时无法满足实际系统需求。

Method: STONe 学习异构域之间稳定的功能映射，直接从稀疏的地面中子测量数据推断高海拔辐射剂量场。它定义了一个在传感器和目标流形之间的非线性算子，该算子在长时间预测范围内保持稳定，无需迭代递归。

Result: STONe 在共享域设置之外推广了算子学习。该模型在 23 年的全球中子数据上进行训练，实现了精确的 180 天预测，推理延迟为毫秒级。

Conclusion: STONe 挑战了算子学习需要域对齐或自回归传播的传统观点，为跨域算子推断建立了通用原则，从而实现了物理、气候和能源系统中复杂时空场的实时预测。

Abstract: Forecasting unobservable physical quantities from sparse, cross-domain sensor
data is a central unsolved problem in scientific machine learning. Existing
neural operators and large-scale forecasters rely on dense, co-located
input-output fields and short temporal contexts, assumptions that fail in
real-world systems where sensing and prediction occur on distinct physical
manifolds and over long timescales. We introduce the Spatio-Temporal Operator
Network (STONe), a non-autoregressive neural operator that learns a stable
functional mapping between heterogeneous domains. By directly inferring
high-altitude radiation dose fields from sparse ground-based neutron
measurements, STONe demonstrates that operator learning can generalize beyond
shared-domain settings. It defines a nonlinear operator between sensor and
target manifolds that remains stable over long forecasting horizons without
iterative recurrence. This challenges the conventional view that operator
learning requires domain alignment or autoregressive propagation. Trained on 23
years of global neutron data, STONe achieves accurate 180-day forecasts with
millisecond inference latency. The framework establishes a general principle
for cross-domain operator inference, enabling real-time prediction of complex
spatiotemporal fields in physics, climate, and energy systems.

</details>


### [113] [Measure-Theoretic Anti-Causal Representation Learning](https://arxiv.org/abs/2510.18052)
*Arman Behnam,Binghui Wang*

Main category: cs.LG

TL;DR: 该文章提出了Anti-Causal Invariant Abstractions (ACIA)，一个针对反因果表示学习的度量理论框架。


<details>
  <summary>Details</summary>
Motivation: 在反因果背景下（标签导致特征而非相反）进行因果表示学习，面临独特的挑战，需要专业方法来解决现有方法的局限性。

Method: ACIA采用两级设计，低级表示捕获标签如何生成观测数据，而高级表示学习跨环境变化的稳定因果模式。通过干预核，ACIA能适应完美和不完美干预，消除对显式因果结构的依赖，有效处理高维数据。

Result: 在合成和真实世界的医疗数据集上的实验表明，ACIA在准确性和不变性指标上均优于现有最新方法。

Conclusion: ACIA提供了理论保证，确保了域外泛化能力，并建立了训练与未见环境之间性能差距的紧密界限，证实了其在稳健反因果学习中的有效性。

Abstract: Causal representation learning in the anti-causal setting (labels cause
features rather than the reverse) presents unique challenges requiring
specialized approaches. We propose Anti-Causal Invariant Abstractions (ACIA), a
novel measure-theoretic framework for anti-causal representation learning. ACIA
employs a two-level design, low-level representations capture how labels
generate observations, while high-level representations learn stable causal
patterns across environment-specific variations. ACIA addresses key limitations
of existing approaches by accommodating prefect and imperfect interventions
through interventional kernels, eliminating dependency on explicit causal
structures, handling high-dimensional data effectively, and providing
theoretical guarantees for out-of-distribution generalization. Experiments on
synthetic and real-world medical datasets demonstrate that ACIA consistently
outperforms state-of-the-art methods in both accuracy and invariance metrics.
Furthermore, our theoretical results establish tight bounds on performance gaps
between training and unseen environments, confirming the efficacy of our
approach for robust anti-causal learning.

</details>


### [114] [Adaptive Divergence Regularized Policy Optimization for Fine-tuning Generative Models](https://arxiv.org/abs/2510.18053)
*Jiajun Fan,Tong Wei,Chaoran Cheng,Yuxin Chen,Ge Liu*

Main category: cs.LG

TL;DR: 该论文介绍了一种名为ADRPO的自适应散度正则化策略优化方法，通过根据优势估计调整正则化强度，在生成模型的强化学习微调中平衡探索与利用。


<details>
  <summary>Details</summary>
Motivation: 在生成模型的强化学习微调中，平衡探索和利用是一个关键挑战。现有方法依赖固定散度正则化，导致模型能力保持与奖励优化之间的两难境地。

Method: ADRPO方法根据优势估计自动调整正则化强度。对于高价值样本，减少正则化强度以促进探索；对于低价值样本，施加强正则化以确保稳定性。

Result: 在文本到图像生成任务中，ADRPO在Wassertstein-2正则化下，使得2B参数的SD3模型在属性绑定、语义一致性、艺术风格迁移和组合控制方面超越了4.8B和12B参数的更大模型。ADRPO还推广到KL正则化的纯文本LLM和多模态推理模型微调，展示了更好的局部最优逃逸和逐步推理能力，甚至使7B模型在多模态音频推理中超越了Gemini 2.5 Pro和GPT-4o Audio等大型商业模型。

Conclusion: ADRPO是一种有效且“即插即用”的解决方案，能够应对不同生成架构和模态中的探索-利用挑战，显著提升了生成模型在多种任务上的性能。

Abstract: Balancing exploration and exploitation during reinforcement learning
fine-tuning of generative models presents a critical challenge, as existing
approaches rely on fixed divergence regularization that creates an inherent
dilemma: strong regularization preserves model capabilities but limits reward
optimization, while weak regularization enables greater alignment but risks
instability or reward hacking. We introduce Adaptive Divergence Regularized
Policy Optimization (ADRPO), which automatically adjusts regularization
strength based on advantage estimates-reducing regularization for high-value
samples while applying stronger regularization to poor samples, enabling
policies to navigate between exploration and aggressive exploitation according
to data quality. Our implementation with Wasserstein-2 regularization for flow
matching generative models achieves remarkable results on text-to-image
generation, achieving better semantic alignment and diversity than offline
methods like DPO and online methods with fixed regularization like ORW-CFM-W2.
ADRPO enables a 2B parameter SD3 model to surpass much larger models with 4.8B
and 12B parameters in attribute binding, semantic consistency, artistic style
transfer, and compositional control while maintaining generation diversity.
ADRPO generalizes to KL-regularized fine-tuning of both text-only LLMs and
multi-modal reasoning models, enhancing existing online RL methods like GRPO.
In LLM fine-tuning, ADRPO demonstrates an emergent ability to escape local
optima through active exploration, while in multi-modal audio reasoning, it
outperforms GRPO through superior step-by-step reasoning, enabling a 7B model
to outperform substantially larger commercial models including Gemini 2.5 Pro
and GPT-4o Audio, offering an effective plug-and-play solution to the
exploration-exploitation challenge across diverse generative architectures and
modalities.

</details>


### [115] [Fine-tuning Flow Matching Generative Models with Intermediate Feedback](https://arxiv.org/abs/2510.18072)
*Jiajun Fan,Chaoran Cheng,Shuaike Shen,Xiangxin Zhou,Ge Liu*

Main category: cs.LG

TL;DR: AC-Flow是一个用于文本到图像生成流模型的鲁棒的Actor-Critic框架，它通过奖励整形、双重稳定性机制和可伸缩的广义评论家加权方案，在保持生成质量和多样性的同时，实现了SOTA的文本到图像对齐性能。


<details>
  <summary>Details</summary>
Motivation: 目前基于流的生成模型在文本到图像生成方面取得了显著成功，但是使用中间反馈进行微调仍然具有挑战性，特别是对于连续时间流匹配模型。大多数现有方法只从结果奖励中学习，难以解决信用分配问题。那些试图通过对累积奖励进行直接回归来学习评论家的方法，在在线设置中经常面临训练不稳定和模型崩溃的问题。

Method: AC-Flow提出了一种鲁棒的Actor-Critic框架，通过三个关键创新来解决这些挑战：（1）奖励整形，提供良好归一化的学习信号，以实现稳定的中间价值学习和梯度控制；（2）新颖的双重稳定性机制，结合优势削减以防止破坏性策略更新，并包含一个预热阶段，允许评论家在影响Actor之前成熟；（3）可扩展的广义评论家加权方案，扩展了传统的奖励加权方法，并通过Wasserstein正则化保持模型多样性。

Result: AC-Flow在Stable Diffusion 3上进行了广泛实验，证明它在文本到图像对齐任务和对未见人类偏好模型的泛化方面取得了最先进的性能。通过计算高效的评论家模型，AC-Flow可以鲁棒地微调流模型，而不会损害生成质量、多样性或稳定性。

Conclusion: AC-Flow为文本到图像生成流模型的微调提供了一个有效且鲁棒的解决方案，解决了现有方法在信用分配、训练稳定性和模型崩溃方面的挑战，从而在对齐和泛化方面显著优于现有技术。

Abstract: Flow-based generative models have shown remarkable success in text-to-image
generation, yet fine-tuning them with intermediate feedback remains
challenging, especially for continuous-time flow matching models. Most existing
approaches solely learn from outcome rewards, struggling with the credit
assignment problem. Alternative methods that attempt to learn a critic via
direct regression on cumulative rewards often face training instabilities and
model collapse in online settings. We present AC-Flow, a robust actor-critic
framework that addresses these challenges through three key innovations: (1)
reward shaping that provides well-normalized learning signals to enable stable
intermediate value learning and gradient control, (2) a novel dual-stability
mechanism that combines advantage clipping to prevent destructive policy
updates with a warm-up phase that allows the critic to mature before
influencing the actor, and (3) a scalable generalized critic weighting scheme
that extends traditional reward-weighted methods while preserving model
diversity through Wasserstein regularization. Through extensive experiments on
Stable Diffusion 3, we demonstrate that AC-Flow achieves state-of-the-art
performance in text-to-image alignment tasks and generalization to unseen human
preference models. Our results demonstrate that even with a computationally
efficient critic model, we can robustly finetune flow models without
compromising generative quality, diversity, or stability.

</details>


### [116] [R2L: Reliable Reinforcement Learning: Guaranteed Return & Reliable Policies in Reinforcement Learning](https://arxiv.org/abs/2510.18074)
*Nadir Farhi*

Main category: cs.LG

TL;DR: 这篇论文提出了一种在不确定性下，解决强化学习中可靠策略问题的新方法，旨在最大化累积回报超过预设阈值的概率，并通过将其转化为标准强化学习问题，实现了性能保障。


<details>
  <summary>Details</summary>
Motivation: 经典强化学习算法侧重于最大化预期回报，但许多实际应用（如路径规划、资源分配和风险下的序贯决策）需要不仅具有高平均性能，还要能保证成功概率的策略。因此，本文旨在解决在强化学习中确定可靠策略的问题，特别关注不确定性下的优化和性能保证的需求。

Method: 本文提出了一种新的表述方法，将目标设定为最大化累积回报超过预设阈值的概率。通过状态增强表示，将这个可靠强化学习问题重新表述为一个标准的强化学习问题，从而可以直接利用现有的强化学习和深度强化学习算法。理论结果证明了这两种表述的等价性，并表明可以通过适当调整Q-learning或Dueling Double DQN等常用方法来推导出可靠策略。

Result: 数值实验表明，所提出的方法能够有效地平衡效率和可靠性，为随机和安全关键环境中的应用提供了潜力。特别在可靠路径规划问题中，该方法能够最大化在给定时间预算内到达目的地的概率。

Conclusion: 本研究提出了一种在强化学习中实现可靠策略的新框架。通过将可靠性问题转化为标准强化学习问题，并通过数值实验验证了其有效性，证明了该方法在需要性能保证的实际应用中的巨大潜力。

Abstract: In this work, we address the problem of determining reliable policies in
reinforcement learning (RL), with a focus on optimization under uncertainty and
the need for performance guarantees. While classical RL algorithms aim at
maximizing the expected return, many real-world applications - such as routing,
resource allocation, or sequential decision-making under risk - require
strategies that ensure not only high average performance but also a guaranteed
probability of success. To this end, we propose a novel formulation in which
the objective is to maximize the probability that the cumulative return exceeds
a prescribed threshold. We demonstrate that this reliable RL problem can be
reformulated, via a state-augmented representation, into a standard RL problem,
thereby allowing the use of existing RL and deep RL algorithms without the need
for entirely new algorithmic frameworks. Theoretical results establish the
equivalence of the two formulations and show that reliable strategies can be
derived by appropriately adapting well-known methods such as Q-learning or
Dueling Double DQN. To illustrate the practical relevance of the approach, we
consider the problem of reliable routing, where the goal is not to minimize the
expected travel time but rather to maximize the probability of reaching the
destination within a given time budget. Numerical experiments confirm that the
proposed formulation leads to policies that effectively balance efficiency and
reliability, highlighting the potential of reliable RL for applications in
stochastic and safety-critical environments.

</details>


### [117] [Batch Distillation Data for Developing Machine Learning Anomaly Detection Methods](https://arxiv.org/abs/2510.18075)
*Justus Arweiler,Indra Jungjohann,Aparna Muraleedharan,Heike Leitte,Jakob Burger,Kerstin Münnemann,Fabian Jirasek,Hans Hasse*

Main category: cs.LG

TL;DR: 该论文介绍了一个为机器学习（ML）异常检测（AD）方法开发的大型实验数据库。


<details>
  <summary>Details</summary>
Motivation: 解决机器学习（ML）异常检测（AD）方法在化学过程中，由于缺乏公开可用的实验数据而受阻的问题。

Method: 建立了一个实验室规模的间歇精馏装置，并通过操作该装置生成了包含无故障和有意引入异常的实验数据。总共进行了119次实验，涵盖了广泛的操作条件和混合物。数据包括时间序列数据、测量不确定性估计、在线台式核磁共振波谱获得的浓度分布、视频和音频记录，以及详细的元数据和专家注释。异常注释基于本文开发的本体。

Result: 创建了一个包含119次实验的综合数据库，涵盖了各种操作条件和混合物的无故障和异常数据。数据包括来自传感器和执行器的时间序列数据、测量不确定性估计、浓度分布、视频和音频记录，以及详细的元数据和专家注释。该数据库已结构化并免费提供。

Conclusion: 所创建的数据库为开发先进的基于ML的AD方法铺平了道路，并通过包含异常原因的信息，促进了可解释和可解释的ML方法以及异常缓解方法的发展。

Abstract: Machine learning (ML) holds great potential to advance anomaly detection (AD)
in chemical processes. However, the development of ML-based methods is hindered
by the lack of openly available experimental data. To address this gap, we have
set up a laboratory-scale batch distillation plant and operated it to generate
an extensive experimental database, covering fault-free experiments and
experiments in which anomalies were intentionally induced, for training
advanced ML-based AD methods. In total, 119 experiments were conducted across a
wide range of operating conditions and mixtures. Most experiments containing
anomalies were paired with a corresponding fault-free one. The database that we
provide here includes time-series data from numerous sensors and actuators,
along with estimates of measurement uncertainty. In addition, unconventional
data sources -- such as concentration profiles obtained via online benchtop NMR
spectroscopy and video and audio recordings -- are provided. Extensive metadata
and expert annotations of all experiments are included. The anomaly annotations
are based on an ontology developed in this work. The data are organized in a
structured database and made freely available via
doi.org/10.5281/zenodo.17395544. This new database paves the way for the
development of advanced ML-based AD methods. As it includes information on the
causes of anomalies, it further enables the development of interpretable and
explainable ML approaches, as well as methods for anomaly mitigation.

</details>


### [118] [MEG-GPT: A transformer-based foundation model for magnetoencephalography data](https://arxiv.org/abs/2510.18080)
*Rukuang Huang,Sungjun Cho,Chetan Gohil,Oiwi Parker Jones,Mark Woolrich*

Main category: cs.LG

TL;DR: MEG-GPT是一个基于Transformer的自回归模型，它结合了时间注意力机制和下一个时间点预测，以及一种新颖的数据驱动的分词器来处理连续的MEG数据。


<details>
  <summary>Details</summary>
Motivation: 传统的脑动力学建模方法未能捕捉到MEG等模态中的丰富结构，而深度学习的最新进展在其他领域取得了显著进展。

Method: 本文引入MEG-GPT，这是一个基于Transformer的自回归模型，它利用时间注意力机制和下一个时间点预测。同时，本文还引入了一种新颖的数据驱动的分词器，用于处理连续的MEG数据。MEG-GPT在大规模MEG数据集（N=612，闭眼休息，Cam-CAN数据）中进行训练。

Result: MEG-GPT可以生成具有真实时空频谱特性的数据，包括瞬态事件和群体变异性。在下游解码任务中表现出色，改进了下游监督预测任务，并在跨会话（准确率从0.54提高到0.59）和跨受试者（准确率从0.41提高到0.49）的零样本泛化方面优于基线方法。此外，该模型可以在较小的标记数据集上进行高效微调，以提高跨受试者解码场景的性能。

Conclusion: 本研究为电生理数据建立了一个强大的基础模型，为计算神经科学和神经解码的应用铺平了道路。

Abstract: Modelling the complex spatiotemporal patterns of large-scale brain dynamics
is crucial for neuroscience, but traditional methods fail to capture the rich
structure in modalities such as magnetoencephalography (MEG). Recent advances
in deep learning have enabled significant progress in other domains, such as
language and vision, by using foundation models at scale. Here, we introduce
MEG-GPT, a transformer based foundation model that uses time-attention and next
time-point prediction. To facilitate this, we also introduce a novel
data-driven tokeniser for continuous MEG data, which preserves the high
temporal resolution of continuous MEG signals without lossy transformations. We
trained MEG-GPT on tokenised brain region time-courses extracted from a
large-scale MEG dataset (N=612, eyes-closed rest, Cam-CAN data), and show that
the learnt model can generate data with realistic spatio-spectral properties,
including transient events and population variability. Critically, it performs
well in downstream decoding tasks, improving downstream supervised prediction
task, showing improved zero-shot generalisation across sessions (improving
accuracy from 0.54 to 0.59) and subjects (improving accuracy from 0.41 to 0.49)
compared to a baseline methods. Furthermore, we show the model can be
efficiently fine-tuned on a smaller labelled dataset to boost performance in
cross-subject decoding scenarios. This work establishes a powerful foundation
model for electrophysiological data, paving the way for applications in
computational neuroscience and neural decoding.

</details>


### [119] [Any-Depth Alignment: Unlocking Innate Safety Alignment of LLMs to Any-Depth](https://arxiv.org/abs/2510.18081)
*Jiawei Zhang,Andrew Estornell,David D. Baek,Bo Li,Xiaojun Xu*

Main category: cs.LG

TL;DR: ADA是一种有效且开销可忽略的推理时防御机制，可以在不改变模型参数的前提下，使大型语言模型在任意生成深度上保持对有害查询的拒绝能力，有效抵御各种对抗性攻击，并保持良性任务的效用。


<details>
  <summary>Details</summary>
Motivation: 大型语言模型（LLMs）存在浅层对齐问题：它们在助手回合开始时能拒绝有害查询，但当有害内容生成进行中时（通过对抗性攻击或有害助手预填充攻击），这种保护会失效。这引出了一个基本问题：能否解锁LLMs固有的浅层对齐，以确保在任意生成深度上的安全性？

Method: 本文提出了一种名为Any-Depth Alignment（ADA）的推理时防御机制。ADA基于一个观察：对齐能力集中在助手头部标记中，这是通过浅层拒绝训练反复使用造成的。
这些标记具有模型强大的对齐先验。
ADA通过在生成过程中重新引入这些标记，促使模型重新评估有害性，并在生成的任何时刻恢复拒绝。

Result: 在不同的开源模型家族（Llama、Gemma、Mistral、Qwen、DeepSeek和gpt-oss）中，ADA在不改变基础模型参数的情况下，实现了强大的安全性能。它对几十到几千个标记的挑战性对抗性预填充攻击实现了近100%的拒绝率。此外，ADA将主要对抗性提示攻击（如GCG、AutoDAN、PAIR和TAP）的平均成功率降低到3%以下。所有这些都在保持良性任务效用的同时，将过度拒绝降到最低。即使基础模型经过后续的指令调整（良性或对抗性），ADA也能保持这种弹性。

Conclusion: ADA通过在生成过程中重新引入助手头部标记，成功地将会话式对齐从其初始位置中解放出来，使其能够在对模型参数没有任何修改的情况下在任意生成深度上工作。这解决了LLMs的浅层对齐问题，显著提高了模型在面对各种对抗性攻击时的安全性，同时保持了实用性。

Abstract: Large Language Models (LLMs) exhibit strong but shallow alignment: they
directly refuse harmful queries when a refusal is expected at the very start of
an assistant turn, yet this protection collapses once a harmful continuation is
underway (either through the adversarial attacks or via harmful
assistant-prefill attacks). This raises a fundamental question: Can the innate
shallow alignment in LLMs be unlocked to ensure safety at arbitrary generation
depths? To achieve this goal, we propose Any-Depth Alignment (ADA), an
effective inference-time defense with negligible overhead. ADA is built based
on our observation that alignment is concentrated in the assistant header
tokens through repeated use in shallow-refusal training, and these tokens
possess the model's strong alignment priors. By reintroducing these tokens
mid-stream, ADA induces the model to reassess harmfulness and recover refusals
at any point in generation. Across diverse open-source model families (Llama,
Gemma, Mistral, Qwen, DeepSeek, and gpt-oss), ADA achieves robust safety
performance without requiring any changes to the base model's parameters. It
secures a near-100% refusal rate against challenging adversarial prefill
attacks ranging from dozens to thousands of tokens. Furthermore, ADA reduces
the average success rate of prominent adversarial prompt attacks (such as GCG,
AutoDAN, PAIR, and TAP) to below 3%. This is all accomplished while preserving
utility on benign tasks with minimal over-refusal. ADA maintains this
resilience even after the base model undergoes subsequent instruction tuning
(benign or adversarial).

</details>


### [120] [Provably Optimal Reinforcement Learning under Safety Filtering](https://arxiv.org/abs/2510.18082)
*Donggeon David Oh,Duy P. Nguyen,Haimin Hu,Jaime F. Fisac*

Main category: cs.LG

TL;DR: 该论文提出了一种新的安全强化学习范式，证明了在保证安全性的前提下，安全过滤器不会降低渐近性能，并通过实验验证了其有效性。


<details>
  <summary>Details</summary>
Motivation: 强化学习在复杂任务中取得了进展，但缺乏正式的安全保证限制了其在安全关键领域的应用。现有的安全过滤器常被认为会牺牲性能并阻碍学习过程。

Method: 1. 提出了安全关键型马尔可夫决策过程 (SC-MDP)，要求绝对避免灾难性故障状态。2. 定义了过滤后的MDP，其中所有动作都通过安全过滤器确保安全。3. 建立了主要理论，证明了在过滤后的MDP中学习的安全性、标准RL收敛性以及与SC-MDP中最佳安全策略相同的渐近回报。

Result: 1. 理论上证明了在充分宽松的安全过滤器下，安全强化学习不会降低渐近性能。2. 在Safety Gymnasium上进行的实验验证了该理论，消除了训练过程中的违规，并达到了与未经过滤的基线相当或更高的最终性能。

Conclusion: 安全执法与性能优化之间可以完全分离。训练和部署RL策略时，应使用最宽松的安全过滤器。

Abstract: Recent advances in reinforcement learning (RL) enable its use on increasingly
complex tasks, but the lack of formal safety guarantees still limits its
application in safety-critical settings. A common practical approach is to
augment the RL policy with a safety filter that overrides unsafe actions to
prevent failures during both training and deployment. However, safety filtering
is often perceived as sacrificing performance and hindering the learning
process. We show that this perceived safety-performance tradeoff is not
inherent and prove, for the first time, that enforcing safety with a
sufficiently permissive safety filter does not degrade asymptotic performance.
We formalize RL safety with a safety-critical Markov decision process (SC-MDP),
which requires categorical, rather than high-probability, avoidance of
catastrophic failure states. Additionally, we define an associated filtered MDP
in which all actions result in safe effects, thanks to a safety filter that is
considered to be a part of the environment. Our main theorem establishes that
(i) learning in the filtered MDP is safe categorically, (ii) standard RL
convergence carries over to the filtered MDP, and (iii) any policy that is
optimal in the filtered MDP-when executed through the same filter-achieves the
same asymptotic return as the best safe policy in the SC-MDP, yielding a
complete separation between safety enforcement and performance optimization. We
validate the theory on Safety Gymnasium with representative tasks and
constraints, observing zero violations during training and final performance
matching or exceeding unfiltered baselines. Together, these results shed light
on a long-standing question in safety-filtered learning and provide a simple,
principled recipe for safe RL: train and deploy RL policies with the most
permissive safety filter that is available.

</details>


### [121] [Enhancing mortality prediction in cardiac arrest ICU patients through meta-modeling of structured clinical data from MIMIC-IV](https://arxiv.org/abs/2510.18103)
*Nursultan Mamatov,Philipp Kellmeyer*

Main category: cs.LG

TL;DR: 该研究开发并评估了结合结构化临床数据和非结构化文本信息（特别是MIMIC-IV数据库中的出院总结和放射学报告）的机器学习模型，用于准确预测ICU住院死亡率。


<details>
  <summary>Details</summary>
Motivation: 在重症监护室（ICU）中，准确地早期预测住院死亡率对于及时的临床干预和高效的资源分配至关重要。

Method: 本研究使用了LASSO和XGBoost进行特征选择，随后使用这两种模型确定的顶级特征训练多变量逻辑回归模型。通过TF-IDF和BERT嵌入结合文本特征显著提高了预测性能。

Result: 最终的逻辑回归模型结合了结构化和文本输入，实现了0.918的AUC，而单独使用结构化数据时AUC为0.753，相对提高了22%。决策曲线分析表明，在0.2-0.8的广泛阈值概率范围内，标准化净收益更优。

Conclusion: 这些结果强调了非结构化临床笔记的额外预后价值，并支持将它们整合到可解释的特征驱动的ICU患者风险预测模型中。

Abstract: Accurate early prediction of in-hospital mortality in intensive care units
(ICUs) is essential for timely clinical intervention and efficient resource
allocation. This study develops and evaluates machine learning models that
integrate both structured clinical data and unstructured textual information,
specifically discharge summaries and radiology reports, from the MIMIC-IV
database. We used LASSO and XGBoost for feature selection, followed by a
multivariate logistic regression trained on the top features identified by both
models. Incorporating textual features using TF-IDF and BERT embeddings
significantly improved predictive performance. The final logistic regression
model, which combined structured and textual input, achieved an AUC of 0.918,
compared to 0.753 when using structured data alone, a relative improvement 22%.
The analysis of the decision curve demonstrated a superior standardized net
benefit in a wide range of threshold probabilities (0.2-0.8), confirming the
clinical utility of the model. These results underscore the added prognostic
value of unstructured clinical notes and support their integration into
interpretable feature-driven risk prediction models for ICU patients.

</details>


### [122] [Gradient Variance Reveals Failure Modes in Flow-Based Generative Models](https://arxiv.org/abs/2510.18118)
*Teodora Reu,Sixtine Dromigny,Michael Bronstein,Francisco Vargas*

Main category: cs.LG

TL;DR: 本文揭示了Rectified Flows在确定性训练下的根本性失效模式，即由于低梯度方差导致模型记忆训练配对，即使插值线相交。研究发现，注入少量噪声可以恢复模型的泛化能力。


<details>
  <summary>Details</summary>
Motivation: Rectified Flows学习的ODE向量场在源分布和目标分布之间是直线轨迹，可以实现近乎一步的推理。然而，这种直线路径目标在确定性训练下存在根本性的失效模式。

Method: 本文通过研究高斯到高斯传输，并利用随机和确定性机制下的损失梯度方差来表征优化在每种设置中偏爱的矢量场。此外，作者证明了即使训练插值线相交，也会存在一个记忆性矢量场，并且优化直线路径目标会收敛到这个不明确的矢量场。

Result: 研究发现，在确定性训练下，低梯度方差会导致模型记忆任意训练配对，即使配对之间的插值线相交。在所有插值线相交的情况下，应用Rectified Flow在推理时会产生与训练时相同的特定配对。经验证，确定性插值会导致记忆，而注入少量噪声可以恢复泛化。

Conclusion: Rectified Flows在确定性训练中存在记忆训练配对的失效模式，这是由于低梯度方差导致的。为了恢复模型的泛化能力，可以通过注入少量噪声来解决这个问题。

Abstract: Rectified Flows learn ODE vector fields whose trajectories are straight
between source and target distributions, enabling near one-step inference. We
show that this straight-path objective conceals fundamental failure modes:
under deterministic training, low gradient variance drives memorization of
arbitrary training pairings, even when interpolant lines between pairs
intersect. To analyze this mechanism, we study Gaussian-to-Gaussian transport
and use the loss gradient variance across stochastic and deterministic regimes
to characterize which vector fields optimization favors in each setting. We
then show that, in a setting where all interpolating lines intersect, applying
Rectified Flow yields the same specific pairings at inference as during
training. More generally, we prove that a memorizing vector field exists even
when training interpolants intersect, and that optimizing the straight-path
objective converges to this ill-defined field. At inference, deterministic
integration reproduces the exact training pairings. We validate our findings
empirically on the CelebA dataset, confirming that deterministic interpolants
induce memorization, while the injection of small noise restores
generalization.

</details>


### [123] [Efficient Long-context Language Model Training by Core Attention Disaggregation](https://arxiv.org/abs/2510.18121)
*Yonghao Zhuang,Junda Chen,Bo Pang,Yi Gu,Yibo Zhu,Yimin Jiang,Ion Stoica,Eric Xing,Hao Zhang*

Main category: cs.LG

TL;DR: CAD（core attention disaggregation）技术通过将核心注意力计算从模型中分离出来，并在独立的设备池上执行，从而提高了长上下文大型语言模型的训练效率。


<details>
  <summary>Details</summary>
Motivation: 在现有系统中，核心注意力计算与模型其他层共存，但其计算量随上下文长度呈二次增长，导致负载不平衡和性能瓶颈。

Method: CAD将核心注意力计算分解为令牌级别的任务，并将其分派到专用的注意力服务器。这些服务器动态地重新批处理任务以平衡计算负载，同时保持内核效率。通过采用乒乓执行方案，实现通信与计算的完全重叠，并在注意力服务器上进行原地执行以减少内存使用。

Result: 在512 H200 GPU和长达512k令牌的上下文长度下，DistCA（实现了CAD的系统）将端到端训练吞吐量提高了1.35倍，消除了数据和管道并行中的性能瓶颈，并实现了近乎完美的计算和内存平衡。

Conclusion: CAD技术通过解耦和优化核心注意力计算，显著提升了长上下文大型语言模型的训练效率和资源利用率。

Abstract: We present core attention disaggregation (CAD), a technique that improves
long-context large language model training by decoupling the core attention
computation, softmax(QK^T)V, from the rest of the model and executing it on a
separate pool of devices. In existing systems, core attention is colocated with
other layers; at long context lengths, its quadratic compute growth compared to
the near-linear growth of other components causes load imbalance and stragglers
across data and pipeline parallel groups. CAD is enabled by two observations.
First, core attention is stateless: it has no trainable parameters and only
minimal transient data, so balancing reduces to scheduling compute-bound tasks.
Second, it is composable: modern attention kernels retain high efficiency when
processing fused batches of token-level shards with arbitrary lengths. CAD
partitions core attention into token-level tasks and dispatches them to
dedicated attention servers, which dynamically rebatch tasks to equalize
compute without sacrificing kernel efficiency. We implement CAD in a system
called DistCA, which uses a ping-pong execution scheme to fully overlap
communication with computation and in-place execution on attention servers to
reduce memory use. On 512 H200 GPUs and context lengths up to 512k tokens,
DistCA improves end-to-end training throughput by up to 1.35x, eliminates data
and pipeline parallel stragglers, and achieves near-perfect compute and memory
balance.

</details>


### [124] [ActivationReasoning: Logical Reasoning in Latent Activation Spaces](https://arxiv.org/abs/2510.18184)
*Lukas Helff,Ruben Härle,Wolfgang Stammer,Felix Friedrich,Manuel Brack,Antonia Wüst,Hikaru Shindo,Patrick Schramowski,Kristian Kersting*

Main category: cs.LG

TL;DR: 该论文介绍了一种名为 ActivationReasoning (AR) 的框架，旨在通过将显式逻辑推理嵌入到大型语言模型的潜在空间中，提高模型的可解释性和可控性。


<details>
  <summary>Details</summary>
Motivation: 大型语言模型在生成流畅文本方面表现出色，但其内部推理过程不透明且难以控制。现有的稀疏自编码器（SAE）虽然能暴露与人类概念相关的潜在特征，但这些特征脆弱且被动，无法实现系统推理或模型控制，这激发了研究人员寻求一种将逻辑推理引入LLM潜在空间的方法。

Method: ActivationReasoning (AR) 框架分三个阶段进行：1. 寻找潜在表征：识别潜在概念表征（例如通过稀疏自编码器），并将其组织成字典。2. 激活命题：在推理时，AR 检测激活概念并将其映射到逻辑命题。3. 逻辑推理：对这些命题应用逻辑规则，以推断更高阶的结构、组合新概念并引导模型行为。

Result: AR 在多跳推理（PrOntoQA）、抽象和对间接概念线索的鲁棒性（Rail2Country）、对自然和多样语言的推理（ProverQA）以及上下文敏感安全性（BeaverTails）方面进行了评估。结果表明，AR 能够稳健地应对推理复杂性，泛化到抽象和上下文敏感任务，并跨模型骨干进行迁移。

Conclusion: 将逻辑结构根植于潜在激活中，不仅可以提高透明度，还可以实现结构化推理、可靠控制，并与期望行为保持一致。这为实现更可靠和可审计的 AI 提供了一条途径。

Abstract: Large language models (LLMs) excel at generating fluent text, but their
internal reasoning remains opaque and difficult to control. Sparse autoencoders
(SAEs) make hidden activations more interpretable by exposing latent features
that often align with human concepts. Yet, these features are fragile and
passive, offering no mechanism for systematic reasoning or model control. To
address this, we introduce ActivationReasoning (AR), a framework that embeds
explicit logical reasoning into the latent space of LLMs. It proceeds in three
stages: (1) Finding latent representations, first latent concept
representations are identified (e.g., via SAEs) and organized into a
dictionary; (2) Activating propositions, at inference time AR detects
activating concepts and maps them to logical propositions; and (3)Logical
reasoning, applying logical rules over these propositions to infer higher-order
structures, compose new concepts, and steer model behavior. We evaluate AR on
multi-hop reasoning (PrOntoQA), abstraction and robustness to indirect concept
cues (Rail2Country), reasoning over natural and diverse language (ProverQA),
and context-sensitive safety (BeaverTails). Across all tasks, AR scales
robustly with reasoning complexity, generalizes to abstract and
context-sensitive tasks, and transfers across model backbones. These results
demonstrate that grounding logical structure in latent activations not only
improves transparency but also enables structured reasoning, reliable control,
and alignment with desired behaviors, providing a path toward more reliable and
auditable AI.

</details>


### [125] [Ensemble based Closed-Loop Optimal Control using Physics-Informed Neural Networks](https://arxiv.org/abs/2510.18195)
*Jostein Barry-Straume,Adwait D. Verulkar,Arash Sarshar,Andrey A. Popov,Adrian Sandu*

Main category: cs.LG

TL;DR: 本文提出了一种分阶段集成框架，用于学习最优代价和相应的最优控制信号，以解决HJB方程的计算困难。


<details>
  <summary>Details</summary>
Motivation: Hamilton-Jacobi-Bellman (HJB) 偏微分方程为最优控制系统设计提供了一个框架，但其数值解计算量大，解析解通常 R 难以获得。

Method: 本文提出了一种多阶段集成框架来学习最优的代价函数，并随后通过HJB方程学习相应的最优控制信号。该框架不使用稳定器项，并且通过单一学习控制信号或集成控制信号策略来控制非线性系统。

Result: 在具有无限时间范围的稳态时不变两态连续非线性系统的闭环控制中，使用集成控制和奇异控制，证明了该方法的成功，该系统考虑了噪声、扰动的系统状态和变化的初始条件。

Conclusion: 本研究提出的多阶段集成框架能够有效解决HJB方程的计算难题，实现了对非线性系统的最优控制，并且在各种复杂条件下表现出良好的性能。

Abstract: The objective of designing a control system is to steer a dynamical system
with a control signal, guiding it to exhibit the desired behavior. The
Hamilton-Jacobi-Bellman (HJB) partial differential equation offers a framework
for optimal control system design. However, numerical solutions to this
equation are computationally intensive, and analytical solutions are frequently
unavailable. Knowledge-guided machine learning methodologies, such as
physics-informed neural networks (PINNs), offer new alternative approaches that
can alleviate the difficulties of solving the HJB equation numerically. This
work presents a multistage ensemble framework to learn the optimal cost-to-go,
and subsequently the corresponding optimal control signal, through the HJB
equation. Prior PINN-based approaches rely on a stabilizing the HJB enforcement
during training. Our framework does not use stabilizer terms and offers a means
of controlling the nonlinear system, via either a singular learned control
signal or an ensemble control signal policy. Success is demonstrated in
closed-loop control, using both ensemble- and singular-control, of a
steady-state time-invariant two-state continuous nonlinear system with an
infinite time horizon, accounting of noisy, perturbed system states and varying
initial conditions.

</details>


### [126] [Towards Fast LLM Fine-tuning through Zeroth-Order Optimization with Projected Gradient-Aligned Perturbations](https://arxiv.org/abs/2510.18228)
*Zhendong Mi,Qitao Tan,Grace Li Zhang,Zhaozhuo Xu,Geng Yuan,Shaoyi Huang*

Main category: cs.LG

TL;DR: P-GAP是一种基于零阶优化的LLM微调方法，通过投影梯度对齐扰动，减少扰动参数数量和方差，显著加速收敛并提高LLM在分类和生成任务上的性能，同时节省训练资源。


<details>
  <summary>Details</summary>
Motivation: 现有的零阶优化方法在微调大型语言模型（LLMs）时存在梯度估计方差大、收敛速度慢和性能次优的问题。

Method: 我们提出了P-GAP方法，该方法首先估计一个低维梯度空间，然后将扰动与该空间内投影梯度的方向对齐。这种方法减少了扰动参数的数量并降低了方差。

Result: 在LLMs上的实验表明，P-GAP始终优于基线方法，在分类任务中准确率提高了6%，在生成任务中准确率提高了12%，同时减少了81%的训练迭代次数和70%的GPU小时。

Conclusion: P-GAP为LLM的零阶微调提供了一种快速、可扩展且资源高效的方法。

Abstract: Fine-tuning large language models (LLMs) using zeroth-order (ZO) optimization
has emerged as a promising alternative to traditional gradient-based methods
due to its reduced memory footprint requirement. However, existing ZO methods
suffer from high variance in gradient estimation, leading to slow convergence
and suboptimal performance on large-scale models. In this work, we propose
P-GAP, a fast LLM fine-tuning approach through zeroth-order optimization with
Projected Gradient-Aligned Perturbations. Specifically, we first estimate a
low-dimensional gradient space and then align perturbations in projected
gradients' direction within the space. This approach enables reduced the number
of perturbed parameters and decreased variance, therefore accelerated
convergence for LLM fine-tuning. Experiments on LLMs show that P-GAP
consistently surpasses the baselines, achieving up to 6% increase in accuracy
on classification tasks and up to 12% higher accuracy on generation tasks, with
up to about 81% less training iterations and 70% less GPU hours. These results
demonstrate that P-GAP enables fast, scalable, and resource-efficient ZO LLM
fine-tuning.

</details>


### [127] [ACTG-ARL: Differentially Private Conditional Text Generation with RL-Boosted Control](https://arxiv.org/abs/2510.18232)
*Yuzheng Hu,Ryan McKenna,Da Yu,Shanshan Wu,Han Zhao,Zheng Xu,Peter Kairouz*

Main category: cs.LG

TL;DR: 本文提出了ACTG-ARL框架，用于在差分隐私下生成高质量的合成文本，该框架通过分层方法和锚定强化学习提高了生成文本的实用性和可控性。


<details>
  <summary>Details</summary>
Motivation: 在不损害用户隐私的前提下，生成高质量的差分隐私（DP）合成文本对于训练和评估语言模型至关重要。然而，现有工作往往未能保留关键统计属性、遭受效用损失且缺乏细粒度控制。

Method: 本文提出了一种分层框架，将DP合成文本生成分解为特征学习和条件文本生成两个子任务。通过系统消融实验，确定了最有效的配置：以富表格模式作为特征、DP表格合成器和DP微调条件生成器（ACTG）。其次，提出了一种名为锚定强化学习（ARL）的后训练方法，结合RL和SFT锚点，以提高ACTG的条件生成指令遵循能力，并防止奖励作弊。

Result: ACTG-ARL算法能够生成高质量的DP合成文本，相比以往工作MAUVE值提高了20%，并在强大的隐私保证下提高了条件生成器的控制能力。

Conclusion: ACTG-ARL框架通过结合分层生成和锚定强化学习，成功地解决了差分隐私下合成文本生成中的质量和控制挑战，显著提升了生成文本的实用性和隐私保护水平。

Abstract: Generating high-quality synthetic text under differential privacy (DP) is
critical for training and evaluating language models without compromising user
privacy. Prior work on synthesizing DP datasets often fail to preserve key
statistical attributes, suffer utility loss from the noise required by DP, and
lack fine-grained control over generation. To address these challenges, we make
two contributions. First, we introduce a hierarchical framework that decomposes
DP synthetic text generation into two subtasks: feature learning and
conditional text generation. This design explicitly incorporates learned
features into the generation process and simplifies the end-to-end synthesis
task. Through systematic ablations, we identify the most effective
configuration: a rich tabular schema as feature, a DP tabular synthesizer, and
a DP fine-tuned conditional generator, which we term ACTG
(Attribute-Conditioned Text Generation). Second, we propose Anchored RL (ARL),
a post-training method that improves the instruction-following ability of ACTG
for conditional generation. ARL combines RL to boost control with an SFT anchor
on best-of-$N$ data to prevent reward hacking. Together, these components form
our end-to-end algorithm ACTG-ARL, which advances both the quality of DP
synthetic text (+20% MAUVE over prior work) and the control of the conditional
generator under strong privacy guarantees.

</details>


### [128] [Fostering the Ecosystem of AI for Social Impact Requires Expanding and Strengthening Evaluation Standards](https://arxiv.org/abs/2510.18238)
*Bryan Wilder,Angela Zhou*

Main category: cs.LG

TL;DR: 该文探讨了社会影响AI/ML研究的审查标准，认为现有标准可能阻碍研究生态系统的可持续性，并提出应拓宽社会影响的理解并更严格评估已部署系统的影响。


<details>
  <summary>Details</summary>
Motivation: 现有的AI/ML社会影响研究的评审标准过于侧重部署和新颖的机器学习方法创新，这可能对更广泛的研究生态系统的可持续性产生负面影响。

Method: 作者通过论证和提出观点的方式，指出当前评审机制的不足，并提出了一种新的评审理念。

Result: 作者认为，目前的评审标准可能激励研究人员采取不利于社会影响研究生态系统可持续发展的做法。评审标准应认可在单一方面（应用或方法学）做出贡献的项目。

Conclusion: 为了促进社会影响AI/ML研究的可持续发展，研究人员和评审员应采纳更广泛的社会影响概念，超越单纯的部署，并对已部署系统的影响进行更严格的评估。

Abstract: There has been increasing research interest in AI/ML for social impact, and
correspondingly more publication venues have refined review criteria for
practice-driven AI/ML research. However, these review guidelines tend to most
concretely recognize projects that simultaneously achieve deployment and novel
ML methodological innovation. We argue that this introduces incentives for
researchers that undermine the sustainability of a broader research ecosystem
of social impact, which benefits from projects that make contributions on
single front (applied or methodological) that may better meet project partner
needs. Our position is that researchers and reviewers in machine learning for
social impact must simultaneously adopt: 1) a more expansive conception of
social impacts beyond deployment and 2) more rigorous evaluations of the impact
of deployed systems.

</details>


### [129] [Learning with Dual-level Noisy Correspondence for Multi-modal Entity Alignment](https://arxiv.org/abs/2510.18240)
*Haobin Li,Yijie Lin,Peng Hu,Mouxing Yang,Xi Peng*

Main category: cs.LG

TL;DR: 该文章旨在通过引入鲁棒的多模态实体对齐框架RULE来解决多模态知识图谱中存在的双层噪声对应问题，并通过实验证明其优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 现有的多模态实体对齐（MMEA）方法通常假设实体内部和图间对应关系是无误的，但这在真实世界的MMKG中由于依赖专家标注而常常被违反。

Method: 本文提出了一个鲁棒的多模态实体对齐框架RULE，它首先通过一个专门的两阶段原则估计实体内部和图间对应关系的可靠性。然后利用估计的可靠性，RULE在属性融合过程中减轻实体内部噪声的负面影响，并在图间差异消除过程中防止过拟合到噪声的图间对应关系。此外，RULE还包含一个对应关系推理模块，揭示图下底层的属性-属性连接，从而保证更准确的等效实体识别。

Result: 在五个基准测试上进行的广泛实验验证了我们方法RULE在面对双层噪声对应（DNC）问题时，与七种最先进的方法相比的有效性。

Conclusion: 该文章成功地揭示并解决了多模态知识图谱中双层噪声对应（DNC）问题，并提出了一个鲁棒的MMEA框架RULE，通过估计和利用对应关系的可靠性，有效减轻了噪声的影响，提高了实体识别的准确性。

Abstract: Multi-modal entity alignment (MMEA) aims to identify equivalent entities
across heterogeneous multi-modal knowledge graphs (MMKGs), where each entity is
described by attributes from various modalities. Existing methods typically
assume that both intra-entity and inter-graph correspondences are faultless,
which is often violated in real-world MMKGs due to the reliance on expert
annotations. In this paper, we reveal and study a highly practical yet
under-explored problem in MMEA, termed Dual-level Noisy Correspondence (DNC).
DNC refers to misalignments in both intra-entity (entity-attribute) and
inter-graph (entity-entity and attribute-attribute) correspondences. To address
the DNC problem, we propose a robust MMEA framework termed RULE. RULE first
estimates the reliability of both intra-entity and inter-graph correspondences
via a dedicated two-fold principle. Leveraging the estimated reliabilities,
RULE mitigates the negative impact of intra-entity noise during attribute
fusion and prevents overfitting to noisy inter-graph correspondences during
inter-graph discrepancy elimination. Beyond the training-time designs, RULE
further incorporates a correspondence reasoning module that uncovers the
underlying attribute-attribute connection across graphs, guaranteeing more
accurate equivalent entity identification. Extensive experiments on five
benchmarks verify the effectiveness of our method against the DNC compared with
seven state-of-the-art methods.The code is available at
\href{https://github.com/XLearning-SCU/RULE}{XLearning-SCU/RULE}

</details>


### [130] [Scaling Laws Meet Model Architecture: Toward Inference-Efficient LLMs](https://arxiv.org/abs/2510.18245)
*Song Bian,Tao Yu,Shivaram Venkataraman,Youngsuk Park*

Main category: cs.LG

TL;DR: 为了提高大型语言模型的性能，我们提出了条件缩放定律，通过200多个模型训练结果验证，寻找推理效率和准确性兼得的模型架构，可以在相同的训练预算下提高2.1%的准确性和42%的推理吞吐量。


<details>
  <summary>Details</summary>
Motivation: 探索模型准确性与推理效率之间的权衡，以及关键架构因素（隐藏层大小、MLP与注意力之间的参数分配、GQA）如何影响推理成本和准确性。

Method: 引入了条件缩放定律来增强Chinchilla框架，并提出了一个搜索框架来识别同时具有推理效率和准确性的架构。训练了200多个模型，参数量从80M到3B，训练tokens从8B到100B，以验证所提出的条件缩放定律。

Result: 条件缩放定律能够可靠地预测最佳架构选择，并且由此产生的模型优于现有的开源基线。在相同的训练预算下，优化后的架构与LLaMA-3.2相比，准确性提高了2.1%，推理吞吐量提高了42%。

Conclusion: 扩大参数数量和训练数据的规模可以提高大型语言模型的性能。同时，通过引入条件缩放定律和搜索框架，可以在推理成本和模型准确性之间找到最佳平衡，显著提高模型的性能和效率。

Abstract: Scaling the number of parameters and the size of training data has proven to
be an effective strategy for improving large language model (LLM) performance.
Yet, as these models grow increasingly powerful and widely deployed, the cost
of inference has become a pressing concern. Despite its importance, the
trade-off between model accuracy and inference efficiency remains
underexplored. In this work, we examine how key architectural factors, hidden
size, the allocation of parameters between MLP and attention (mlp-to-attention
ratio), and grouped-query attention (GQA), influence both inference cost and
accuracy. We introduce a conditional scaling law that augments the Chinchilla
framework with architectural information, along with a search framework for
identifying architectures that are simultaneously inference-efficient and
accurate. To validate our approach, we train more than 200 models spanning 80M
to 3B parameters and 8B to 100B training tokens, and fit the proposed
conditional scaling law. Our results show that the conditional scaling law
reliably predicts optimal architectural choices and that the resulting models
outperform existing open-source baselines. Under the same training budget,
optimized architectures achieve up to 2.1% higher accuracy and 42% greater
inference throughput compared to LLaMA-3.2.

</details>


### [131] [NTKMTL: Mitigating Task Imbalance in Multi-Task Learning from Neural Tangent Kernel Perspective](https://arxiv.org/abs/2510.18258)
*Xiaohan Qin,Xiaoxing Wang,Ning Liao,Junchi Yan*

Main category: cs.LG

TL;DR: 这篇论文提出了一种新的多任务学习方法NTKMTL，它利用神经正切核（NTK）理论来平衡不同任务的收敛速度，从而解决任务不平衡问题。


<details>
  <summary>Details</summary>
Motivation: 多任务学习（MTL）通过任务间的知识迁移来增强泛化能力，但任务不平衡仍然是一个主要挑战。尽管平衡不同任务的收敛速度是解决该问题的有效方法，但在复杂的MTL系统中准确描述多个任务的训练动态和收敛速度非常具有挑战性。

Method: 本研究通过利用神经正切核（NTK）理论来分析MTL中的训练动态，并提出了NTKMTL方法。具体而言，引入了一个扩展的MTL NTK矩阵，并采用谱分析来平衡多个任务的收敛速度，从而缓解任务不平衡问题。此外，基于共享表示的近似，进一步提出了NTKMTL-SR，在保持竞争性性能的同时实现了训练效率。

Result: 实验结果表明，所提出的方法在多任务监督学习和多任务强化学习等广泛基准测试中均达到了最先进的性能。

Conclusion: NTKMTL通过利用神经正切核理论有效解决了多任务学习中的任务不平衡问题，并通过平衡任务收敛速度显著提升了模型性能和训练效率。

Abstract: Multi-Task Learning (MTL) enables a single model to learn multiple tasks
simultaneously, leveraging knowledge transfer among tasks for enhanced
generalization, and has been widely applied across various domains. However,
task imbalance remains a major challenge in MTL. Although balancing the
convergence speeds of different tasks is an effective approach to address this
issue, it is highly challenging to accurately characterize the training
dynamics and convergence speeds of multiple tasks within the complex MTL
system. To this end, we attempt to analyze the training dynamics in MTL by
leveraging Neural Tangent Kernel (NTK) theory and propose a new MTL method,
NTKMTL. Specifically, we introduce an extended NTK matrix for MTL and adopt
spectral analysis to balance the convergence speeds of multiple tasks, thereby
mitigating task imbalance. Based on the approximation via shared
representation, we further propose NTKMTL-SR, achieving training efficiency
while maintaining competitive performance. Extensive experiments demonstrate
that our methods achieve state-of-the-art performance across a wide range of
benchmarks, including both multi-task supervised learning and multi-task
reinforcement learning. Source code is available at
https://github.com/jianke0604/NTKMTL.

</details>


### [132] [From Competition to Synergy: Unlocking Reinforcement Learning for Subject-Driven Image Generation](https://arxiv.org/abs/2510.18263)
*Ziwei Huang,Ying Shu,Hao Fang,Quanyu Long,Wenya Wang,Qiushi Guo,Tiezheng Ge,Leilei Gan*

Main category: cs.LG

TL;DR: 本文提出Customized-GRPO，一种新的方法，通过协同感知奖励塑造和时间感知动态加权来解决主体驱动图像生成模型中身份保留和提示遵循之间的权衡，解决了现有GRPO方法的局限性。


<details>
  <summary>Details</summary>
Motivation: 目前主体驱动的图像生成模型在身份保留和提示遵循之间面临根本性权衡。尽管在线强化学习（RL），特别是GRPO，提供了一个有前景的解决方案，但简单的GRPO应用会导致性能下降，因为奖励的线性聚合和静态权重会引起冲突的梯度信号，并与扩散过程的时间动态不一致。

Method: 本文提出了Customized-GRPO框架，包含两个关键创新点：1. 协同感知奖励塑造（SARS）：一种非线性机制，明确惩罚冲突的奖励信号并放大协同信号，提供更清晰、更果断的梯度。2. 时间感知动态加权（TDW）：通过在早期优先考虑提示遵循，后期优先考虑身份保留，使优化压力与模型的时间动态对齐。

Result: 广泛的实验表明，我们的方法显著优于简单的GRPO基线模型，成功地缓解了竞争性性能下降。我们的模型实现了卓越的平衡，生成的图像既保留了关键的身份特征，又准确地遵循了复杂的文本提示。

Conclusion: Customized-GRPO通过其创新性的协同感知奖励塑造和时间感知动态加权机制，有效地解决了主体驱动图像生成中身份保留和提示遵循之间的矛盾，显著提升了生成图片的质量和可控性。

Abstract: Subject-driven image generation models face a fundamental trade-off between
identity preservation (fidelity) and prompt adherence (editability). While
online reinforcement learning (RL), specifically GPRO, offers a promising
solution, we find that a naive application of GRPO leads to competitive
degradation, as the simple linear aggregation of rewards with static weights
causes conflicting gradient signals and a misalignment with the temporal
dynamics of the diffusion process. To overcome these limitations, we propose
Customized-GRPO, a novel framework featuring two key innovations: (i)
Synergy-Aware Reward Shaping (SARS), a non-linear mechanism that explicitly
penalizes conflicted reward signals and amplifies synergistic ones, providing a
sharper and more decisive gradient. (ii) Time-Aware Dynamic Weighting (TDW),
which aligns the optimization pressure with the model's temporal dynamics by
prioritizing prompt-following in the early, identity preservation in the later.
Extensive experiments demonstrate that our method significantly outperforms
naive GRPO baselines, successfully mitigating competitive degradation. Our
model achieves a superior balance, generating images that both preserve key
identity features and accurately adhere to complex textual prompts.

</details>


### [133] [Online Time Series Forecasting with Theoretical Guarantees](https://arxiv.org/abs/2510.18281)
*Zijian Li,Changze Zhou,Minghao Fu,Sanjay Manjunath,Fan Feng,Guangyi Chen,Yingyao Hu,Ruichu Cai,Kun Zhang*

Main category: cs.LG

TL;DR: 本文提出了一种在线时间序列预测理论框架（TOT），能够在存在未知分布偏移的情况下，通过引入潜变量来提高预测精度，并提供了理论保证。


<details>
  <summary>Details</summary>
Motivation: 在线时间序列预测在面对未知分布偏移时面临挑战，现有的方法未能充分解决潜变量对预测的影响。

Method: 提出TOT框架，通过引入潜变量来收紧贝叶斯风险，并证明了其在潜变量估计不确定性下的持续有效性。进一步，通过最小相邻观测识别潜变量，并设计了一个模型无关的蓝图，利用时间解码器匹配观测变量分布，并使用两个独立的噪声估计器来建模潜变量的因果推断和观测变量的混合过程。

Result: 在合成数据上的实验结果支持了理论主张。在多个基准测试中，基于TOT的即插即用实现相对于多个基线模型都取得了普遍的改进。

Conclusion: TOT框架通过有效引入潜变量解决了在线时间序列预测中的分布偏移问题，提高了预测精度，并在实际应用中展现了有效性。

Abstract: This paper is concerned with online time series forecasting, where unknown
distribution shifts occur over time, i.e., latent variables influence the
mapping from historical to future observations. To develop an automated way of
online time series forecasting, we propose a Theoretical framework for Online
Time-series forecasting (TOT in short) with theoretical guarantees.
Specifically, we prove that supplying a forecaster with latent variables
tightens the Bayes risk, the benefit endures under estimation uncertainty of
latent variables and grows as the latent variables achieve a more precise
identifiability. To better introduce latent variables into online forecasting
algorithms, we further propose to identify latent variables with minimal
adjacent observations. Based on these results, we devise a model-agnostic
blueprint by employing a temporal decoder to match the distribution of observed
variables and two independent noise estimators to model the causal inference of
latent variables and mixing procedures of observed variables, respectively.
Experiment results on synthetic data support our theoretical claims. Moreover,
plug-in implementations built on several baselines yield general improvement
across multiple benchmarks, highlighting the effectiveness in real-world
applications.

</details>


### [134] [Physics-Informed Parametric Bandits for Beam Alignment in mmWave Communications](https://arxiv.org/abs/2510.18299)
*Hao Qin,Thang Duong,Ming Li,Chicheng Zhang*

Main category: cs.LG

TL;DR: 这篇论文提出了两种基于物理信息的波束赋形算法：pretc和prgreedy，用于毫米波通信中的波束对准和跟踪。


<details>
  <summary>Details</summary>
Motivation: 在毫米波通信中，波束对准和跟踪对于对抗显著的路径损耗至关重要。传统的多臂老虎机算法在大型波束空间下收敛时间长，而现有方法对奖励函数的单峰或多峰结构假设在实践中往往不成立，导致算法收敛到次优波束。

Method: 本文提出了两种物理信息的多臂老虎机算法pretc和prgreedy。这些算法利用毫米波信道的稀疏多径特性，将每个路径的参数视为黑盒，并根据采样的历史奖励维持对其的最优估计。pretc算法首先进行随机探索，然后根据估计的奖励函数选择最优波束。prgreedy算法在线执行这种估计，并根据当前估计选择最佳波束。这些算法也可以很容易地适应移动场景下的波束跟踪。

Result: 通过使用合成的DeepMIMO数据集和真实的DeepSense6G数据集进行的实验，本文证明了这两种算法在各种信道环境下的广泛场景中都优于现有方法，显示了它们的泛化能力和鲁棒性。

Conclusion: 本文提出的pretc和prgreedy算法通过利用毫米波信道的稀疏多径特性，解决了传统波束对准算法的局限性，在波束对准和跟踪方面取得了显著的性能提升。

Abstract: In millimeter wave (mmWave) communications, beam alignment and tracking are
crucial to combat the significant path loss. As scanning the entire directional
space is inefficient, designing an efficient and robust method to identify the
optimal beam directions is essential. Since traditional bandit algorithms
require a long time horizon to converge under large beam spaces, many existing
works propose efficient bandit algorithms for beam alignment by relying on
unimodality or multimodality assumptions on the reward function's structure.
However, such assumptions often do not hold (or cannot be strictly satisfied)
in practice, which causes such algorithms to converge to choosing suboptimal
beams.
  In this work, we propose two physics-informed bandit algorithms
\textit{pretc} and \textit{prgreedy} that exploit the sparse multipath property
of mmWave channels - a generic but realistic assumption - which is connected to
the Phase Retrieval Bandit problem. Our algorithms treat the parameters of each
path as black boxes and maintain optimal estimates of them based on sampled
historical rewards. \textit{pretc} starts with a random exploration phase and
then commits to the optimal beam under the estimated reward function.
\textit{prgreedy} performs such estimation in an online manner and chooses the
best beam under current estimates. Our algorithms can also be easily adapted to
beam tracking in the mobile setting. Through experiments using both the
synthetic DeepMIMO dataset and the real-world DeepSense6G dataset, we
demonstrate that both algorithms outperform existing approaches in a wide range
of scenarios across diverse channel environments, showing their
generalizability and robustness.

</details>


### [135] [Higher Embedding Dimension Creates a Stronger World Model for a Simple Sorting Task](https://arxiv.org/abs/2510.18315)
*Brady Bhalla,Honglu Fan,Nancy Chen,Tony Yue YU*

Main category: cs.LG

TL;DR: 这篇文章研究了嵌入维度如何影响在用强化学习训练以执行冒泡排序式相邻交换的Transformer中，内部“世界模型”的出现。


<details>
  <summary>Details</summary>
Motivation: 研究嵌入维度如何影响Transformer内部“世界模型”的形成。

Method: 通过强化学习训练Transformer执行冒泡排序式相邻交换，并进行数百次实验观察其内部机制。

Result: 模型即使在非常小的嵌入维度下也能达到高精度，但更大的维度会产生更忠实、一致和鲁棒的内部表示。更高的嵌入维度会加强结构化内部表示的形成，并带来更好的可解释性。观察到两个一致的机制：1. 注意力权重矩阵的最后一行单调地编码了token的全局顺序；2. 所选的转置与这些编码值的最大相邻差异对齐。

Conclusion: Transformer构建了结构化的内部世界模型，并且模型大小除了提高最终性能外，还改善了表示质量。

Abstract: We investigate how embedding dimension affects the emergence of an internal
"world model" in a transformer trained with reinforcement learning to perform
bubble-sort-style adjacent swaps. Models achieve high accuracy even with very
small embedding dimensions, but larger dimensions yield more faithful,
consistent, and robust internal representations. In particular, higher
embedding dimensions strengthen the formation of structured internal
representation and lead to better interpretability. After hundreds of
experiments, we observe two consistent mechanisms: (1) the last row of the
attention weight matrix monotonically encodes the global ordering of tokens;
and (2) the selected transposition aligns with the largest adjacent difference
of these encoded values. Our results provide quantitative evidence that
transformers build structured internal world models and that model size
improves representation quality in addition to end performance. We release our
metrics and analyses, which can be used to probe similar algorithmic tasks.

</details>


### [136] [Scalable, Explainable and Provably Robust Anomaly Detection with One-Step Flow Matching](https://arxiv.org/abs/2510.18328)
*Zhong Li,Qi Huang,Yuxuan Zhu,Lincen Yang,Mohammad Mohammadi Amiri,Niki van Stein,Matthijs van Leeuwen*

Main category: cs.LG

TL;DR: TCCM是一种新颖的半监督表格数据异常检测方法，它通过预测时间条件收缩向量来学习速度场，实现了轻量级训练、高效评分和可解释性。在ADBench基准测试中，TCCM在检测精度和推理成本之间取得了良好的平衡。


<details>
  <summary>Details</summary>
Motivation: 现有的连续时间模型（如DTE）在异常检测准确性方面表现领先，但推理成本较高，存在推理瓶颈。

Method: TCCM受流匹配启发，但简化了框架，通过预测时间条件收缩向量，在每个采样时间步长向固定目标（原点）收缩来学习速度场。TCCM设计上的优势包括：1. 轻量级和可扩展的训练目标，无需在训练和推理过程中求解常微分方程；2. 高效的评分策略，即“一步偏差”；3. 可解释性和可证明的鲁棒性，学习到的速度场直接在输入空间中操作，异常分数在特征上是可归因的，并且分数函数相对于输入是Lipschitz连续的。

Result: 广泛的实验表明，TCCM在检测精度和推理成本之间取得了良好的平衡，优于最先进的方法，尤其是在高维和大规模数据集上。

Conclusion: TCCM通过其独特的设计，有效地解决了现有连续时间模型在异常检测中面临的推理效率问题，并在保持高检测精度的同时，提供了良好的可解释性和鲁棒性。

Abstract: We introduce Time-Conditioned Contraction Matching (TCCM), a novel method for
semi-supervised anomaly detection in tabular data. TCCM is inspired by flow
matching, a recent generative modeling framework that learns velocity fields
between probability distributions and has shown strong performance compared to
diffusion models and generative adversarial networks. Instead of directly
applying flow matching as originally formulated, TCCM builds on its core idea
-- learning velocity fields between distributions -- but simplifies the
framework by predicting a time-conditioned contraction vector toward a fixed
target (the origin) at each sampled time step. This design offers three key
advantages: (1) a lightweight and scalable training objective that removes the
need for solving ordinary differential equations during training and inference;
(2) an efficient scoring strategy called one time-step deviation, which
quantifies deviation from expected contraction behavior in a single forward
pass, addressing the inference bottleneck of existing continuous-time models
such as DTE (a diffusion-based model with leading anomaly detection accuracy
but heavy inference cost); and (3) explainability and provable robustness, as
the learned velocity field operates directly in input space, making the anomaly
score inherently feature-wise attributable; moreover, the score function is
Lipschitz-continuous with respect to the input, providing theoretical
guarantees under small perturbations. Extensive experiments on the ADBench
benchmark show that TCCM strikes a favorable balance between detection accuracy
and inference cost, outperforming state-of-the-art methods -- especially on
high-dimensional and large-scale datasets. The source code is available at our
GitHub repository.

</details>


### [137] [Why Policy Gradient Algorithms Work for Undiscounted Total-Reward MDPs](https://arxiv.org/abs/2510.18340)
*Jongmin Lee,Ernest K. Ryu*

Main category: cs.LG

TL;DR: 本文分析了用于无限视野马尔可夫决策过程（MDP）的策略梯度方法，其中不使用折扣因子（$\gamma = 1$），这种设置在大型语言模型的策略学习中很常见。


<details>
  <summary>Details</summary>
Motivation: 现有的策略梯度方法收敛性理论大多假设折扣因子$\gamma < 1$。然而，在大型语言模型中，策略梯度强化学习通常在无折扣的总奖励设置下进行，这使得现有理论不再适用。

Method: 本文基于两点洞察力对无折扣预期总奖励无限视野MDPs的策略梯度方法进行了分析：1. 在策略对所有行动都赋予严格正概率的情况下，MDP状态的瞬态和循环态分类是不变的。2. 引入了“瞬态访问度量”来替代在$\gamma = 1$时可能失效的传统状态访问度量。

Result: 通过上述两点洞察，本文为无折扣总奖励设置下的策略梯度方法提供了新的分析框架。

Conclusion: 本文通过引入新的概念和利用状态分类的不变性，为无折扣设置下的策略梯度方法提供了理论分析，填补了现有理论在大型语言模型应用中的空白。

Abstract: The classical policy gradient method is the theoretical and conceptual
foundation of modern policy-based reinforcement learning (RL) algorithms. Most
rigorous analyses of such methods, particularly those establishing convergence
guarantees, assume a discount factor $\gamma < 1$. In contrast, however, a
recent line of work on policy-based RL for large language models uses the
undiscounted total-reward setting with $\gamma = 1$, rendering much of the
existing theory inapplicable. In this paper, we provide analyses of the policy
gradient method for undiscounted expected total-reward infinite-horizon MDPs
based on two key insights: (i) the classification of the MDP states into
recurrent and transient states is invariant over the set of policies that
assign strictly positive probability to every action (as is typical in deep RL
models employing a softmax output layer) and (ii) the classical state
visitation measure (which may be ill-defined when $\gamma = 1$) can be replaced
with a new object that we call the transient visitation measure.

</details>


### [138] [Computable universal online learning](https://arxiv.org/abs/2510.18352)
*Dariusz Kalociński,Tomasz Steifer*

Main category: cs.LG

TL;DR: 本文探讨了通用在线学习何时可以作为计算机程序实现。


<details>
  <summary>Details</summary>
Motivation: 以往的机器学习理论研究忽略了学习的可实现性问题。

Method: 我们研究了通用在线学习模型，并讨论了可计算的通用在线学习。

Result: 通用在线学习并不意味着可计算的通用在线学习，即使假设类在计算上相对简单。 我们对可计算的通用在线学习的未知变量进行了研究，并给出了可学习类的精确描述。 我们还考虑了普适通用在线学习的一个变体，并精确展示了何时可以实现。

Conclusion: 本文的研究结果为在线二元分类的现有理论和归纳推理的相关问题提供了更现实的视角。

Abstract: Understanding when learning is possible is a fundamental task in the theory
of machine learning. However, many characterizations known from the literature
deal with abstract learning as a mathematical object and ignore the crucial
question: when can learning be implemented as a computer program? We address
this question for universal online learning, a generalist theoretical model of
online binary classification, recently characterized by Bousquet et al.
(STOC'21). In this model, there is no hypothesis fixed in advance; instead,
Adversary -- playing the role of Nature -- can change their mind as long as
local consistency with the given class of hypotheses is maintained. We require
Learner to achieve a finite number of mistakes while using a strategy that can
be implemented as a computer program. We show that universal online learning
does not imply computable universal online learning, even if the class of
hypotheses is relatively easy from a computability-theoretic perspective. We
then study the agnostic variant of computable universal online learning and
provide an exact characterization of classes that are learnable in this sense.
We also consider a variant of proper universal online learning and show exactly
when it is possible. Together, our results give a more realistic perspective on
the existing theory of online binary classification and the related problem of
inductive inference.

</details>


### [139] [Towards Unsupervised Open-Set Graph Domain Adaptation via Dual Reprogramming](https://arxiv.org/abs/2510.18363)
*Zhen Zhang,Bingsheng He*

Main category: cs.LG

TL;DR: 这篇论文介绍了一种名为GraphRTA的新框架，用于解决无监督开放集图域适应问题，该框架通过图重编程和模型重编程，在分类已知类别的同时识别未知节点类型。


<details>
  <summary>Details</summary>
Motivation: 现有的图域适应模型主要关注闭集设置，即源域和目标域共享相同的标签空间。然而，在现实世界中，目标域可能包含源域中不存在的类别。

Method: 本论文提出了一种名为GraphRTA的新框架。它通过修改目标图结构和节点特征来重编程图，以更好地分离已知和未知类别。同时，它通过修剪特定于域的参数来执行模型重编程，以减少对源图的偏倚，同时保留捕获跨图可迁移模式的参数。此外，它还为未知类别扩展了分类器，从而无需手动指定开放集识别中的阈值。

Result: 在几个公共数据集上的综合实验表明，与最近的最新基线相比，我们提出的模型可以达到令人满意的性能。

Conclusion: GraphRTA框架通过图重编程和模型重编程，成功解决了无监督开放集图域适应问题，并在已知类别分类和未知节点类型识别方面取得了良好效果。

Abstract: Unsupervised Graph Domain Adaptation has become a promising paradigm for
transferring knowledge from a fully labeled source graph to an unlabeled target
graph. Existing graph domain adaptation models primarily focus on the
closed-set setting, where the source and target domains share the same label
spaces. However, this assumption might not be practical in the real-world
scenarios, as the target domain might include classes that are not present in
the source domain. In this paper, we investigate the problem of unsupervised
open-set graph domain adaptation, where the goal is to not only correctly
classify target nodes into the known classes, but also recognize previously
unseen node types into the unknown class. Towards this end, we propose a novel
framework called GraphRTA, which conducts reprogramming on both the graph and
model sides. Specifically, we reprogram the graph by modifying target graph
structure and node features, which facilitates better separation of known and
unknown classes. Meanwhile, we also perform model reprogramming by pruning
domain-specific parameters to reduce bias towards the source graph while
preserving parameters that capture transferable patterns across graphs.
Additionally, we extend the classifier with an extra dimension for the unknown
class, thus eliminating the need of manually specified threshold in open-set
recognition. Comprehensive experiments on several public datasets demonstrate
that our proposed model can achieve satisfied performance compared with recent
state-of-the-art baselines. Our source codes and datasets are publicly
available at https://github.com/cszhangzhen/GraphRTA.

</details>


### [140] [Training Diverse Graph Experts for Ensembles: A Systematic Empirical Study](https://arxiv.org/abs/2510.18370)
*Gangda Deng,Yuxin Yang,Ömer Faruk Akgül,Hanqing Zeng,Yinglong Xia,Rajgopal Kannan,Viktor Prasanna*

Main category: cs.LG

TL;DR: 这篇论文对GNN集成中的专家级多样化技术进行了首次系统实证研究。作者评估了20种多样化策略，并构建和分析了200多个集成变体。


<details>
  <summary>Details</summary>
Motivation: GNN的性能受限于实际图中存在的异构性，而MoE框架通过集成多个多样化的GNN可以显著提高性能。

Method: 作者评估了20种多样化策略，包括随机重新初始化、超参数调整、架构变化、方向性建模和训练数据划分，并在14个节点分类基准上构建和分析了200多个集成变体。

Result: 作者通过综合评估，研究了每种技术在专家多样性、互补性和集成性能方面的表现。

Conclusion: 研究结果为专家训练和图数据上有效的MoE框架设计提供了可操作的指导。

Abstract: Graph Neural Networks (GNNs) have become essential tools for learning on
relational data, yet the performance of a single GNN is often limited by the
heterogeneity present in real-world graphs. Recent advances in
Mixture-of-Experts (MoE) frameworks demonstrate that assembling multiple,
explicitly diverse GNNs with distinct generalization patterns can significantly
improve performance. In this work, we present the first systematic empirical
study of expert-level diversification techniques for GNN ensembles. Evaluating
20 diversification strategies -- including random re-initialization,
hyperparameter tuning, architectural variation, directionality modeling, and
training data partitioning -- across 14 node classification benchmarks, we
construct and analyze over 200 ensemble variants. Our comprehensive evaluation
examines each technique in terms of expert diversity, complementarity, and
ensemble performance. We also uncovers mechanistic insights into training
maximally diverse experts. These findings provide actionable guidance for
expert training and the design of effective MoE frameworks on graph data. Our
code is available at https://github.com/Hydrapse/bench-gnn-diversification.

</details>


### [141] [Learning from N-Tuple Data with M Positive Instances: Unbiased Risk Estimation and Theoretical Guarantees](https://arxiv.org/abs/2510.18406)
*Miao Zhang,Junpeng Li,ChangChun HUa,Yana Yang*

Main category: cs.LG

TL;DR: 这篇论文研究了一种弱监督学习设置，其中每个训练样本是包含m个正例的n元组，但只观察到每个元组的计数值m。这个NTMP（N-tuple with M positives）监督出现在例如带有区域提议的图像分类和多实例测量中。


<details>
  <summary>Details</summary>
Motivation: 在弱监督学习中，通常使用粗略的聚合信号而不是实例标签进行操作。作者研究了一种特殊设置，其中每个训练样本是一个n元组，包含m个正例，但只观察到每个元组中的m计数。这种NTMP（N-tuple with M positives）监督在例如带有区域提议的图像分类和多实例测量中出现。

Method: 作者证明了元组计数通过将元组生成过程与潜在实例边缘联系起来，可以得到一个可训练的无偏风险估计器（URE）。从固定的(n,m)开始，作者推导出一个闭式URE，并将其扩展到可变元组大小、可变计数及其组合。当有效混合率与类别先验分离时，识别成立。作者通过Rademacher复杂度建立了泛化界限，并在温和的正则性假设下证明了标准速率的统计一致性。为了提高有限样本的稳定性，作者引入了简单的ReLU校正到URE中，以保持渐近正确性。

Result: 在转换为NTMP任务的基准测试中，该方法始终优于代表性的弱监督基线，并产生了有利的精确度-召回率和F1权衡。它在类别先验不平衡和各种元组配置下保持鲁棒性，表明仅计数监督可以通过理论基础和实践稳定的目标得到有效利用。

Conclusion: 计数监督可以通过理论基础和实践稳定的目标得到有效利用，从而在弱监督学习场景中取得很好的效果。

Abstract: Weakly supervised learning often operates with coarse aggregate signals
rather than instance labels. We study a setting where each training example is
an $n$-tuple containing exactly m positives, while only the count m per tuple
is observed. This NTMP (N-tuple with M positives) supervision arises in, e.g.,
image classification with region proposals and multi-instance measurements. We
show that tuple counts admit a trainable unbiased risk estimator (URE) by
linking the tuple-generation process to latent instance marginals. Starting
from fixed (n,m), we derive a closed-form URE and extend it to variable tuple
sizes, variable counts, and their combination. Identification holds whenever
the effective mixing rate is separated from the class prior. We establish
generalization bounds via Rademacher complexity and prove statistical
consistency with standard rates under mild regularity assumptions. To improve
finite-sample stability, we introduce simple ReLU corrections to the URE that
preserve asymptotic correctness. Across benchmarks converted to NTMP tasks, the
approach consistently outperforms representative weak-supervision baselines and
yields favorable precision-recall and F1 trade-offs. It remains robust under
class-prior imbalance and across diverse tuple configurations, demonstrating
that count-only supervision can be exploited effectively through a
theoretically grounded and practically stable objective.

</details>


### [142] [Learning Boltzmann Generators via Constrained Mass Transport](https://arxiv.org/abs/2510.18460)
*Christopher von Klitzing,Denis Blessing,Henrik Schopmans,Pascal Friederich,Gerhard Neumann*

Main category: cs.LG

TL;DR: 该文章介绍了一种名为CMT的变分框架，用于解决高维多峰概率分布的有效采样问题，特别是在Boltzmann生成器中的应用。


<details>
  <summary>Details</summary>
Motivation: 目前现有的经典变分方法容易导致模式崩溃，而基于退火的方法则会受到质量传输的影响，并且需要大量时间调整。

Method: CMT方法通过在连续步骤之间对KL散度和熵衰减施加约束来生成中间分布，从而解决了上述问题。

Result: 在标准的BG基准测试和本文引入的ELIL四肽上，CMT始终优于最先进的变分方法，有效样本量提高了2.5倍以上，同时避免了模式崩溃。

Conclusion: CMT通过解决现有方法的局限性，在处理复杂的物理系统采样问题上取得了显著进展，为高维多峰概率分布采样领域带来了新的突破。

Abstract: Efficient sampling from high-dimensional and multimodal unnormalized
probability distributions is a central challenge in many areas of science and
machine learning. We focus on Boltzmann generators (BGs) that aim to sample the
Boltzmann distribution of physical systems, such as molecules, at a given
temperature. Classical variational approaches that minimize the reverse
Kullback-Leibler divergence are prone to mode collapse, while annealing-based
methods, commonly using geometric schedules, can suffer from mass teleportation
and rely heavily on schedule tuning. We introduce Constrained Mass Transport
(CMT), a variational framework that generates intermediate distributions under
constraints on both the KL divergence and the entropy decay between successive
steps. These constraints enhance distributional overlap, mitigate mass
teleportation, and counteract premature convergence. Across standard BG
benchmarks and the here introduced ELIL tetrapeptide, the largest system
studied to date without access to samples from molecular dynamics, CMT
consistently surpasses state-of-the-art variational methods, achieving more
than 2.5x higher effective sample size while avoiding mode collapse.

</details>


### [143] [Learning to Navigate Under Imperfect Perception: Conformalised Segmentation for Safe Reinforcement Learning](https://arxiv.org/abs/2510.18485)
*Daniel Bethell,Simos Gerasimou,Radu Calinescu,Calum Imrie*

Main category: cs.LG

TL;DR: COPPOL是一种共形驱动的感知策略学习方法，它将无分布、有限样本的安全保证集成到语义分割中，从而产生具有严格漏检边界的校准危险图，并通过下游强化学习规划实现风险感知成本场。COPPOL提高了危险覆盖率，减少了导航中的危险违规，并且对分布变化具有鲁棒性。


<details>
  <summary>Details</summary>
Motivation: 目前的方法在危险检测能力方面存在假设，而不确定性感知方法缺乏有限样本保证。

Method: COPPOL将无分布、有限样本的安全保证集成到语义分割中，生成具有严格漏检边界的校准危险图，并通过风险感知成本场进行下游RL规划。

Result: COPPOL将危险覆盖率提高了6倍，实现了对不安全区域的近乎完全检测，同时将导航过程中危险违规减少了50%。该方法对分布变化具有鲁棒性，保持了安全性和效率。

Conclusion: COPPOL在安全关键环境中实现了可靠的导航，因为它提供了准确的危险感知和原则性的不确定性处理，从而提高了下游安全处理能力。

Abstract: Reliable navigation in safety-critical environments requires both accurate
hazard perception and principled uncertainty handling to strengthen downstream
safety handling. Despite the effectiveness of existing approaches, they assume
perfect hazard detection capabilities, while uncertainty-aware perception
approaches lack finite-sample guarantees. We present COPPOL, a conformal-driven
perception-to-policy learning approach that integrates distribution-free,
finite-sample safety guarantees into semantic segmentation, yielding calibrated
hazard maps with rigorous bounds for missed detections. These maps induce
risk-aware cost fields for downstream RL planning. Across two satellite-derived
benchmarks, COPPOL increases hazard coverage (up to 6x) compared to comparative
baselines, achieving near-complete detection of unsafe regions while reducing
hazardous violations during navigation (up to approx 50%). More importantly,
our approach remains robust to distributional shift, preserving both safety and
efficiency.

</details>


### [144] [Alibaba International E-commerce Product Search Competition DILAB Team Technical Report](https://arxiv.org/abs/2510.18499)
*Hyewon Lee,Junghyun Oh,Minkyung Song,Soyoung Park,Seunghoon Han*

Main category: cs.LG

TL;DR: 该研究介绍了DILAB团队开发的多语言电商搜索系统，该系统在最终排行榜上获得第五名，总分0.8819。


<details>
  <summary>Details</summary>
Motivation: 解决多语言查询-商品理解中的挑战。

Method: 设计了一个多阶段流水线，整合了数据精炼、轻量级预处理和自适应建模。数据精炼阶段增强了数据集一致性和类别覆盖。语言标记和噪声过滤提高了输入质量。在建模阶段，探索了多种架构和微调策略，并使用精选的验证集优化了超参数，以平衡查询-类别（QC）和查询-商品（QI）任务的性能。

Result: DILAB团队开发的多语言电商搜索系统在最终排行榜上获得第五名，总分0.8819，在评估指标上表现出稳定且高性能的结果。所提出的框架在不同语言和领域中表现出鲁棒性和适应性。

Conclusion: 系统的数据整理和迭代评估对于多语言搜索系统是有效的。

Abstract: This study presents the multilingual e-commerce search system developed by
the DILAB team, which achieved 5th place on the final leaderboard with a
competitive overall score of 0.8819, demonstrating stable and high-performing
results across evaluation metrics. To address challenges in multilingual
query-item understanding, we designed a multi-stage pipeline integrating data
refinement, lightweight preprocessing, and adaptive modeling. The data
refinement stage enhanced dataset consistency and category coverage, while
language tagging and noise filtering improved input quality. In the modeling
phase, multiple architectures and fine-tuning strategies were explored, and
hyperparameters optimized using curated validation sets to balance performance
across query-category (QC) and query-item (QI) tasks. The proposed framework
exhibited robustness and adaptability across languages and domains,
highlighting the effectiveness of systematic data curation and iterative
evaluation for multilingual search systems. The source code is available at
https://github.com/2noweyh/DILAB-Alibaba-Ecommerce-Search.

</details>


### [145] [Partial VOROS: A Cost-aware Performance Metric for Binary Classifiers with Precision and Capacity Constraints](https://arxiv.org/abs/2510.18520)
*Christopher Ratigan,Kyle Heuton,Carissa Wang,Lenore Cowen,Michael C. Hughes*

Main category: cs.LG

TL;DR: 本文提出了一种新的评估指标VoroS，用于解决传统ROC曲线在医院警报系统等应用中无法捕捉精度约束、容量限制和不对称成本等关键因素的问题。


<details>
  <summary>Details</summary>
Motivation: 传统ROC分析无法捕捉实际部署中的关键因素，例如避免虚警疲劳的最低精度约束、代表医护人员能力的预测阳性数量上限，以及假阳性和假阴性的不对称成本。

Method: 本文首先展示了满足给定精度和容量约束的分类器子集如何表示为ROC空间中的可行区域，并建立了该可行区域的几何结构。然后，定义了“劣势分类器的部分面积（partial area of lesser classifiers）”这一性能指标，该指标与成本单调相关，并且只考虑ROC空间的可行部分。最后，通过对所需成本参数范围内的面积进行平均，得到了ROC曲面上的部分体积，即VoroS。

Result: 在MIMIC-IV数据集上使用生命体征历史预测死亡风险的实验中，本文提出的成本感知指标在医院警报应用中对分类器进行排序方面优于其他替代方法。

Conclusion: 本文提出了一种称为VoroS的成本感知评估指标，该指标通过引入可行区域和考虑不对称成本，解决了传统ROC曲线在实际应用中存在的局限性，特别适用于需要考虑精度约束和容量限制的医院警报系统。

Abstract: The ROC curve is widely used to assess binary classification performance. Yet
for some applications such as alert systems for hospitalized patient
monitoring, conventional ROC analysis cannot capture crucial factors that
impact deployment, such as enforcing a minimum precision constraint to avoid
false alarm fatigue or imposing an upper bound on the number of predicted
positives to represent the capacity of hospital staff. The usual area under the
curve metric also does not reflect asymmetric costs for false positives and
false negatives. In this paper we address all three of these issues. First, we
show how the subset of classifiers that meet given precision and capacity
constraints can be represented as a feasible region in ROC space. We establish
the geometry of this feasible region. We then define the partial area of lesser
classifiers, a performance metric that is monotonic with cost and only accounts
for the feasible portion of ROC space. Averaging this area over a desired range
of cost parameters results in the partial volume over the ROC surface, or
partial VOROS. In experiments predicting mortality risk using vital sign
history on the MIMIC-IV dataset, we show this cost-aware metric is better than
alternatives for ranking classifiers in hospital alert applications.

</details>


### [146] [Pay Attention to the Triggers: Constructing Backdoors That Survive Distillation](https://arxiv.org/abs/2510.18541)
*Giovanni De Muri,Mark Vero,Robin Staab,Martin Vechev*

Main category: cs.LG

TL;DR: 这篇文章探讨了大型语言模型（LLM）知识蒸馏中的安全风险，特别是当教师模型被植入后门时的影响。


<details>
  <summary>Details</summary>
Motivation: 研究现有LLM后门方法在知识蒸馏中无法有效转移到学生模型的问题，并指出这低估了知识蒸馏的安全风险。

Method: 提出了一种新的后门技术T-MTB，该技术通过构建复合型后门触发器，使得后门在知识蒸馏过程中能够有效转移到学生模型。T-MTB选择在蒸馏数据集中单独频繁出现的特定token组合作为触发器。

Result: 使用T-MTB，作者在越狱和内容调制两种攻击场景以及四种LLM模型家族中，广泛研究并展示了可转移后门带来的安全风险。

Conclusion: 知识蒸馏过程中存在的后门风险被低估，T-MTB技术能够成功构建可转移的后门，揭示了LLM知识蒸馏中潜在的严重安全问题。

Abstract: LLMs are often used by downstream users as teacher models for knowledge
distillation, compressing their capabilities into memory-efficient models.
However, as these teacher models may stem from untrusted parties, distillation
can raise unexpected security risks. In this paper, we investigate the security
implications of knowledge distillation from backdoored teacher models. First,
we show that prior backdoors mostly do not transfer onto student models. Our
key insight is that this is because existing LLM backdooring methods choose
trigger tokens that rarely occur in usual contexts. We argue that this
underestimates the security risks of knowledge distillation and introduce a new
backdooring technique, T-MTB, that enables the construction and study of
transferable backdoors. T-MTB carefully constructs a composite backdoor
trigger, made up of several specific tokens that often occur individually in
anticipated distillation datasets. As such, the poisoned teacher remains
stealthy, while during distillation the individual presence of these tokens
provides enough signal for the backdoor to transfer onto the student. Using
T-MTB, we demonstrate and extensively study the security risks of transferable
backdoors across two attack scenarios, jailbreaking and content modulation, and
across four model families of LLMs.

</details>


### [147] [RAISE: A Unified Framework for Responsible AI Scoring and Evaluation](https://arxiv.org/abs/2510.18559)
*Loc Phuc Truong Nguyen,Hung Thanh Do*

Main category: cs.LG

TL;DR: RAISE是一个统一的框架，旨在通过量化可解释性、公平性、鲁棒性和可持续性这四个维度来评估AI系统，并将这些评估聚合成一个整体的责任分数。


<details>
  <summary>Details</summary>
Motivation: 随着AI系统进入高风险领域，评估必须超越预测准确性，将可解释性、公平性、鲁棒性和可持续性纳入考量。

Method: 本文提出了RAISE（Responsible AI Scoring and Evaluation），一个统一的框架，用于量化模型在这四个维度上的表现，并将其汇总为一个单一的，整体的责任分数。研究人员在金融、医疗保健和社会经济领域的结构化数据集上评估了三种深度学习模型：多层感知器（MLP）、表格ResNet和特征分词器Transformer。

Result: 研究结果揭示了关键的权衡：MLP在可持续性和鲁棒性方面表现出色；Transformer在可解释性和公平性方面表现突出，但环境成本很高；表格ResNet则提供了一个平衡的特性。

Conclusion: 没有单一模型能在所有责任标准上都占据主导地位，这强调了对负责任模型选择进行多维度评估的必要性。

Abstract: As AI systems enter high-stakes domains, evaluation must extend beyond
predictive accuracy to include explainability, fairness, robustness, and
sustainability. We introduce RAISE (Responsible AI Scoring and Evaluation), a
unified framework that quantifies model performance across these four
dimensions and aggregates them into a single, holistic Responsibility Score. We
evaluated three deep learning models: a Multilayer Perceptron (MLP), a Tabular
ResNet, and a Feature Tokenizer Transformer, on structured datasets from
finance, healthcare, and socioeconomics. Our findings reveal critical
trade-offs: the MLP demonstrated strong sustainability and robustness, the
Transformer excelled in explainability and fairness at a very high
environmental cost, and the Tabular ResNet offered a balanced profile. These
results underscore that no single model dominates across all responsibility
criteria, highlighting the necessity of multi-dimensional evaluation for
responsible model selection. Our implementation is available at:
https://github.com/raise-framework/raise.

</details>


### [148] [HeFS: Helper-Enhanced Feature Selection via Pareto-Optimized Genetic Search](https://arxiv.org/abs/2510.18575)
*Yusi Fan,Tian Wang,Zhiying Yan,Chang Liu,Qiong Zhou,Qi Lu,Zhehao Guo,Ziqi Deng,Wenyu Zhu,Ruochi Zhang,Fengfeng Zhou*

Main category: cs.LG

TL;DR: HeFS（Helper-Enhanced Feature Selection）框架通过识别辅助特征来改进现有特征选择算法生成的特征子集，从而提高分类性能。


<details>
  <summary>Details</summary>
Motivation: 传统的特征选择方法存在早熟收敛和难以捕捉复杂特征关系的问题，在高维数据集中尤为突出。

Method: HeFS在遗传算法中采用偏向初始化和比例引导变异机制，通过Pareto多目标优化来最大化预测精度和特征互补性，从而系统地搜索残差空间以识别 Helper Set。

Result: 在18个基准数据集上，HeFS在胃癌分类、药物毒性预测和计算机科学应用等挑战性领域中，始终能识别出被忽视但信息丰富的特征，并实现优于现有技术的性能。

Conclusion: HeFS框架通过引入辅助特征并采用多目标优化策略，有效解决了传统特征选择方法的局限性，显著提高了高维数据集的分类性能。

Abstract: Feature selection is a combinatorial optimization problem that is NP-hard.
Conventional approaches often employ heuristic or greedy strategies, which are
prone to premature convergence and may fail to capture subtle yet informative
features. This limitation becomes especially critical in high-dimensional
datasets, where complex and interdependent feature relationships prevail. We
introduce the HeFS (Helper-Enhanced Feature Selection) framework to refine
feature subsets produced by existing algorithms. HeFS systematically searches
the residual feature space to identify a Helper Set - features that complement
the original subset and improve classification performance. The approach
employs a biased initialization scheme and a ratio-guided mutation mechanism
within a genetic algorithm, coupled with Pareto-based multi-objective
optimization to jointly maximize predictive accuracy and feature
complementarity. Experiments on 18 benchmark datasets demonstrate that HeFS
consistently identifies overlooked yet informative features and achieves
superior performance over state-of-the-art methods, including in challenging
domains such as gastric cancer classification, drug toxicity prediction, and
computer science applications. The code and datasets are available at
https://healthinformaticslab.org/supp/.

</details>


### [149] [Retaining by Doing: The Role of On-Policy Data in Mitigating Forgetting](https://arxiv.org/abs/2510.18874)
*Howard Chen,Noam Razin,Karthik Narasimhan,Danqi Chen*

Main category: cs.LG

TL;DR: 本文比较了SFT和RL两种后训练方法在语言模型适应新任务时的灾难性遗忘现象，发现RL比SFT具有更少的遗忘，同时在目标任务性能上相当或更高，并探讨了其中的原因和实际应用价值。


<details>
  <summary>Details</summary>
Motivation: 在语言模型适应新任务的后训练过程中，存在降低现有能力的灾难性遗忘风险，本文旨在通过系统比较两种广泛采用的后训练方法（SFT和RL）的遗忘模式，以期找到缓解这种现象的指导方针。

Method: 本文系统比较了SFT和RL两种后训练方法的遗忘模式。通过实验揭示了在不同LM家族（Llama, Qwen）和任务（指令遵循、通用知识和算术推理）中，RL导致更少的遗忘。通过将LM建模为两种分布的混合体，分析了RL模式搜索的特性，并验证了在实际设置中使用on-policy数据是RL对抗遗忘的关键。

Result: RL在实现相当或更高目标任务性能的同时，导致了比SFT更少的遗忘。研究发现，RL的模式搜索特性，源于其使用on-policy数据，使得在学习目标任务时能够保持先验知识的完整性。研究也表明，使用on-policy数据是RL对遗忘鲁棒性的根本原因。

Conclusion: RL在语言模型后训练中能有效缓解灾难性遗忘，优于SFT，主要原因在于RL的on-policy数据使用。这一发现为未来语言模型的微调和持续学习提供了新的方向，提示可以利用近似on-policy数据来提高效率并减少遗忘。

Abstract: Adapting language models (LMs) to new tasks via post-training carries the
risk of degrading existing capabilities -- a phenomenon classically known as
catastrophic forgetting. In this paper, toward identifying guidelines for
mitigating this phenomenon, we systematically compare the forgetting patterns
of two widely adopted post-training methods: supervised fine-tuning (SFT) and
reinforcement learning (RL). Our experiments reveal a consistent trend across
LM families (Llama, Qwen) and tasks (instruction following, general knowledge,
and arithmetic reasoning): RL leads to less forgetting than SFT while achieving
comparable or higher target task performance. To investigate the cause for this
difference, we consider a simplified setting in which the LM is modeled as a
mixture of two distributions, one corresponding to prior knowledge and the
other to the target task. We identify that the mode-seeking nature of RL, which
stems from its use of on-policy data, enables keeping prior knowledge intact
when learning the target task. We then verify this insight by demonstrating
that the use on-policy data underlies the robustness of RL to forgetting in
practical settings, as opposed to other algorithmic choices such as the KL
regularization or advantage estimation. Lastly, as a practical implication, our
results highlight the potential of mitigating forgetting using approximately
on-policy data, which can be substantially more efficient to obtain than fully
on-policy data.

</details>


### [150] [Unrolled-SINDy: A Stable Explicit Method for Non linear PDE Discovery from Sparsely Sampled Data](https://arxiv.org/abs/2510.18611)
*Fayad Ali Banna,Antoine Caradot,Eduardo Brandao,Jean-Philippe Colombier,Rémi Emonet,Marc Sebban*

Main category: cs.LG

TL;DR: 本文介绍了Unrolled-SINDy，一种通过展开方案提高显式方法稳定性的方法，用于解决稀疏采样时间数据下物理动力学控制微分方程识别问题。


<details>
  <summary>Details</summary>
Motivation: 现有的SINDy方法在处理时间稀疏采样数据时存在局限性，导致无法识别真实世界的物理动力学控制微分方程。

Method: 本文提出了Unrolled-SINDy方法，该方法利用展开方案将数值时间步长与可用数据的采样率解耦，从而恢复因局部截断误差大而无法通过原始SINDy优化问题最小化得到的方程参数。该方法可以通过迭代闭式方法或梯度下降方案实现。

Result: 实验证明了本文方法的通用性。在传统SINDy和最先进的噪声鲁棒iNeuralSINDy上，使用不同的数值方案（Euler，RK4），本文提出的展开方案能够解决非展开方法无法处理的问题。

Conclusion: Unrolled-SINDy通过有效处理时间稀疏采样数据，显著扩展了SINDy方法在物理动力学方程发现领域的应用范围，并提高了方程参数识别的准确性。

Abstract: Identifying from observation data the governing differential equations of a
physical dynamics is a key challenge in machine learning. Although approaches
based on SINDy have shown great promise in this area, they still fail to
address a whole class of real world problems where the data is sparsely sampled
in time. In this article, we introduce Unrolled-SINDy, a simple methodology
that leverages an unrolling scheme to improve the stability of explicit methods
for PDE discovery. By decorrelating the numerical time step size from the
sampling rate of the available data, our approach enables the recovery of
equation parameters that would not be the minimizers of the original SINDy
optimization problem due to large local truncation errors. Our method can be
exploited either through an iterative closed-form approach or by a gradient
descent scheme. Experiments show the versatility of our method. On both
traditional SINDy and state-of-the-art noise-robust iNeuralSINDy, with
different numerical schemes (Euler, RK4), our proposed unrolling scheme allows
to tackle problems not accessible to non-unrolled methods.

</details>


### [151] [A Rectification-Based Approach for Distilling Boosted Trees into Decision Trees](https://arxiv.org/abs/2510.18615)
*Gilles Audemard,Sylvie Coste-Marquis,Pierre Marquis,Mehdi Sabiri,Nicolas Szczepanski*

Main category: cs.LG

TL;DR: 这篇文章介绍了一种将增强树提取到决策树中的新方法，旨在生成一个在预测性能和可解释性方面达到可接受折衷的机器学习模型。


<details>
  <summary>Details</summary>
Motivation: 为了在预测性能和可解释性之间实现可接受的折衷，需要将增强树模型蒸馏成决策树模型。

Method: 本文提出使用名为“修正（rectification）”的校正方法来实现蒸馏过程。

Result: 实证结果表明，与通过重新训练模型实现的蒸馏方法相比，本文提出的方法能提供更好的结果。

Conclusion: “修正”方法可以有效地将增强树蒸馏成决策树，从而在保持预测性能的同时提高模型的可解释性。

Abstract: We present a new approach for distilling boosted trees into decision trees,
in the objective of generating an ML model offering an acceptable compromise in
terms of predictive performance and interpretability. We explain how the
correction approach called rectification can be used to implement such a
distillation process. We show empirically that this approach provides
interesting results, in comparison with an approach to distillation achieved by
retraining the model.

</details>


### [152] [Hardness of Learning Regular Languages in the Next Symbol Prediction Setting](https://arxiv.org/abs/2510.18634)
*Satwik Bhattamishra,Phil Blunsom,Varun Kanade*

Main category: cs.LG

TL;DR: 本文探讨了在 Next Symbol Prediction (NSP) 设置中，语言学习的计算复杂性。尽管 NSP 提供了比传统分类设置更丰富的标签，但我们证明了学习 DFA 和布尔公式等概念类仍然是计算上困难的。


<details>
  <summary>Details</summary>
Motivation: 在 Next Symbol Prediction (NSP) 设置中，学习器接收语言的积极例子，并获得关于前缀是否在语言中以及哪些下一个符号可以导致接受字符串的信息。此设置已用于经验性分析神经序列模型，并且 NSP 设置的高效算法可用于学习语言模型的（截断）支持。

Method: 我们对 NSP 设置进行了形式化，使其适用于 PAC 学习分析。通过构建一个使得几乎所有额外标签都无信息的方法，我们将传统学习问题简化为使用 NSP 标签的学习问题。

Result: 尽管 NSP 设置提供了比传统分类设置更丰富的标签，但学习 DFA 和布尔公式等概念类仍然是计算上困难的。

Conclusion: 在密码学假设下，NSP 设置中学习 DFA 的问题在计算上是困难的。

Abstract: We study the learnability of languages in the Next Symbol Prediction (NSP)
setting, where a learner receives only positive examples from a language
together with, for every prefix, (i) whether the prefix itself is in the
language and (ii) which next symbols can lead to an accepting string. This
setting has been used in prior works to empirically analyze neural sequence
models, and additionally, we observe that efficient algorithms for the NSP
setting can be used to learn the (truncated) support of language models. We
formalize the setting so as to make it amenable to PAC-learning analysis. While
the setting provides a much richer set of labels than the conventional
classification setting, we show that learning concept classes such as DFAs and
Boolean formulas remains computationally hard. The proof is via a construction
that makes almost all additional labels uninformative, yielding a reduction
from the conventional learning problem to learning with NSP labels. Under
cryptographic assumptions, the reduction implies that the problem of learning
DFAs is computationally hard in the NSP setting.

</details>


### [153] [Informed Learning for Estimating Drought Stress at Fine-Scale Resolution Enables Accurate Yield Prediction](https://arxiv.org/abs/2510.18648)
*Miro Miranda,Marcela Charfuelan,Matias Valdenegro Toro,Andreas Dengel*

Main category: cs.LG

TL;DR: 这篇论文提出了一种结合物理模型和机器学习模型优点的作物产量预测方法，能够准确预测作物产量，并具有较高的可解释性。


<details>
  <summary>Details</summary>
Motivation: 传统的作物模拟模型缺乏性能，而机器学习模型缺乏可解释性和对物理原理的遵循。本文旨在弥补这一差距，通过耦合两种方法的优势来准确预测作物产量。

Method: 该研究将作物产量定义为时间的函数，并通过预测作物干旱胁迫和对水分稀缺的敏感性来预测作物产量。为了保证物理一致性，提出了一种新颖的物理信息损失函数，并利用多光谱卫星图像、气象数据和精细尺度产量数据。此外，为了解决模型中的不确定性，本文采用了一种深度集成方法。

Result: 该方法在作物产量预测方面超越了LSTM和Transformer等最先进的模型，决定系数（R²分数）高达0.82，同时具有较高的可解释性。

Conclusion: 该方法为工业、政策制定者和农民在气候变化条件下建立更具弹性的农业提供了决策支持。

Abstract: Water is essential for agricultural productivity. Assessing water shortages
and reduced yield potential is a critical factor in decision-making for
ensuring agricultural productivity and food security. Crop simulation models,
which align with physical processes, offer intrinsic explainability but often
perform poorly. Conversely, machine learning models for crop yield modeling are
powerful and scalable, yet they commonly operate as black boxes and lack
adherence to the physical principles of crop growth. This study bridges this
gap by coupling the advantages of both worlds. We postulate that the crop yield
is inherently defined by the water availability. Therefore, we formulate crop
yield as a function of temporal water scarcity and predict both the crop
drought stress and the sensitivity to water scarcity at fine-scale resolution.
Sequentially modeling the crop yield response to water enables accurate yield
prediction. To enforce physical consistency, a novel physics-informed loss
function is proposed. We leverage multispectral satellite imagery,
meteorological data, and fine-scale yield data. Further, to account for the
uncertainty within the model, we build upon a deep ensemble approach. Our
method surpasses state-of-the-art models like LSTM and Transformers in crop
yield prediction with a coefficient of determination ($R^2$-score) of up to
0.82 while offering high explainability. This method offers decision support
for industry, policymakers, and farmers in building a more resilient
agriculture in times of changing climate conditions.

</details>


### [154] [Learning Time-Varying Turn-Taking Behavior in Group Conversations](https://arxiv.org/abs/2510.18649)
*Madeline Navarro,Lisa O'Bryan,Santiago Segarra*

Main category: cs.LG

TL;DR: 本文提出了一种灵活的概率模型，用于根据个人特征和过去的说话行为预测群体对话中的轮流模式。


<details>
  <summary>Details</summary>
Motivation: 以往的会话动态模型难以推广到单个群体之外，并且常常采用不适用于所有群体的普适性公式来描述说话行为。

Method: 本文提出了一种对现有会话模型的泛化，该模型根据个体的性格特质和先前的说话行为，预测任何群体中个体的说话轮次。此外，该方法还能够学习说话倾向如何根据个体上次说话的时间而变化。

Result: 通过在合成数据和真实世界对话数据上的应用，验证了所提出的方法。结果表明，以往的行为模型可能不总是现实的。

Conclusion: 本文提出了一种数据驱动但理论扎实的模型，为理解和预测群体对话中的轮流模式提供了新的视角。

Abstract: We propose a flexible probabilistic model for predicting turn-taking patterns
in group conversations based solely on individual characteristics and past
speaking behavior. Many models of conversation dynamics cannot yield insights
that generalize beyond a single group. Moreover, past works often aim to
characterize speaking behavior through a universal formulation that may not be
suitable for all groups. We thus develop a generalization of prior conversation
models that predicts speaking turns among individuals in any group based on
their individual characteristics, that is, personality traits, and prior
speaking behavior. Importantly, our approach provides the novel ability to
learn how speaking inclination varies based on when individuals last spoke. We
apply our model to synthetic and real-world conversation data to verify the
proposed approach and characterize real group interactions. Our results
demonstrate that previous behavioral models may not always be realistic,
motivating our data-driven yet theoretically grounded approach.

</details>


### [155] [Reasoning Language Model Inference Serving Unveiled: An Empirical Study](https://arxiv.org/abs/2510.18672)
*Qi Li,Junpan Wu,Xiang Liu,Yuxin Wang,Zeyu Li,Zhenheng Tang,Yuhan Chen,Shaohuai Shi,Xiaowen Chu*

Main category: cs.LG

TL;DR: 本文对RLLM的服务进行了全面的研究，揭示了RLLM与传统LLM在服务行为上的显著差异，并评估了现有推理优化技术对RLLM的有效性。


<details>
  <summary>Details</summary>
Motivation: 探索RLLM的服务性能和行为，以解决其在实际部署和利用中存在的问题。

Method: 我们首先对RLLM和传统LLM之间的服务性能进行了初步研究，然后进一步调查了现有的推理优化技术是否适用于RLLM。

Result: RLLM的服务行为存在显著差异，包括显著的内存使用和波动、掉队请求、自适应运行时间、领域偏好。模型量化方法和推测性解码可以在对RLLM精度影响较小的情况下提高服务系统效率，而前缀缓存、KV缓存量化甚至可能降低小型RLLM的精度或服务性能。

Conclusion: 本文的工作为研究社区和工业界提供了关于推进RLLM推理服务的见解。

Abstract: The reasoning large language model (RLLM) has been proven competitive in
solving complex reasoning tasks such as mathematics, coding, compared to
general LLM. However, the serving performance and behavior of RLLM remains
unexplored, which may undermine the deployment and utilization of RLLM in
real-world scenario. To close this gap, in this paper, we conduct a
comprehensive study of RLLM service. We first perform a pilot study on
comparing the serving performance between RLLM and traditional LLM and reveal
that there are several distinct differences regarding serving behavior: (1)
significant memory usage and fluctuations; (2) straggler requests; (3) adaptive
running time; (4) domain preference. Then we further investigate whether
existing inference optimization techniques are valid for RLLM. Our main
takeaways are that model quantization methods and speculative decoding can
improve service system efficiency with small compromise to RLLM accuracy, while
prefix caching, KV cache quantization may even degrade accuracy or serving
performance for small RLLM. Lastly, we conduct evaluation under real world
workload modeled by Gamma distribution to verify our findings. Empirical
results of real world workload evaluation across different dataset are aligned
with our main findings regarding RLLM serving. We hope our work can provide the
research community and industry with insights to advance RLLM inference
serving.

</details>


### [156] [Learning Task-Agnostic Representations through Multi-Teacher Distillation](https://arxiv.org/abs/2510.18680)
*Philippe Formont,Maxime Darrin,Banafsheh Karimian,Jackie CK Cheung,Eric Granger,Ismail Ben Ayed,Mohammadhadi Shateri,Pablo Piantanida*

Main category: cs.LG

TL;DR: 本文提出了一个任务无关的蒸馏框架，通过利用教师模型的多样性来丰富学生模型的表示，该框架在多种数据模态上提升了下游任务的表现，并且不依赖于任务特定的标签或先验知识。


<details>
  <summary>Details</summary>
Motivation: 现有的多教师蒸馏方法通常针对特定任务，而不是任务无关的，这限制了它们在更广泛应用中的潜力。

Method: 本文引入了一个基于“多数投票”目标函数的任务无关框架。该目标函数以学生和教师嵌入之间的互信息为界限，从而推导出一个不依赖于任务特定标签或先验知识的蒸馏损失。

Result: 该方法在文本、视觉和分子建模等多种模态上进行了评估，结果表明它能有效利用教师多样性，产生在分类、聚类或回归等多种下游任务中表现更好的表示。此外，研究者还训练并发布了最先进的嵌入模型，进一步提升了不同模态下的下游性能。

Conclusion: 本文提出的任务无关蒸馏框架通过“多数投票”目标函数和互信息约束，成功地利用了多教师模型的优势，在不依赖特定任务标签的情况下，提升了多种下游任务的性能，并贡献了高质量的嵌入模型。

Abstract: Casting complex inputs into tractable representations is a critical step
across various fields. Diverse embedding models emerge from differences in
architectures, loss functions, input modalities and datasets, each capturing
unique aspects of the input. Multi-teacher distillation leverages this
diversity to enrich representations but often remains tailored to specific
tasks. In this paper, we introduce a task-agnostic framework based on a
``majority vote" objective function. We demonstrate that this function is
bounded by the mutual information between student and teachers' embeddings,
leading to a task-agnostic distillation loss that eliminates dependence on
task-specific labels or prior knowledge. Our evaluations across text, vision
models, and molecular modeling show that our method effectively leverages
teacher diversity, resulting in representations enabling better performance for
a wide range of downstream tasks such as classification, clustering, or
regression. Additionally, we train and release state-of-the-art embedding
models, enhancing downstream performance in various modalities.

</details>


### [157] [OmniCast: A Masked Latent Diffusion Model for Weather Forecasting Across Time Scales](https://arxiv.org/abs/2510.18707)
*Tung Nguyen,Tuan Pham,Troy Arcomano,Veerabhadra Kotamarthi,Ian Foster,Sandeep Madireddy,Aditya Grover*

Main category: cs.LG

TL;DR: OmniCast是一个可扩展且熟练的概率模型，它统一了跨时间尺度的天气预报。


<details>
  <summary>Details</summary>
Motivation: 目前基于深度学习的数据驱动方法在中等范围内取得了显著成功，但在较长的次季节到季节（S2S）范围内，由于其自回归方法中的误差积累，预测效果不佳。

Method: OmniCast包含两个组件：一个将原始天气数据编码为连续、低维潜在空间的VAE模型，以及一个基于扩散的Transformer模型，该模型根据初始条件令牌生成一系列未来潜在令牌。在训练期间，随机掩盖未来的令牌，并训练Transformer，使其根据条件和可见令牌，使用每个令牌的扩散头来估计它们的分布。在推理期间，Transformer通过迭代地揭示随机子集的令牌来生成未来令牌的完整序列。

Result: OmniCast在中等范围时间尺度上与领先的概率方法竞争激烈，速度提高了10到20倍，并在次季节到季节尺度上，在准确性、基于物理和概率指标方面实现了最先进的性能。

Conclusion: OmniCast可以生成长达100年的稳定预测。

Abstract: Accurate weather forecasting across time scales is critical for anticipating
and mitigating the impacts of climate change. Recent data-driven methods based
on deep learning have achieved significant success in the medium range, but
struggle at longer subseasonal-to-seasonal (S2S) horizons due to error
accumulation in their autoregressive approach. In this work, we propose
OmniCast, a scalable and skillful probabilistic model that unifies weather
forecasting across timescales. OmniCast consists of two components: a VAE model
that encodes raw weather data into a continuous, lower-dimensional latent
space, and a diffusion-based transformer model that generates a sequence of
future latent tokens given the initial conditioning tokens. During training, we
mask random future tokens and train the transformer to estimate their
distribution given conditioning and visible tokens using a per-token diffusion
head. During inference, the transformer generates the full sequence of future
tokens by iteratively unmasking random subsets of tokens. This joint sampling
across space and time mitigates compounding errors from autoregressive
approaches. The low-dimensional latent space enables modeling long sequences of
future latent states, allowing the transformer to learn weather dynamics beyond
initial conditions. OmniCast performs competitively with leading probabilistic
methods at the medium-range timescale while being 10x to 20x faster, and
achieves state-of-the-art performance at the subseasonal-to-seasonal scale
across accuracy, physics-based, and probabilistic metrics. Furthermore, we
demonstrate that OmniCast can generate stable rollouts up to 100 years ahead.
Code and model checkpoints are available at
https://github.com/tung-nd/omnicast.

</details>


### [158] [Improving the Generation and Evaluation of Synthetic Data for Downstream Medical Causal Inference](https://arxiv.org/abs/2510.18768)
*Harry Amad,Zhaozhi Qian,Dennis Frauen,Julianna Piskorz,Stefan Feuerriegel,Mihaela van der Schaar*

Main category: cs.LG

TL;DR: 这篇论文介绍了一种名为STEAM的新方法，用于生成包含治疗信息的合成医疗数据，旨在解决真实医疗数据难以获取的问题，并优化了下游因果推断任务的效用。


<details>
  <summary>Details</summary>
Motivation: 由于监管障碍，真实世界的医疗数据集通常难以获取，这阻碍了医学干预措施的开发和评估。合成数据可以成为一个有价值的资产，支持医疗分析和新的因果推断方法开发。

Method: 作者提出了一套合成数据应满足的必要条件，以最大限度地提高下游因果推断的效用，包括保留（i）协变量分布，（ii）治疗分配机制和（iii）结果生成机制。基于这些必要条件，他们提出了一套评估合成数据的指标，并开发了STEAM方法。

Result: STEAM方法在所提出的评估指标上，比现有生成模型取得了最先进的性能，尤其是在真实数据生成过程复杂性增加的情况下。

Conclusion: STEAM是一种生成医学治疗效果分析合成数据的新方法，它通过模拟包含治疗信息的数据生成过程并优化上述必要条件，有效地解决了真实医疗数据获取困难的问题，并提高了下游因果推断任务的效用。

Abstract: Causal inference is essential for developing and evaluating medical
interventions, yet real-world medical datasets are often difficult to access
due to regulatory barriers. This makes synthetic data a potentially valuable
asset that enables these medical analyses, along with the development of new
inference methods themselves. Generative models can produce synthetic data that
closely approximate real data distributions, yet existing methods do not
consider the unique challenges that downstream causal inference tasks, and
specifically those focused on treatments, pose. We establish a set of
desiderata that synthetic data containing treatments should satisfy to maximise
downstream utility: preservation of (i) the covariate distribution, (ii) the
treatment assignment mechanism, and (iii) the outcome generation mechanism.
Based on these desiderata, we propose a set of evaluation metrics to assess
such synthetic data. Finally, we present STEAM: a novel method for generating
Synthetic data for Treatment Effect Analysis in Medicine that mimics the
data-generating process of data containing treatments and optimises for our
desiderata. We empirically demonstrate that STEAM achieves state-of-the-art
performance across our metrics as compared to existing generative models,
particularly as the complexity of the true data-generating process increases.

</details>


### [159] [CAGE: Curvature-Aware Gradient Estimation For Accurate Quantization-Aware Training](https://arxiv.org/abs/2510.18784)
*Soroush Tabesh,Mher Safaryan,Dan Alistarh*

Main category: cs.LG

TL;DR: CAGE（Curvature-Aware Gradient Estimation）是一种新的QAT方法，它通过曲率感知校正来增强STE梯度，以抵消量化引起的损失增加。


<details>
  <summary>Details</summary>
Motivation: 低位量化感知训练（QAT）与原生训练之间存在很大的精度差距，作者旨在弥合这一差距。

Method: CAGE方法源于QAT的多目标视图，它平衡了损失最小化和量化约束的依从性，产生了一个取决于局部曲率信息的原则性校正项。该方法在理论上引入了量化优化的帕累托最优解概念，并建立了CAGE在平滑非凸设置下强大的收敛保证。在实现方面，CAGE是优化器无关的，但作者提供了一个利用Adam统计数据的高效实现。

Result: 在预训练高达8亿参数的Llama风格模型时，CAGE在W4A4机制下，比离群值缓解方法多恢复了超过10%的量化引起的损失增加。

Conclusion: 曲率感知梯度校正可以弥补当前离群值处理方法之外剩余的性能差距。

Abstract: Despite significant work on low-bit quantization-aware training (QAT), there
is still a large accuracy gap between such techniques and native training. To
address this, we introduce CAGE (Curvature-Aware Gradient Estimation), a new
QAT method that augments the straight-through estimator (STE) gradient with a
curvature-aware correction designed to counteract the loss increase induced by
quantization. CAGE is derived from a multi-objective view of QAT that balances
loss minimization with adherence to quantization constraints, yielding a
principled correction term that depends on local curvature information. On the
theoretical side, we introduce the notion of Pareto-optimal solutions for
quantized optimization, and establish that CAGE yields strong convergence
guarantees in the smooth non-convex setting. In terms of implementation, our
approach is optimizer-agnostic, but we provide a highly-efficient
implementation that leverages Adam statistics. When pre-training Llama-style
models of up to 800M-parameters, CAGE recovers over 10% of the
quantization-induced loss increase in the W4A4 regime over outlier-mitigation
methods. These results indicate that curvature-aware gradient corrections can
bridge the remaining performance gap beyond current outlier-handling methods.

</details>


### [160] [Stick-Breaking Embedded Topic Model with Continuous Optimal Transport for Online Analysis of Document Streams](https://arxiv.org/abs/2510.18786)
*Federica Granese,Serena Villata,Charles Bouveyron*

Main category: cs.LG

TL;DR: SB-SETM模型利用截断stick-breaking和主题嵌入合并策略，能够处理数据流并自动推断主题数量。


<details>
  <summary>Details</summary>
Motivation: 在线主题模型与现实世界场景更吻合，但相对于离线模型，其研究关注度较低，面临额外挑战。

Method: SB-SETM扩展了嵌入式主题模型（ETM），通过合并连续部分文档批次形成的模型来处理数据流。它利用截断的stick-breaking结构来推断每个时间步的活跃主题数量，并引入基于连续最优传输的主题嵌入合并策略。

Result: SB-SETM在模拟场景中优于基线，并在2022-2023年俄乌战争新闻文章的真实语料库上进行了广泛测试。

Conclusion: SB-SETM通过引入新的主题数量推断和主题嵌入合并策略，有效地解决了在线主题模型在数据流处理方面的挑战，并在实际应用中展现了优越的性能。

Abstract: Online topic models are unsupervised algorithms to identify latent topics in
data streams that continuously evolve over time. Although these methods
naturally align with real-world scenarios, they have received considerably less
attention from the community compared to their offline counterparts, due to
specific additional challenges. To tackle these issues, we present SB-SETM, an
innovative model extending the Embedded Topic Model (ETM) to process data
streams by merging models formed on successive partial document batches. To
this end, SB-SETM (i) leverages a truncated stick-breaking construction for the
topic-per-document distribution, enabling the model to automatically infer from
the data the appropriate number of active topics at each timestep; and (ii)
introduces a merging strategy for topic embeddings based on a continuous
formulation of optimal transport adapted to the high dimensionality of the
latent topic space. Numerical experiments show SB-SETM outperforming baselines
on simulated scenarios. We extensively test it on a real-world corpus of news
articles covering the Russian-Ukrainian war throughout 2022-2023.

</details>


### [161] [When LRP Diverges from Leave-One-Out in Transformers](https://arxiv.org/abs/2510.18810)
*Weiqiu You,Siqi Zeng,Yao-Hung Hubert Tsai,Makoto Yamada,Han Zhao*

Main category: cs.LG

TL;DR: 本文分析了Transformer模型中LRP对LOO的近似能力受到的两个主要因素的影响。


<details>
  <summary>Details</summary>
Motivation: 探索LRP在现代Transformer模型中应用于特征重要性评估的可靠性

Method: 从理论和实验两方面分析了AttnLRP的双线性传播规则对实现不变性公理的违反，并重新审视了CP-LRP作为诊断基线，研究了在不通过softmax层进行相关性传播时对LOO对齐的影响。

Result: AttnLRP的双线性传播规则违反了实现不变性公理，尤其是在线性注意力层。通过价值矩阵传播相关性可以显著提高CP-LRP与LOO的对齐程度，尤其是在Transformer模型的中间到后期层。

Conclusion: 双线性分解敏感性和softmax传播误差可能共同损害了LRP在Transformer模型中近似LOO的能力。

Abstract: Leave-One-Out (LOO) provides an intuitive measure of feature importance but
is computationally prohibitive. While Layer-Wise Relevance Propagation (LRP)
offers a potentially efficient alternative, its axiomatic soundness in modern
Transformers remains largely under-examined. In this work, we first show that
the bilinear propagation rules used in recent advances of AttnLRP violate the
implementation invariance axiom. We prove this analytically and confirm it
empirically in linear attention layers. Second, we also revisit CP-LRP as a
diagnostic baseline and find that bypassing relevance propagation through the
softmax layer -- backpropagating relevance only through the value matrices --
significantly improves alignment with LOO, particularly in middle-to-late
Transformer layers. Overall, our results suggest that (i) bilinear
factorization sensitivity and (ii) softmax propagation error potentially
jointly undermine LRP's ability to approximate LOO in Transformers.

</details>


### [162] [Online SFT for LLM Reasoning: Surprising Effectiveness of Self-Tuning without Rewards](https://arxiv.org/abs/2510.18814)
*Mengqi Li,Lei Zhao,Anthony Man-Cho So,Ruoyu Sun,Xiao Li*

Main category: cs.LG

TL;DR: 该论文提出了OSFT（自帮助在线监督微调）范式，通过模型自生成数据进行即时微调，在数学推理任务上取得了与GRPO等RLVR方法相当的性能，且更为高效和鲁棒。


<details>
  <summary>Details</summary>
Motivation: 探索一种更高效、低成本的LLM推理能力提升方法，避免了传统强化学习方法对奖励的需求和复杂性。

Method: OSFT范式：模型生成自己的响应，并立即利用这些自生成的数据进行微调。该方法是无奖励的，并且默认情况下只使用一次rollout。

Result: OSFT在具有挑战性的数学推理任务上实现了与GRPO等强大的可验证奖励强化学习（RLVR）方法相当的下游性能。消融研究表明OSFT具有高效率和鲁棒性。

Conclusion: OSFT提供了一种高效且有前途的替代方案，可以替代更复杂、基于奖励的训练范式，主要机制在于促进模型自身从预训练中学习到的现有偏好（潜在知识），从而提高推理能力。

Abstract: We present a simple, self-help online supervised finetuning (OSFT) paradigm
for LLM reasoning. In this paradigm, the model generates its own responses and
is immediately finetuned on this self-generated data. OSFT is a highly
efficient training strategy for LLM reasoning, as it is reward-free and uses
just one rollout by default. Experiment results show that OSFT achieves
downstream performance on challenging mathematical reasoning tasks comparable
to strong reinforcement learning with verifiable rewards (RLVR) methods such as
GRPO. Our ablation study further demonstrates the efficiency and robustness of
OSFT. The major mechanism of OSFT lies in facilitating the model's own existing
preference (latent knowledge) learned from pretraining, which leads to
reasoning ability improvement. We believe that OSFT offers an efficient and
promising alternative to more complex, reward-based training paradigms. Our
code is available at https://github.com/ElementQi/OnlineSFT.

</details>


### [163] [Actor-Free Continuous Control via Structurally Maximizable Q-Functions](https://arxiv.org/abs/2510.18828)
*Yigit Korkmaz,Urvi Bhuwania,Ayush Jain,Erdem Bıyık*

Main category: cs.LG

TL;DR: 这篇论文提出了一种纯粹基于值函数的连续控制框架，该框架重新审视了Q函数的结构最大化，通过一系列关键的架构和算法选择，实现了高效稳定的学习，并在性能和样本效率上与最先进的基线方法相当，特别是在动作空间受限的环境中表现更优。


<details>
  <summary>Details</summary>
Motivation: 传统的基于值函数的方法在连续动作空间中计算Q值不可行，导致其应用受限。而Actor-Critic方法在训练过程中存在不稳定性。

Method: 本文提出了一种纯粹基于值函数的方法，用于连续控制，该方法重新审视了Q函数的结构最大化，并引入了一系列关键的架构和算法选择，以实现高效稳定的学习。它是一种无Actor的Q学习方法。

Result: 所提出的无Actor的Q学习方法在各种标准模拟任务中进行了评估，其性能和样本效率与最先进的基线方法相当，并且无需学习单独的Actor。特别是在动作空间受限的环境中，该方法优于传统的基于梯度的最大化的Actor-Critic方法。

Conclusion: 本文成功地提出了一种纯粹基于值函数的连续控制框架，解决了传统方法在连续动作空间中的局限性以及Actor-Critic方法的不稳定性问题，在不引入额外Actor的情况下实现了与SOTA方法相当或更优的性能。

Abstract: Value-based algorithms are a cornerstone of off-policy reinforcement learning
due to their simplicity and training stability. However, their use has
traditionally been restricted to discrete action spaces, as they rely on
estimating Q-values for individual state-action pairs. In continuous action
spaces, evaluating the Q-value over the entire action space becomes
computationally infeasible. To address this, actor-critic methods are typically
employed, where a critic is trained on off-policy data to estimate Q-values,
and an actor is trained to maximize the critic's output. Despite their
popularity, these methods often suffer from instability during training. In
this work, we propose a purely value-based framework for continuous control
that revisits structural maximization of Q-functions, introducing a set of key
architectural and algorithmic choices to enable efficient and stable learning.
We evaluate the proposed actor-free Q-learning approach on a range of standard
simulation tasks, demonstrating performance and sample efficiency on par with
state-of-the-art baselines, without the cost of learning a separate actor.
Particularly, in environments with constrained action spaces, where the value
functions are typically non-smooth, our method with structural maximization
outperforms traditional actor-critic methods with gradient-based maximization.
We have released our code at https://github.com/USC-Lira/Q3C.

</details>


### [164] [A Hybrid Enumeration Framework for Optimal Counterfactual Generation in Post-Acute COVID-19 Heart Failure](https://arxiv.org/abs/2510.18841)
*Jingya Cheng,Alaleh Azhir,Jiazi Tian,Hossein Estiri*

Main category: cs.LG

TL;DR: 这篇论文提出了一个用于个体风险估计和干预分析的反事实推理框架，并以COVID-19急性后遗症（PASC）患者的临床应用为例进行了阐述。


<details>
  <summary>Details</summary>
Motivation: 该研究的动机是为个体风险估计和干预分析提供一个反事实推理框架，以弥合因果推理和预测建模之间的鸿沟，并通过临床应用展示其有效性，特别是针对COVID-19后急性后遗症（PASC）患者。

Method: 该框架整合了正则化预测建模与反事实搜索，结合了精确枚举和基于优化的方法（包括NICE和MOC算法），以高效探索高维干预空间。

Result: 该模型对超过2700名SARS-CoV-2感染和既往心力衰竭的个体进行了应用，取得了强大的判别性能（AUROC：0.88，95% CI：0.84-0.91），并生成了可解释的、患者特定的反事实，量化了改变合并症模式或治疗因素如何改变预测结果。

Conclusion: 这项工作表明，反事实推理可以被形式化为预测函数上的优化问题，为复杂生物医学系统中的个性化推理提供了一种严格、可解释且计算高效的方法。

Abstract: Counterfactual inference provides a mathematical framework for reasoning
about hypothetical outcomes under alternative interventions, bridging causal
reasoning and predictive modeling. We present a counterfactual inference
framework for individualized risk estimation and intervention analysis,
illustrated through a clinical application to post-acute sequelae of COVID-19
(PASC) among patients with pre-existing heart failure (HF). Using longitudinal
diagnosis, laboratory, and medication data from a large health-system cohort,
we integrate regularized predictive modeling with counterfactual search to
identify actionable pathways to PASC-related HF hospital admissions. The
framework combines exact enumeration with optimization-based methods, including
the Nearest Instance Counterfactual Explanations (NICE) and Multi-Objective
Counterfactuals (MOC) algorithms, to efficiently explore high-dimensional
intervention spaces. Applied to more than 2700 individuals with confirmed
SARS-CoV-2 infection and prior HF, the model achieved strong discriminative
performance (AUROC: 0.88, 95% CI: 0.84-0.91) and generated interpretable,
patient-specific counterfactuals that quantify how modifying comorbidity
patterns or treatment factors could alter predicted outcomes. This work
demonstrates how counterfactual reasoning can be formalized as an optimization
problem over predictive functions, offering a rigorous, interpretable, and
computationally efficient approach to personalized inference in complex
biomedical systems.

</details>


<div id='cs.MA'></div>

# cs.MA [[Back]](#toc)

### [165] [TACLA: An LLM-Based Multi-Agent Tool for Transactional Analysis Training in Education](https://arxiv.org/abs/2510.17913)
*Monika Zamojska,Jarosław A. Chudziak*

Main category: cs.MA

TL;DR: 该论文介绍了一个新颖的多智能体架构TACLA，通过整合人际关系分析（TA）的核心原则，并模拟智能体独特的父母、成人和儿童自我状态，以克服大型语言模型在模拟细致的人类社会动态方面的局限性。


<details>
  <summary>Details</summary>
Motivation: 大型语言模型在模拟细致的人类社会动态方面存在挑战，尤其是在实现心理深度和一致的角色行为方面。

Method: TACLA（Transactional Analysis Contextual LLM-based Agents）通过将智能体建模为由不同的父母、成人和儿童自我状态组成的协调系统，每个状态都有自己的模式记忆。一个编排器智能体根据上下文触发器和智能体的生活脚本来优先激活自我状态，确保心理上真实的响应。

Result: 在教育场景中验证了TACLA，它展示了学生智能体中真实的自我状态转变，并根据不同的教师干预策略有效地模拟了冲突的降级和升级。

Conclusion: TACLA能够创建动态的、心理上真实的社会模拟，推动了教育及其他领域有效人工智能工具的开发。

Abstract: Simulating nuanced human social dynamics with Large Language Models (LLMs)
remains a significant challenge, particularly in achieving psychological depth
and consistent persona behavior crucial for high-fidelity training tools. This
paper introduces TACLA (Transactional Analysis Contextual LLM-based Agents), a
novel Multi-Agent architecture designed to overcome these limitations. TACLA
integrates core principles of Transactional Analysis (TA) by modeling agents as
an orchestrated system of distinct Parent, Adult, and Child ego states, each
with its own pattern memory. An Orchestrator Agent prioritizes ego state
activation based on contextual triggers and an agent's life script, ensuring
psychologically authentic responses. Validated in an educational scenario,
TACLA demonstrates realistic ego state shifts in Student Agents, effectively
modeling conflict de-escalation and escalation based on different teacher
intervention strategies. Evaluation shows high conversational credibility and
confirms TACLA's capacity to create dynamic, psychologically-grounded social
simulations, advancing the development of effective AI tools for education and
beyond.

</details>


### [166] [From Agent Simulation to Social Simulator: A Comprehensive Review (Part 1)](https://arxiv.org/abs/2510.18271)
*Xiao Xue,Deyu Zhou,Ming Zhang,Fei-Yue Wang*

Main category: cs.MA

TL;DR: 这篇综述是关于Agent-Based Modeling (ABM) 的历史发展和经典案例的第一部分。


<details>
  <summary>Details</summary>
Motivation: 传统物理仿真方法在社会领域面临挑战，因此需要ABM。

Method: 文章首先讨论了ABM的发展历史和设计原则，然后详细介绍了用于模拟社会系统的基础模型（个体模型、环境模型和基于规则的模型），最后提出了社会仿真的经典案例（思想实验、机制探索和并行优化）。

Result: 通过对ABM历史、设计原则、基础模型和经典案例的介绍，帮助读者理解ABM在社会仿真中的应用。

Conclusion: 本文通过对ABM的全面回顾，展示了ABM在解决社会领域传统仿真方法挑战方面的潜力和应用。

Abstract: This is the first part of the comprehensive review, focusing on the
historical development of Agent-Based Modeling (ABM) and its classic cases. It
begins by discussing the development history and design principles of
Agent-Based Modeling (ABM), helping readers understand the significant
challenges that traditional physical simulation methods face in the social
domain. Then, it provides a detailed introduction to foundational models for
simulating social systems, including individual models, environmental models,
and rule-based models. Finally, it presents classic cases of social simulation,
covering three types: thought experiments, mechanism exploration, and parallel
optimization.

</details>


### [167] [Socialized Learning and Emergent Behaviors in Multi-Agent Systems based on Multimodal Large Language Models](https://arxiv.org/abs/2510.18515)
*Sureyya Akin,Shruti T. Tiwari,Ram Bhattacharya,Sagar A. Raman,Kiran Mohanty,Sita Krishnan*

Main category: cs.MA

TL;DR: 该研究引入了多模态社会化学习框架（M-S2L），通过整合多模态大语言模型（M-LLMs）与社会学习机制，旨在培养AI智能体的新兴社会智能。


<details>
  <summary>Details</summary>
Motivation: 该研究旨在通过整合多模态大语言模型（M-LLMs）与社会学习机制，为AI智能体培养新兴社会智能。

Method: M-S2L框架为智能体提供了多模态感知（视觉和文本）和结构化行动能力，实现了物理操作和基于多模态的交流。它结合了直接强化学习与两种新颖的社会学习途径：多模态观察学习和反馈驱动的交流学习，并通过情景记忆系统增强了长期社会上下文。研究在协作组装环境（CAE）中评估了M-S2L，智能体团队必须在信息不对称的情况下根据模糊蓝图构建复杂设备。

Result: M-S2L智能体在任务完成率和完成时间方面始终优于仅文本和无社会学习基线，特别是在动态问题解决场景中。消融研究证实了多模态和社交学习的必要性。分析揭示了结合视觉指针的简洁文本的有效沟通协议的出现，以及快速的角色专业化。定性案例研究表明智能体具备共享意识、动态重新规划和自适应问题解决的能力。

Conclusion: 将多模态感知与明确的社会学习相结合对于在多智能体系统中发展类人协作智能至关重要。

Abstract: This search introduces the Multimodal Socialized Learning Framework (M-S2L),
designed to foster emergent social intelligence in AI agents by integrating
Multimodal Large Language Models (M-LLMs) with social learning mechanisms. The
framework equips agents with multimodal perception (vision and text) and
structured action capabilities, enabling physical manipulation and grounded
multimodal communication (e.g., text with visual pointers). M-S2L combines
direct reinforcement learning with two novel social learning pathways:
multimodal observational learning and communication-driven learning from
feedback, augmented by an episodic memory system for long-term social context.
  We evaluate M-S2L in a Collaborative Assembly Environment (CAE), where agent
teams must construct complex devices from ambiguous blueprints under
informational asymmetry. Across tasks of increasing complexity, M-S2L agents
consistently outperform Text-Only and No-Social-Learning baselines in Task
Completion Rate and Time to Completion, particularly in dynamic problem-solving
scenarios. Ablation studies confirm the necessity of both multimodality and
socialized learning. Our analysis reveals the emergence of efficient
communication protocols integrating visual pointers with concise text,
alongside rapid role specialization leading to stable labor division.
Qualitative case studies demonstrate agents' abilities for shared awareness,
dynamic re-planning, and adaptive problem-solving, suggesting a nascent form of
machine social cognition. These findings indicate that integrating multimodal
perception with explicit social learning is critical for developing human-like
collaborative intelligence in multi-agent systems.

</details>


### [168] [Computational Foundations for Strategic Coopetition: Formalizing Interdependence and Complementarity](https://arxiv.org/abs/2510.18802)
*Vik Pant,Eric Yu*

Main category: cs.MA

TL;DR: 这篇技术报告旨在通过开发计算基础来弥合概念建模语言（如 i*）和经典博弈论之间的差距，从而形式化竞争合作的两个关键维度：相互依赖和互补性。


<details>
  <summary>Details</summary>
Motivation: 现代社会技术系统以战略竞争为特征，参与者同时合作创造价值，又竞争获取价值。i*等概念建模语言提供了丰富的战略依赖定性表示，但缺乏动态权衡的定量分析机制。相反，经典博弈论提供了数学严谨性，但 HHP 剥夺了 HHP 的 CXTF 丰富性。

Method: 本文通过开发计算基础来弥合这一差距，该计算基础 HHP 形式化了竞争合作的两个关键维度：相互依赖和互补性。我们将相互依赖建立在 i* 结构依赖分析中，通过结构化转换框架将依赖者-被依赖者-依赖关系转换为定量相互依赖系数。我们遵循 Brandenburger 和 Nalebuff 的附加价值概念形式化互补性，用经过验证的参数化模型协同价值创造。我们将结构依赖与价值分配中的议价能力相结合，并引入了纳什均衡包含结构相互依赖的博弈论公式。

Result: 验证结合了跨功率和对数价值函数规范的综合实验测试，证明了功能形式的鲁棒性，以及在三星-索尼 S-LCD 合资企业 (2004-2011) 中的实证应用，其中对数规范实现了卓越的实证拟合（验证得分 45/60），而幂函数提供了理论可 S-LCD。

Conclusion: 本技术报告 HHP 作为一项 HHP 协调研究计划的基础参考，该计划 HHP 检查需求工程和多主体系统中的战略竞争，并 HHP 附带解决信任动态、团队生产和互惠机制的工作。

Abstract: Modern socio-technical systems are characterized by strategic coopetition
where actors simultaneously cooperate to create value and compete to capture
it. While conceptual modeling languages like i* provide rich qualitative
representations of strategic dependencies, they lack mechanisms for
quantitative analysis of dynamic trade-offs. Conversely, classical game theory
offers mathematical rigor but strips away contextual richness. This technical
report bridges this gap by developing computational foundations that formalize
two critical dimensions of coopetition: interdependence and complementarity. We
ground interdependence in i* structural dependency analysis, translating
depender-dependee-dependum relationships into quantitative interdependence
coefficients through a structured translation framework. We formalize
complementarity following Brandenburger and Nalebuff's Added Value concept,
modeling synergistic value creation with validated parameterization. We
integrate structural dependencies with bargaining power in value appropriation
and introduce a game-theoretic formulation where Nash Equilibrium incorporates
structural interdependence. Validation combines comprehensive experimental
testing across power and logarithmic value function specifications,
demonstrating functional form robustness, with empirical application to the
Samsung-Sony S-LCD joint venture (2004-2011), where logarithmic specifications
achieve superior empirical fit (validation score 45/60) while power functions
provide theoretical tractability. This technical report serves as the
foundational reference for a coordinated research program examining strategic
coopetition in requirements engineering and multi-agent systems, with companion
work addressing trust dynamics, team production, and reciprocity mechanisms.

</details>


<div id='cs.IT'></div>

# cs.IT [[Back]](#toc)

### [169] [A Markov-Chain Characterization of Finite-State Dimension and a Generalization of Agafonov's Theorem](https://arxiv.org/abs/2510.18736)
*Laurent Bienvenu,Hugo Gimbert,Subin Pulari*

Main category: cs.IT

TL;DR: 这篇文章探讨了有限状态维度与Borel正态性的关系，并将其推广到更广泛的序列，提供了一种新的信息论刻画方法。


<details>
  <summary>Details</summary>
Motivation: 研究有限状态维度与序列信息率的关系，并推广Schnorr-Stimm定理。

Method: 通过条件Kullback-Leibler散度来刻画有限状态维度，并将其应用于Agafonov定理的推广。

Result: 有限状态维度可以通过序列模拟马尔可夫链产生的极限分布与其平稳分布之间的条件Kullback-Leibler散度来表征。Agafonov定理被推广到任意序列，建立了序列的有限状态维度与其自动子序列的有限状态维度之间的紧密定量关系。

Conclusion: 有限状态维度与信息率、Borel正态性以及马尔可夫链模拟存在深刻的联系，并且可以通过信息论工具进行更普适的刻画。

Abstract: Finite-state dimension quantifies the asymptotic rate of information in an
infinite sequence as perceived by finite automata. For a fixed alphabet, the
infinite sequences that have maximal finite-state dimension are exactly those
that are Borel normal, i.e., in which all words of any given length appear with
the same frequency. A theorem of Schnorr and Stimm (1972) shows that a real
number is Borel normal if and only if, for every finite-state irreducible
Markov chain with fair transitions, when the chain is simulated using the
binary expansion of the given number, the empirical distribution of states
converges to its stationary distribution. In this paper we extend this
correspondence beyond normal numbers. We show that the finite-state dimension
of a sequence can be characterized in terms of the conditional Kullback-Leibler
divergence between the limiting distributions arising from the simulation of
Markov chains using the given sequence and their stationary distributions. This
provides a new information-theoretic characterization of finite-state dimension
which generalizes the Schnorr-Stimm result.
  As an application, we prove a generalization of Agafonov's theorem for normal
numbers. Agafonov's theorem states that a sequence is normal if and only if
every subsequence selected by a finite automaton is also normal. We extend this
to arbitrary sequences by establishing a tight quantitative relationship
between the finite-state dimension of a sequence and the finite-state
dimensions of its automatic subsequences.

</details>


<div id='cs.GT'></div>

# cs.GT [[Back]](#toc)

### [170] [On Condorcet's Jury Theorem with Abstention](https://arxiv.org/abs/2510.18062)
*Reshef Meir,Ganesh Ghalme*

Main category: cs.GT

TL;DR: 本文探讨了在投票者面临不同参与成本和可能启发式信念的背景下，Condorcet陪审团定理的适用性，并识别了导致多重稳定均衡的关键信念属性。


<details>
  <summary>Details</summary>
Motivation: 在不对称的投票环境中，研究投票者行为（如参与成本和对关键性的启发式信念）如何影响Condorcet陪审团定理的结论，尤其是在多数裁决下选举结果逼近平局的情况。

Method: 建立一个有成本投票模型，其中投票者在参与成本高于其关键性估计时会弃权。通过分析“弱消失关键性”和“强消失关键性”两种启发式信念属性对均衡的影响。

Result: “弱消失关键性”会导致多个稳定的均衡，其中选举结果接近平局。而“强消失关键性”（如标准投票演算模型）会产生一个独特的、琐碎的均衡，即只有零成本的投票者参与。当存在非琐碎均衡时，本文还刻画了陪审团定理在何种条件下成立：低于某个阈值时，多数偏好的候选人以接近于1的概率获胜；高于该阈值时，两位候选人以同等概率获胜。

Conclusion: 投票者的启发式信念属性，特别是关键性估算的消失方式，对选举均衡的性质和Condorcet陪审团定理的适用范围有重要影响。在特定条件下，即使存在参与成本和启发式信念，多数偏好仍能胜出，但在其他条件下，选举可能趋于平局。

Abstract: The well-known Condorcet Jury Theorem states that, under majority rule, the
better of two alternatives is chosen with probability approaching one as the
population grows. We study an asymmetric setting where voters face varying
participation costs and share a possibly heuristic belief about their
pivotality (ability to influence the outcome).
  In a costly voting setup where voters abstain if their participation cost is
greater than their pivotality estimate, we identify a single property of the
heuristic belief -- weakly vanishing pivotality -- that gives rise to multiple
stable equilibria in which elections are nearly tied. In contrast, strongly
vanishing pivotality (as in the standard Calculus of Voting model) yields a
unique, trivial equilibrium where only zero-cost voters participate as the
population grows. We then characterize when nontrivial equilibria satisfy a
version of the Jury Theorem: below a sharp threshold, the majority-preferred
candidate wins with probability approaching one; above it, both candidates
either win with equal probability.

</details>


### [171] [Likelihood of the Existence of Average Justified Representation](https://arxiv.org/abs/2510.18718)
*Qishen Han,Biaoshuai Tao,Lirong Xia,Chengkai Zhang,Houyu Zhou*

Main category: cs.GT

TL;DR: 本文研究了在Erdos-Renyi模型下，批准制多赢者选举中平均正当代表（AJR）存在的可能性。我们提供了当候选人数量m是常数且选民数量n趋于无穷大时，AJR委员会存在性的完整刻画。


<details>
  <summary>Details</summary>
Motivation: 现有的多赢者选举机制中，满足AJR的获胜委员会不一定总是存在，因此需要研究其存在的可能性。

Method: 本文在Erdos-Renyi模型下，通过对参数p进行分析，研究了AJR委员会存在的可能性。

Result: 研究结果表明存在两个相变点p1和p2。当p < p1或p > p2时，AJR委员会存在的概率为1-o(1)；当p1 < p < p2时，概率为o(1)；当p = p1或p = p2时，概率介于0和1之间。

Conclusion: 本文刻画了在Erdos-Renyi模型下多赢者选举中AJR委员会存在的条件，揭示了其存在的相变现象。

Abstract: We study the approval-based multi-winner election problem where $n$ voters
jointly decide a committee of $k$ winners from $m$ candidates. We focus on the
axiom \emph{average justified representation} (AJR) proposed by Fernandez,
Elkind, Lackner, Garcia, Arias-Fisteus, Basanta-Val, and Skowron (2017). AJR
postulates that every group of voters with a common preference should be
sufficiently represented in that their average satisfaction should be no less
than their Hare quota. Formally, for every group of
$\lceil\ell\cdot\frac{n}{k}\rceil$ voters with $\ell$ common approved
candidates, the average number of approved winners for this group should be at
least $\ell$. It is well-known that a winning committee satisfying AJR is not
guaranteed to exist for all multi-winner election instances. In this paper, we
study the likelihood of the existence of AJR under the Erd\H{o}s--R\'enyi
model. We consider the Erd\H{o}s--R\'enyi model parameterized by $p\in[0,1]$
that samples multi-winner election instances from the distribution where each
voter approves each candidate with probability $p$ (and the events that voters
approve candidates are independent), and we provide a clean and complete
characterization of the existence of AJR committees in the case where $m$ is a
constant and $n$ tends to infinity. We show that there are two phase transition
points $p_1$ and $p_2$ (with $p_1\leq p_2$) for the parameter $p$ such that: 1)
when $p<p_1$ or $p>p_2$, an AJR committee exists with probability $1-o(1)$, 2)
when $p_1<p<p_2$, an AJR committee exists with probability $o(1)$, and 3) when
$p=p_1$ or $p=p_2$, the probability that an AJR committee exists is bounded
away from both $0$ and $1$.

</details>
