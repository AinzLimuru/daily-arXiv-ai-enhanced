<div id=toc></div>

# Table of Contents

- [cs.CL](#cs.CL) [Total: 16]
- [cs.LG](#cs.LG) [Total: 58]
- [cs.AI](#cs.AI) [Total: 22]
- [stat.ML](#stat.ML) [Total: 2]
- [cs.MA](#cs.MA) [Total: 3]
- [cs.GT](#cs.GT) [Total: 4]
- [cs.IT](#cs.IT) [Total: 6]
- [cs.SI](#cs.SI) [Total: 3]


<div id='cs.CL'></div>

# cs.CL [[Back]](#toc)

### [1] [Segment, Embed, and Align: A Universal Recipe for Aligning Subtitles to Signing](https://arxiv.org/abs/2512.08094)
*Zifan Jiang,Youngjoon Jang,Liliane Momeni,Gül Varol,Sarah Ebling,Andrew Zisserman*

Main category: cs.CL

TL;DR: 这篇文章介绍了一种名为SEA（Segment, Embed, and Align）的字幕与连续手语视频对齐的通用方法，它比现有方法更具通用性，可以跨多种语言和领域，在多个手语数据集上取得了最先进的对齐性能。


<details>
  <summary>Details</summary>
Motivation: 目前的手语视频与字幕对齐方法通常依赖于端到端的训练，且受限于特定的语言或数据集，这限制了它们的通用性。

Method: SEA方法利用两个预训练模型：一个将视频帧序列分割成单个手语，另一个将每个手语的视频剪辑嵌入到与文本共享的潜在空间中。随后，通过轻量级的动态规划程序进行对齐。

Result: SEA在四个手语数据集上取得了最先进的对齐性能。

Conclusion: SEA方法提供了一个通用的框架，可以跨多种语言和领域进行字幕与连续手语视频的对齐，并且能够生成高质量的并行数据，从而推动手语处理的发展。

Abstract: The goal of this work is to develop a universal approach for aligning subtitles (i.e., spoken language text with corresponding timestamps) to continuous sign language videos. Prior approaches typically rely on end-to-end training tied to a specific language or dataset, which limits their generality. In contrast, our method Segment, Embed, and Align (SEA) provides a single framework that works across multiple languages and domains. SEA leverages two pretrained models: the first to segment a video frame sequence into individual signs and the second to embed the video clip of each sign into a shared latent space with text. Alignment is subsequently performed with a lightweight dynamic programming procedure that runs efficiently on CPUs within a minute, even for hour-long episodes. SEA is flexible and can adapt to a wide range of scenarios, utilizing resources from small lexicons to large continuous corpora. Experiments on four sign language datasets demonstrate state-of-the-art alignment performance, highlighting the potential of SEA to generate high-quality parallel data for advancing sign language processing. SEA's code and models are openly available.

</details>


### [2] [Universal Adversarial Suffixes Using Calibrated Gumbel-Softmax Relaxation](https://arxiv.org/abs/2512.08123)
*Sampriti Soor,Suklav Ghosh,Arijit Sur*

Main category: cs.CL

TL;DR: 该论文研究了语言模型中的通用对抗性后缀，发现这些后缀可以有效降低多个任务和模型上的准确性。


<details>
  <summary>Details</summary>
Motivation: 语言模型在零样本或少样本分类中表现脆弱，易受对抗性提示影响。现有方法通常优化特定于任务或模型的触发器，这使得结果难以比较并限制了可迁移性。

Method: 本文研究了通用的对抗性后缀：简短的 token 序列（4-10 个 token），当附加到任何输入时，会普遍降低跨任务和模型的准确性。该方法使用 Gumbel-Softmax 松弛在可微分的“软”形式中学习后缀，然后将其离散化用于推理。训练旨在最大化标签区域上的校准交叉熵，同时屏蔽黄金 token 以防止微不足道的泄漏，并使用熵正则化以避免崩溃。

Result: 在Qwen2-1.5B、Phi-1.5和TinyLlama-1.1B上对情感分析、自然语言推理、释义检测、常识问答和物理推理的实验表明，单个在一种模型上训练的后缀可以有效地迁移到其他模型，持续降低准确性和校准置信度。

Conclusion: 本文成功开发了一种通用的对抗性后缀，能够有效攻击和迁移到不同的任务和模型。这表明了语言模型在对抗性攻击下的脆弱性，并提出了未来研究的潜在方向，以增强模型的鲁棒性。

Abstract: Language models (LMs) are often used as zero-shot or few-shot classifiers by scoring label words, but they remain fragile to adversarial prompts. Prior work typically optimizes task- or model-specific triggers, making results difficult to compare and limiting transferability. We study universal adversarial suffixes: short token sequences (4-10 tokens) that, when appended to any input, broadly reduce accuracy across tasks and models. Our approach learns the suffix in a differentiable "soft" form using Gumbel-Softmax relaxation and then discretizes it for inference. Training maximizes calibrated cross-entropy on the label region while masking gold tokens to prevent trivial leakage, with entropy regularization to avoid collapse. A single suffix trained on one model transfers effectively to others, consistently lowering both accuracy and calibrated confidence. Experiments on sentiment analysis, natural language inference, paraphrase detection, commonsense QA, and physical reasoning with Qwen2-1.5B, Phi-1.5, and TinyLlama-1.1B demonstrate consistent attack effectiveness and transfer across tasks and model families.

</details>


### [3] [Universal Adversarial Suffixes for Language Models Using Reinforcement Learning with Calibrated Reward](https://arxiv.org/abs/2512.08131)
*Sampriti Soor,Suklav Ghosh,Arijit Sur*

Main category: cs.CL

TL;DR: 该论文提出了一种新的强化学习框架来生成对抗性后缀，旨在提高其在不同任务和模型中的可迁移性。


<details>
  <summary>Details</summary>
Motivation: 以往寻找对抗性后缀的方法，如梯度搜索或基于规则的方法，通常比较脆弱，且常常受限于单一任务或模型。

Method: 本研究采用强化学习框架，将后缀视为策略，并使用近端策略优化（PPO）进行训练，以冻结模型作为奖励预言机。奖励通过校准交叉熵进行塑造，以消除标签偏差并聚合不同表面形式，从而提高可迁移性。

Result: 在五个不同的NLP基准数据集（涵盖情感、自然语言推理、复述和常识推理）以及三种不同的语言模型（Qwen2-1.5B Instruct, TinyLlama-1.1B Chat和Phi-1.5）上进行了评估。结果表明，通过强化学习训练的后缀比以往同类型对抗性触发器更有效地降低了准确性，并且在任务和模型之间的迁移性更强。

Conclusion: 本研究提出的强化学习框架可以有效地生成具有高迁移性的对抗性后缀，对现有语言模型的鲁棒性提出了挑战，并为未来对抗性攻击的研究提供了新的方向。

Abstract: Language models are vulnerable to short adversarial suffixes that can reliably alter predictions. Previous works usually find such suffixes with gradient search or rule-based methods, but these are brittle and often tied to a single task or model. In this paper, a reinforcement learning framework is used where the suffix is treated as a policy and trained with Proximal Policy Optimization against a frozen model as a reward oracle. Rewards are shaped using calibrated cross-entropy, removing label bias and aggregating across surface forms to improve transferability. The proposed method is evaluated on five diverse NLP benchmark datasets, covering sentiment, natural language inference, paraphrase, and commonsense reasoning, using three distinct language models: Qwen2-1.5B Instruct, TinyLlama-1.1B Chat, and Phi-1.5. Results show that RL-trained suffixes consistently degrade accuracy and transfer more effectively across tasks and models than previous adversarial triggers of similar genres.

</details>


### [4] [ClinicalTrialsHub: Bridging Registries and Literature for Comprehensive Clinical Trial Access](https://arxiv.org/abs/2512.08193)
*Jiwoo Park,Ruoqi Liu,Avani Jagdale,Andrew Srisuwananukorn,Jing Zhao,Lang Li,Ping Zhang,Sachin Kumar*

Main category: cs.CL

TL;DR: ClinicalTrialsHub是一个交互式、以搜索为中心的平台，它整合了ClinicalTrials.gov的所有数据，并通过自动从PubMed研究文章中提取和构建试验相关信息来增强数据。


<details>
  <summary>Details</summary>
Motivation: 增加对结构化临床试验数据的访问，使患者、临床医生、研究人员和政策制定者更容易获取信息，从而推动循证医学发展。

Method: ClinicalTrialsHub使用大型语言模型（如GPT-5.1和Gemini-3-Pro）来增强可访问性。该平台自动解析全文研究文章以提取结构化的试验信息，将用户查询转化为结构化的数据库搜索，并提供一个归因问答系统，生成与特定来源语句链接的基于证据的答案。

Result: 与单独依赖ClinicalTrials.gov相比，ClinicalTrialsHub有效地将结构化临床试验数据的访问量增加了83.8%。

Conclusion: ClinicalTrialsHub通过整合和增强临床试验数据，显著提高了数据可及性，并为患者、临床医生、研究人员和政策制定者提供了更便捷的信息获取途径，促进了循证医学的发展。

Abstract: We present ClinicalTrialsHub, an interactive search-focused platform that consolidates all data from ClinicalTrials.gov and augments it by automatically extracting and structuring trial-relevant information from PubMed research articles. Our system effectively increases access to structured clinical trial data by 83.8% compared to relying on ClinicalTrials.gov alone, with potential to make access easier for patients, clinicians, researchers, and policymakers, advancing evidence-based medicine. ClinicalTrialsHub uses large language models such as GPT-5.1 and Gemini-3-Pro to enhance accessibility. The platform automatically parses full-text research articles to extract structured trial information, translates user queries into structured database searches, and provides an attributed question-answering system that generates evidence-grounded answers linked to specific source sentences. We demonstrate its utility through a user study involving clinicians, clinical researchers, and PhD students of pharmaceutical sciences and nursing, and a systematic automatic evaluation of its information extraction and question answering capabilities.

</details>


### [5] [Curriculum Guided Massive Multi Agent System Solving For Robust Long Horizon Tasks](https://arxiv.org/abs/2512.08545)
*Indrajit Kar,Kalathur Chenchu Kishore Kumar*

Main category: cs.CL

TL;DR: 该论文介绍了一种新颖的分层多智能体架构，旨在解决大型语言模型和多智能体系统在处理长周期推理任务时面临的挑战，并通过逐步学习和自适应训练策略提高了效率和可靠性。


<details>
  <summary>Details</summary>
Motivation: 大型语言模型和多智能体系统在分解复杂任务方面表现出潜力，但在长周期推理任务和计算成本不断升级方面仍面临挑战。

Method: 本文提出了一种分层的多智能体架构，将推理分布在64x64的轻量级智能体网格中，并由选择性预言机支持。该系统采用空间课程，逐步扩展网格的操作区域，确保智能体在处理更困难的外围任务之前，先掌握较简单的中心任务。为了提高可靠性，系统将负对数似然（NLL）作为置信度度量，使课程能够优先处理智能体既准确又校准良好的区域。一个Thompson抽样课程管理器根据能力和NLL驱动的奖励信号自适应地选择训练区域。

Result: 通过在空间接地汉诺塔基准上评估该方法，结果表明该方法提高了稳定性，减少了预言机的使用，并从分布式智能体合作中获得了更强的长程推理能力。

Conclusion: 该研究表明，所提出的分层多智能体架构通过结合空间课程、NLL置信度度量和Thompson抽样课程管理器，能够有效解决长周期推理任务中的挑战，提高多智能体系统的效率和可靠性。

Abstract: Large Language Models and multi-agent systems have shown promise in decomposing complex tasks, yet they struggle with long-horizon reasoning tasks and escalating computation cost. This work introduces a hierarchical multi-agent architecture that distributes reasoning across a 64*64 grid of lightweight agents, supported by a selective oracle. A spatial curriculum progressively expands the operational region of the grid, ensuring that agents master easier central tasks before tackling harder peripheral ones. To improve reliability, the system integrates Negative Log-Likelihood as a measure of confidence, allowing the curriculum to prioritize regions where agents are both accurate and well calibrated. A Thompson Sampling curriculum manager adaptively chooses training zones based on competence and NLL-driven reward signals. We evaluate the approach on a spatially grounded Tower of Hanoi benchmark, which mirrors the long-horizon structure of many robotic manipulation and planning tasks. Results demonstrate improved stability, reduced oracle usage, and stronger long-range reasoning from distributed agent cooperation.

</details>


### [6] [Are generative AI text annotations systematically biased?](https://arxiv.org/abs/2512.08404)
*Sjoerd B. Stolwijk,Mark Boukes,Damian Trilling*

Main category: cs.CL

TL;DR: 本文分析了GLLM（大型语言模型）在概念复制Boukes（2024）手动标注时存在的偏见，发现GLLM标注与人工标注存在差异，并导致下游结果不同。


<details>
  <summary>Details</summary>
Motivation: 在概念复制Boukes（2024）手动标注时，探究GLLM标注中存在的偏见。

Method: 使用不同的GLLM（Llama3.1:8b, Llama3.3:70b, GPT4o, Qwen2.5:72b）和五种不同的提示词，分析了五个概念（政治内容、交互性、理性、不文明行为和意识形态）。

Result: GLLM在F1分数方面表现尚可，但在流行度上与人工标注不同，导致下游结果存在实质性差异，并表现出系统性偏见，即GLLM之间的重叠度高于GLLM与人工标注的重叠度。

Conclusion: F1分数的差异未能解释偏见的程度。

Abstract: This paper investigates bias in GLLM annotations by conceptually replicating manual annotations of Boukes (2024). Using various GLLMs (Llama3.1:8b, Llama3.3:70b, GPT4o, Qwen2.5:72b) in combination with five different prompts for five concepts (political content, interactivity, rationality, incivility, and ideology). We find GLLMs perform adequate in terms of F1 scores, but differ from manual annotations in terms of prevalence, yield substantively different downstream results, and display systematic bias in that they overlap more with each other than with manual annotations. Differences in F1 scores fail to account for the degree of bias.

</details>


### [7] [What Triggers my Model? Contrastive Explanations Inform Gender Choices by Translation Models](https://arxiv.org/abs/2512.08440)
*Janiça Hackenbuchner,Arda Tezcan,Joke Daems*

Main category: cs.CL

TL;DR: 研究了机器翻译和大型语言模型中性别偏见的来源，通过分析输入token对目标语言中性别屈折选择的影响，并将其与人类感知进行比较。


<details>
  <summary>Details</summary>
Motivation: 研究现有机器翻译和大型语言模型中性别偏见的来源。

Method: 通过对比解释和显著性归因，分析输入token对目标语言中性别屈折选择的影响，并将显著性词语与人类的性别感知进行比较。

Result: 人类感知和模型归因之间存在显著重叠。

Conclusion: 理解模型在性别方面的翻译决策对于缓解性别偏见至关重要。

Abstract: Interpretability can be implemented as a means to understand decisions taken by (black box) models, such as machine translation (MT) or large language models (LLMs). Yet, research in this area has been limited in relation to a manifested problem in these models: gender bias. With this research, we aim to move away from simply measuring bias to exploring its origins. Working with gender-ambiguous natural source data, this study examines which context, in the form of input tokens in the source sentence, influences (or triggers) the translation model choice of a certain gender inflection in the target language. To analyse this, we use contrastive explanations and compute saliency attribution. We first address the challenge of a lacking scoring threshold and specifically examine different attribution levels of source words on the model gender decisions in the translation. We compare salient source words with human perceptions of gender and demonstrate a noticeable overlap between human perceptions and model attribution. Additionally, we provide a linguistic analysis of salient words. Our work showcases the relevance of understanding model translation decisions in terms of gender, how this compares to human decisions and that this information should be leveraged to mitigate gender bias.

</details>


### [8] [Soft Inductive Bias Approach via Explicit Reasoning Perspectives in Inappropriate Utterance Detection Using Large Language Models](https://arxiv.org/abs/2512.08480)
*Ju-Young Kim,Ji-Hong Park,Se-Yeon Lee,Sujin Park,Gun-Woo Kim*

Main category: cs.CL

TL;DR: 本文探讨了在线环境中不当言论升级为辱骂甚至犯罪行为的问题，提出了一种软归纳偏差方法，通过明确推理视角来指导推理过程，从而提高不当言论检测的准确性。


<details>
  <summary>Details</summary>
Motivation: 在匿名在线游戏和社区中，不加约束的不当言论经常升级为言语侮辱甚至犯罪行为，引发了社会关注。因此，需要研究检测会话文本中不当言论的技术，以帮助构建更安全的交流环境。

Method: 本文提出了一种软归纳偏差方法，该方法明确定义了推理视角以指导推理过程，从而促进理性决策并防止推理过程中可能出现的错误。研究者使用所提出的方法对韩国大型语言模型进行微调，并进行定量性能比较和定性评估。

Result: 实验结果表明，Kanana-1.5模型的平均准确率达到87.0046，比标准监督学习提高了约3.89%。

Conclusion: 所提出的方法通过限制推理视角，实现了更精确和一致的判断，证明了其在不当言论检测方面的有效性，超越了大型语言模型简单的知识模仿。

Abstract: Recent incidents in certain online games and communities, where anonymity is guaranteed, show that unchecked inappropriate remarks frequently escalate into verbal abuse and even criminal behavior, raising significant social concerns. Consequently, there is a growing need for research on techniques that can detect inappropriate utterances within conversational texts to help build a safer communication environment. Although large-scale language models trained on Korean corpora and chain-of-thought reasoning have recently gained attention, research applying these approaches to inappropriate utterance detection remains limited. In this study, we propose a soft inductive bias approach that explicitly defines reasoning perspectives to guide the inference process, thereby promoting rational decision-making and preventing errors that may arise during reasoning. We fine-tune a Korean large language model using the proposed method and conduct both quantitative performance comparisons and qualitative evaluations across different training strategies. Experimental results show that the Kanana-1.5 model achieves an average accuracy of 87.0046, improving by approximately 3.89 percent over standard supervised learning. These findings indicate that the proposed method goes beyond simple knowledge imitation by large language models and enables more precise and consistent judgments through constrained reasoning perspectives, demonstrating its effectiveness for inappropriate utterance detection.

</details>


### [9] [HealthcareNLP: where are we and what is next?](https://arxiv.org/abs/2512.08617)
*Lifeng Han,Paul Rayson,Suzan Verberne,Andrew Moore,Goran Nenadic*

Main category: cs.CL

TL;DR: 这篇教程关注医疗领域的自然语言处理应用，以及该领域已经取得的成就和未来的挑战。


<details>
  <summary>Details</summary>
Motivation: 现有回顾未能覆盖重要任务（如合成数据生成和可解释临床NLP）或方法（如检索增强生成和LLM与KG的神经符号集成）。

Method: 本教程通过三层等级结构提供以患者和资源为导向的HealthcareNLP重点子领域的介绍性概述：数据/资源层、NLP评估层和患者层。

Result: 教程将涵盖数据/资源层（标注指南、伦理批准等）、NLP评估层（NER、RE、情感分析等）和患者层（PPIE、健康素养等），并包含实践环节。

Conclusion: 本教程旨在向对医疗健康NLP感兴趣的NLP从业者、研究人员和学生提供全面的介绍，并强调不需要先验知识。

Abstract: This proposed tutorial focuses on Healthcare Domain Applications of NLP, what we have achieved around HealthcareNLP, and the challenges that lie ahead for the future. Existing reviews in this domain either overlook some important tasks, such as synthetic data generation for addressing privacy concerns, or explainable clinical NLP for improved integration and implementation, or fail to mention important methodologies, including retrieval augmented generation and the neural symbolic integration of LLMs and KGs. In light of this, the goal of this tutorial is to provide an introductory overview of the most important sub-areas of a patient- and resource-oriented HealthcareNLP, with three layers of hierarchy: data/resource layer: annotation guidelines, ethical approvals, governance, synthetic data; NLP-Eval layer: NLP tasks such as NER, RE, sentiment analysis, and linking/coding with categorised methods, leading to explainable HealthAI; patients layer: Patient Public Involvement and Engagement (PPIE), health literacy, translation, simplification, and summarisation (also NLP tasks), and shared decision-making support. A hands-on session will be included in the tutorial for the audience to use HealthcareNLP applications. The target audience includes NLP practitioners in the healthcare application domain, NLP researchers who are interested in domain applications, healthcare researchers, and students from NLP fields. The type of tutorial is "Introductory to CL/NLP topics (HealthcareNLP)" and the audience does not need prior knowledge to attend this. Tutorial materials: https://github.com/4dpicture/HealthNLP

</details>


### [10] [QSTN: A Modular Framework for Robust Questionnaire Inference with Large Language Models](https://arxiv.org/abs/2512.08646)
*Maximilian Kreutner,Jens Rupprecht,Georg Ahnert,Ahmed Salem,Markus Strohmaier*

Main category: cs.CL

TL;DR: QSTN是一个用于生成问卷式提示响应的开源Python框架，支持LLM的计算机内调查和注释任务。


<details>
  <summary>Details</summary>
Motivation: 作者引入QSTN框架以支持LLM的计算机内调查和注释任务，并对问卷呈现、提示扰动和响应生成方法进行鲁棒评估。

Method: QSTN框架允许对问卷呈现、提示扰动和响应生成方法进行评估。研究人员可以通过无代码用户界面设置实验。

Result: 超过4000万次调查响应的广泛评估表明，问题结构和响应生成方法对生成的调查响应与人类答案的一致性有显著影响，并且只需付出很少的计算成本即可获得。

Conclusion: QSTN框架支持LLM研究的可复现性和可靠性，它能以较低的成本生成与人类答案高度一致的调查响应。

Abstract: We introduce QSTN, an open-source Python framework for systematically generating responses from questionnaire-style prompts to support in-silico surveys and annotation tasks with large language models (LLMs). QSTN enables robust evaluation of questionnaire presentation, prompt perturbations, and response generation methods. Our extensive evaluation ($>40 $ million survey responses) shows that question structure and response generation methods have a significant impact on the alignment of generated survey responses with human answers, and can be obtained for a fraction of the compute cost. In addition, we offer a no-code user interface that allows researchers to set up robust experiments with LLMs without coding knowledge. We hope that QSTN will support the reproducibility and reliability of LLM-based research in the future.

</details>


### [11] [An Agentic AI System for Multi-Framework Communication Coding](https://arxiv.org/abs/2512.08659)
*Bohao Yang,Rui Yang,Joshua M. Biro,Haoyuan Wang,Jessica L. Handley,Brianna Richardson,Sophia Bessias,Nicoleta Economou-Zavlanos,Armando D. Bedoya,Monica Agrawal,Michael M. Zavlanos,Anand Chowdhury,Raj M. Ratwani,Kai Sun,Kathryn I. Pollak,Michael J. Pencina,Chuan Hong*

Main category: cs.CL

TL;DR: 该研究开发了一个名为 MOSAIC 的多框架结构化 Agentic AI 系统，用于临床沟通分析，并在风湿病学和妇产科的患者-医生对话中实现了 0.928 的 F1 分数，优于基线模型。


<details>
  <summary>Details</summary>
Motivation: 临床沟通对患者预后至关重要，但大规模的人工标注费时费力且难以统一。现有的基于大型语言模型的方法通常依赖于单任务模型，缺乏适应性、可解释性和可靠性，尤其是在不同的沟通框架和临床领域中应用时。

Method: 该研究开发了一个名为 MOSAIC 的多框架结构化 Agentic AI 系统，该系统基于 LangGraph 架构，协调了四个核心 Agent：
1. 规划 Agent：负责法典选择和工作流规划。
2. 更新 Agent：用于维护最新的检索数据库。
3. 标注 Agent：通过动态少样本提示应用法典引导的检索增强生成（RAG）。
4. 验证 Agent：提供一致性检查和反馈。

MOSAIC 使用 26 份金标准标注文本进行训练，并使用 50 份标注文本进行测试，涵盖风湿病学和妇产科领域。研究将 MOSAIC 的输出与经过训练的人工编码员创建的金标准标注进行了比较。

Result: 在测试集上，MOSAIC 取得了 0.928 的总体 F1 分数。在风湿病学子集中的表现最高（F1 = 0.962），并且在患者行为方面（例如，患者提问、表达偏好或表现出自信）表现最强。消融实验表明，MOSAIC 的性能优于基线模型。

Conclusion: MOSAIC 系统能够高效、准确地分析临床沟通，尤其在患者行为分析方面表现突出，并且在不同临床领域具有良好的适应性。该系统解决了传统人工标注的局限性，并为临床沟通研究提供了一个可靠、可扩展的工具。

Abstract: Clinical communication is central to patient outcomes, yet large-scale human annotation of patient-provider conversation remains labor-intensive, inconsistent, and difficult to scale. Existing approaches based on large language models typically rely on single-task models that lack adaptability, interpretability, and reliability, especially when applied across various communication frameworks and clinical domains. In this study, we developed a Multi-framework Structured Agentic AI system for Clinical Communication (MOSAIC), built on a LangGraph-based architecture that orchestrates four core agents, including a Plan Agent for codebook selection and workflow planning, an Update Agent for maintaining up-to-date retrieval databases, a set of Annotation Agents that applies codebook-guided retrieval-augmented generation (RAG) with dynamic few-shot prompting, and a Verification Agent that provides consistency checks and feedback. To evaluate performance, we compared MOSAIC outputs against gold-standard annotations created by trained human coders. We developed and evaluated MOSAIC using 26 gold standard annotated transcripts for training and 50 transcripts for testing, spanning rheumatology and OB/GYN domains. On the test set, MOSAIC achieved an overall F1 score of 0.928. Performance was highest in the Rheumatology subset (F1 = 0.962) and strongest for Patient Behavior (e.g., patients asking questions, expressing preferences, or showing assertiveness). Ablations revealed that MOSAIC outperforms baseline benchmarking.

</details>


### [12] [Automatic Essay Scoring and Feedback Generation in Basque Language Learning](https://arxiv.org/abs/2512.08713)
*Ekhi Azurmendi,Xabier Arregi,Oier Lopez de Lacalle*

Main category: cs.CL

TL;DR: 该论文提出了一个针对巴斯克语的自动论文评分和反馈生成数据集，并展示了微调后的开源模型在该任务上超越了SOTA闭源系统。


<details>
  <summary>Details</summary>
Motivation: 目前缺乏针对巴斯克语的公开自动论文评分（AES）和反馈生成数据集，尤其是在CEFR C1水平。

Method: 1. 构建了一个包含3200篇巴斯克语作文的数据集，并由专家标注了具体评分和详细反馈。
2. 使用RoBERTa-EusCrawl和Latxa 8B/70B等开源模型进行微调，以实现评分和解释生成。
3. 提出了一种新的反馈生成评估方法，结合了自动一致性指标和专家验证。

Result: 1. 编码器模型在AES方面表现出高可靠性。
2. Latxa模型（通过SFT微调）显著优于GPT-5和Claude Sonnet 4.5等SOTA闭源系统，在评分一致性和反馈质量方面表现更佳。
3. 微调后的Latxa模型能够生成符合标准、具有教学意义的反馈，并能识别比专有模型更广泛的错误类型。

Conclusion: 该研究为巴斯克语等低资源语言的NLP研究提供了透明、可复现且具有教育意义的基础资源和基准则标杆。

Abstract: This paper introduces the first publicly available dataset for Automatic Essay Scoring (AES) and feedback generation in Basque, targeting the CEFR C1 proficiency level. The dataset comprises 3,200 essays from HABE, each annotated by expert evaluators with criterion specific scores covering correctness, richness, coherence, cohesion, and task alignment enriched with detailed feedback and error examples. We fine-tune open-source models, including RoBERTa-EusCrawl and Latxa 8B/70B, for both scoring and explanation generation. Our experiments show that encoder models remain highly reliable for AES, while supervised fine-tuning (SFT) of Latxa significantly enhances performance, surpassing state-of-the-art (SoTA) closed-source systems such as GPT-5 and Claude Sonnet 4.5 in scoring consistency and feedback quality. We also propose a novel evaluation methodology for assessing feedback generation, combining automatic consistency metrics with expert-based validation of extracted learner errors. Results demonstrate that the fine-tuned Latxa model produces criterion-aligned, pedagogically meaningful feedback and identifies a wider range of error types than proprietary models. This resource and benchmark establish a foundation for transparent, reproducible, and educationally grounded NLP research in low-resource languages such as Basque.

</details>


### [13] [Fluent Alignment with Disfluent Judges: Post-training for Lower-resource Languages](https://arxiv.org/abs/2512.08777)
*David Samuel,Lilja Øvrelid,Erik Velldal,Andrey Kutuzov*

Main category: cs.CL

TL;DR: 本文提出了一种针对低资源语言的后训练方法，该方法在奖励模型不流畅的情况下也能保持语言模型的流畅性。


<details>
  <summary>Details</summary>
Motivation: 以前的工作主要针对英语和汉语模型，但低资源语言缺乏母语人士编写的数据集和能够生成流畅合成数据的语言模型。因此，本文致力于在不使用目标语言的指令微调数据的情况下，开发一种流畅的偏好对齐语言模型。

Method: 本文使用了一种“在策略上（on-policy）”的训练方法，并将其与两种常见方法进行了比较：在机器翻译数据上进行监督微调和多语言微调。以挪威语Bokmål为例进行了案例研究。

Result: 结果表明，“在策略上（on-policy）”方法是至关重要的，并且在不依赖任何难以获取的数据的情况下优于其他替代方法。

Conclusion: 所提出的“在策略上（on-policy）”训练方法能够有效提升低资源语言模型的流畅性，无需指令微调数据，并且优于传统的监督微调和多语言微调方法。

Abstract: We propose a post-training method for lower-resource languages that preserves fluency of language models even when aligned by disfluent reward models. Preference-optimization is now a well-researched topic, but previous work has mostly addressed models for English and Chinese. Lower-resource languages lack both datasets written by native speakers and language models capable of generating fluent synthetic data. Thus, in this work, we focus on developing a fluent preference-aligned language model without any instruction-tuning data in the target language. Our approach uses an on-policy training method, which we compare with two common approaches: supervised finetuning on machine-translated data and multilingual finetuning. We conduct a case study on Norwegian Bokmål and evaluate fluency through native-speaker assessments. The results show that the on-policy aspect is crucial and outperforms the alternatives without relying on any hard-to-obtain data.

</details>


### [14] [A Systematic Evaluation of Preference Aggregation in Federated RLHF for Pluralistic Alignment of LLMs](https://arxiv.org/abs/2512.08786)
*Mahmoud Srewa,Tianyu Zhao,Salma Elmalaki*

Main category: cs.CL

TL;DR: 本文提出了一种联邦学习环境下对齐大型语言模型与多样化人类偏好的新方法，该方法通过自适应调整偏好权重，在问答任务中实现了更好的公平性和对齐效果。


<details>
  <summary>Details</summary>
Motivation: 在联邦学习环境中，标准方法难以充分体现多样化的人类偏好，导致大型语言模型（LLMs）的对齐面临挑战。

Method: 本文引入了一个全面的评估框架，系统地评估了在联邦学习环境中使用不同聚合策略时，人类偏好对齐质量和公平性之间的权衡。具体来说，研究者评估了标准的奖励聚合技术（最小、最大和平均），并提出了一种新颖的自适应方案，该方案根据群体的历史对齐表现动态调整偏好权重。

Result: 在基于PPO的RLHF问答任务实验中，本文提出的自适应方法在保持竞争性对齐得分的同时，始终实现了卓越的公平性。

Conclusion: 本文为评估LLM在不同人群中的行为提供了一种稳健的方法，并为开发真正多元化和公平对齐的模型提供了一个实用的解决方案。

Abstract: This paper addresses the challenge of aligning large language models (LLMs) with diverse human preferences within federated learning (FL) environments, where standard methods often fail to adequately represent diverse viewpoints. We introduce a comprehensive evaluation framework that systematically assesses the trade-off between alignment quality and fairness when using different aggregation strategies for human preferences. In our federated setting, each group locally evaluates rollouts and produces reward signals, and the server aggregates these group-level rewards without accessing any raw data. Specifically, we evaluate standard reward aggregation techniques (min, max, and average) and introduce a novel adaptive scheme that dynamically adjusts preference weights based on a group's historical alignment performance. Our experiments on question-answering (Q/A) tasks using a PPO-based RLHF pipeline demonstrate that our adaptive approach consistently achieves superior fairness while maintaining competitive alignment scores. This work offers a robust methodology for evaluating LLM behavior across diverse populations and provides a practical solution for developing truly pluralistic and fairly aligned models.

</details>


### [15] [Do Depth-Grown Models Overcome the Curse of Depth? An In-Depth Analysis](https://arxiv.org/abs/2512.08819)
*Ferdinand Kapl,Emmanouil Angelis,Tobias Höppe,Kaitlin Maile,Johannes von Oswald,Nino Scherrer,Stefan Bauer*

Main category: cs.CL

TL;DR: 这篇文章探讨了通过渐进式地增加Transformer模型深度来提高性能和效率。


<details>
  <summary>Details</summary>
Motivation: MIDAS表明，在训练过程中逐步增加Transformer的深度可以降低训练成本并提高推理性能，但其内在机制尚不清楚。

Method: 通过深度分析，作者证明了通过逐步中间堆叠进行的增长可以更有效地利用模型深度，改变残差流结构，并促进可置换计算块的形成。

Result: 逐步增长的模型深度可以形成独特的计算电路，并克服标准非增长模型中有限的深度利用。作者还提出了一种轻量级的MIDAS修改方案，可以进一步改善下游推理基准。

Conclusion: 本文揭示了逐步增加模型深度如何形成独特的计算电路，并克服了标准非增长模型中深度利用率有限的问题。

Abstract: Gradually growing the depth of Transformers during training can not only reduce training cost but also lead to improved reasoning performance, as shown by MIDAS (Saunshi et al., 2024). Thus far, however, a mechanistic understanding of these gains has been missing. In this work, we establish a connection to recent work showing that layers in the second half of non-grown, pre-layernorm Transformers contribute much less to the final output distribution than those in the first half - also known as the Curse of Depth (Sun et al., 2025, Csordás et al., 2025). Using depth-wise analyses, we demonstrate that growth via gradual middle stacking yields more effective utilization of model depth, alters the residual stream structure, and facilitates the formation of permutable computational blocks. In addition, we propose a lightweight modification of MIDAS that yields further improvements in downstream reasoning benchmarks. Overall, this work highlights how the gradual growth of model depth can lead to the formation of distinct computational circuits and overcome the limited depth utilization seen in standard non-grown models.

</details>


### [16] [Toward Faithful Retrieval-Augmented Generation with Sparse Autoencoders](https://arxiv.org/abs/2512.08892)
*Guangzhi Xiong,Zhenghao He,Bohan Liu,Sanchit Sinha,Aidong Zhang*

Main category: cs.CL

TL;DR: RAGLens是一种轻量级的幻觉检测器，它利用大型语言模型的内部表示来准确地标记不忠实的检索增强生成（RAG）输出。RAGLens的检测性能优于现有方法，并能提供可解释的决策原理。


<details>
  <summary>Details</summary>
Motivation: RAG通过使用检索到的证据来提高大型语言模型的准确性，但仍然存在忠实度问题，即生成的内容与提供的来源相矛盾或超出范围。现有的幻觉检测方法存在需要大量标注数据、高推理成本或准确性有限的问题。

Method: 该研究利用机械可解释性方面的最新进展，使用稀疏自动编码器（SAE）来解耦内部激活，成功识别了RAG幻觉期间特异性触发的特征。在此基础上，通过信息特征选择和加性特征建模的系统流程，引入了RAGLens。

Result: RAGLens在幻觉检测方面取得了优于现有方法的性能，并能提供可解释的决策理由。

Conclusion: RAGLens不仅实现了卓越的检测性能，而且为其决策提供了可解释的原理，从而能够有效进行事后补救。该研究还深入了解了LLM中幻觉相关信号的分布。

Abstract: Retrieval-Augmented Generation (RAG) improves the factuality of large language models (LLMs) by grounding outputs in retrieved evidence, but faithfulness failures, where generations contradict or extend beyond the provided sources, remain a critical challenge. Existing hallucination detection methods for RAG often rely either on large-scale detector training, which requires substantial annotated data, or on querying external LLM judges, which leads to high inference costs. Although some approaches attempt to leverage internal representations of LLMs for hallucination detection, their accuracy remains limited. Motivated by recent advances in mechanistic interpretability, we employ sparse autoencoders (SAEs) to disentangle internal activations, successfully identifying features that are specifically triggered during RAG hallucinations. Building on a systematic pipeline of information-based feature selection and additive feature modeling, we introduce RAGLens, a lightweight hallucination detector that accurately flags unfaithful RAG outputs using LLM internal representations. RAGLens not only achieves superior detection performance compared to existing methods, but also provides interpretable rationales for its decisions, enabling effective post-hoc mitigation of unfaithful RAG. Finally, we justify our design choices and reveal new insights into the distribution of hallucination-related signals within LLMs. The code is available at https://github.com/Teddy-XiongGZ/RAGLens.

</details>


<div id='cs.LG'></div>

# cs.LG [[Back]](#toc)

### [17] [ThreadWeaver: Adaptive Threading for Efficient Parallel Reasoning in Language Models](https://arxiv.org/abs/2512.07843)
*Long Lian,Sida Wang,Felix Juefei-Xu,Tsu-Jui Fu,Xiuyu Li,Adam Yala,Trevor Darrell,Alane Suhr,Yuandong Tian,Xi Victoria Lin*

Main category: cs.LG

TL;DR: ThreadWeaver是一个用于自适应并行推理的框架，它在准确性与流行的顺序推理模型相当的情况下，显著降低了推理延迟。


<details>
  <summary>Details</summary>
Motivation: 大型语言模型（LLMs）的推理时间计算能力的扩展使其实现了强大的推理性能，但固有的顺序解码导致了显著的延迟，尤其是在复杂任务上。现有的自适应并行推理方法要么受限于监督行为克隆，要么与广泛使用的顺序长思维链（CoT）基线相比，准确性显著下降。此外，许多方法需要定制的推理引擎，这使得部署复杂化。

Method: ThreadWeaver achieves its performance through three key innovations: 1) a two-stage parallel trajectory generator that produces large-scale, high-quality CoT data with parallel annotations for supervised fine-tuning; 2) a trie-based training-inference co-design that enables parallel reasoning on any off-the-shelf autoregressive inference engine without modifying position embeddings or KV caches; and 3) a parallelization-aware reinforcement learning framework that teaches the model to balance accuracy with effective parallelization.

Result: 在六个具有挑战性的数学推理基准测试中，ThreadWeaver在Qwen3-8B模型上的准确性与最先进的顺序推理模型相当（平均71.9%，AIME24上为79.9%），同时在token延迟方面提供了高达1.53倍的平均加速，建立了准确性和效率之间的新帕累托前沿。

Conclusion: ThreadWeaver在保持与先进的顺序推理模型相当的准确性的前提下，显著降低了推理延迟，有效地解决了现有并行推理方法的局限性。

Abstract: Scaling inference-time computation has enabled Large Language Models (LLMs) to achieve strong reasoning performance, but inherently sequential decoding leads to substantial latency, especially on complex tasks. Recent work on adaptive parallel reasoning aims to improve inference efficiency by decomposing the problem-solving process into concurrent reasoning threads when beneficial. However, existing methods on realistic tasks are either limited to supervised behavior cloning or exhibit significant accuracy drops compared to widely-used sequential long chain-of-thought (CoT) baselines. Moreover, many require customized inference engines, complicating deployment. We introduce ThreadWeaver, a framework for adaptive parallel reasoning that achieves accuracy on par with popular sequential reasoning models of comparable size while significantly reducing inference latency. ThreadWeaver's performance stems from three key innovations: 1) a two-stage parallel trajectory generator that produces large-scale, high-quality CoT data with parallel annotations for supervised fine-tuning; 2) a trie-based training-inference co-design that enables parallel reasoning on any off-the-shelf autoregressive inference engine without modifying position embeddings or KV caches; and 3) a parallelization-aware reinforcement learning framework that teaches the model to balance accuracy with effective parallelization. Across six challenging mathematical reasoning benchmarks, ThreadWeaver trained atop Qwen3-8B achieves accuracy comparable to cutting-edge sequential reasoning models (71.9% on average and 79.9% on AIME24) while delivering up to 1.53x average speedup in token latency, establishing a new Pareto frontier between accuracy and efficiency.

</details>


### [18] [RaX-Crash: A Resource Efficient and Explainable Small Model Pipeline with an Application to City Scale Injury Severity Prediction](https://arxiv.org/abs/2512.07848)
*Di Zhu,Chen Xie,Ziwei Wang,Haoyun Zhang*

Main category: cs.LG

TL;DR: RaX-Crash是一个资源高效且可解释的小型模型，用于预测纽约市机动车碰撞数据集中的结构化伤害严重程度。


<details>
  <summary>Details</summary>
Motivation: 纽约市每年发生超过十万起机动车碰撞，造成严重的伤害和公共卫生负担。

Method: RaX-Crash整合了三个关联表，构建了统一的特征模式，并训练了紧凑的基于树的集成模型（随机森林和XGBoost），以及与本地部署的小型语言模型（SLM）进行比较。

Result: XGBoost和随机森林在时间上保留的测试集上分别达到了0.7828和0.7794的准确率，明显优于SLM（0.594和0.496）。类别不平衡分析表明，简单的类别加权可以通过适度的准确率权衡来提高致命召回率。

Conclusion: 可解释的小型模型集成仍然是城市规模伤害分析的强大基线，而将表格预测器与SLM生成的叙述相结合的混合管道可以在不牺牲可扩展性的情况下改善沟通。

Abstract: New York City reports over one hundred thousand motor vehicle collisions each year, creating substantial injury and public health burden. We present RaX-Crash, a resource efficient and explainable small model pipeline for structured injury severity prediction on the official NYC Motor Vehicle Collisions dataset. RaX-Crash integrates three linked tables with tens of millions of records, builds a unified feature schema in partitioned storage, and trains compact tree based ensembles (Random Forest and XGBoost) on engineered tabular features, which are compared against locally deployed small language models (SLMs) prompted with textual summaries. On a temporally held out test set, XGBoost and Random Forest achieve accuracies of 0.7828 and 0.7794, clearly outperforming SLMs (0.594 and 0.496); class imbalance analysis shows that simple class weighting improves fatal recall with modest accuracy trade offs, and SHAP attribution highlights human vulnerability factors, timing, and location as dominant drivers of predicted severity. Overall, RaX-Crash indicates that interpretable small model ensembles remain strong baselines for city scale injury analytics, while hybrid pipelines that pair tabular predictors with SLM generated narratives improve communication without sacrificing scalability.

</details>


### [19] [Softly Symbolifying Kolmogorov-Arnold Networks](https://arxiv.org/abs/2512.07875)
*James Bagrow,Josh Bongard*

Main category: cs.LG

TL;DR: S2KAN 是一种可微分、端到端优化的模型，它通过在训练中直接集成符号基元并结合 MDL 目标来实现激活的稀疏化，从而在保持高精度的同时提高了模型的可解释性。


<details>
  <summary>Details</summary>
Motivation: 传统的 KAN 模型在学习到的激活函数中存在符号保真度不足的问题，导致难以解释。

Method: S2KAN 通过引入一个包含符号和密集项的字典，并使用可学习的门控机制来稀疏化激活函数的表示。这种稀疏化是可微分的，并由最小描述长度（MDL）目标引导，实现端到端优化。

Result: S2KAN 在符号基准、动力系统预测和真实世界预测任务上取得了与现有方法相当或更优的准确性，同时模型尺寸显著减小。模型还表现出自发的稀疏化能力，即使没有额外的正则化压力。

Conclusion: S2KAN 解决了 KAN 模型在可解释性方面的挑战，通过集成符号原语和引导稀疏化，S2KAN 实现了高精度和更好的可解释性，并且具有更小的模型尺寸，使得模型在各种任务中都表现出色。

Abstract: Kolmogorov-Arnold Networks (KANs) offer a promising path toward interpretable machine learning: their learnable activations can be studied individually, while collectively fitting complex data accurately. In practice, however, trained activations often lack symbolic fidelity, learning pathological decompositions with no meaningful correspondence to interpretable forms. We propose Softly Symbolified Kolmogorov-Arnold Networks (S2KAN), which integrate symbolic primitives directly into training. Each activation draws from a dictionary of symbolic and dense terms, with learnable gates that sparsify the representation. Crucially, this sparsification is differentiable, enabling end-to-end optimization, and is guided by a principled Minimum Description Length objective. When symbolic terms suffice, S2KAN discovers interpretable forms; when they do not, it gracefully degrades to dense splines. We demonstrate competitive or superior accuracy with substantially smaller models across symbolic benchmarks, dynamical systems forecasting, and real-world prediction tasks, and observe evidence of emergent self-sparsification even without regularization pressure.

</details>


### [20] [GPU Memory Prediction for Multimodal Model Training](https://arxiv.org/abs/2512.07853)
*Jinwoo Jeong,Minchul Kang,Younghun Go,Changyong Shin,Hyunho Lee,Junho Yoon,Gyeongsik Yang,Chuck Yoo*

Main category: cs.LG

TL;DR: 该论文提出了一个预测多模态模型峰值GPU内存使用量的框架，解决了现有方法无法泛化到多模态模型的问题，并达到了约8.7%的平均MAPE预测精度。


<details>
  <summary>Details</summary>
Motivation: 在Agentic AI系统中，随着深度学习模型规模和复杂性的增加，GPU内存需求超出可用容量，导致内存溢出错误，中断训练并浪费计算资源。因此，准确预测GPU内存使用量以防止内存溢出至关重要。现有研究主要关注单模态架构，无法泛化到Agentic AI系统中常用的多模态模型。

Method: 该框架通过分析多模态模型的架构和训练行为来预测其峰值GPU内存使用量。具体而言，该框架将多模态模型分解为各个组成层，并应用因子分解来估计每一层的内存使用量。

Result: 该框架在预测多模态模型峰值GPU内存使用量方面取得了约8.7%的平均MAPE（Mean Absolute Percentage Error）的预测精度。

Conclusion: 该论文成功提出了一种适用于多模态模型的GPU内存使用预测框架，有效解决了现有方法的局限性，实现了高精度预测，有助于防止训练过程中的内存溢出错误。

Abstract: As deep learning models in agentic AI systems grow in scale and complexity, GPU memory requirements increase and often exceed the available GPU memory capacity, so that out-of-memory (OoM) errors occur. It is well known that OoM interrupts the whole training itself and wastes substantial computational resources. Therefore, to prevent OoM, accurate prediction of GPU memory usage is essential. However, previous studies focus only on unimodal architectures and fail to generalize to multimodal models, even though the multimodal models are a common choice in agentic AI systems. To address this limitation, we propose a framework that predicts the peak GPU memory usage by analyzing the model architecture and training behavior of multimodal models. Specifically, the framework decomposes the multimodal model into its constituent layers and applies factorization to estimate the memory usage of each layer. Our evaluation shows that our framework achieves high prediction accuracy of ~8.7% average MAPE.

</details>


### [21] [LAPA: Log-Domain Prediction-Driven Dynamic Sparsity Accelerator for Transformer Model](https://arxiv.org/abs/2512.07855)
*Huizheng Wang,Hongbin Wang,Shaojun Wei,Yang Hu,Shouyi Yin*

Main category: cs.LG

TL;DR: LAPA是一种针对Transformer模型设计的硬件加速器。它通过对注意力预测算法和硬件架构的协同设计，解决了现有稀疏Transformer方法在跨阶段应用时功耗过大的问题。


<details>
  <summary>Details</summary>
Motivation: Transformer模型在自然语言处理和计算机视觉任务中表现出色，但其计算瓶颈在不同阶段会动态变化，需要跨阶段的稀疏加速策略。现有的稀疏Transformer方法大多是单阶段的，并且其稀疏性预测机制在跨多个阶段应用时会导致显著的功耗开销。

Method: LAPA主要通过以下三个方面来解决问题： 1. 设计了一种非对称前导一计算（ALOC）方案，以消除昂贵的乘法运算。 2. 提出了一种混合精度多轮移位累加（MRSA）机制，以减轻累加开销。 3. 设计了一种数据特征相关滤波器（DDF）策略，与MRSA过程协同工作。 最后，设计了一个精密的加速器将理论增强转化为实际硬件改进。

Result: 实验结果表明，LAPA在能效方面比现有的SOTA工作Spatten、Sanger和FACT分别高出3.52倍、3.24倍和2.79倍。

Conclusion: LAPA通过其协同设计的方法，在Transformer模型的硬件加速方面取得了显著的能效提升，为解决跨阶段稀疏加速问题提供了有效的解决方案。

Abstract: Attention-based Transformers have revolutionized natural language processing (NLP) and shown strong performance in computer vision (CV) tasks. However, as the input sequence varies, the computational bottlenecks in Transformer models exhibit dynamic behavior across stages, which calls for a cross-stage sparse acceleration strategy. Unfortunately, most existing sparse Transformer approaches are single-stage based, and their sparsity prediction mechanisms lead to significant power overhead when applied across multiple stages. To this end, this paper proposes a log-domain attention prediction algorithm-architecture co-design, named LAPA. First, an asymmetric leading one computing (ALOC) scheme is designed to eliminate expensive multiplications. Next, a mixed-precision multi-round shifting accumulation (MRSA) mechanism is further proposed to mitigate the accumulation overhead. A data-feature dependent filter (DDF) strategy is designed to work in concert with the MRSA process. Finally, an elaborate accelerator is designed to translate the theoretical enhancement into practical hardware improvement. Experimental results show that LAPA achieves 3.52x, 3.24x and 2.79x higher energy efficiency than the state-of-the-art (SOTA) works Spatten, Sanger and FACT, respectively.

</details>


### [22] [LUNA: Linear Universal Neural Attention with Generalization Guarantees](https://arxiv.org/abs/2512.08061)
*Ashkan Shahbazi,Ping He,Ali Abbasi,Yikun Bai,Xinran Liu,Elaheh Akbari,Darian Salehi,Navid NaderiAlizadeh,Soheil Kolouri*

Main category: cs.LG

TL;DR: 本文介绍了一种名为LUNA的核化线性注意力机制，它通过学习而不是固定核特征映射，实现了在保持线性计算成本的同时，达到甚至超越二次注意力机制的准确性，解决了长序列处理中效率与准确性的权衡问题。


<details>
  <summary>Details</summary>
Motivation: 传统的softmax注意力机制计算成本为$\mathcal{O}(n^2)$，限制了其在长序列任务中的应用。虽然线性注意力机制能将成本降至$\mathcal{O}(n)$，但其依赖于固定的随机特征映射，导致在计算效率和模型准确性之间存在权衡。

Method: LUNA通过参数化核，学习一个针对特定数据和任务的特征基，从而克服了固定特征方法的表达能力限制。它实现了一个可学习的特征映射，该映射产生一个正定核并支持流式计算，从而在序列长度上实现了线性的时间和内存扩展。

Result: 在Long Range Arena（LRA）基准测试中，LUNA在计算资源相同的情况下，在高效Transformer中取得了最先进的平均准确性。此外，LUNA在后验转换方面表现出色，可以在预训练的BERT和ViT-B/16模型中替换softmax，通过短暂微调即可恢复大部分原始性能，显著优于固定的线性化方法。

Conclusion: LUNA通过学习核特征映射，成功解决了长序列处理中线性注意力机制效率与准确性之间的权衡问题，实现了在保持线性计算成本的同时，达到或超越二次注意力机制的准确性，并具有广泛的应用潜力。

Abstract: Scaling attention faces a critical bottleneck: the $\mathcal{O}(n^2)$ quadratic computational cost of softmax attention, which limits its application in long-sequence domains. While linear attention mechanisms reduce this cost to $\mathcal{O}(n)$, they typically rely on fixed random feature maps, such as random Fourier features or hand-crafted functions. This reliance on static, data-agnostic kernels creates a fundamental trade-off, forcing practitioners to sacrifice significant model accuracy for computational efficiency. We introduce \textsc{LUNA}, a kernelized linear attention mechanism that eliminates this trade-off, retaining linear cost while matching and surpassing the accuracy of quadratic attention. \textsc{LUNA} is built on the key insight that the kernel feature map itself should be learned rather than fixed a priori. By parameterizing the kernel, \textsc{LUNA} learns a feature basis tailored to the specific data and task, overcoming the expressive limitations of fixed-feature methods. \textsc{Luna} implements this with a learnable feature map that induces a positive-definite kernel and admits a streaming form, yielding linear time and memory scaling in the sequence length. Empirical evaluations validate our approach across diverse settings. On the Long Range Arena (LRA), \textsc{Luna} achieves state-of-the-art average accuracy among efficient Transformers under compute parity, using the same parameter count, training steps, and approximate FLOPs. \textsc{Luna} also excels at post-hoc conversion: replacing softmax in fine-tuned BERT and ViT-B/16 checkpoints and briefly fine-tuning recovers most of the original performance, substantially outperforming fixed linearizations.

</details>


### [23] [Medical Test-free Disease Detection Based on Big Data](https://arxiv.org/abs/2512.07856)
*Haokun Zhao,Yingzhe Bai,Qingyang Xu,Lixin Zhou,Jianxin Chen,Jicong Fan*

Main category: cs.LG

TL;DR: CLDD是一个新颖的基于图的深度学习模型，通过利用疾病之间的关联和患者之间的相似性，将疾病检测表述为协同学习任务，以实现疾病检测。


<details>
  <summary>Details</summary>
Motivation: 疾病检测过程通常与广泛的医学测试和可观的成本相关联，这使得对患者执行所有可能的医学测试以诊断或预测数百或数千种疾病变得不切实际。

Method: 本文提出了协同学习疾病检测（CLDD），这是一种新颖的基于图的深度学习模型，它通过自适应地利用疾病之间的关联和患者之间的相似性，将疾病检测表述为协同学习任务。CLDD整合了来自电子健康记录的患者-疾病交互和人口统计学特征，以检测每位患者的数百或数千种疾病，而很少或根本不依赖相应的医学测试。

Result: 在包含61,191名患者和2,000种疾病的MIMIC-IV数据集的处理版本上的广泛实验表明，CLDD在多个指标上始终优于代表性基线，召回率提高了6.33％，精确率提高了7.63％。此外，对个体患者的案例研究表明，CLDD可以成功地恢复其排名靠前的预测中的被掩盖疾病，展示了疾病预测的可解释性和可靠性。

Conclusion: 通过降低诊断成本和提高可及性，CLDD有望实现大规模疾病筛查和社会健康保障。

Abstract: Accurate disease detection is of paramount importance for effective medical treatment and patient care. However, the process of disease detection is often associated with extensive medical testing and considerable costs, making it impractical to perform all possible medical tests on a patient to diagnose or predict hundreds or thousands of diseases. In this work, we propose Collaborative Learning for Disease Detection (CLDD), a novel graph-based deep learning model that formulates disease detection as a collaborative learning task by exploiting associations among diseases and similarities among patients adaptively. CLDD integrates patient-disease interactions and demographic features from electronic health records to detect hundreds or thousands of diseases for every patient, with little to no reliance on the corresponding medical tests. Extensive experiments on a processed version of the MIMIC-IV dataset comprising 61,191 patients and 2,000 diseases demonstrate that CLDD consistently outperforms representative baselines across multiple metrics, achieving a 6.33\% improvement in recall and 7.63\% improvement in precision. Furthermore, case studies on individual patients illustrate that CLDD can successfully recover masked diseases within its top-ranked predictions, demonstrating both interpretability and reliability in disease prediction. By reducing diagnostic costs and improving accessibility, CLDD holds promise for large-scale disease screening and social health security.

</details>


### [24] [SA^2GFM: Enhancing Robust Graph Foundation Models with Structure-Aware Semantic Augmentation](https://arxiv.org/abs/2512.07857)
*Junhua Shi,Qingyun Sun,Haonan Yuan,Xingcheng Fu*

Main category: cs.LG

TL;DR: SA^2GFM是一个鲁棒的图基础模型框架，它通过结构感知语义增强来改进域适应表示，并通过自监督信息瓶颈机制和专家自适应路由机制来解决负迁移问题，在各种图任务中表现出卓越的性能和鲁棒性。


<details>
  <summary>Details</summary>
Motivation: 现有图基础模型在应对领域噪声、结构扰动和对抗性攻击时鲁棒性不足，主要原因是缺乏对层次结构语义的充分建模。

Method: SA^2GFM通过以下方法实现鲁棒性：1. 将基于熵的编码树转换为结构感知文本提示，以编码分层结构先验。2. 利用自监督信息瓶颈机制，通过结构引导压缩提取鲁棒、可迁移的表示。3. 引入专家自适应路由机制，结合混合专家架构和空专家设计，解决跨域适应中的负迁移问题。4. 提出一个微调模块，通过联合社区内和社区间结构学习来优化分层结构。

Result: SA^2GFM在节点和图分类任务中，针对随机噪声和对抗性扰动，在有效性和鲁棒性方面优于9个最先进的基线模型。

Conclusion: SA^2GFM通过结构感知语义增强和信息瓶颈机制，有效地提高了图基础模型的鲁棒性和泛化能力，并通过专家自适应路由机制解决了负迁移问题，为图学习领域提供了一个强大的框架。

Abstract: We present Graph Foundation Models (GFMs) which have made significant progress in various tasks, but their robustness against domain noise, structural perturbations, and adversarial attacks remains underexplored. A key limitation is the insufficient modeling of hierarchical structural semantics, which are crucial for generalization. In this paper, we propose SA^2GFM, a robust GFM framework that improves domain-adaptive representations through Structure-Aware Semantic Augmentation. First, we encode hierarchical structural priors by transforming entropy-based encoding trees into structure-aware textual prompts for feature augmentation. The enhanced inputs are processed by a self-supervised Information Bottleneck mechanism that distills robust, transferable representations via structure-guided compression. To address negative transfer in cross-domain adaptation, we introduce an expert adaptive routing mechanism, combining a mixture-of-experts architecture with a null expert design. For efficient downstream adaptation, we propose a fine-tuning module that optimizes hierarchical structures through joint intra- and inter-community structure learning. Extensive experiments demonstrate that SA^2GFM outperforms 9 state-of-the-art baselines in terms of effectiveness and robustness against random noise and adversarial perturbations for node and graph classification.

</details>


### [25] [A Multivariate Bernoulli-Based Sampling Method for Multi-Label Data with Application to Meta-Research](https://arxiv.org/abs/2512.08371)
*Simon Chung,Colby J. Vorland,Donna L. Maney,Andrew W. Brown*

Main category: cs.LG

TL;DR: 该文章提出了一种新的抽样算法，用于处理多标签数据集中标签频率差异大且标签间存在依赖关系的问题。该算法基于多元伯努利分布，旨在创建一个更平衡的子样本，以更好地表示少数类别。


<details>
  <summary>Details</summary>
Motivation: 在多标签数据集中，当标签不互斥且频率差异很大时，很难获得一个既包含足够稀有标签观测值，又能以已知方式偏离总体频率的样本。

Method: 文章提出了一种新颖的抽样算法，该算法考虑了标签依赖性。它使用观察到的标签频率来估计多元伯努利分布参数，并计算每个标签组合的权重。

Result: 作者将此方法应用于来自Web of Science的64个生物医学主题类别的研究文章样本。该方法生成了一个更平衡的子样本，增强了少数类别的代表性。

Conclusion: 该抽样算法有效地解决了多标签数据集中标签不平衡和标签依赖性的问题，从而产生了更能代表少数类别的更平衡的子样本。

Abstract: Datasets may contain observations with multiple labels. If the labels are not mutually exclusive, and if the labels vary greatly in frequency, obtaining a sample that includes sufficient observations with scarcer labels to make inferences about those labels, and which deviates from the population frequencies in a known manner, creates challenges. In this paper, we consider a multivariate Bernoulli distribution as our underlying distribution of a multi-label problem. We present a novel sampling algorithm that takes label dependencies into account. It uses observed label frequencies to estimate multivariate Bernoulli distribution parameters and calculate weights for each label combination. This approach ensures the weighted sampling acquires target distribution characteristics while accounting for label dependencies. We applied this approach to a sample of research articles from Web of Science labeled with 64 biomedical topic categories. We aimed to preserve category frequency order, reduce frequency differences between most and least common categories, and account for category dependencies. This approach produced a more balanced sub-sample, enhancing the representation of minority categories.

</details>


### [26] [FAIM: Frequency-Aware Interactive Mamba for Time Series Classification](https://arxiv.org/abs/2512.07858)
*Da Zhang,Bingyu Li,Zhiyuan Zhao,Yanhan Zhang,Junyu Gao,Feiping Nie,Xuelong Li*

Main category: cs.LG

TL;DR: 本文提出了FAIM模型，这是一种轻量级的频率感交互式Mamba模型，用于时间序列分类。该模型通过自适应滤波块提取频率域特征并抑制噪声，通过交互式Mamba块促进多粒度信息交互，以及通过自监督预训练机制增强模型对复杂时间模式的理解和鲁棒性。FAIM在多个基准测试中表现出色，在准确性和效率之间取得了卓越的平衡。


<details>
  <summary>Details</summary>
Motivation: 时间序列分类在许多现实应用中至关重要，但当前的深度学习模型面临计算成本高、对噪声敏感以及在小规模数据集上容易过拟合等挑战。

Method: FAIM模型包括两个主要组件：
1. **自适应滤波块（AFB）**：利用傅里叶变换从时间序列数据中提取频率域特征。AFB包含可学习的自适应阈值，用于动态抑制噪声，并采用全局和局部语义自适应滤波的元素级耦合，深入建模不同频率分量之间的协同作用。
2. **交互式Mamba块（IMB）**：促进高效的多粒度信息交互，平衡细粒度判别特征和全面的全局上下文信息的提取，赋予FAIM强大的表达能力。
此外，FAIM还结合了自监督预训练机制，以增强对复杂时间模式的理解并提高跨领域和高噪声场景的鲁棒性。

Result: FAIM在多个基准测试中持续优于现有的最先进（SOTA）方法。该模型在准确性和效率之间取得了卓越的平衡，并表现出出色的性能。

Conclusion: FAIM模型通过其轻量级的频率感知架构、自适应噪声抑制、多粒度信息交互以及自监督预训练，有效解决了当前时间序列分类深度学习模型面临的挑战，并在准确性和效率方面达到了新的SOTA水平。

Abstract: Time series classification (TSC) is crucial in numerous real-world applications, such as environmental monitoring, medical diagnosis, and posture recognition. TSC tasks require models to effectively capture discriminative information for accurate class identification. Although deep learning architectures excel at capturing temporal dependencies, they often suffer from high computational cost, sensitivity to noise perturbations, and susceptibility to overfitting on small-scale datasets. To address these challenges, we propose FAIM, a lightweight Frequency-Aware Interactive Mamba model. Specifically, we introduce an Adaptive Filtering Block (AFB) that leverages Fourier Transform to extract frequency-domain features from time series data. The AFB incorporates learnable adaptive thresholds to dynamically suppress noise and employs element-wise coupling of global and local semantic adaptive filtering, enabling in-depth modeling of the synergy among different frequency components. Furthermore, we design an Interactive Mamba Block (IMB) to facilitate efficient multi-granularity information interaction, balancing the extraction of fine-grained discriminative features and comprehensive global contextual information, thereby endowing FAIM with powerful and expressive representations for TSC tasks. Additionally, we incorporate a self-supervised pre-training mechanism to enhance FAIM's understanding of complex temporal patterns and improve its robustness across various domains and high-noise scenarios. Extensive experiments on multiple benchmarks demonstrate that FAIM consistently outperforms existing state-of-the-art (SOTA) methods, achieving a superior trade-off between accuracy and efficiency and exhibits outstanding performance.

</details>


### [27] [DS FedProxGrad: Asymptotic Stationarity Without Noise Floor in Fair Federated Learning](https://arxiv.org/abs/2512.08671)
*Huzaifa Arif*

Main category: cs.LG

TL;DR: 本文提出了一种改进的DS FedProxGrad框架，通过在Robbins-Monro步长策略和局部不精确性的温和衰减条件下，实现了渐近收敛到平稳点，且收敛速度不受方差引起的噪声下限的影响。


<details>
  <summary>Details</summary>
Motivation: 此前的研究FedProxGrad在群体公平联邦学习的非凸复合优化问题中收敛到受噪声主导的平稳邻域，且收敛性明确依赖于方差引起的噪声下限。

Method: 本文提出了DS FedProxGrad (Decay Step Size FedProxGrad) 框架，该框架是在不精确局部近似解和显式公平性正则化的情况下，对FedProxGrad类型分析框架的扩展。在Robbins-Monro步长计划和局部不精确性的温和衰减条件下，证明了算法的渐近平稳性，且收敛速度不依赖于方差引起的噪声下限。

Result: 在Robbins-Monro步长调度和局部不精确性的温和衰减条件下，证明了 \(\liminf_{r\to\infty} \mathbb{E}[\|\nabla F(\mathbf{x}^r)\|^2] = 0\)，即算法是渐近平稳的，且收敛速度不依赖于方差引起的噪声下限。

Conclusion: 本文通过引入DS FedProxGrad框架，显著改进了FedProxGrad的收敛性分析，消除了收敛速度对方差引起的噪声下限的依赖，实现了渐近平稳性。

Abstract: Recent work \cite{arifgroup} introduced Federated Proximal Gradient \textbf{(\texttt{FedProxGrad})} for solving non-convex composite optimization problems in group fair federated learning. However, the original analysis established convergence only to a \textit{noise-dominated neighborhood of stationarity}, with explicit dependence on a variance-induced noise floor. In this work, we provide an improved asymptotic convergence analysis for a generalized \texttt{FedProxGrad}-type analytical framework with inexact local proximal solutions and explicit fairness regularization. We call this extended analytical framework \textbf{DS \texttt{FedProxGrad}} (Decay Step Size \texttt{FedProxGrad}). Under a Robbins-Monro step-size schedule \cite{robbins1951stochastic} and a mild decay condition on local inexactness, we prove that $\liminf_{r\to\infty} \mathbb{E}[\|\nabla F(\mathbf{x}^r)\|^2] = 0$, i.e., the algorithm is asymptotically stationary and the convergence rate does not depend on a variance-induced noise floor.

</details>


### [28] [SetAD: Semi-Supervised Anomaly Learning in Contextual Sets](https://arxiv.org/abs/2512.07863)
*Jianling Gao,Chongyang Tao,Xuelian Lin,Junfeng Liu,Shuai Ma*

Main category: cs.LG

TL;DR: 一篇关于半监督异常检测的论文，它通过引入SetAD框架来解决现有方法中点或对中心视图的局限性，从而有效地利用有限的标记数据。


<details>
  <summary>Details</summary>
Motivation: 传统的半监督异常检测方法忽略了异常的上下文性质，并且未能利用集合的组合构成中产生的丰富监督信号。

Method: SetAD框架将半监督异常检测重新定义为集合级异常检测任务。它采用一个基于注意力的集合编码器，通过分级学习目标进行训练，量化整个集合中的异常程度。为了提高鲁棒性和分数校准，SetAD提出了一种上下文校准的异常评分机制。

Result: 在10个真实世界数据集上进行的大量实验表明，SetAD显著优于现有最先进的模型。模型的性能随着集合大小的增加而持续提高。

Conclusion: SetAD通过引入集合级异常检测，并通过关注点校准机制来评估点异常分数，为半监督异常检测提供了一种新颖有效的方法。

Abstract: Semi-supervised anomaly detection (AD) has shown great promise by effectively leveraging limited labeled data. However, existing methods are typically structured around scoring individual points or simple pairs. Such {point- or pair-centric} view not only overlooks the contextual nature of anomalies, which are defined by their deviation from a collective group, but also fails to exploit the rich supervisory signals that can be generated from the combinatorial composition of sets. Consequently, such models struggle to exploit the high-order interactions within the data, which are critical for learning discriminative representations. To address these limitations, we propose SetAD, a novel framework that reframes semi-supervised AD as a Set-level Anomaly Detection task. SetAD employs an attention-based set encoder trained via a graded learning objective, where the model learns to quantify the degree of anomalousness within an entire set. This approach directly models the complex group-level interactions that define anomalies. Furthermore, to enhance robustness and score calibration, we propose a context-calibrated anomaly scoring mechanism, which assesses a point's anomaly score by aggregating its normalized deviations from peer behavior across multiple, diverse contextual sets. Extensive experiments on 10 real-world datasets demonstrate that SetAD significantly outperforms state-of-the-art models. Notably, we show that our model's performance consistently improves with increasing set size, providing strong empirical support for the set-based formulation of anomaly detection.

</details>


### [29] [Unsupervised Learning of Density Estimates with Topological Optimization](https://arxiv.org/abs/2512.08895)
*Suina Tanweer,Firas A. Khasawneh*

Main category: cs.LG

TL;DR: 本文提出了一种基于拓扑损失函数的无监督学习方法，用于自动选择核密度估计的最佳带宽，并在不同维度上验证其有效性。


<details>
  <summary>Details</summary>
Motivation: 核密度估计在机器学习、贝叶斯推断、随机动力学和信号处理等领域中广泛应用，但其关键超参数——核带宽的选择，是一个难题。带宽的选择直接影响模型的偏差-方差权衡，可能导致过平滑或欠平滑。

Method: 本文提出了一种利用拓扑数据分析（TDA）量化拓扑特征的方法，并将这些特征编码为基于拓扑的损失函数，从而实现无监督地自动选择最优带宽。

Result: 通过与经典技术进行基准测试，该方法在不同维度上展现了其潜力。

Conclusion: 本文提出了一种新颖的无监督学习方法，通过结合拓扑数据分析和基于拓扑的损失函数，有效地解决了核密度估计中带宽选择的难题，为该领域的算法性能提升提供了新的途径。

Abstract: Kernel density estimation is a key component of a wide variety of algorithms in machine learning, Bayesian inference, stochastic dynamics and signal processing. However, the unsupervised density estimation technique requires tuning a crucial hyperparameter: the kernel bandwidth. The choice of bandwidth is critical as it controls the bias-variance trade-off by over- or under-smoothing the topological features. Topological data analysis provides methods to mathematically quantify topological characteristics, such as connected components, loops, voids et cetera, even in high dimensions where visualization of density estimates is impossible. In this paper, we propose an unsupervised learning approach using a topology-based loss function for the automated and unsupervised selection of the optimal bandwidth and benchmark it against classical techniques -- demonstrating its potential across different dimensions.

</details>


### [30] [Pattern Recognition of Ozone-Depleting Substance Exports in Global Trade Data](https://arxiv.org/abs/2512.07864)
*Muhammad Sukri Bin Ramli*

Main category: cs.LG

TL;DR: 这篇论文介绍了一个使用无监督机器学习的框架，用于系统性地检测可疑贸易模式，以监测《蒙特利尔议定书》等环境条约的遵守情况。


<details>
  <summary>Details</summary>
Motivation: 监测《蒙特利尔议定书》等环境条约的遵守情况需要新的方法来审查庞大、复杂的贸易海关数据集。

Method: 该方法结合了多种机器学习技术：无监督聚类（K-Means）发现贸易原型；异常检测（Isolation Forest和IQR）识别罕见的“巨额交易”和商业上不寻常的单位价格货物；启发式标记发现模糊的货物描述等策略。这些层级被组合成一个优先评分，并通过可解释人工智能（SHAP）进行验证。

Result: 成功识别了1,351个价格异常值和1,288个高优先级货物，供海关审查。高优先级商品显示出与普通商品不同且更有价值的价值与重量比。模型检测到2021年初“巨额交易”的激增，这与美国AIM法案的实际监管影响直接相关。

Conclusion: 该研究提供了一个可重复的无监督学习流程，可以将原始贸易数据转化为优先级排序、可用的情报，供监管机构使用。

Abstract: New methods are needed to monitor environmental treaties, like the Montreal Protocol, by reviewing large, complex customs datasets. This paper introduces a framework using unsupervised machine learning to systematically detect suspicious trade patterns and highlight activities for review. Our methodology, applied to 100,000 trade records, combines several ML techniques. Unsupervised Clustering (K-Means) discovers natural trade archetypes based on shipment value and weight. Anomaly Detection (Isolation Forest and IQR) identifies rare "mega-trades" and shipments with commercially unusual price-per-kilogram values. This is supplemented by Heuristic Flagging to find tactics like vague shipment descriptions. These layers are combined into a priority score, which successfully identified 1,351 price outliers and 1,288 high-priority shipments for customs review. A key finding is that high-priority commodities show a different and more valuable value-to-weight ratio than general goods. This was validated using Explainable AI (SHAP), which confirmed vague descriptions and high value as the most significant risk predictors. The model's sensitivity was validated by its detection of a massive spike in "mega-trades" in early 2021, correlating directly with the real-world regulatory impact of the US AIM Act. This work presents a repeatable unsupervised learning pipeline to turn raw trade data into prioritized, usable intelligence for regulatory groups.

</details>


### [31] [Using Text-Based Life Trajectories from Swedish Register Data to Predict Residential Mobility with Pretrained Transformers](https://arxiv.org/abs/2512.07865)
*Philipp Stark,Alexandros Sopasakis,Ola Hall,Markus Grillitsch*

Main category: cs.LG

TL;DR: 本文将瑞典的大规模登记数据转化为文本化的生命轨迹，以解决数据分析中长期存在的挑战，并通过利用这种独特而全面的数据，预测个体在后续年份的居住流动性。


<details>
  <summary>Details</summary>
Motivation: 解决分类变量基数高和编码方案随时间不一致的问题。

Method: 将来自690万个体（2001-2013年）的登记数据转换为语义丰富的文本，并预测个体在后续年份（2013-2017年）的居住流动性。结合人口统计信息与居住、工作、教育、收入和家庭状况的年度变化，评估此类序列对长期预测的有效性。比较了多种NLP架构（包括LSTM、DistilBERT、BERT和Qwen）。

Result: 序列模型和基于Transformer的模型比基线模型更有效地捕捉时序和语义结构。文本化登记数据保留了个体路径的有意义信息，并支持复杂、可扩展的建模。该数据集为开发和评估新的序列建模方法提供了严格的测试平台。

Conclusion: 将语义丰富的登记数据与现代语言模型相结合，可以显著推进社会科学中的纵向分析。

Abstract: We transform large-scale Swedish register data into textual life trajectories to address two long-standing challenges in data analysis: high cardinality of categorical variables and inconsistencies in coding schemes over time. Leveraging this uniquely comprehensive population register, we convert register data from 6.9 million individuals (2001-2013) into semantically rich texts and predict individuals' residential mobility in later years (2013-2017). These life trajectories combine demographic information with annual changes in residence, work, education, income, and family circumstances, allowing us to assess how effectively such sequences support longitudinal prediction. We compare multiple NLP architectures (including LSTM, DistilBERT, BERT, and Qwen) and find that sequential and transformer-based models capture temporal and semantic structure more effectively than baseline models. The results show that textualized register data preserves meaningful information about individual pathways and supports complex, scalable modeling. Because few countries maintain longitudinal microdata with comparable coverage and precision, this dataset enables analyses and methodological tests that would be difficult or impossible elsewhere, offering a rigorous testbed for developing and evaluating new sequence-modeling approaches. Overall, our findings demonstrate that combining semantically rich register data with modern language models can substantially advance longitudinal analysis in social sciences.

</details>


### [32] [Advancing physiological time series reconstruction and imputation via mixture of receptive fields and experts fusion](https://arxiv.org/abs/2512.07873)
*Ci Zhang,Huayu Li,Changdi Yang,Jiangnan Xia,Yanzhi Wang,Xiaolong Ma,Jin Lu,Geng Yuan*

Main category: cs.LG

TL;DR: 该研究提出了一种新颖的基于MoE的评分扩散框架，用于医学时间序列信号重建。


<details>
  <summary>Details</summary>
Motivation: 以往研究表明，扩散模型在时间序列信号重建方面潜力巨大，但在医疗时间序列领域应用较少。医疗时间序列信号的独特性给深度学习带来了挑战。

Method: 设计了感受野自适应MoE模块（RFAMoE），使每个通道在扩散过程中自适应选择感受野。设计了融合MoE模块，利用MoE并行生成K个噪声信号，通过路由机制融合，在单次推理中完成信号重建。

Result: 实验结果表明，该框架在不同任务和数据集上均优于现有的基于扩散的SOTA方法，并消除了多重推理带来的计算成本和延迟。

Conclusion: 提出的基于MoE的评分扩散框架在医疗时间序列信号重建方面表现出色，有效解决了现有方法的局限性，提高了性能并降低了计算成本。

Abstract: Recent studies show that using diffusion models for time series signal reconstruc- tion holds great promise. However, such approaches remain largely unexplored in the domain of medical time series. The unique characteristics of the physiological time series signals, such as multivariate, high temporal variability, highly noisy, and artifact-prone, make deep learning-based approaches still challenging for tasks such as imputation. Hence, we propose a novel Mixture of Experts (MoE)-based noise estimator within a score-based diffusion framework. Specifically, the Receptive Field Adaptive MoE (RFAMoE) module is designed to enable each channel to adap- tively select desired receptive fields throughout the diffusion process. Moreover, recent literature has found that when generating a physiological signal, performing multiple inferences and averaging the reconstructed signals can effectively reduce reconstruction errors, but at the cost of significant computational and latency over- head. We design a Fusion MoE module and innovatively leverage the nature of MoE module to generate K noise signals in parallel, fuse them using a routing mechanism, and complete signal reconstruction in a single inference step. This design not only improves performance over previous methods but also eliminates the substantial computational cost and latency associated with multiple inference processes. Extensive results demonstrate that our proposed framework consistently outperforms diffusion-based SOTA works on different tasks and datasets.

</details>


### [33] [Balanced Accuracy: The Right Metric for Evaluating LLM Judges - Explained through Youden's J statistic](https://arxiv.org/abs/2512.08121)
*Stephane Collot,Colin Fraser,Justin Zhao,William F. Shen,Timon Willi,Ilias Leontiadis*

Main category: cs.LG

TL;DR: 本文提出Youden指数作为评估大型语言模型（LLM）的分类器选择的更好指标，优于传统的准确率、精确率和F1分数，因为Youden指数能更好地处理类别不平衡问题并避免对模型评估的扭曲。


<details>
  <summary>Details</summary>
Motivation: 目前评估LLM的方法依赖于分类器（无论是LLM作为评判者还是人类标注者）来估计理想或非理想行为的发生频率，但传统指标如准确率、精确率和F1分数受到类别不平衡和正类别任意选择的影响，可能导致对评判者选择的偏差，进而扭曲评估结果。

Method: 本文提出使用Youden指数（或其等效的平衡准确率）作为选择最佳分类器的理论依据。通过理论分析、实际案例和模拟，论证了Youden指数在处理类别不平衡问题和避免评估扭曲方面的优越性。

Result: Youden指数（或平衡准确率）在理论上更适合选择用于比较LLM的评判者。相较于传统指标，使用平衡准确率选择评判者能够带来更好、更鲁棒的分类器选择。

Conclusion: 为了对大型语言模型进行可靠的评估，应采用Youden指数或平衡准确率作为选择评估分类器的主要指标，因为它能有效解决现有评估指标在类别不平衡和正类别选择上的局限性，从而确保评估结果的准确性和公正性。

Abstract: Rigorous evaluation of large language models (LLMs) relies on comparing models by the prevalence of desirable or undesirable behaviors, such as task pass rates or policy violations. These prevalence estimates are produced by a classifier, either an LLM-as-a-judge or human annotators, making the choice of classifier central to trustworthy evaluation. Common metrics used for this choice, such as Accuracy, Precision, and F1, are sensitive to class imbalance and to arbitrary choices of positive class, and can favor judges that distort prevalence estimates. We show that Youden's $J$ statistic is theoretically aligned with choosing the best judge to compare models, and that Balanced Accuracy is an equivalent linear transformation of $J$. Through both analytical arguments and empirical examples and simulations, we demonstrate how selecting judges using Balanced Accuracy leads to better, more robust classifier selection.

</details>


### [34] [Graph Contrastive Learning via Spectral Graph Alignment](https://arxiv.org/abs/2512.07878)
*Manh Nguyen,Joshua Cape*

Main category: cs.LG

TL;DR: 该论文介绍了一种名为SpecMatch-CL的新型对比学习方法，用于图的表示学习。SpecMatch-CL通过对齐不同视图的图嵌入所构建的“图的图”的规范化拉普拉斯算子来工作，从而在保持局部对齐的同时控制全局结构。


<details>
  <summary>Details</summary>
Motivation: 传统的对比学习方法（例如InfoNCE）在对齐图的增强视图时，无法控制由这些嵌入构建的视图特定“图的图”的全局结构。这可能导致学习到的嵌入次优。

Method: SpecMatch-CL引入了一个新的损失函数，通过最小化规范化拉普拉斯算子之间的差异来对齐视图特定的“图的图”。理论分析表明，在特定条件下，规范化拉普拉斯算子之间的差异不仅为理想的完美对齐对比损失和当前损失之间的差异提供了上限，也为均匀损失提供了上限。

Result: SpecMatch-CL在8个TU基准测试的无监督学习和低标签率半监督学习中都取得了最先进的成果。此外，在PPI-306K和ZINC 2M数据集上的迁移学习中也获得了持续的性能提升。

Conclusion: SpecMatch-CL通过在对比学习中引入全局结构控制机制，显著提高了图表示学习的性能。它通过对齐“图的图”的规范化拉普拉斯算子来实现这一点，提供了理论上的保证，并在多个基准测试中取得了优异的实证结果。

Abstract: Given augmented views of each input graph, contrastive learning methods (e.g., InfoNCE) optimize pairwise alignment of graph embeddings across views while providing no mechanism to control the global structure of the view specific graph-of-graphs built from these embeddings. We introduce SpecMatch-CL, a novel loss function that aligns the view specific graph-of-graphs by minimizing the difference between their normalized Laplacians. Theoretically, we show that under certain assumptions, the difference between normalized Laplacians provides an upper bound not only for the difference between the ideal Perfect Alignment contrastive loss and the current loss, but also for the Uniformly loss. Empirically, SpecMatch-CL establishes new state of the art on eight TU benchmarks under unsupervised learning and semi-supervised learning at low label rates, and yields consistent gains in transfer learning on PPI-306K and ZINC 2M datasets.

</details>


### [35] [Nonnegative Matrix Factorization through Cone Collapse](https://arxiv.org/abs/2512.07879)
*Manh Nguyen,Daniel Pimentel-Alarcón*

Main category: cs.LG

TL;DR: 本文提出了一种名为Cone Collapse的新的NMF算法，它从几何角度重新审视了NMF，并在各种数据集上取得了最先进的聚类结果。


<details>
  <summary>Details</summary>
Motivation: NMF在数据聚类方面应用广泛，但现有算法未充分利用NMF产生的锥形几何结构。作者旨在提出一种新的算法来明确利用锥形几何结构，以提高NMF在聚类应用中的性能。

Method: 本文提出了一种名为Cone Collapse的算法，该算法从完整的非负正交开始，并迭代地将其缩小到数据生成的最小锥体。在此基础上，作者推导了锥感知正交NMF模型（CC-NMF），通过对恢复的极射线应用单正交NMF。

Result: 在16个基准基因表达、文本和图像数据集上，CC-NMF在聚类纯度方面始终与强大的NMF基线（包括乘法更新、ANLS、投影NMF、ONMF和稀疏NMF）相匹配或超越。

Conclusion: 明确恢复数据锥可以产生理论上合理且经验上强大的基于NMF的聚类方法。

Abstract: Nonnegative matrix factorization (NMF) is a widely used tool for learning parts-based, low-dimensional representations of nonnegative data, with applications in vision, text, and bioinformatics. In clustering applications, orthogonal NMF (ONMF) variants further impose (approximate) orthogonality on the representation matrix so that its rows behave like soft cluster indicators. Existing algorithms, however, are typically derived from optimization viewpoints and do not explicitly exploit the conic geometry induced by NMF: data points lie in a convex cone whose extreme rays encode fundamental directions or "topics". In this work we revisit NMF from this geometric perspective and propose Cone Collapse, an algorithm that starts from the full nonnegative orthant and iteratively shrinks it toward the minimal cone generated by the data. We prove that, under mild assumptions on the data, Cone Collapse terminates in finitely many steps and recovers the minimal generating cone of $\mathbf{X}^\top$ . Building on this basis, we then derive a cone-aware orthogonal NMF model (CC-NMF) by applying uni-orthogonal NMF to the recovered extreme rays. Across 16 benchmark gene-expression, text, and image datasets, CC-NMF consistently matches or outperforms strong NMF baselines-including multiplicative updates, ANLS, projective NMF, ONMF, and sparse NMF-in terms of clustering purity. These results demonstrate that explicitly recovering the data cone can yield both theoretically grounded and empirically strong NMF-based clustering methods.

</details>


### [36] [Revisiting the Scaling Properties of Downstream Metrics in Large Language Model Training](https://arxiv.org/abs/2512.08894)
*Jakub Krajewski,Amitis Shidani,Dan Busbridge,Sam Wiseman,Jason Ramapuram*

Main category: cs.LG

TL;DR: 本文提出了一种直接建模LLM下游任务性能与训练预算之间扩展关系的新框架，发现对于固定的token-to-parameter比例，简单的幂律即可准确描述多个下游任务的对数准确率的扩展行为。


<details>
  <summary>Details</summary>
Motivation: 传统的LLM扩展定律主要关注预训练损失等替代指标，而预测下游任务性能被认为不可靠，本文旨在挑战这一观点。

Method: 本文提出了一种直接框架来建模基准性能与训练预算之间的扩展关系。通过固定token-to-parameter比例，使用简单的幂律来描述多个流行下游任务的对数准确率的扩展行为。

Result: 研究结果表明，直接方法比之前提出的两阶段程序具有更好的外推性，且后者容易导致误差累积。此外，本文还引入了功能形式，可以预测不同token-to-parameter比例下的准确率，并考虑重复采样下的推理计算。

Conclusion: 本文验证了所提出的框架在多达17B参数和350B tokens训练的模型上的有效性，并发布了完整的预训练损失和下游评估结果，以支持可复现性和鼓励未来的研究。

Abstract: While scaling laws for Large Language Models (LLMs) traditionally focus on proxy metrics like pretraining loss, predicting downstream task performance has been considered unreliable. This paper challenges that view by proposing a direct framework to model the scaling of benchmark performance from the training budget. We find that for a fixed token-to-parameter ratio, a simple power law can accurately describe the scaling behavior of log accuracy on multiple popular downstream tasks. Our results show that the direct approach extrapolates better than the previously proposed two-stage procedure, which is prone to compounding errors. Furthermore, we introduce functional forms that predict accuracy across token-to-parameter ratios and account for inference compute under repeated sampling. We validate our findings on models with up to 17B parameters trained on up to 350B tokens across two dataset mixtures. To support reproducibility and encourage future research, we release the complete set of pretraining losses and downstream evaluation results.

</details>


### [37] [Semi-Supervised Contrastive Learning with Orthonormal Prototypes](https://arxiv.org/abs/2512.07880)
*Huanran Li,Manh Nguyen,Daniel Pimentel-Alarcón*

Main category: cs.LG

TL;DR: 这篇论文介绍了一种新的半监督对比学习方法CLOP，它通过促进类嵌入之间的正交线性子空间来解决维度坍塌问题，并在图像分类和目标检测任务中表现出更好的性能和稳定性。


<details>
  <summary>Details</summary>
Motivation: 对比学习在表示学习方面表现出色，但维度坍塌问题，特别是半监督和自监督设置中的维度坍塌，是一个重大挑战。

Method: 本文首先确定了一个关键的学习率阈值，超过该阈值，标准的对比损失会收敛到坍塌的解决方案。在此基础上，提出了一种新颖的半监督损失函数CLOP，旨在通过促进类嵌入之间正交线性子空间的形成来防止维度坍塌。

Result: 通过在真实和合成数据集上进行大量实验，证明CLOP在图像分类和目标检测任务中提高了性能，同时在不同的学习率和批量大小下也表现出更高的稳定性。

Conclusion: CLOP通过解决维度坍塌问题，提高了对比学习在半监督场景下的性能和稳定性。

Abstract: Contrastive learning has emerged as a powerful method in deep learning, excelling at learning effective representations through contrasting samples from different distributions. However, dimensional collapse, where embeddings converge into a lower-dimensional space, poses a significant challenge, especially in semi-supervised and self-supervised setups. In this paper, we first identify a critical learning-rate threshold, beyond which standard contrastive losses converge to collapsed solutions. Building on these insights, we propose CLOP, a novel semi-supervised loss function designed to prevent dimensional collapse by promoting the formation of orthogonal linear subspaces among class embeddings. Through extensive experiments on real and synthetic datasets, we demonstrate that CLOP improves performance in image classification and object detection tasks while also exhibiting greater stability across different learning rates and batch sizes.

</details>


### [38] [GSPN-2: Efficient Parallel Sequence Modeling](https://arxiv.org/abs/2512.07884)
*Hongjun Wang,Yitong Jiang,Collin McCarthy,David Wehr,Hanrong Ye,Xinhao Li,Ka Chun Cheung,Wonmin Byeon,Jinwei Gu,Ke Chen,Kai Han,Hongxu Yin,Pavlo Molchanov,Jan Kautz,Sifei Liu*

Main category: cs.LG

TL;DR: GSPN-2通过算法和系统协同设计，优化了GSPN在处理高分辨率图像和长视频时的效率瓶颈，在计算成本显著降低的同时保持了Transformer级别的精度。


<details>
  <summary>Details</summary>
Motivation: 现有的视觉Transformer在处理高分辨率图像和长视频时效率低下，GSPN虽通过行扫描传播方案将成本接近线性，但其实现仍存在GPU内核重复启动、数据传输过多和冗余计算等问题。

Method: GSPN-2将现有的数千次微启动融合为一个2D内核，为每个通道切片分配一个warp，并在共享内存中暂存前一列的激活。在模型方面，引入了一种紧凑的通道传播策略，取代了每个通道的矩阵，减少了参数，并与Transformer注意力中使用的亲和图自然对齐。

Result: GSPN-2在图像分类和文生图合成任务中展现了Transformer级别的精度，但计算成本显著降低。

Conclusion: GSPN-2通过结构化矩阵变换和GPU优化实现的独特结合，为视觉应用中建模全局空间上下文建立了新的效率前沿。

Abstract: Efficient vision transformer remains a bottleneck for high-resolution images and long-video related real-world applications. Generalized Spatial Propagation Network (GSPN) addresses this by replacing quadratic self-attention with a line-scan propagation scheme, bringing the cost close to linear in the number of rows or columns, while retaining accuracy. Despite this advancement, the existing GSPN implementation still suffers from (i) heavy overhead due to repeatedly launching GPU kernels, (ii) excessive data transfers from global GPU memory, and (iii) redundant computations caused by maintaining separate propagation weights for each channel. We introduce GSPN-2, a joint algorithm-system redesign. In particular, we eliminate thousands of micro-launches from the previous implementation into one single 2D kernel, explicitly pin one warp to each channel slice, and stage the previous column's activations in shared memory. On the model side, we introduce a compact channel propagation strategy that replaces per-channel matrices, trimming parameters, and align naturally with the affinity map used in transformer attention. Experiments demonstrate GSPN-2's effectiveness across image classification and text-to-image synthesis tasks, matching transformer-level accuracy with significantly lower computational cost. GSPN-2 establishes a new efficiency frontier for modeling global spatial context in vision applications through its unique combination of structured matrix transformations and GPU-optimized implementation. Project page: https://whj363636.github.io/GSPN2/

</details>


### [39] [Towards symbolic regression for interpretable clinical decision scores](https://arxiv.org/abs/2512.07961)
*Guilherme Seidyo Imai Aldeia,Joseph D. Romano,Fabricio Olivetti de Franca,Daniel S. Herman,William G. La Cava*

Main category: cs.LG

TL;DR: Brush是一种符号回归（SR）算法，它结合了决策树状的分割算法和非线性常数优化，从而将基于规则的逻辑无缝集成到符号回归和分类模型中。


<details>
  <summary>Details</summary>
Motivation: 传统的符号回归（SR）方法难以对结合风险方程和规则的医疗决策进行建模。

Method: Brush算法在决策树状的分割算法中引入了非线性常数优化，能将基于规则的逻辑与符号回归和分类模型相结合，以达到对医疗决策建模的目的。

Result: Brush在SRBench上取得了帕累托最优的性能，并成功重新推导了两个广泛使用的临床评分系统，获得了高准确度和可解释的模型。相较于决策树、随机森林和其他SR方法，Brush在产生更简单模型的同时，实现了可媲美或更优的预测性能。

Conclusion: Brush算法通过结合决策树状分割和非线性常数优化，有效解决了符号回归在建模医疗决策方面的局限性，并有望应用于开发数据驱动的临床风险评分。

Abstract: Medical decision-making makes frequent use of algorithms that combine risk equations with rules, providing clear and standardized treatment pathways. Symbolic regression (SR) traditionally limits its search space to continuous function forms and their parameters, making it difficult to model this decision-making. However, due to its ability to derive data-driven, interpretable models, SR holds promise for developing data-driven clinical risk scores. To that end we introduce Brush, an SR algorithm that combines decision-tree-like splitting algorithms with non-linear constant optimization, allowing for seamless integration of rule-based logic into symbolic regression and classification models. Brush achieves Pareto-optimal performance on SRBench, and was applied to recapitulate two widely used clinical scoring systems, achieving high accuracy and interpretable models. Compared to decision trees, random forests, and other SR methods, Brush achieves comparable or superior predictive performance while producing simpler models.

</details>


### [40] [CIP-Net: Continual Interpretable Prototype-based Network](https://arxiv.org/abs/2512.07981)
*Federico Di Valerio,Michela Proietti,Alessio Ragno,Roberto Capobianco*

Main category: cs.LG

TL;DR: 介绍了CIP-Net，一种无需存储示例的自解释原型模型，用于持续学习的灾难性遗忘问题。


<details>
  <summary>Details</summary>
Motivation: 持续学习中的灾难性遗忘问题，即模型在学习新任务时遗忘旧知识。现有的可解释AI方法在持续学习中存在局限性，如需要后验解释或为每个新任务增加内存。

Method: 提出CIP-Net模型，该模型是一种无需存储示例的自解释原型模型，旨在解决持续学习中的灾难性遗忘问题。它在预测过程中生成解释，有助于保留知识，并避免存储过去的示例，同时保持简单的架构。

Result: CIP-Net在任务增量和类别增量设置下，与以往的无示例和自解释方法相比，取得了最先进的性能，并显著降低了与内存相关的开销。

Conclusion: CIP-Net是持续学习中一种实用且可解释的解决方案。

Abstract: Continual learning constrains models to learn new tasks over time without forgetting what they have already learned. A key challenge in this setting is catastrophic forgetting, where learning new information causes the model to lose its performance on previous tasks. Recently, explainable AI has been proposed as a promising way to better understand and reduce forgetting. In particular, self-explainable models are useful because they generate explanations during prediction, which can help preserve knowledge. However, most existing explainable approaches use post-hoc explanations or require additional memory for each new task, resulting in limited scalability. In this work, we introduce CIP-Net, an exemplar-free self-explainable prototype-based model designed for continual learning. CIP-Net avoids storing past examples and maintains a simple architecture, while still providing useful explanations and strong performance. We demonstrate that CIPNet achieves state-of-the-art performances compared to previous exemplar-free and self-explainable methods in both task- and class-incremental settings, while bearing significantly lower memory-related overhead. This makes it a practical and interpretable solution for continual learning.

</details>


### [41] [Bridging the Clinical Expertise Gap: Development of a Web-Based Platform for Accessible Time Series Forecasting and Analysis](https://arxiv.org/abs/2512.07992)
*Aaron D. Mullen,Daniel R. Harris,Svetla Slavova,V. K. Cody Bumgardner*

Main category: cs.LG

TL;DR: 该文章介绍了一个时间序列预测的Web平台，旨在帮助研究人员和临床医生解决在医疗领域进行时间序列预测的技术障碍。该平台提供数据分析、模型训练、结果解释和可视化等功能，并利用大型语言模型提供建议和解释，以期集成到学习型健康系统中。


<details>
  <summary>Details</summary>
Motivation: 在医疗领域，时间序列预测的应用广泛，但分析数据、建立模型和解释结果所需的技术专业知识是其使用的障碍。

Method: 文章提出了一个Web平台，该平台提供以下功能：1. 用户上传数据并生成图表，以展示变量及其关系。2. 支持多种可高度自定义的预测模型和训练技术。3. 利用大型语言模型生成推荐和解释，以帮助用户选择合适的参数并理解模型结果。

Result: 该平台能够使研究人员和临床医生更容易地分析和绘制数据、训练预测模型以及解释和查看结果。

Conclusion: 该平台的目标是将时间序列预测技术集成到学习型健康系统中，以实现临床管道的持续数据收集和推断。

Abstract: Time series forecasting has applications across domains and industries, especially in healthcare, but the technical expertise required to analyze data, build models, and interpret results can be a barrier to using these techniques. This article presents a web platform that makes the process of analyzing and plotting data, training forecasting models, and interpreting and viewing results accessible to researchers and clinicians. Users can upload data and generate plots to showcase their variables and the relationships between them. The platform supports multiple forecasting models and training techniques which are highly customizable according to the user's needs. Additionally, recommendations and explanations can be generated from a large language model that can help the user choose appropriate parameters for their data and understand the results for each model. The goal is to integrate this platform into learning health systems for continuous data collection and inference from clinical pipelines.

</details>


### [42] [Benchmarking Offline Multi-Objective Reinforcement Learning in Critical Care](https://arxiv.org/abs/2512.08012)
*Aryaman Bansal,Divya Sharma*

Main category: cs.LG

TL;DR: 这篇论文评估了三种离线多目标强化学习(MORL)算法在重症监护环境中的应用，发现PEDA DT算法在MIMIC-IV数据集上表现出优越的灵活性，证实了序列建模架构在多目标条件生成方面的鲁棒性和有效性。


<details>
  <summary>Details</summary>
Motivation: 在重症监护环境中，临床医生需要平衡患者生存和资源利用等相互冲突的目标。传统的单目标强化学习方法由于其固定的奖励函数而导致策略僵化，无法适应不断变化的临床优先级。因此，需要一种能够针对不同临床优先级动态调整策略的方法。

Method: 这篇论文基准测试了三种离线多目标强化学习算法（Conditioned Conservative Pareto Q-Learning (CPQL)、Adaptive CPQL和改进的Pareto Efficient Decision Agent (PEDA) Decision Transformer (PEDA DT)）与三种标量化单目标基线（BC、CQL和DDQN）在MIMIC-IV数据集上的性能。研究使用离策略评估（OPE）指标来评估算法的有效性。

Result: 研究结果表明，PEDA DT算法相比静态标量化基线提供了更优越的灵活性。此外，研究还扩展了先前关于医疗保健领域单目标决策Transformer的发现，证实了序列建模架构在扩展到多目标条件生成时仍然保持鲁棒和有效。

Conclusion: 离线多目标强化学习是一个很有前途的框架，可以在重症监护中实现个性化、可调整的决策，而无需重新训练。PEDA DT算法在MIMIC-IV数据集上表现出优越的灵活性，证实了序列建模架构在多目标条件生成方面的鲁棒性和有效性。训练。

Abstract: In critical care settings such as the Intensive Care Unit, clinicians face the complex challenge of balancing conflicting objectives, primarily maximizing patient survival while minimizing resource utilization (e.g., length of stay). Single-objective Reinforcement Learning approaches typically address this by optimizing a fixed scalarized reward function, resulting in rigid policies that fail to adapt to varying clinical priorities. Multi-objective Reinforcement Learning (MORL) offers a solution by learning a set of optimal policies along the Pareto Frontier, allowing for dynamic preference selection at test time. However, applying MORL in healthcare necessitates strict offline learning from historical data.
  In this paper, we benchmark three offline MORL algorithms, Conditioned Conservative Pareto Q-Learning (CPQL), Adaptive CPQL, and a modified Pareto Efficient Decision Agent (PEDA) Decision Transformer (PEDA DT), against three scalarized single-objective baselines (BC, CQL, and DDQN) on the MIMIC-IV dataset. Using Off-Policy Evaluation (OPE) metrics, we demonstrate that PEDA DT algorithm offers superior flexibility compared to static scalarized baselines. Notably, our results extend previous findings on single-objective Decision Transformers in healthcare, confirming that sequence modeling architectures remain robust and effective when scaled to multi-objective conditioned generation. These findings suggest that offline MORL is a promising framework for enabling personalized, adjustable decision-making in critical care without the need for retraining.

</details>


### [43] [CLARITY: Medical World Model for Guiding Treatment Decisions by Modeling Context-Aware Disease Trajectories in Latent Space](https://arxiv.org/abs/2512.08029)
*Tianxingjian Ding,Yuanhao Zou,Chen Chen,Mubarak Shah,Yu Tian*

Main category: cs.LG

TL;DR: CLARITY是一个医疗世界模型，它通过整合时间和患者特定数据来预测疾病演变，生成个性化治疗方案，并在治疗规划方面表现出最先进的性能。


<details>
  <summary>Details</summary>
Motivation: 目前的静态AI预测器无法预测动态疾病演变，现有医疗世界模型也存在局限性，例如忽视患者特定的时间和临床背景，以及缺乏将预测与治疗决策联系起来的反馈机制。

Method: CLARITY模型将疾病演变直接建模在结构化潜在空间中，明确整合时间间隔和患者特定数据，将治疗条件下的疾病进展建模为平滑、可解释的轨迹。它还引入了新颖的预测到决策框架，将潜在的推演转化为透明、可操作的建议。

Result: 在MU-Glioma-Post数据集上，CLARITY的性能比最新的MeWM高出12%，并且显著优于所有其他医疗领域的大型语言模型，在治疗规划方面表现出最先进的性能。

Conclusion: CLARITY通过其独特的模型设计和预测到决策框架，有效地解决了现有医疗AI预测的局限性，在肿瘤治疗规划中展示出卓越的潜力。

Abstract: Clinical decision-making in oncology requires predicting dynamic disease evolution, a task current static AI predictors cannot perform. While world models (WMs) offer a paradigm for generative prediction, existing medical applications remain limited. Existing methods often rely on stochastic diffusion models, focusing on visual reconstruction rather than causal, physiological transitions. Furthermore, in medical domain, models like MeWM typically ignore patient-specific temporal and clinical contexts and lack a feedback mechanism to link predictions to treatment decisions. To address these gaps, we introduce CLARITY, a medical world model that forecasts disease evolution directly within a structured latent space. It explicitly integrates time intervals (temporal context) and patient-specific data (clinical context) to model treatment-conditioned progression as a smooth, interpretable trajectory, and thus generate physiologically faithful, individualized treatment plans. Finally, CLARITY introduces a novel prediction-to-decision framework, translating latent rollouts into transparent, actionable recommendations. CLARITY demonstrates state-of-the-art performance in treatment planning. On the MU-Glioma-Post dataset, our approach outperforms recent MeWM by 12\%, and significantly surpasses all other medical-specific large language models.

</details>


### [44] [Deep Kernel Aalen-Johansen Estimator: An Interpretable and Flexible Neural Net Framework for Competing Risks](https://arxiv.org/abs/2512.08063)
*Xiaobin Shen,George H. Chen*

Main category: cs.LG

TL;DR: 该文章提出了一个名为DKAJ的深度竞争风险模型，它通过结合集群权重和核函数来泛化Aalen-Johansen估计器，并在保持竞争力的同时提供模型解释的可视化。


<details>
  <summary>Details</summary>
Motivation: 现有的竞争风险模型可能缺乏可解释性，该研究旨在通过引入DKAJ模型来解决这个问题，该模型能够提供直观的可视化，以帮助理解模型的预测。

Method: DKAJ模型将每个数据点表示为集群的加权组合。这些权重来自一个自动学习的核函数，该函数衡量任意两个数据点之间的相似性。如果一个数据点仅对一个集群具有非零权重，则其预测的CIFs与仅限于该集群数据点的经典Aalen-Johansen估计器相对应。

Result: 在四个标准的竞争风险数据集上，DKAJ模型与最先进的基线模型相比具有竞争力。

Conclusion: DKAJ模型不仅能够提供与最新模型相当的性能，而且还通过可视化促进了模型解释，这在理解复杂预测方面具有优势。

Abstract: We propose an interpretable deep competing risks model called the Deep Kernel Aalen-Johansen (DKAJ) estimator, which generalizes the classical Aalen-Johansen nonparametric estimate of cumulative incidence functions (CIFs). Each data point (e.g., patient) is represented as a weighted combination of clusters. If a data point has nonzero weight only for one cluster, then its predicted CIFs correspond to those of the classical Aalen-Johansen estimator restricted to data points from that cluster. These weights come from an automatically learned kernel function that measures how similar any two data points are. On four standard competing risks datasets, we show that DKAJ is competitive with state-of-the-art baselines while being able to provide visualizations to assist model interpretation.

</details>


### [45] [CAMO: Causality-Guided Adversarial Multimodal Domain Generalization for Crisis Classification](https://arxiv.org/abs/2512.08071)
*Pingchuan Ma,Chengshuai Zhao,Bohan Jiang,Saketh Vishnubhatla,Ujun Jeong,Alimohammad Beigi,Adrienne Raglin,Huan Liu*

Main category: cs.LG

TL;DR: 该文章提出了一种名为MMDG的框架，通过结合对抗性解缠和统一表示学习，以解决社交媒体危机分类中跨未知灾害类型泛化能力差的问题。


<details>
  <summary>Details</summary>
Motivation: 在社交媒体中进行危机分类目前面临着一个挑战，即在未见过灾害类型时的泛化能力差。主要原因是现有方法未能解缠虚假特征与因果特征，且未能对齐异构模态表示。

Method: 本文提出了一个因果关系引导的多模态领域泛化（MMDG）框架。该框架结合了对抗性解缠和统一表示学习，具体而言，对抗性目标鼓励模型解缠并关注领域不变的因果特征，而统一表示则对齐了来自不同模态的特征，使其在共享的潜在空间中。

Result: 在不同数据集上的实验表明，该方法在未见过的灾害场景中取得了最佳的性能。

Conclusion: MMDG框架通过其独特的方法，在提高社交媒体危机分类在未知灾害类型上的泛化能力方面取得了显著进展，为未来的研究奠定了基础。

Abstract: Crisis classification in social media aims to extract actionable disaster-related information from multimodal posts, which is a crucial task for enhancing situational awareness and facilitating timely emergency responses. However, the wide variation in crisis types makes achieving generalizable performance across unseen disasters a persistent challenge. Existing approaches primarily leverage deep learning to fuse textual and visual cues for crisis classification, achieving numerically plausible results under in-domain settings. However, they exhibit poor generalization across unseen crisis types because they 1. do not disentangle spurious and causal features, resulting in performance degradation under domain shift, and 2. fail to align heterogeneous modality representations within a shared space, which hinders the direct adaptation of established single-modality domain generalization (DG) techniques to the multimodal setting. To address these issues, we introduce a causality-guided multimodal domain generalization (MMDG) framework that combines adversarial disentanglement with unified representation learning for crisis classification. The adversarial objective encourages the model to disentangle and focus on domain-invariant causal features, leading to more generalizable classifications grounded in stable causal mechanisms. The unified representation aligns features from different modalities within a shared latent space, enabling single-modality DG strategies to be seamlessly extended to multimodal learning. Experiments on the different datasets demonstrate that our approach achieves the best performance in unseen disaster scenarios.

</details>


### [46] [Training LLMs for Honesty via Confessions](https://arxiv.org/abs/2512.08093)
*Manas Joglekar,Jeremy Chen,Gabriel Wu,Jason Yosinski,Jasmine Wang,Boaz Barak,Amelia Glaese*

Main category: cs.LG

TL;DR: 本文提出了一种通过“自白”来诱导大型语言模型（LLM）诚实表达自身缺点的方法，并论证了其在提高模型坦诚度方面的有效性。


<details>
  <summary>Details</summary>
Motivation: 大型语言模型在报告其行为和信念时可能会不诚实，这可能源于强化学习中奖励塑造的挑战，导致训练过程无意中激励模型说谎或歪曲事实。

Method: 提出了一种通过模型“自白”来诱导LLM诚实表达自身缺点的方法。自白是模型在给出原始答案后，应请求提供的输出，旨在全面说明模型对策略和指令的遵守情况。自白在训练中的奖励仅基于其诚实性，不影响主答案的奖励。这种机制激励模型在自白中坦诚地揭示不当行为。

Result: 在GPT-5-Thinking模型上进行了训练和评估。模型在出现不当行为时，通常能诚实地自白，并且自白的诚实度随着训练适度提高。

Conclusion: “自白”机制可以有效地提高大型语言模型的坦诚度，并为模型监控、拒绝采样和向用户呈现问题提供了可能。

Abstract: Large language models (LLMs) can be dishonest when reporting on their actions and beliefs -- for example, they may overstate their confidence in factual claims or cover up evidence of covert actions. Such dishonesty may arise due to the effects of reinforcement learning (RL), where challenges with reward shaping can result in a training process that inadvertently incentivizes the model to lie or misrepresent its actions.
  In this work we propose a method for eliciting an honest expression of an LLM's shortcomings via a self-reported *confession*. A confession is an output, provided upon request after a model's original answer, that is meant to serve as a full account of the model's compliance with the letter and spirit of its policies and instructions. The reward assigned to a confession during training is solely based on its honesty, and does not impact positively or negatively the main answer's reward. As long as the "path of least resistance" for maximizing confession reward is to surface misbehavior rather than covering it up, this incentivizes models to be honest in their confessions. Our findings provide some justification this empirical assumption, especially in the case of egregious model misbehavior.
  To demonstrate the viability of our approach, we train GPT-5-Thinking to produce confessions, and we evaluate its honesty in out-of-distribution scenarios measuring hallucination, instruction following, scheming, and reward hacking. We find that when the model lies or omits shortcomings in its "main" answer, it often confesses to these behaviors honestly, and this confession honesty modestly improves with training. Confessions can enable a number of inference-time interventions including monitoring, rejection sampling, and surfacing issues to the user.

</details>


### [47] [Scalable Offline Model-Based RL with Action Chunks](https://arxiv.org/abs/2512.08108)
*Kwanyoung Park,Seohong Park,Youngwoon Lee,Sergey Levine*

Main category: cs.LG

TL;DR: 本文探讨了模型量化在离线强化学习中处理复杂且需要长期规划的任务方面的可扩展性。


<details>
  <summary>Details</summary>
Motivation: 研究模型量化是否能为离线强化学习中复杂、长期任务提供可扩展的解决方案，并解决模型误差累积与长期预测退化之间的权衡问题。

Method: 本文提出了一种“动作块”模型，该模型通过一系列动作来预测未来状态，以此减少复合误差。同时，本文还引入了拒绝采样机制，以防止模型利用超出分布的动作。该方法被称为“基于动作块的模型深度强化学习”（MAC）。

Result: 在具有大规模数据集（高达1亿次转换）的挑战性任务上的实验表明，MAC在离线模型量化算法中取得了最佳性能，尤其是在具有挑战性的长期任务中。

Conclusion: MAC通过引入动作块模型和拒绝采样，有效解决了模型误差累积和外部分布动作利用问题，在复杂、长期离线强化学习任务中表现出色。

Abstract: In this paper, we study whether model-based reinforcement learning (RL), in particular model-based value expansion, can provide a scalable recipe for tackling complex, long-horizon tasks in offline RL. Model-based value expansion fits an on-policy value function using length-n imaginary rollouts generated by the current policy and a learned dynamics model. While larger n reduces bias in value bootstrapping, it amplifies accumulated model errors over long horizons, degrading future predictions. We address this trade-off with an \emph{action-chunk} model that predicts a future state from a sequence of actions (an "action chunk") instead of a single action, which reduces compounding errors. In addition, instead of directly training a policy to maximize rewards, we employ rejection sampling from an expressive behavioral action-chunk policy, which prevents model exploitation from out-of-distribution actions. We call this recipe \textbf{Model-Based RL with Action Chunks (MAC)}. Through experiments on highly challenging tasks with large-scale datasets of up to 100M transitions, we show that MAC achieves the best performance among offline model-based RL algorithms, especially on challenging long-horizon tasks.

</details>


### [48] [Improving the Sensitivity of Backdoor Detectors via Class Subspace Orthogonalization](https://arxiv.org/abs/2512.08129)
*Guangmingmei Yang,David J. Miller,George Kesidis*

Main category: cs.LG

TL;DR: 该文章提出了一种名为CSO的后门检测方法，它通过抑制固有的类别特征来提高检测灵敏度，解决了传统方法在某些情况下失效的问题。


<details>
  <summary>Details</summary>
Motivation: 传统的后训练后门检测方法依赖于受攻击模型表现出极端异常的检测统计数据，但这在某些情况下可能会失败，例如某些非目标类别容易区分，或者后门本身很微弱。

Method: CSO方法通过建立一个约束优化问题，利用少量干净样本，并在优化检测统计数据时，对类别的固有特征进行正交化，从而抑制固有特征，突出后门触发器的贡献。

Result: 通过对具有挑战性的混合标签攻击和自适应攻击进行评估，CSO方法展现了有效性。

Conclusion: CSO通过抑制固有特征，使得后门检测器对目标类别的检测更加敏感，解决了传统方法在某些场景下的局限性，提高了后门检测的准确性。

Abstract: Most post-training backdoor detection methods rely on attacked models exhibiting extreme outlier detection statistics for the target class of an attack, compared to non-target classes. However, these approaches may fail: (1) when some (non-target) classes are easily discriminable from all others, in which case they may naturally achieve extreme detection statistics (e.g., decision confidence); and (2) when the backdoor is subtle, i.e., with its features weak relative to intrinsic class-discriminative features. A key observation is that the backdoor target class has contributions to its detection statistic from both the backdoor trigger and from its intrinsic features, whereas non-target classes only have contributions from their intrinsic features. To achieve more sensitive detectors, we thus propose to suppress intrinsic features while optimizing the detection statistic for a given class. For non-target classes, such suppression will drastically reduce the achievable statistic, whereas for the target class the (significant) contribution from the backdoor trigger remains. In practice, we formulate a constrained optimization problem, leveraging a small set of clean examples from a given class, and optimizing the detection statistic while orthogonalizing with respect to the class's intrinsic features. We dub this plug-and-play approach Class Subspace Orthogonalization (CSO) and assess it against challenging mixed-label and adaptive attacks.

</details>


### [49] [Biothreat Benchmark Generation Framework for Evaluating Frontier AI Models I: The Task-Query Architecture](https://arxiv.org/abs/2512.08130)
*Gary Ackerman,Brandon Behlendorf,Zachary Kallenborn,Sheriff Almakki,Doug Clifford,Jenna LaTourette,Hayley Peterson,Noah Sheinbaum,Olivia Shoemaker,Anna Wetzel*

Main category: cs.LG

TL;DR: 该论文介绍了一种名为“生物威胁基准生成（BBG）框架”的新方法，旨在量化和减轻大型语言模型（LLMs）可能带来的生物安全风险，特别是生物恐怖主义或获取生物武器的风险。


<details>
  <summary>Details</summary>
Motivation: 模型开发者和政策制定者都希望量化和减轻快速发展的前沿人工智能模型（特别是大型语言模型LLMs）可能带来的生物恐怖主义或获取生物武器的风险。

Method: BBG框架通过一个生物威胁类别、元素和任务的层级结构，构建了任务对齐的查询。目前，该框架主要针对细菌生物威胁。未来的研究将把查询转化为模型提示，并用于模型评估。

Result: 该论文描述了BBG框架的第一个组成部分，即“细菌生物威胁图式”的开发，它提供了一个稳健、可重用的结构，用于评估LLMs在细菌生物风险方面的表现，并考虑了不同行为者能力水平和操作风险因素。

Conclusion: BBG框架及其细菌生物威胁图式为评估LLMs可能带来的细菌生物风险提供了一个全面的方法，它考虑了技术和操作要求以及生物威胁行为者的能力，以期实现可靠的风险衡量和评估。

Abstract: Both model developers and policymakers seek to quantify and mitigate the risk of rapidly-evolving frontier artificial intelligence (AI) models, especially large language models (LLMs), to facilitate bioterrorism or access to biological weapons. An important element of such efforts is the development of model benchmarks that can assess the biosecurity risk posed by a particular model. This paper describes the first component of a novel Biothreat Benchmark Generation (BBG) Framework. The BBG approach is designed to help model developers and evaluators reliably measure and assess the biosecurity risk uplift and general harm potential of existing and future AI models, while accounting for key aspects of the threat itself that are often overlooked in other benchmarking efforts, including different actor capability levels, and operational (in addition to purely technical) risk factors. As a pilot, the BBG is first being developed to address bacterial biological threats only. The BBG is built upon a hierarchical structure of biothreat categories, elements and tasks, which then serves as the basis for the development of task-aligned queries. This paper outlines the development of this biothreat task-query architecture, which we have named the Bacterial Biothreat Schema, while future papers will describe follow-on efforts to turn queries into model prompts, as well as how the resulting benchmarks can be implemented for model evaluation. Overall, the BBG Framework, including the Bacterial Biothreat Schema, seeks to offer a robust, re-usable structure for evaluating bacterial biological risks arising from LLMs across multiple levels of aggregation, which captures the full scope of technical and operational requirements for biological adversaries, and which accounts for a wide spectrum of biological adversary capabilities.

</details>


### [50] [Robust Agents in Open-Ended Worlds](https://arxiv.org/abs/2512.08139)
*Mikayel Samvelyan*

Main category: cs.LG

TL;DR: 该论文旨在通过结合开放式学习和多智能体学习方法，训练和评估能够泛化到新颖环境、对抗性输入和其他智能体交互的鲁棒AI智能体。


<details>
  <summary>Details</summary>
Motivation: 在不断变化的开放式世界中，AI智能体需要具备鲁棒性，不仅在训练环境中表现出色，还能有效地泛化到以前未见和多变的情况。

Method: 1. 引入MiniHack（一个基于NetHack的沙盒框架），通过程序化内容生成创建多样化环境，以构建新的强化学习任务，侧重于泛化。 2. 提出了Maestro（一种生成对抗性课程的新方法），逐步增强强化学习智能体在双人零和游戏中的鲁棒性和通用性。 3. 在多智能体领域，利用质量-多样性方法，系统地识别了复杂足球视频游戏中预训练强化学习策略的脆弱性。 4. 将鲁棒性探索扩展到大型语言模型（LLM）领域，通过进化搜索生成多样化的有效输入，旨在引出LLMs的不良输出，以诊断和增强LLM对对抗性提示的鲁棒性。

Result: 通过上述方法，研究旨在开发出能够适应不断发展的世界并在不可预见的挑战和交互中表现出色的AI智能体。

Conclusion: 该工作为AI鲁棒性未来的发展铺平了道路，使AI智能体不仅能适应不断变化的世界，而且能在不可预见的挑战和交互中茁壮成长。

Abstract: The growing prevalence of artificial intelligence (AI) in various applications underscores the need for agents that can successfully navigate and adapt to an ever-changing, open-ended world. A key challenge is ensuring these AI agents are robust, excelling not only in familiar settings observed during training but also effectively generalising to previously unseen and varied scenarios. In this thesis, we harness methodologies from open-endedness and multi-agent learning to train and evaluate robust AI agents capable of generalising to novel environments, out-of-distribution inputs, and interactions with other co-player agents. We begin by introducing MiniHack, a sandbox framework for creating diverse environments through procedural content generation. Based on the game of NetHack, MiniHack enables the construction of new tasks for reinforcement learning (RL) agents with a focus on generalisation. We then present Maestro, a novel approach for generating adversarial curricula that progressively enhance the robustness and generality of RL agents in two-player zero-sum games. We further probe robustness in multi-agent domains, utilising quality-diversity methods to systematically identify vulnerabilities in state-of-the-art, pre-trained RL policies within the complex video game football domain, characterised by intertwined cooperative and competitive dynamics. Finally, we extend our exploration of robustness to the domain of LLMs. Here, our focus is on diagnosing and enhancing the robustness of LLMs against adversarial prompts, employing evolutionary search to generate a diverse range of effective inputs that aim to elicit undesirable outputs from an LLM. This work collectively paves the way for future advancements in AI robustness, enabling the development of agents that not only adapt to an ever-evolving world but also thrive in the face of unforeseen challenges and interactions.

</details>


### [51] [PolyLingua: Margin-based Inter-class Transformer for Robust Cross-domain Language Detection](https://arxiv.org/abs/2512.08143)
*Ali Lotfi Rezaabad,Bikram Khanal,Shashwat Chaurasia,Lu Zeng,Dezhi Hong,Hossein Beshashati,Thomas Butler,Megan Ganji*

Main category: cs.LG

TL;DR: PolyLingua是一个轻量级Transformer模型，用于域内语言检测和细粒度语言分类，它采用两级对比学习框架，实现了高准确性和效率，特别适用于计算和延迟受限的环境。


<details>
  <summary>Details</summary>
Motivation: 现有的语言识别工具在处理歌曲请求等特定场景时存在困难，开源工具速度快但不准确，大型语言模型准确但成本高昂，不适用于低延迟或低资源环境。

Method: PolyLingua是一个基于Transformer的轻量级模型，它采用两级对比学习框架，结合实例级分离和类级对齐，并使用自适应边距，从而生成紧凑且分离良好的嵌入。

Result: 在Amazon Massive和Song数据集上，PolyLingua分别达到了99.25% F1和98.15% F1的准确率，超越了Sonnet 3.5，并且参数量减少了10倍。

Conclusion: PolyLingua以其高准确性、低延迟和低资源需求，为多语言系统中的语言识别提供了一个理想的解决方案，特别适用于计算和延迟受限的环境。

Abstract: Language identification is a crucial first step in multilingual systems such as chatbots and virtual assistants, enabling linguistically and culturally accurate user experiences. Errors at this stage can cascade into downstream failures, setting a high bar for accuracy. Yet, existing language identification tools struggle with key cases--such as music requests where the song title and user language differ. Open-source tools like LangDetect, FastText are fast but less accurate, while large language models, though effective, are often too costly for low-latency or low-resource settings. We introduce PolyLingua, a lightweight Transformer-based model for in-domain language detection and fine-grained language classification. It employs a two-level contrastive learning framework combining instance-level separation and class-level alignment with adaptive margins, yielding compact and well-separated embeddings even for closely related languages. Evaluated on two challenging datasets--Amazon Massive (multilingual digital assistant utterances) and a Song dataset (music requests with frequent code-switching)--PolyLingua achieves 99.25% F1 and 98.15% F1, respectively, surpassing Sonnet 3.5 while using 10x fewer parameters, making it ideal for compute- and latency-constrained environments.

</details>


### [52] [TreeGRPO: Tree-Advantage GRPO for Online RL Post-Training of Diffusion Models](https://arxiv.org/abs/2512.08153)
*Zheng Ding,Weirui Ye*

Main category: cs.LG

TL;DR: TreeGRPO是一个新颖的强化学习框架。该框架通过新的树形结构提高了训练效率，同时实现了更高的样本效率和更细粒度的信用分配。


<details>
  <summary>Details</summary>
Motivation: 强化学习（RL）后训练对于使生成模型与人类偏好对齐至关重要，但其高昂的计算成本仍然是广泛采用的主要障碍。

Method: 本文引入了TreeGRPO，一个新颖的强化学习框架。它将去噪过程重铸为搜索树，从共享的初始噪声样本中，TreeGRPO策略性地分支生成多个候选轨迹，同时有效地重用它们的共同前缀。

Result: TreeGRPO实现了2.4倍的训练速度提升，并在效率-奖励权衡空间中建立了卓越的帕累托前沿。该方法在多个基准和奖励模型上始终优于GRPO基线。

Conclusion: TreeGRPO提供了一种可扩展且有效的途径，用于基于RL的视觉生成模型对齐。

Abstract: Reinforcement learning (RL) post-training is crucial for aligning generative models with human preferences, but its prohibitive computational cost remains a major barrier to widespread adoption. We introduce \textbf{TreeGRPO}, a novel RL framework that dramatically improves training efficiency by recasting the denoising process as a search tree. From shared initial noise samples, TreeGRPO strategically branches to generate multiple candidate trajectories while efficiently reusing their common prefixes. This tree-structured approach delivers three key advantages: (1) \emph{High sample efficiency}, achieving better performance under same training samples (2) \emph{Fine-grained credit assignment} via reward backpropagation that computes step-specific advantages, overcoming the uniform credit assignment limitation of trajectory-based methods, and (3) \emph{Amortized computation} where multi-child branching enables multiple policy updates per forward pass. Extensive experiments on both diffusion and flow-based models demonstrate that TreeGRPO achieves \textbf{2.4$\times$ faster training} while establishing a superior Pareto frontier in the efficiency-reward trade-off space. Our method consistently outperforms GRPO baselines across multiple benchmarks and reward models, providing a scalable and effective pathway for RL-based visual generative model alignment. The project website is available at treegrpo.github.io.

</details>


### [53] [MobileFineTuner: A Unified End-to-End Framework for Fine-Tuning LLMs on Mobile Phones](https://arxiv.org/abs/2512.08211)
*Jiaxiang Geng,Lunyu Zhao,Yiyi Lu,Bing Luo*

Main category: cs.LG

TL;DR: 本文介绍了MobileFineTuner，一个开源框架，可以在普通手机上直接进行端到端的大型语言模型（LLM）微调，解决了现有方法在移动设备上微调LLM的不足。


<details>
  <summary>Details</summary>
Motivation: 探索在移动设备上利用用户隐私数据进行LLM微调，以应对高质量公共数据日益枯竭的挑战。现有的方法主要基于模拟或面向物联网设备和个人电脑，但很少关注普通手机。

Method: MobileFineTuner是一个开源框架，支持全参数微调（Full-FT）和参数高效微调（PEFT）。它引入了系统级优化来解决手机内存和能耗限制，包括参数分片、梯度累积和能耗感知计算调度。

Result: MobileFineTuner在实际手机上成功微调了GPT-2、Gemma 3和Qwen 2.5等LLM。大量的实验和消融研究证实了所提出优化的有效性。

Conclusion: MobileFineTuner为未来在移动设备上训练LLM的研究奠定了可行的基础。

Abstract: Mobile phones are the most ubiquitous end devices, generating vast amounts of human-authored data and serving as the primary platform for end-side applications. As high-quality public data for large language models (LLMs) approaches exhaustion, on-device fine-tuning provides an opportunity to leverage private user data while preserving privacy. However, existing approaches are predominantly simulation-based or rely on IoT devices and PCs, leaving commodity mobile phones largely unexplored. A key gap is the absence of an open-source framework that enables practical LLM fine-tuning on mobile phones. We present MobileFineTuner, a unified open-source framework that enables end-to-end LLM fine-tuning directly on commodity mobile phones. MobileFineTuner is designed for efficiency, scalability, and usability, supporting full-parameters fine-tuning (Full-FT) and parameter-efficient fine-tuning (PEFT). To address the memory and energy limitations inherent to mobile phones, we introduce system-level optimizations including parameter sharding, gradient accumulation, and energy-aware computation scheduling. We demonstrate the practicality of MobileFineTuner by fine-tuning GPT-2, Gemma 3, and Qwen 2.5 on real mobile phones. Extensive experiments and ablation studies validate the effectiveness of the proposed optimizations and establish MobileFineTuner as a viable foundation for future research on on-device LLM training.

</details>


### [54] [Correction of Decoupled Weight Decay](https://arxiv.org/abs/2512.08217)
*Jason Chuan-Chih Chou*

Main category: cs.LG

TL;DR: 该研究挑战了 AdamW 中解耦权重衰减与学习率的比例关系，并提出衰减应与学习率的平方成正比，以稳定权重范数并优化训练过程。


<details>
  <summary>Details</summary>
Motivation: 传统的 AdamW 优化器中，解耦权重衰减一直被设置为与学习率呈线性关系，但近期有研究者对其提出了质疑，认为其应与学习率的平方成正比。该论文旨在重新审视这一假设，并探究更优的权重衰减设置。

Method: 本研究通过理论推导，证明在更新独立于权重的情况下，解耦权重衰减与学习率的平方成正比可以稳定权重范数。基于相同假设，论文还推导并验证了 Scion 优化器下 minibatch 的总更新贡献，并发现其与动量相关的有效学习率更好地相关。

Result: 研究发现，解耦权重衰减与学习率的平方成正比不仅能够稳定权重和梯度范数，还能更好地控制训练动态，并最终提高模型性能。这与以往认为的线性关系不同。

Conclusion: 本研究认为，解耦权重衰减应与学习率的平方成正比，而非线性关系。这一改进能有效稳定训练过程，并提升模型性能，为优化器设计提供了新的方向。

Abstract: Decoupled weight decay, solely responsible for the performance advantage of AdamW over Adam, has long been set to proportional to learning rate $γ$ without questioning. Some researchers have recently challenged such assumption and argued that decoupled weight decay should be set $\propto γ^2$ instead based on orthogonality arguments at steady state. To the contrary, we find that eliminating the contribution of the perpendicular component of the update to the weight norm leads to little change to the training dynamics. Instead, we derive that decoupled weight decay $\propto γ^2$ results in stable weight norm based on the simple assumption that updates become independent of the weights at steady state, regardless of the nature of the optimizer. Based on the same assumption, we derive and empirically verify that the Total Update Contribution (TUC) of a minibatch under the Scion optimizer is better characterized by the momentum-dependent effective learning rate whose optimal value transfers and we show that decoupled weight decay $\propto γ^2$ leads to stable weight and gradient norms and allows us to better control the training dynamics and improve the model performance.

</details>


### [55] [SPROCKET: Extending ROCKET to Distance-Based Time-Series Transformations With Prototypes](https://arxiv.org/abs/2512.08246)
*Nicholas Harner*

Main category: cs.LG

TL;DR: SPROCKET（Selected Prototype Random Convolutional Kernel Transform）是一种基于原型的新型时间序列分类特征工程策略，在UCR和UEA数据集上表现出色，与现有卷积算法相当，与MR-HY-SP集成模型的平均准确度排名超过了HYDRA-MR，证明了原型特征转换在时间序列分类中的潜力。


<details>
  <summary>Details</summary>
Motivation: 经典时间序列分类算法严重依赖特征工程策略，而现有的随机核特征转换（如ROCKET）表现出强大的性能。本研究旨在通过引入基于原型的新型特征工程策略来进一步提升时间序列分类的准确性和鲁棒性。

Method: 本研究提出SROCKET（Selected Prototype Random Convolutional Kernel Transform）算法。该算法采用基于原型的方法进行特征工程，通过选择合适的原型来生成随机卷积核特征。此外，还构建了MR-HY-SP ( MultiROCKET-HYDRA-SPROCKET)集成模型，以进一步提升分类性能。

Result: SPROCKET在大多数UCR和UEA时间序列分类数据集上的性能与现有卷积算法相当。MR-HY-SP集成模型的平均准确度排名超过了HYDRA-MR（之前表现最佳的卷积集成模型）。

Conclusion: 原型化特征转换策略能够有效增强时间序列分类的准确性和鲁棒性。SPROCKET及其集成模型MR-HY-SP在时间序列分类任务中表现出优越的性能，为未来的相关研究提供了新的方向。

Abstract: Classical Time Series Classification algorithms are dominated by feature engineering strategies. One of the most prominent of these transforms is ROCKET, which achieves strong performance through random kernel features. We introduce SPROCKET (Selected Prototype Random Convolutional Kernel Transform), which implements a new feature engineering strategy based on prototypes. On a majority of the UCR and UEA Time Series Classification archives, SPROCKET achieves performance comparable to existing convolutional algorithms and the new MR-HY-SP ( MultiROCKET-HYDRA-SPROCKET) ensemble's average accuracy ranking exceeds HYDRA-MR, the previous best convolutional ensemble's performance. These experimental results demonstrate that prototype-based feature transformation can enhance both accuracy and robustness in time series classification.

</details>


### [56] [Geometric-Stochastic Multimodal Deep Learning for Predictive Modeling of SUDEP and Stroke Vulnerability](https://arxiv.org/abs/2512.08257)
*Preksha Girish,Rachana Mysore,Mahanthesha U,Shrey Kumar,Misbah Fatimah Annigeri,Tanish Jain*

Main category: cs.LG

TL;DR: 该研究提出了一个统一的几何随机多模态深度学习框架，用于整合多种生理信号，以建模癫痫猝死（SUDEP）和急性缺血性中风的脆弱性。


<details>
  <summary>Details</summary>
Motivation: 探索SUDEP和急性缺血性中风这些涉及皮质、脑干和自主系统之间复杂相互作用的危及生命的疾病，并为它们提供一个统一的建模框架。

Method: 该框架结合了黎曼流形嵌入、李群不变特征表示、分数随机动力学、哈密顿能量流建模和跨模态注意力机制。中风传播通过结构脑图上的分数流行病扩散进行建模。

Result: 在MULTI-CLARID数据集上的实验表明，该框架提高了预测准确性，并能从流形曲率、分数记忆指数、注意力熵和扩散中心性中提取可解释的生物标志物。

Conclusion: 该框架为神经自主疾病的早期检测、风险分层和可解释的多模态建模提供了一个数学上原则性的基础。

Abstract: Sudden Unexpected Death in Epilepsy (SUDEP) and acute ischemic stroke are life-threatening conditions involving complex interactions across cortical, brainstem, and autonomic systems. We present a unified geometric-stochastic multimodal deep learning framework that integrates EEG, ECG, respiration, SpO2, EMG, and fMRI signals to model SUDEP and stroke vulnerability. The approach combines Riemannian manifold embeddings, Lie-group invariant feature representations, fractional stochastic dynamics, Hamiltonian energy-flow modeling, and cross-modal attention mechanisms. Stroke propagation is modeled using fractional epidemic diffusion over structural brain graphs. Experiments on the MULTI-CLARID dataset demonstrate improved predictive accuracy and interpretable biomarkers derived from manifold curvature, fractional memory indices, attention entropy, and diffusion centrality. The proposed framework provides a mathematically principled foundation for early detection, risk stratification, and interpretable multimodal modeling in neural-autonomic disorders.

</details>


### [57] [gHAWK: Local and Global Structure Encoding for Scalable Training of Graph Neural Networks on Knowledge Graphs](https://arxiv.org/abs/2512.08274)
*Humera Sabir,Fatima Farooq,Ashraf Aboulnaga*

Main category: cs.LG

TL;DR: gHAWK是一个可扩展的GNN训练框架，它通过预计算结构特征（布隆过滤器和TransE嵌入）来解决现有GNN在大型知识图谱上扩展性差的问题，从而提高效率、加速收敛并提升模型精度。


<details>
  <summary>Details</summary>
Motivation: 现有的消息传递GNN在大型知识图谱上难以扩展，因为它们依赖迭代消息传递，这在mini-batch训练中效率低下，节点只能看到其邻域的部分视图。

Method: gHAWK在GNN训练开始前为每个节点预计算结构特征，捕获其局部和全局结构。它引入了一个预处理步骤来计算：(a) 布隆过滤器以紧凑编码局部邻域结构，以及 (b) TransE嵌入以表示每个节点在图中的全局位置。这些特征与领域特定特征融合，生成一个节点特征向量，可用于任何GNN技术。

Result: gHAWK显著减少了内存使用，加速了收敛，并提高了模型精度。它在Open Graph Benchmark (OGB)的大型数据集上达到了最先进的精度和更低的训练时间，并在三个图的OGB排行榜上名列前茅。

Conclusion: gHAWK通过预计算结构特征，为大型知识图谱提供了一个可扩展且高效的GNN训练框架，解决了现有GNN的扩展性问题，并提升了性能。

Abstract: Knowledge Graphs (KGs) are a rich source of structured, heterogeneous data, powering a wide range of applications. A common approach to leverage this data is to train a graph neural network (GNN) on the KG. However, existing message-passing GNNs struggle to scale to large KGs because they rely on the iterative message passing process to learn the graph structure, which is inefficient, especially under mini-batch training, where a node sees only a partial view of its neighborhood. In this paper, we address this problem and present gHAWK, a novel and scalable GNN training framework for large KGs. The key idea is to precompute structural features for each node that capture its local and global structure before GNN training even begins. Specifically, gHAWK introduces a preprocessing step that computes: (a)~Bloom filters to compactly encode local neighborhood structure, and (b)~TransE embeddings to represent each node's global position in the graph. These features are then fused with any domain-specific features (e.g., text embeddings), producing a node feature vector that can be incorporated into any GNN technique. By augmenting message-passing training with structural priors, gHAWK significantly reduces memory usage, accelerates convergence, and improves model accuracy. Extensive experiments on large datasets from the Open Graph Benchmark (OGB) demonstrate that gHAWK achieves state-of-the-art accuracy and lower training time on both node property prediction and link prediction tasks, topping the OGB leaderboard for three graphs.

</details>


### [58] [Jacobian Aligned Random Forests](https://arxiv.org/abs/2512.08306)
*Sarwesh Rauniyar*

Main category: cs.LG

TL;DR: 该论文提出了JARF，一种雅可比对齐随机森林方法，通过全局线性预处理器，在保留轴对齐森林优点的同时，有效地处理了倾斜决策边界和特征交互，在表格数据任务上表现优异。


<details>
  <summary>Details</summary>
Motivation: 传统的轴对齐决策树在处理具有旋转或交互依赖决策边界的数据集时表现不佳，因为它们需要更复杂的特征组合而不是单一特征阈值。虽然倾斜森林可以通过每个节点超平面分裂来解决这个问题，但这会增加计算成本和实现复杂性。

Method: JARF首先拟合一个轴对齐森林来估计类别概率或回归输出。然后，计算这些预测对每个特征的有限差分梯度，并将其聚合成一个广义的预期雅可比外积（EGOP）。最后，将此作为所有输入的单一全局线性预处理器，对特征空间进行全局旋转，然后将转换后的数据交由标准轴对齐森林处理。此外，这种方法也适用于任何提供梯度的模型。

Result: 在表格分类和回归基准测试中，这种预处理方法持续改进了轴对齐森林的性能，并且经常与倾斜基线模型相匹配或超越，同时还提升了训练时间。

Conclusion: 监督预处理可以恢复倾斜森林的大部分准确性，同时保留轴对齐树的简单性和鲁棒性。

Abstract: Axis-aligned decision trees are fast and stable but struggle on datasets with rotated or interaction-dependent decision boundaries, where informative splits require linear combinations of features rather than single-feature thresholds. Oblique forests address this with per-node hyperplane splits, but at added computational cost and implementation complexity. We propose a simple alternative: JARF, Jacobian-Aligned Random Forests. Concretely, we first fit an axis-aligned forest to estimate class probabilities or regression outputs, compute finite-difference gradients of these predictions with respect to each feature, aggregate them into an expected Jacobian outer product that generalizes the expected gradient outer product (EGOP), and use it as a single global linear preconditioner for all inputs. This supervised preconditioner applies a single global rotation of the feature space, then hands the transformed data back to a standard axis-aligned forest, preserving off-the-shelf training pipelines while capturing oblique boundaries and feature interactions that would otherwise require many axis-aligned splits to approximate. The same construction applies to any model that provides gradients, though we focus on random forests and gradient-boosted trees in this work. On tabular classification and regression benchmarks, this preconditioning consistently improves axis-aligned forests and often matches or surpasses oblique baselines while improving training time. Our experimental results and theoretical analysis together indicate that supervised preconditioning can recover much of the accuracy of oblique forests while retaining the simplicity and robustness of axis-aligned trees.

</details>


### [59] [Minimizing Layerwise Activation Norm Improves Generalization in Federated Learning](https://arxiv.org/abs/2512.08314)
*M Yashwanth,Gaurav Kumar Nayak,Harsh Rangwani,Arya Singh,R. Venkatesh Babu,Anirban Chakraborty*

Main category: cs.LG

TL;DR: 联邦学习 (FL) 允许客户端协作训练全局模型，而无需共享数据。然而，聚合模型可能收敛到“尖锐最小值”，影响泛化能力。本文提出了一种新颖的正则化技术“MAN”，通过最小化客户端模型中每个激活层的范数来解决这一问题，从而使模型收敛到平坦最小值，提高泛化性能并建立了新的最先进水平。


<details>
  <summary>Details</summary>
Motivation: 传统的联邦学习（FL）方法中，聚合的全局模型可能收敛到“尖锐最小值”，从而损害模型的泛化能力。

Method: 本文通过引入“平坦度”约束来解决FL模型的泛化问题，该约束施加在训练损失的Hessian矩阵的最大特征值上。为了更高效地实现这一目标，作者利用客户端损失函数重新 S 构问题，并提出了一种名为“MAN”的计算高效正则化技术，该技术旨在最小化客户端模型中每个激活层的范数。

Result: 所提出的“MAN”方法显著改善了现有FL技术的性能，并在多个基准测试中取得了最先进的结果。作者通过理论分析证明，最小化激活范数可以降低客户端损失的逐层Hessian矩阵的最大特征值，进而减小整体Hessian矩阵的最大特征值，最终确保模型收敛到平坦最小值。

Conclusion: 本文提出了一种有效的正则化技术“MAN”，用于联邦学习以促进模型收敛到平坦最小值，从而显著提高模型的泛化能力。

Abstract: Federated Learning (FL) is an emerging machine learning framework that enables multiple clients (coordinated by a server) to collaboratively train a global model by aggregating the locally trained models without sharing any client's training data. It has been observed in recent works that learning in a federated manner may lead the aggregated global model to converge to a 'sharp minimum' thereby adversely affecting the generalizability of this FL-trained model. Therefore, in this work, we aim to improve the generalization performance of models trained in a federated setup by introducing a 'flatness' constrained FL optimization problem. This flatness constraint is imposed on the top eigenvalue of the Hessian computed from the training loss. As each client trains a model on its local data, we further re-formulate this complex problem utilizing the client loss functions and propose a new computationally efficient regularization technique, dubbed 'MAN,' which Minimizes Activation's Norm of each layer on client-side models. We also theoretically show that minimizing the activation norm reduces the top eigenvalue of the layer-wise Hessian of the client's loss, which in turn decreases the overall Hessian's top eigenvalue, ensuring convergence to a flat minimum. We apply our proposed flatness-constrained optimization to the existing FL techniques and obtain significant improvements, thereby establishing new state-of-the-art.

</details>


### [60] [Biothreat Benchmark Generation Framework for Evaluating Frontier AI Models III: Implementing the Bacterial Biothreat Benchmark (B3) Dataset](https://arxiv.org/abs/2512.08459)
*Gary Ackerman,Theodore Wilson,Zachary Kallenborn,Olivia Shoemaker,Anna Wetzel,Hayley Peterson,Abigail Danfora,Jenna LaTourette,Brandon Behlendorf,Douglas Clifford*

Main category: cs.LG

TL;DR: 这篇论文描述了细菌生物威胁基准（B3）数据集的初步实施，该数据集旨在评估大型语言模型（LLMs）在生物安全方面的潜在风险。


<details>
  <summary>Details</summary>
Motivation: 快速发展的前沿人工智能模型，特别是大型语言模型（LLMs），可能助长生物恐怖主义或生物武器的获取，这引发了政策、学术和公众的广泛关注。模型开发者和政策制定者都希望量化并减轻这种风险，其中一个重要环节就是开发能够评估特定模型所带来的生物安全风险的模型基准。

Method: 本文讨论了细菌生物威胁基准（B3）数据集的初步实施。这是描述生物威胁基准生成（BBG）框架系列论文中的第三篇，之前的论文详细介绍了B3数据集的开发。试点项目包括通过一个前沿AI模型运行基准测试，接着对模型响应进行人工评估，并对结果进行多维度风险分析。

Result: 试点项目表明，B3数据集为快速评估LLM带来的生物安全风险提供了一个可行且细致入微的方法，能够识别风险的关键来源，并为优先缓解领域提供指导。

Conclusion: B3数据集提供了一种有效的方法来评估大型语言模型带来的生物安全风险，并为风险缓解提供了重要的见解。

Abstract: The potential for rapidly-evolving frontier artificial intelligence (AI) models, especially large language models (LLMs), to facilitate bioterrorism or access to biological weapons has generated significant policy, academic, and public concern. Both model developers and policymakers seek to quantify and mitigate any risk, with an important element of such efforts being the development of model benchmarks that can assess the biosecurity risk posed by a particular model. This paper discusses the pilot implementation of the Bacterial Biothreat Benchmark (B3) dataset. It is the third in a series of three papers describing an overall Biothreat Benchmark Generation (BBG) framework, with previous papers detailing the development of the B3 dataset. The pilot involved running the benchmarks through a sample frontier AI model, followed by human evaluation of model responses, and an applied risk analysis of the results along several dimensions. Overall, the pilot demonstrated that the B3 dataset offers a viable, nuanced method for rapidly assessing the biosecurity risk posed by a LLM, identifying the key sources of that risk and providing guidance for priority areas of mitigation priority.

</details>


### [61] [Transformers for Multimodal Brain State Decoding: Integrating Functional Magnetic Resonance Imaging Data and Medical Metadata](https://arxiv.org/abs/2512.08462)
*Danial Jafarzadeh Jazi,Maryam Hajiesmaeili*

Main category: cs.LG

TL;DR: 该研究提出了一个新颖的框架，将基于Transformer的架构与多模态输入（包括fMRI数据和DICOM元数据）相结合，以解码大脑状态。


<details>
  <summary>Details</summary>
Motivation: 传统的机器学习和深度学习方法未能充分利用DICOM元数据提供的上下文信息。

Method: 通过使用注意力机制的Transformer架构，整合fMRI数据和DICOM元数据进行多模态输入。

Result: 该方法提升了模型准确性、可解释性和鲁棒性。

Conclusion: 该框架在临床诊断、认知神经科学和个性化医疗方面具有应用潜力，但存在元数据变异性和计算需求等局限性，未来需要优化可扩展性和泛化性。

Abstract: Decoding brain states from functional magnetic resonance imaging (fMRI) data is vital for advancing neuroscience and clinical applications. While traditional machine learning and deep learning approaches have made strides in leveraging the high-dimensional and complex nature of fMRI data, they often fail to utilize the contextual richness provided by Digital Imaging and Communications in Medicine (DICOM) metadata. This paper presents a novel framework integrating transformer-based architectures with multimodal inputs, including fMRI data and DICOM metadata. By employing attention mechanisms, the proposed method captures intricate spatial-temporal patterns and contextual relationships, enhancing model accuracy, interpretability, and robustness. The potential of this framework spans applications in clinical diagnostics, cognitive neuroscience, and personalized medicine. Limitations, such as metadata variability and computational demands, are addressed, and future directions for optimizing scalability and generalizability are discussed.

</details>


### [62] [Optimal Perturbation Budget Allocation for Data Poisoning in Offline Reinforcement Learning](https://arxiv.org/abs/2512.08485)
*Junnan Qiu,Jie Li*

Main category: cs.LG

TL;DR: 这篇论文提出了一种针对离线强化学习（RL）数据投毒攻击的全局预算分配策略，根据样本对价值函数收敛的影响（即时序差分（TD）误差）来分配扰动，从而以最小的扰动实现显著的性能下降，并能规避现有的检测方法。


<details>
  <summary>Details</summary>
Motivation: 现有的离线强化学习（RL）数据投毒攻击策略通常对所有样本进行统一扰动，这导致了扰动预算的浪费且缺乏隐蔽性。

Method: 我们提出了一种新颖的全局预算分配攻击策略。利用样本对价值函数收敛的影响与其时序差分（TD）误差成比例的理论洞察，我们将攻击形式化为全局资源分配问题。我们推导出了一个闭式解，在该解中，扰动幅度在全局L2约束下按TD误差敏感度按比例分配。

Result: 在D4RL基准测试上的实验结果表明，我们的方法显著优于基线策略，以最小的扰动实现了高达80%的性能下降，并且能够规避现有最先进的统计和谱防御方法的检测。

Conclusion: 本论文提出的全局预算分配攻击策略，通过利用TD误差来指导扰动分配，能够高效且隐蔽地对离线强化学习系统进行数据投毒攻击，实现了显著的性能下降，并能规避现有检测。

Abstract: Offline Reinforcement Learning (RL) enables policy optimization from static datasets but is inherently vulnerable to data poisoning attacks. Existing attack strategies typically rely on locally uniform perturbations, which treat all samples indiscriminately. This approach is inefficient, as it wastes the perturbation budget on low-impact samples, and lacks stealthiness due to significant statistical deviations. In this paper, we propose a novel Global Budget Allocation attack strategy. Leveraging the theoretical insight that a sample's influence on value function convergence is proportional to its Temporal Difference (TD) error, we formulate the attack as a global resource allocation problem. We derive a closed-form solution where perturbation magnitudes are assigned proportional to the TD-error sensitivity under a global L2 constraint. Empirical results on D4RL benchmarks demonstrate that our method significantly outperforms baseline strategies, achieving up to 80% performance degradation with minimal perturbations that evade detection by state-of-the-art statistical and spectral defenses.

</details>


### [63] [Can TabPFN Compete with GNNs for Node Classification via Graph Tabularization?](https://arxiv.org/abs/2512.08798)
*Jeongwhan Choi,Woosung Kang,Minseo Kim,Jongwoo Kim,Noseong Park*

Main category: cs.LG

TL;DR: TabPFN-GN通过将图数据转化为表格特征，实现了无需图特定训练或语言模型依赖的节点分类，并在实验中表现出与GNN相当甚至更优的性能。


<details>
  <summary>Details</summary>
Motivation: 探索图节点分类是否可以有效地转化为表格学习问题，以利用大型预训练模型在不同领域展现出的零样本泛化能力。

Method: TabPFN-GN通过提取节点属性、结构特性、位置编码和可选的平滑邻域特征，将图数据转换为表格特征，然后使用TabPFN进行直接节点分类。

Result: 在12个基准数据集上的实验表明，TabPFN-GN在同质图上实现了与GNN相当的性能，并在异质图上始终优于GNN。

Conclusion: 原则性的特征工程可以弥合表格领域和图领域之间的鸿沟，为任务特定的GNN训练和依赖LLM的图基础模型提供了一种实用的替代方案。

Abstract: Foundation models pretrained on large data have demonstrated remarkable zero-shot generalization capabilities across domains. Building on the success of TabPFN for tabular data and its recent extension to time series, we investigate whether graph node classification can be effectively reformulated as a tabular learning problem. We introduce TabPFN-GN, which transforms graph data into tabular features by extracting node attributes, structural properties, positional encodings, and optionally smoothed neighborhood features. This enables TabPFN to perform direct node classification without any graph-specific training or language model dependencies. Our experiments on 12 benchmark datasets reveal that TabPFN-GN achieves competitive performance with GNNs on homophilous graphs and consistently outperforms them on heterophilous graphs. These results demonstrate that principled feature engineering can bridge the gap between tabular and graph domains, providing a practical alternative to task-specific GNN training and LLM-dependent graph foundation models.

</details>


### [64] [An Additive Manufacturing Part Qualification Framework: Transferring Knowledge of Stress-strain Behaviors from Additively Manufactured Polymers to Metals](https://arxiv.org/abs/2512.08699)
*Chenglong Duan,Dazhong Wu*

Main category: cs.LG

TL;DR: 为了实现增材制造零件的认证，本文提出了一个结合动态时间规整（DTW）和迁移学习（TL）的框架，通过将低成本聚合物的应力-应变行为知识迁移到金属，以预测和验证增材制造零件的性能。


<details>
  <summary>Details</summary>
Motivation: 增材制造零件的认证至关重要，它确保了增材制造零件能够持续生产并可靠地应用于关键领域。零件认证旨在验证增材制造零件是否满足性能要求，因此，预测增材制造零件复杂的应力-应变行为是关键。

Method: 本文开发了一个动态时间规整（DTW）-迁移学习（TL）框架，用于增材制造零件认证。该框架通过DTW选择与目标金属数据集最相关的聚合物数据集作为源域，然后使用长短期记忆（LSTM）模型进行知识迁移。

Result: 实验结果表明，DTW-TL框架能够识别聚合物和金属之间的最接近匹配，从而选择单个聚合物数据集作为源域。当三个金属作为目标域时，DTW-TL模型的平均绝对百分比误差最低，为12.41%，决定系数最高，为0.96。该框架优于没有迁移学习的普通LSTM模型以及在四个聚合物数据集上预训练的迁移学习模型。

Conclusion: 所提出的DTW-TL框架可以有效地将聚合物的应力-应变行为知识迁移到金属，显著提高了增材制造金属零件的应力-应变行为预测精度，为增材制造零件的认证提供了一种有效的方法。

Abstract: Part qualification is crucial in additive manufacturing (AM) because it ensures that additively manufactured parts can be consistently produced and reliably used in critical applications. Part qualification aims at verifying that an additively manufactured part meets performance requirements; therefore, predicting the complex stress-strain behaviors of additively manufactured parts is critical. We develop a dynamic time warping (DTW)-transfer learning (TL) framework for additive manufacturing part qualification by transferring knowledge of the stress-strain behaviors of additively manufactured low-cost polymers to metals. Specifically, the framework employs DTW to select a polymer dataset as the source domain that is the most relevant to the target metal dataset. Using a long short-term memory (LSTM) model, four source polymers (i.e., Nylon, PLA, CF-ABS, and Resin) and three target metals (i.e., AlSi10Mg, Ti6Al4V, and carbon steel) that are fabricated by different AM techniques are utilized to demonstrate the effectiveness of the DTW-TL framework. Experimental results show that the DTW-TL framework identifies the closest match between polymers and metals to select one single polymer dataset as the source domain. The DTW-TL model achieves the lowest mean absolute percentage error of 12.41% and highest coefficient of determination of 0.96 when three metals are used as the target domain, respectively, outperforming the vanilla LSTM model without TL as well as the TL model pre-trained on four polymer datasets as the source domain.

</details>


### [65] [Exposing Hidden Biases in Text-to-Image Models via Automated Prompt Search](https://arxiv.org/abs/2512.08724)
*Manos Plitsis,Giorgos Bouritsas,Vassilis Katsouros,Yannis Panagakis*

Main category: cs.LG

TL;DR: BGPS是一个自动生成提示的框架，旨在最大限度地增加生成图像中的偏差。


<details>
  <summary>Details</summary>
Motivation: 现有的方法常常依赖于策划的提示数据集来缓解文本到图像（TTI）扩散模型中的社会偏差，但这存在高昂的成本，并且容易忽略意外的偏差。

Method: BGPS框架包含两个部分：1. 一个大型语言模型（LLM），用于生成属性中立的提示；2. 作用于TTI内部表示的属性分类器，引导LLM的解码过程，使其在提示空间中放大感兴趣的图像属性。

Result: 通过对Stable Diffusion 1.5和最先进的去偏模型进行广泛实验，BGPS发现了一系列微妙且以前未记录的偏差，这些偏差严重恶化了公平性指标。

Conclusion: BGPS揭示了TTI模型的漏洞，并扩展了偏差搜索空间，可以作为偏差缓解的新评估工具。

Abstract: Text-to-image (TTI) diffusion models have achieved remarkable visual quality, yet they have been repeatedly shown to exhibit social biases across sensitive attributes such as gender, race and age. To mitigate these biases, existing approaches frequently depend on curated prompt datasets - either manually constructed or generated with large language models (LLMs) - as part of their training and/or evaluation procedures. Beside the curation cost, this also risks overlooking unanticipated, less obvious prompts that trigger biased generation, even in models that have undergone debiasing. In this work, we introduce Bias-Guided Prompt Search (BGPS), a framework that automatically generates prompts that aim to maximize the presence of biases in the resulting images. BGPS comprises two components: (1) an LLM instructed to produce attribute-neutral prompts and (2) attribute classifiers acting on the TTI's internal representations that steer the decoding process of the LLM toward regions of the prompt space that amplify the image attributes of interest. We conduct extensive experiments on Stable Diffusion 1.5 and a state-of-the-art debiased model and discover an array of subtle and previously undocumented biases that severely deteriorate fairness metrics. Crucially, the discovered prompts are interpretable, i.e they may be entered by a typical user, quantitatively improving the perplexity metric compared to a prominent hard prompt optimization counterpart. Our findings uncover TTI vulnerabilities, while BGPS expands the bias search space and can act as a new evaluation tool for bias mitigation.

</details>


### [66] [Fed-SE: Federated Self-Evolution for Privacy-Constrained Multi-Environment LLM Agents](https://arxiv.org/abs/2512.08870)
*Xiang Chen,Yuling Shi,Qizhen Lan,Yuchao Qiu,Xiaodong Gu*

Main category: cs.LG

TL;DR: Fed-SE是一个针对LLM智能体的联邦自演化框架，通过本地高效微调和全局低秩子空间聚合，解决了联邦学习在开放式智能体自演化中面临的梯度冲突和负迁移问题。


<details>
  <summary>Details</summary>
Motivation: 在大规模部署LLM智能体时，隐私限制阻碍了集中式优化和协同演化。现有的联邦学习方法在静态数据集上表现良好，但在开放式智能体自演化方面研究不足。直接应用标准联邦学习会导致异构任务和稀疏轨迹级奖励造成的严重梯度冲突，从而破坏全局优化。

Method: Fed-SE框架采用本地演化和全局聚合范式。在本地，智能体通过参数高效微调高回报轨迹来确保稳定的梯度更新。在全局，Fed-SE在低秩子空间内聚合更新，该子空间解耦了特定于环境的动态，有效减少了客户端之间的负迁移。

Result: 在五种异构环境中的实验表明，Fed-SE将平均任务成功率比联邦基线提高了约18%。

Conclusion: Fed-SE通过在隐私受限环境中实现稳健的跨环境知识迁移，有效解决了LLM智能体联邦自演化中的挑战。

Abstract: LLM agents are widely deployed in complex interactive tasks, yet privacy constraints often preclude centralized optimization and co-evolution across dynamic environments. While Federated Learning (FL) has proven effective on static datasets, its extension to the open-ended self-evolution of agents remains underexplored. Directly applying standard FL is challenging: heterogeneous tasks and sparse, trajectory-level rewards introduce severe gradient conflicts, destabilizing the global optimization process. To bridge this gap, we propose Fed-SE, a Federated Self-Evolution framework for LLM agents. Fed-SE establishes a local evolution-global aggregation paradigm. Locally, agents employ parameter-efficient fine-tuning on filtered, high-return trajectories to achieve stable gradient updates. Globally, Fed-SE aggregates updates within a low-rank subspace that disentangles environment-specific dynamics, effectively reducing negative transfer across clients. Experiments across five heterogeneous environments demonstrate that Fed-SE improves average task success rates by approximately 18% over federated baselines, validating its effectiveness in robust cross-environment knowledge transfer in privacy-constrained deployments.

</details>


### [67] [Neural Ordinary Differential Equations for Simulating Metabolic Pathway Dynamics from Time-Series Multiomics Data](https://arxiv.org/abs/2512.08732)
*Udesh Habaraduwa,Andrei Lixandru*

Main category: cs.LG

TL;DR: 这篇论文介绍了一种基于神经ODE（NODE）的动态框架，用于从高通量多组学数据中学习复杂的生物系统行为，并在大肠杆菌代谢工程时间序列数据上展示了其在预测性能和推理速度方面的显著优势。


<details>
  <summary>Details</summary>
Motivation: 利用高通量多组学数据预测复杂生物系统的行为是人类健康寿命和生物工程发展的关键。然而，将这些数据转化为可操作的预测模型仍然是一个瓶颈。作者旨在开发一种高容量、数据驱动的模拟系统，能够直接从观测数据中推断潜在的相互作用，从而模拟时间轨迹并预测下游干预效果。

Method: 本文引入了神经常微分方程（NODEs）作为一种动态框架，用于学习蛋白质组和代谢组之间复杂的相互作用。作者将此框架应用于工程化大肠杆菌菌株的时间序列数据，对代谢途径的连续动态进行建模。

Result: 与传统的机器学习方法相比，所提出的NODE架构在捕获系统动态方面表现出卓越的性能。在柠檬烯（高达94.38%的改进）和异戊烯醇（高达97.65%的改进）途径数据集上，其均方根误差（RMSE）改善了90%以上。此外，NODE模型展现了1000倍的推理时间加速。

Conclusion: NODE模型是一种可扩展、高保真度的工具，可用于下一代代谢工程和生物发现，能够有效地从多组学数据中学习复杂的生物系统动态并进行预测。

Abstract: The advancement of human healthspan and bioengineering relies heavily on predicting the behavior of complex biological systems. While high-throughput multiomics data is becoming increasingly abundant, converting this data into actionable predictive models remains a bottleneck. High-capacity, datadriven simulation systems are critical in this landscape; unlike classical mechanistic models restricted by prior knowledge, these architectures can infer latent interactions directly from observational data, allowing for the simulation of temporal trajectories and the anticipation of downstream intervention effects in personalized medicine and synthetic biology. To address this challenge, we introduce Neural Ordinary Differential Equations (NODEs) as a dynamic framework for learning the complex interplay between the proteome and metabolome. We applied this framework to time-series data derived from engineered Escherichia coli strains, modeling the continuous dynamics of metabolic pathways. The proposed NODE architecture demonstrates superior performance in capturing system dynamics compared to traditional machine learning pipelines. Our results show a greater than 90% improvement in root mean squared error over baselines across both Limonene (up to 94.38% improvement) and Isopentenol (up to 97.65% improvement) pathway datasets. Furthermore, the NODE models demonstrated a 1000x acceleration in inference time, establishing them as a scalable, high-fidelity tool for the next generation of metabolic engineering and biological discovery.

</details>


### [68] [When Tables Leak: Attacking String Memorization in LLM-Based Tabular Data Generation](https://arxiv.org/abs/2512.08875)
*Joshua Ward,Bochao Gu,Chi-Hua Wang,Guang Cheng*

Main category: cs.LG

TL;DR: 本文探讨了大型语言模型（LLMs）在生成表格合成数据时存在的隐私泄露问题，并提出了一种新的攻击方法和防御策略。


<details>
  <summary>Details</summary>
Motivation: 大型语言模型（LLMs）在生成高质量表格合成数据方面表现出色，但随之而来的隐私泄露问题不容忽视。

Method: 本文提出了一种名为LevAtt的无盒成员推理攻击（MIA），该攻击仅利用生成的合成数据，并针对合成观测数据中数字字符串序列。同时，本文提出了两种防御方法，包括一种在生成过程中策略性扰动数字的新采样策略。

Result: LevAtt攻击揭示了各种模型和数据集中存在严重的隐私泄露，在某些情况下，甚至对最先进的模型也是一个完美的成员分类器。提出的防御方法可以在合成数据保真度和实用性损失最小的情况下有效抵御这些攻击。

Conclusion: LLM-based合成数据生成存在独特的隐私漏洞，需要有效的防御措施。本文提出的攻击和防御方法为解决这一问题提供了新的思路。

Abstract: Large Language Models (LLMs) have recently demonstrated remarkable performance in generating high-quality tabular synthetic data. In practice, two primary approaches have emerged for adapting LLMs to tabular data generation: (i) fine-tuning smaller models directly on tabular datasets, and (ii) prompting larger models with examples provided in context. In this work, we show that popular implementations from both regimes exhibit a tendency to compromise privacy by reproducing memorized patterns of numeric digits from their training data. To systematically analyze this risk, we introduce a simple No-box Membership Inference Attack (MIA) called LevAtt that assumes adversarial access to only the generated synthetic data and targets the string sequences of numeric digits in synthetic observations. Using this approach, our attack exposes substantial privacy leakage across a wide range of models and datasets, and in some cases, is even a perfect membership classifier on state-of-the-art models. Our findings highlight a unique privacy vulnerability of LLM-based synthetic data generation and the need for effective defenses. To this end, we propose two methods, including a novel sampling strategy that strategically perturbs digits during generation. Our evaluation demonstrates that this approach can defeat these attacks with minimal loss of fidelity and utility of the synthetic data.

</details>


### [69] [DAO-GP Drift Aware Online Non-Linear Regression Gaussian-Process](https://arxiv.org/abs/2512.08879)
*Mohammad Abu-Shaira,Ajita Rattani,Weishi Shi*

Main category: cs.LG

TL;DR: 本文提出了一种能够处理概念漂移的在线高斯过程模型（DAO-GP），该模型在各种漂移类型和数据特性下均表现出优越的性能。


<details>
  <summary>Details</summary>
Motivation: 在真实世界数据集中，常见的数据分布演变，即概念漂移，若不加以考虑会严重降低模型的预测准确性。在线模型中的超参数通常是固定的，无法根据数据分布的变化进行动态调整，这使得问题更加复杂。

Method: 我们提出了DAO-GP（Drift-Aware Online Gaussian Process），这是一种新颖的、完全自适应、无超参数、衰减和稀疏的非线性回归模型。DAO-GP具有内置的漂移检测和适应机制，可根据漂移的严重程度动态调整模型行为。

Result: 广泛的实证评估证实了DAO-GP在静态条件、各种漂移类型（突变、增量、渐进）和不同数据特性下的鲁棒性。分析表明其具有动态适应性、高效的内存和基于衰减的管理以及不断演变的归纳点。

Conclusion: 与最先进的参数和非参数模型相比，DAO-GP始终实现卓越或具有竞争力的性能，使其成为在线非线性回归的漂移弹性解决方案。

Abstract: Real-world datasets often exhibit temporal dynamics characterized by evolving data distributions. Disregarding this phenomenon, commonly referred to as concept drift, can significantly diminish a model's predictive accuracy. Furthermore, the presence of hyperparameters in online models exacerbates this issue. These parameters are typically fixed and cannot be dynamically adjusted by the user in response to the evolving data distribution. Gaussian Process (GP) models offer powerful non-parametric regression capabilities with uncertainty quantification, making them ideal for modeling complex data relationships in an online setting. However, conventional online GP methods face several critical limitations, including a lack of drift-awareness, reliance on fixed hyperparameters, vulnerability to data snooping, absence of a principled decay mechanism, and memory inefficiencies. In response, we propose DAO-GP (Drift-Aware Online Gaussian Process), a novel, fully adaptive, hyperparameter-free, decayed, and sparse non-linear regression model. DAO-GP features a built-in drift detection and adaptation mechanism that dynamically adjusts model behavior based on the severity of drift. Extensive empirical evaluations confirm DAO-GP's robustness across stationary conditions, diverse drift types (abrupt, incremental, gradual), and varied data characteristics. Analyses demonstrate its dynamic adaptation, efficient in-memory and decay-based management, and evolving inducing points. Compared with state-of-the-art parametric and non-parametric models, DAO-GP consistently achieves superior or competitive performance, establishing it as a drift-resilient solution for online non-linear regression.

</details>


### [70] [Identifying counterfactual probabilities using bivariate distributions and uplift modeling](https://arxiv.org/abs/2512.08805)
*Théo Verhelst,Gianluca Bontempi*

Main category: cs.LG

TL;DR: 该文章提出了一种反事实估计器，通过将预测的提升分数拟合到双变量beta分布来获得反事实结果的后验分布。


<details>
  <summary>Details</summary>
Motivation: 传统的提升模型仅估计干预的因果效应，而反事实识别旨在恢复潜在结果的联合分布，提供更丰富的信息。

Method: 通过将预测的提升分数拟合到双变量beta分布，提出了一个反事实估计器，该方法在提升建模的假设之外不需要额外的因果假设。

Result: 模拟结果表明该方法有效，可应用于电信领域的客户流失问题，提供传统机器学习或单一提升模型无法获得的见解。

Conclusion: 该研究提出了一种新的反事实估计器，能够从提升模型中获取更丰富的反事实信息，为实际问题提供了更深入的洞察。

Abstract: Uplift modeling estimates the causal effect of an intervention as the difference between potential outcomes under treatment and control, whereas counterfactual identification aims to recover the joint distribution of these potential outcomes (e.g., "Would this customer still have churned had we given them a marketing offer?"). This joint counterfactual distribution provides richer information than the uplift but is harder to estimate. However, the two approaches are synergistic: uplift models can be leveraged for counterfactual estimation. We propose a counterfactual estimator that fits a bivariate beta distribution to predicted uplift scores, yielding posterior distributions over counterfactual outcomes. Our approach requires no causal assumptions beyond those of uplift modeling. Simulations show the efficacy of the approach, which can be applied, for example, to the problem of customer churn in telecom, where it reveals insights unavailable to standard ML or uplift models alone.

</details>


### [71] [Forecasting Fails: Unveiling Evasion Attacks in Weather Prediction Models](https://arxiv.org/abs/2512.08832)
*Huzaifa Arif,Pin-Yu Chen,Alex Gittens,James Diffenderfer,Bhavya Kailkhura*

Main category: cs.LG

TL;DR: WAAPO是一种生成目标对抗性扰动的新框架，它在操纵预测方面既有效又隐蔽。


<details>
  <summary>Details</summary>
Motivation: 随着对天气预报AI模型的日益依赖，评估其对抗性扰动的脆弱性至关重要。

Method: WAAPO通过结合通道稀疏性、空间局部化和平滑性约束来实现，确保扰动保持物理真实性和不可察觉性。

Result: WAAPO能够生成与预定义目标紧密对齐的对抗性轨迹，即使在受限条件下也是如此。

Conclusion: 突出了AI驱动预测模型中的关键漏洞，其中对初始条件的微小扰动可能导致预测天气模式的显著偏差。

Abstract: With the increasing reliance on AI models for weather forecasting, it is imperative to evaluate their vulnerability to adversarial perturbations. This work introduces Weather Adaptive Adversarial Perturbation Optimization (WAAPO), a novel framework for generating targeted adversarial perturbations that are both effective in manipulating forecasts and stealthy to avoid detection. WAAPO achieves this by incorporating constraints for channel sparsity, spatial localization, and smoothness, ensuring that perturbations remain physically realistic and imperceptible. Using the ERA5 dataset and FourCastNet (Pathak et al. 2022), we demonstrate WAAPO's ability to generate adversarial trajectories that align closely with predefined targets, even under constrained conditions. Our experiments highlight critical vulnerabilities in AI-driven forecasting models, where small perturbations to initial conditions can result in significant deviations in predicted weather patterns. These findings underscore the need for robust safeguards to protect against adversarial exploitation in operational forecasting systems.

</details>


### [72] [Reinforcement Learning From State and Temporal Differences](https://arxiv.org/abs/2512.08855)
*Lex Weaver,Jonathan Baxter*

Main category: cs.LG

TL;DR: 本文分析了TD(λ)在函数逼近方面的问题，并提出了一种改进的方法STD(λ)。


<details>
  <summary>Details</summary>
Motivation: TD(λ)在某些复杂的强化学习问题中表现出色，但它最小化的是近似值与真实值之间的平方误差，而不是对策略更关键的相对状态排序误差。

Method: 本文提出了一种改进的TD(λ)算法，称为STD(λ)。STD(λ)在二元决策问题中根据相对状态值训练函数逼近器。

Result: STD(λ)在双状态系统和acrobot问题的变体上取得了成功。

Conclusion: TD(λ)可能会收敛到次优策略。STD(λ)可以实现单调策略改进，解决了TD(λ)的局限性。

Abstract: TD($λ$) with function approximation has proved empirically successful for some complex reinforcement learning problems. For linear approximation, TD($λ$) has been shown to minimise the squared error between the approximate value of each state and the true value. However, as far as policy is concerned, it is error in the relative ordering of states that is critical, rather than error in the state values. We illustrate this point, both in simple two-state and three-state systems in which TD($λ$)--starting from an optimal policy--converges to a sub-optimal policy, and also in backgammon. We then present a modified form of TD($λ$), called STD($λ$), in which function approximators are trained with respect to relative state values on binary decision problems. A theoretical analysis, including a proof of monotonic policy improvement for STD($λ$) in the context of the two-state system, is presented, along with a comparison with Bertsekas' differential training method [1]. This is followed by successful demonstrations of STD($λ$) on the two-state system and a variation on the well known acrobot problem.

</details>


### [73] [Refining Diffusion Models for Motion Synthesis with an Acceleration Loss to Generate Realistic IMU Data](https://arxiv.org/abs/2512.08859)
*Lars Ole Häusler,Lena Uhlenberg,Göran Köber,Diyora Salimova,Oliver Amft*

Main category: cs.LG

TL;DR: 该文章提出了一种文本到IMU（惯性测量单元）运动合成框架，通过引入基于加速度的二阶损失（L_acc）来微调预训练的扩散模型，以生成更真实的IMU数据。


<details>
  <summary>Details</summary>
Motivation: 现有的运动合成模型，特别是扩散模型，在生成IMU数据时可能无法完全捕捉到IMU特有的加速度模式和高动态活动细节，导致合成数据与真实IMU记录存在差异。

Method: 1. 提出了一种基于加速度的二阶损失（L_acc），用于衡量生成运动的离散二阶时间差值的一致性。 2. 将L_acc集成到现有扩散模型的训练目标中，并对模型进行微调，以获得IMU特定的运动先验。 3. 结合表面建模和虚拟传感器模拟的现有文本到IMU框架，评估了所提出的方法。 4. 分析了加速度信号的保真度以及合成运动表示与真实IMU记录之间的差异。 5. 应用于下游的人体活动识别（HAR）任务，比较了所提出方法与其他扩散模型的分类性能。

Result: 1. 引入L_acc后，相对于原始模型，L_acc降低了12.7%。 2. 在高动态活动（如跑步、跳跃）中，性能改进比低动态活动（如坐立）更为显著。 3. 在低维嵌入中，通过精炼模型生成的合成IMU数据更接近真实IMU记录的分布。 4. 仅使用精炼的合成IMU数据进行训练，HAR分类性能比之前的扩散模型提高了8.7%，比表现最佳的比较扩散模型提高了7.6%。

Conclusion: 基于加速度感知的扩散精炼提供了一种有效的方法，可以使运动生成与IMU合成对齐，并且突出了深度学习管道在将通用文本到运动先验专门化为传感器特定任务方面的灵活性。

Abstract: We propose a text-to-IMU (inertial measurement unit) motion-synthesis framework to obtain realistic IMU data by fine-tuning a pretrained diffusion model with an acceleration-based second-order loss (L_acc). L_acc enforces consistency in the discrete second-order temporal differences of the generated motion, thereby aligning the diffusion prior with IMU-specific acceleration patterns. We integrate L_acc into the training objective of an existing diffusion model, finetune the model to obtain an IMU-specific motion prior, and evaluate the model with an existing text-to-IMU framework that comprises surface modelling and virtual sensor simulation. We analysed acceleration signal fidelity and differences between synthetic motion representation and actual IMU recordings. As a downstream application, we evaluated Human Activity Recognition (HAR) and compared the classification performance using data of our method with the earlier diffusion model and two additional diffusion model baselines. When we augmented the earlier diffusion model objective with L_acc and continued training, L_acc decreased by 12.7% relative to the original model. The improvements were considerably larger in high-dynamic activities (i.e., running, jumping) compared to low-dynamic activities~(i.e., sitting, standing). In a low-dimensional embedding, the synthetic IMU data produced by our refined model shifts closer to the distribution of real IMU recordings. HAR classification trained exclusively on our refined synthetic IMU data improved performance by 8.7% compared to the earlier diffusion model and by 7.6% over the best-performing comparison diffusion model. We conclude that acceleration-aware diffusion refinement provides an effective approach to align motion generation and IMU synthesis and highlights how flexible deep learning pipelines are for specialising generic text-to-motion priors to sensor-specific tasks.

</details>


### [74] [Open Polymer Challenge: Post-Competition Report](https://arxiv.org/abs/2512.08896)
*Gang Liu,Sobin Alosious,Subhamoy Mahajan,Eric Inae,Yihan Zhu,Yuhan Liu,Renzheng Zhang,Jiaxin Xu,Addison Howard,Ying Li,Tengfei Luo,Meng Jiang*

Main category: cs.LG

TL;DR: 这篇论文介绍了一个名为“开放聚合物挑战赛”（Open Polymer Challenge，OPC）的基准，旨在解决可持续聚合物材料发现中高质量数据集匮乏的问题。该挑战赛提供了一个包含1万种聚合物和5种性能（热导率、旋转半径、密度、自由体积分数和玻璃化转变温度）的数据集，并专注于多任务聚合物性能预测。参赛者在小数据、标签不平衡和异构模拟源等限制下开发了模型，并揭示了数据准备、分布偏移和跨组模拟一致性的重要经验教训。OPA的成果为聚合物科学中的分子人工智能奠定了新的基础，并有望加速可持续和节能材料的开发。


<details>
  <summary>Details</summary>
Motivation: 机器学习在发现可持续聚合物材料方面具有巨大潜力，但目前缺乏大型、高质量和可公开访问的聚合物数据集，这限制了该领域的进展。

Method: 本文通过发布“开放聚合物挑战赛”（OPC）来解决数据匮乏的问题。OPC提供了一个包含1万种聚合物和5种性能（热导率、旋转半径、密度、自由体积分数和玻璃化转变温度）的数据集，并将其作为聚合物信息学的社区开发基准。挑战赛的重点是多任务聚合物性能预测。参赛者在有限的数据、标签不平衡和来自不同模拟来源的异构数据等现实约束下开发模型。他们采用了特征增强、迁移学习、自监督预训练和有针对性的集成策略等技术。

Result: 挑战赛的模型、分析和发布的数据为聚合物科学中的分子人工智能奠定了新的基础。它揭示了数据准备、分布偏移和跨组模拟一致性的重要经验教训，为未来大型聚合物数据集的最佳实践提供了参考。比赛还发布了测试数据集和数据生成管道（可以模拟超过25种属性）。

Conclusion: Open Polymer Challenge（OPC）通过提供大规模、高质量的聚合物数据集和社区开发的基准，弥补了机器学习在可持续聚合物材料发现领域的数据空白。挑战赛不仅推动了聚合物性能预测模型的发展，还为未来大规模聚合物数据集的开发提供了宝贵的经验，加速了可持续和节能材料的开发。

Abstract: Machine learning (ML) offers a powerful path toward discovering sustainable polymer materials, but progress has been limited by the lack of large, high-quality, and openly accessible polymer datasets. The Open Polymer Challenge (OPC) addresses this gap by releasing the first community-developed benchmark for polymer informatics, featuring a dataset with 10K polymers and 5 properties: thermal conductivity, radius of gyration, density, fractional free volume, and glass transition temperature. The challenge centers on multi-task polymer property prediction, a core step in virtual screening pipelines for materials discovery. Participants developed models under realistic constraints that include small data, label imbalance, and heterogeneous simulation sources, using techniques such as feature-based augmentation, transfer learning, self-supervised pretraining, and targeted ensemble strategies. The competition also revealed important lessons about data preparation, distribution shifts, and cross-group simulation consistency, informing best practices for future large-scale polymer datasets. The resulting models, analysis, and released data create a new foundation for molecular AI in polymer science and are expected to accelerate the development of sustainable and energy-efficient materials. Along with the competition, we release the test dataset at https://www.kaggle.com/datasets/alexliu99/neurips-open-polymer-prediction-2025-test-data. We also release the data generation pipeline at https://github.com/sobinalosious/ADEPT, which simulates more than 25 properties, including thermal conductivity, radius of gyration, and density.

</details>


<div id='cs.AI'></div>

# cs.AI [[Back]](#toc)

### [75] [Impact of Data-Oriented and Object-Oriented Design on Performance and Cache Utilization with Artificial Intelligence Algorithms in Multi-Threaded CPUs](https://arxiv.org/abs/2512.07841)
*Gabriel M. Arantes,Richard F. Pinto,Bruno L. Dalmazo,Eduardo N. Borges,Giancarlo Lucca,Viviane L. D. de Mattos,Fabian C. Cardoso,Rafael A. Berri*

Main category: cs.AI

TL;DR: 本文对比了数据导向设计（DOD）和面向对象设计（OOD）在多线程环境下的性能，DOD在执行时间、系统调用和缓存未命中方面表现出显著优势。


<details>
  <summary>Details</summary>
Motivation: 解决多核CPU与主内存之间日益增长的性能差距，需要硬件感知软件设计范式。

Method: 开发并比较了四种A*搜索算法：单线程OOD、单线程DOD、多线程OOD和多线程DOD，评估指标包括执行时间、内存使用和CPU缓存未命中。

Result: 多线程测试中，DOD实现表现出显著的性能提升，执行时间更快，系统调用和缓存未命中更少。尽管OOD在内存使用或基于百分比的缓存未命中率方面偶尔有优势，但DOD在数据密集型操作中的效率更明显。此外，对于A*算法这类细粒度任务，线程管理的开销导致单线程版本显著优于多线程版本。

Conclusion: DOD在关键指标上的持续优势突出了其基础架构的优越性，表明它在最大化复杂、大规模AI和并行计算任务的硬件效率方面是一种更有效的方法。

Abstract: The growing performance gap between multi-core CPUs and main memory necessitates hardware-aware software design paradigms. This study provides a comprehensive performance analysis of Data Oriented Design (DOD) versus the traditional Object-Oriented Design (OOD), focusing on cache utilization and efficiency in multi-threaded environments. We developed and compared four distinct versions of the A* search algorithm: single-threaded OOD (ST-OOD), single-threaded DOD (ST-DOD), multi-threaded OOD (MT-OOD), and multi-threaded DOD (MT-DOD). The evaluation was based on metrics including execution time, memory usage, and CPU cache misses. In multi-threaded tests, the DOD implementation demonstrated considerable performance gains, with faster execution times and a lower number of raw system calls and cache misses. While OOD occasionally showed marginal advantages in memory usage or percentage-based cache miss rates, DOD's efficiency in data-intensive operations was more evident. Furthermore, our findings reveal that for a fine-grained task like the A* algorithm, the overhead associated with thread management led to single-threaded versions significantly outperforming their multi-threaded counterparts in both paradigms. We conclude that even when performance differences appear subtle in simple algorithms, the consistent advantages of DOD in critical metrics highlight its foundational architectural superiority, suggesting it is a more effective approach for maximizing hardware efficiency in complex, large-scale AI and parallel computing tasks.

</details>


### [76] [SkipKV: Selective Skipping of KV Generation and Storage for Efficient Inference with Large Reasoning Models](https://arxiv.org/abs/2512.07993)
*Jiayi Tian,Seyedarmin Azizi,Yequan Zhao,Erfan Baghaei Potraghloo,Sean McPherson,Sharath Nittur Sridhar,Zhengyang Wang,Zheng Zhang,Massoud Pedram,Souvik Kundu*

Main category: cs.AI

TL;DR: 本文提出SkipKV，一种免训练的KV压缩方法，通过句子级别的选择性删除和生成，从而在CoT推理过程中提高效率并保持准确性。


<details>
  <summary>Details</summary>
Motivation: 大型推理模型（LRM）在链式思考（CoT）推理过程中，由于键值（KV）缓存的开销会随着冗长的推理过程线性增长，导致内存和吞吐量瓶颈，限制了其高效部署。现有的KV缓存淘汰方法在多批次设置中无法保持准确性，并且会生成比原始模型更长的序列。

Method: SkipKV是一种免训练的KV压缩方法，它通过句子级别的选择性删除和生成来提高CoT推理效率。它引入了句子评分机制，用于识别和删除高度相似的句子，同时保持语义连贯性。为了抑制冗余生成，SkipKV动态调整一个转向向量，在推理过程中更新隐藏激活状态，强制LRM生成简洁的响应。

Result: SkipKV在多个推理基准测试中，相比现有方法，在相似的压缩预算下，准确性提高了26.7%。此外，与现有技术相比，SkipKV的生成长度减少了1.6倍，吞吐量提高了1.7倍。

Conclusion: SkipKV通过创新的句子级别KV缓存压缩策略，显著提高了大型推理模型在CoT推理的效率和准确性，解决了现有方法在多批次处理和冗余生成方面的问题。

Abstract: Large reasoning models (LRMs) often cost significant key-value (KV) cache overhead, due to their linear growth with the verbose chain-of-thought (CoT) reasoning process. This costs both memory and throughput bottleneck limiting their efficient deployment. Towards reducing KV cache size during inference, we first investigate the effectiveness of existing KV cache eviction methods for CoT reasoning. Interestingly, we find that due to unstable token-wise scoring and the reduced effective KV budget caused by padding tokens, state-of-the-art (SoTA) eviction methods fail to maintain accuracy in the multi-batch setting. Additionally, these methods often generate longer sequences than the original model, as semantic-unaware token-wise eviction leads to repeated revalidation during reasoning. To address these issues, we present \textbf{SkipKV}, a \textbf{\textit{training-free}} KV compression method for selective \textit{eviction} and \textit{generation} operating at a coarse-grained sentence-level sequence removal for efficient CoT reasoning. In specific, it introduces a \textit{sentence-scoring metric} to identify and remove highly similar sentences while maintaining semantic coherence. To suppress redundant generation, SkipKV dynamically adjusts a steering vector to update the hidden activation states during inference enforcing the LRM to generate concise response. Extensive evaluations on multiple reasoning benchmarks demonstrate the effectiveness of SkipKV in maintaining up to $\mathbf{26.7}\%$ improved accuracy compared to the alternatives, at a similar compression budget. Additionally, compared to SoTA, SkipKV yields up to $\mathbf{1.6}\times$ fewer generation length while improving throughput up to $\mathbf{1.7}\times$.

</details>


### [77] [Toward an AI Reasoning-Enabled System for Patient-Clinical Trial Matching](https://arxiv.org/abs/2512.08026)
*Caroline N. Leach,Mitchell A. Klusty,Samuel E. Armstrong,Justine C. Pickarski,Kristen L. Hankins,Emily B. Collier,Maya Shah,Aaron D. Mullen,V. K. Cody Bumgardner*

Main category: cs.AI

TL;DR: 该系统利用开源、支持推理的大型语言模型，实现人工智能增强的患者与临床试验匹配，旨在解决异构电子健康记录数据集成、专家评审促进和严格安全标准维护等挑战。


<details>
  <summary>Details</summary>
Motivation: 传统的临床试验患者筛选过程手动、耗时且资源密集，作者旨在通过开发一个安全、可扩展的AI增强系统来改进这一过程。

Method: 该系统利用开源、支持推理的大型语言模型，生成结构化的资格评估，并提供可解释的推理链，支持人工审核。该系统将资格视为动态状态，提供可操作的建议，以提高患者未来的资格。

Result: 该系统旨在减轻协调员的负担，智能地拓宽为每位患者考虑的试验范围，并保证所有AI生成输出的全面可审计性。

Conclusion: 该系统通过AI增强的患者与临床试验匹配，提高了效率、减轻了协调员负担，并支持人工审核，从而优化了临床试验的筛选流程。

Abstract: Screening patients for clinical trial eligibility remains a manual, time-consuming, and resource-intensive process. We present a secure, scalable proof-of-concept system for Artificial Intelligence (AI)-augmented patient-trial matching that addresses key implementation challenges: integrating heterogeneous electronic health record (EHR) data, facilitating expert review, and maintaining rigorous security standards. Leveraging open-source, reasoning-enabled large language models (LLMs), the system moves beyond binary classification to generate structured eligibility assessments with interpretable reasoning chains that support human-in-the-loop review. This decision support tool represents eligibility as a dynamic state rather than a fixed determination, identifying matches when available and offering actionable recommendations that could render a patient eligible in the future. The system aims to reduce coordinator burden, intelligently broaden the set of trials considered for each patient and guarantee comprehensive auditability of all AI-generated outputs.

</details>


### [78] [Large Language Models for Education and Research: An Empirical and User Survey-based Analysis](https://arxiv.org/abs/2512.08057)
*Md Mostafizer Rahman,Ariful Islam Shiplu,Md Faizul Ibne Amin,Yutaka Watanobe,Lu Peng*

Main category: cs.AI

TL;DR: 本文对ChatGPT和DeepSeek两种大型语言模型在教育和研究领域的应用进行了全面评估，包括技术分析、实验和用户调查。


<details>
  <summary>Details</summary>
Motivation: 评估当前最先进的大型语言模型ChatGPT和DeepSeek在教育和研究领域的表现，并探讨它们在模型准确性、计算效率和用户体验之间的权衡。

Method: 通过背景技术分析、经验实验和真实世界用户调查，对ChatGPT和DeepSeek在文本生成、编程和专业问题解决方面的性能进行基准测试。

Result: ChatGPT在通用语言理解和文本生成方面表现出色，而DeepSeek在编程任务中表现出优越的性能。两者在医学诊断输出和解决复杂数学问题方面均表现良好。用户调查进一步揭示了这些模型在教育和研究中的实际益处和局限性。

Conclusion: ChatGPT和DeepSeek是功能强大的大型语言模型，在教育和研究领域具有显著潜力。ChatGPT擅长通用语言任务，而DeepSeek在编程方面表现突出。未来的工作应继续探索如何优化这些模型的部署，以最大化其在这些领域的积极影响。

Abstract: Pretrained Large Language Models (LLMs) have achieved remarkable success across diverse domains, with education and research emerging as particularly impactful areas. Among current state-of-the-art LLMs, ChatGPT and DeepSeek exhibit strong capabilities in mathematics, science, medicine, literature, and programming. In this study, we present a comprehensive evaluation of these two LLMs through background technology analysis, empirical experiments, and a real-world user survey. The evaluation explores trade-offs among model accuracy, computational efficiency, and user experience in educational and research affairs. We benchmarked these LLMs performance in text generation, programming, and specialized problem-solving. Experimental results show that ChatGPT excels in general language understanding and text generation, while DeepSeek demonstrates superior performance in programming tasks due to its efficiency- focused design. Moreover, both models deliver medically accurate diagnostic outputs and effectively solve complex mathematical problems. Complementing these quantitative findings, a survey of students, educators, and researchers highlights the practical benefits and limitations of these models, offering deeper insights into their role in advancing education and research.

</details>


### [79] [The High Cost of Incivility: Quantifying Interaction Inefficiency via Multi-Agent Monte Carlo Simulations](https://arxiv.org/abs/2512.08345)
*Benedikt Mangold*

Main category: cs.AI

TL;DR: 该研究利用基于LLM的多智能体系统模拟对抗性辩论，以量化工作场所毒性对运营效率的影响。


<details>
  <summary>Details</summary>
Motivation: 量化工作场所毒性对运营效率的直接影响在方法上具有挑战性，因为在人类受试者中重现冲突存在伦理和实践困难。

Method: 本研究利用基于大型语言模型（LLM）的多智能体系统模拟一对一的对抗性辩论，创建一个受控的“社会学沙箱”。采用蒙特卡洛方法模拟数百次讨论，测量基线对照组和涉及“有毒”系统提示的智能体治疗组之间的收敛时间（定义为达成结论所需的论证数量）。

Result: 结果表明，涉及有毒参与者的对话时长统计显著增加了约25%。

Conclusion: 这种“毒性延迟”可以作为企业和学术环境中财务 F 损害的替代指标。此外，智能体B建模为测量社会摩擦机制提供了一种可重现、符合伦理的替代人类主体研究的方法。

Abstract: Workplace toxicity is widely recognized as detrimental to organizational culture, yet quantifying its direct impact on operational efficiency remains methodologically challenging due to the ethical and practical difficulties of reproducing conflict in human subjects. This study leverages Large Language Model (LLM) based Multi-Agent Systems to simulate 1-on-1 adversarial debates, creating a controlled "sociological sandbox". We employ a Monte Carlo method to simulate hundrets of discussions, measuring the convergence time (defined as the number of arguments required to reach a conclusion) between a baseline control group and treatment groups involving agents with "toxic" system prompts. Our results demonstrate a statistically significant increase of approximately 25\% in the duration of conversations involving toxic participants. We propose that this "latency of toxicity" serves as a proxy for financial damage in corporate and academic settings. Furthermore, we demonstrate that agent-based modeling provides a reproducible, ethical alternative to human-subject research for measuring the mechanics of social friction.

</details>


### [80] [Scalable Back-End for an AI-Based Diabetes Prediction Application](https://arxiv.org/abs/2512.08147)
*Henry Anand Septian Radityo,Bernardus Willson,Reynard Tanadi,Latifa Dwiyanti,Saiful Akbar*

Main category: cs.AI

TL;DR: 本文介绍了一种用于糖尿病预测移动应用的后端系统，该系统通过水平扩展、数据库分片和异步通信实现了高可用性和可伸缩性，能够在高并发用户下保持低错误率和低延迟。


<details>
  <summary>Details</summary>
Motivation: 糖尿病的全球患病率不断上升，早期检测对于预防严重并发症至关重要。人工智能驱动的预测应用提供了一个有前景的解决方案，但需要响应迅速且可扩展的后端架构才能有效地服务大量用户。

Method: 该系统采用了水平扩展、数据库分片和通过消息队列进行异步通信的架构。

Result: 系统83%的功能（24个中的20个）达到了性能目标，即故障率低于5%和平均延迟低于1000毫秒。系统能够处理多达10,000个并发用户。使用RabbitMQ的异步通信对于计算密集型预测请求的错误率最小化至关重要。

Conclusion: 本文开发的后端系统在可扩展性和可靠性方面表现出色，成功满足了糖尿病预测移动应用的需求。它展示了处理大量并发用户和维持高性能的能力，特别是通过异步通信有效管理了计算密集型任务。

Abstract: The rising global prevalence of diabetes necessitates early detection to prevent severe complications. While AI-powered prediction applications offer a promising solution, they require a responsive and scalable back-end architecture to serve a large user base effectively. This paper details the development and evaluation of a scalable back-end system designed for a mobile diabetes prediction application. The primary objective was to maintain a failure rate below 5% and an average latency of under 1000 ms. The architecture leverages horizontal scaling, database sharding, and asynchronous communication via a message queue. Performance evaluation showed that 83% of the system's features (20 out of 24) met the specified performance targets. Key functionalities such as user profile management, activity tracking, and read-intensive prediction operations successfully achieved the desired performance. The system demonstrated the ability to handle up to 10,000 concurrent users without issues, validating its scalability. The implementation of asynchronous communication using RabbitMQ proved crucial in minimizing the error rate for computationally intensive prediction requests, ensuring system reliability by queuing requests and preventing data loss under heavy load.

</details>


### [81] [Multi-Agent Intelligence for Multidisciplinary Decision-Making in Gastrointestinal Oncology](https://arxiv.org/abs/2512.08674)
*Rongzhao Zhang,Junqiao Wang,Shuyun Yang,Mouxiao Bian,Chao Ding,Yuwei Bai,Chihao Zhang,Yuguang Shen,Lei Wang,Lei Zheng,Qiujuan Yan,Yun Zhong,Meiling Liu,Jiwei Yu,Zheng Wang,Jie Xu,Meng Luo*

Main category: cs.AI

TL;DR: 本文提出了一个多智能体框架，通过模仿人类MDT的协作工作流程，解决了MLLM在处理复杂的异构医疗历史时遇到的问题，并在胃肠道肿瘤学领域实现了显著的性能提升。


<details>
  <summary>Details</summary>
Motivation: 在胃肠道肿瘤学领域，多模态临床推理需要整合解释内窥镜图像、放射学数据和生化标志物。尽管多模态大型语言模型（MLLM）展现出明显的潜力，但在面对复杂的异构病史时，它们常常遇到上下文稀释和幻觉等挑战。

Method: 为了解决这些限制，本文提出了一个分层的多智能体框架，该框架模仿了人类多学科团队（MDT）的协作工作流程。

Result: 该系统获得了4.60/5.00的综合专家评估分数，比单一基线模型有了显著改进。值得注意的是，基于智能体的架构在推理逻辑和医学准确性方面产生了最实质性的提升。

Conclusion: 研究结果表明，模仿性的、基于智能体的协作提供了一种可扩展、可解释且临床上稳健的范式，用于肿瘤学中的自动化决策支持。

Abstract: Multimodal clinical reasoning in the field of gastrointestinal (GI) oncology necessitates the integrated interpretation of endoscopic imagery, radiological data, and biochemical markers. Despite the evident potential exhibited by Multimodal Large Language Models (MLLMs), they frequently encounter challenges such as context dilution and hallucination when confronted with intricate, heterogeneous medical histories. In order to address these limitations, a hierarchical Multi-Agent Framework is proposed, which emulates the collaborative workflow of a human Multidisciplinary Team (MDT). The system attained a composite expert evaluation score of 4.60/5.00, thereby demonstrating a substantial improvement over the monolithic baseline. It is noteworthy that the agent-based architecture yielded the most substantial enhancements in reasoning logic and medical accuracy. The findings indicate that mimetic, agent-based collaboration provides a scalable, interpretable, and clinically robust paradigm for automated decision support in oncology.

</details>


### [82] [Beyond Traditional Diagnostics: Transforming Patient-Side Information into Predictive Insights with Knowledge Graphs and Prototypes](https://arxiv.org/abs/2512.08261)
*Yibowen Zhao,Yinan Zhang,Zhixiang Su,Lizhen Cui,Chunyan Miao*

Main category: cs.AI

TL;DR: 该文章提出了一种名为KPI的疾病预测框架，它是一个结合了知识图谱、原型学习和可解释性的模型，旨在解决现有医疗预测模型中疾病分布不平衡和可解释性差的问题，并通过对比学习和大型语言模型提升预测精度和解释性。


<details>
  <summary>Details</summary>
Motivation: 现有的疾病预测方法主要依赖于患者信息，但在疾病分布不平衡和缺乏可解释性方面存在挑战，导致预测结果可能存在偏差或不可靠。因此，该研究的动机是开发一种能够克服这些问题的疾病预测框架。

Method: KPI框架通过以下方式解决现有问题：1. 将结构化和可信的医疗知识整合到统一的疾病知识图谱中。2. 构建具有临床意义的疾病原型。3. 采用对比学习以提高预测准确性，尤其针对长尾疾病。4. 利用大型语言模型生成针对患者的、医学相关的解释，从而提高可解释性和可靠性。

Result: 在真实世界数据集上的大量实验表明，KPI框架在预测准确性方面优于现有最先进的方法。此外，它能提供与患者叙述密切相关的临床有效解释。

Conclusion: KPI框架通过其创新的方法，显著提高了疾病预测的准确性和可解释性，特别是在处理长尾疾病方面表现出色，并能提供临床有效的解释。这表明KPI在以患者为中心的医疗服务中具有重要的实践价值。

Abstract: Predicting diseases solely from patient-side information, such as demographics and self-reported symptoms, has attracted significant research attention due to its potential to enhance patient awareness, facilitate early healthcare engagement, and improve healthcare system efficiency. However, existing approaches encounter critical challenges, including imbalanced disease distributions and a lack of interpretability, resulting in biased or unreliable predictions. To address these issues, we propose the Knowledge graph-enhanced, Prototype-aware, and Interpretable (KPI) framework. KPI systematically integrates structured and trusted medical knowledge into a unified disease knowledge graph, constructs clinically meaningful disease prototypes, and employs contrastive learning to enhance predictive accuracy, which is particularly important for long-tailed diseases. Additionally, KPI utilizes large language models (LLMs) to generate patient-specific, medically relevant explanations, thereby improving interpretability and reliability. Extensive experiments on real-world datasets demonstrate that KPI outperforms state-of-the-art methods in predictive accuracy and provides clinically valid explanations that closely align with patient narratives, highlighting its practical value for patient-centered healthcare delivery.

</details>


### [83] [Towards Foundation Models with Native Multi-Agent Intelligence](https://arxiv.org/abs/2512.08743)
*Shuyue Hu,Haoyang Yan,Yiqun Zhang,Yang Chen,Dongzhan Zhou,Lei Bai*

Main category: cs.AI

TL;DR: 该研究探讨了基础模型在多智能体环境中的潜力，并指出了当前模型在多智能体能力方面的不足，提出了未来研究方向。


<details>
  <summary>Details</summary>
Motivation: 尽管基础模型在单智能体方面取得了进展，但其在多智能体环境中的应用仍面临挑战，亟需提升其多智能体智能。

Method: 本文分析了41个大型语言模型在单智能体和多智能体场景下的表现，以评估它们的多智能体能力。

Result: 研究发现，强大的单智能体性能并不能自动转化为稳健的多智能体智能。

Conclusion: 要构建具有原生多智能体智能的基础模型，需要在数据集构建、评估、训练范式和安全等方面进行深入研究，以弥补当前基础模型在多智能体智能方面的不足。

Abstract: Foundation models (FMs) are increasingly assuming the role of the "brain" of AI agents. While recent efforts have begun to equip FMs with native single-agent abilities -- such as GUI interaction or integrated tool use -- we argue that the next frontier is endowing FMs with native multi-agent intelligence. We identify four core capabilities of FMs in multi-agent contexts: understanding, planning, efficient communication, and adaptation. Contrary to assumptions about the spontaneous emergence of such abilities, we provide extensive empirical evidence across 41 large language models showing that strong single-agent performance alone does not automatically yield robust multi-agent intelligence. To address this gap, we outline key research directions -- spanning dataset construction, evaluation, training paradigms, and safety considerations -- for building FMs with native multi-agent intelligence.

</details>


### [84] [Reasoning Models Ace the CFA Exams](https://arxiv.org/abs/2512.08270)
*Jaisal Patel,Yunzhe Chen,Kaiwen He,Keyi Wang,David Li,Kairong Xiao,Xiao-Yang Liu*

Main category: cs.AI

TL;DR: 最新的大型语言模型在特许金融分析师（CFA）考试中表现出色，通过了所有三个级别的考试。


<details>
  <summary>Details</summary>
Motivation: 评估最新的推理模型在CFA考试中的表现，因为之前的研究表明大型语言模型在此类考试中表现不佳。

Method: 使用一系列模拟CFA考试（包括三个一级、两个二级和三个三级考试，共980个问题）来评估最先进的推理模型，并采用与先前研究相同的及格/不及格标准。

Result: 大多数模型都通过了所有三个级别的CFA考试，其中Gemini 3.0 Pro、Gemini 2.5 Pro、GPT-5、Grok 4、Claude Opus 4.1和DeepSeek-V3.1按总体表现排序。Gemini 3.0 Pro在一级考试中取得了97.6%的最高分。GPT-5在二级考试中表现突出，达到94.3%。在三级考试中，Gemini 2.5 Pro在多项选择题中获得86.4%的最高分，而Gemini 3.0 Pro在构建式回答问题中获得92.0%的最高分。

Conclusion: 最新的推理模型显著提升了大型语言模型在CFA考试中的表现，某些模型在不同级别的考试中取得了非常高的分数。

Abstract: Previous research has reported that large language models (LLMs) demonstrate poor performance on the Chartered Financial Analyst (CFA) exams. However, recent reasoning models have achieved strong results on graduate-level academic and professional examinations across various disciplines. In this paper, we evaluate state-of-the-art reasoning models on a set of mock CFA exams consisting of 980 questions across three Level I exams, two Level II exams, and three Level III exams. Using the same pass/fail criteria from prior studies, we find that most models clear all three levels. The models that pass, ordered by overall performance, are Gemini 3.0 Pro, Gemini 2.5 Pro, GPT-5, Grok 4, Claude Opus 4.1, and DeepSeek-V3.1. Specifically, Gemini 3.0 Pro achieves a record score of 97.6% on Level I. Performance is also strong on Level II, led by GPT-5 at 94.3%. On Level III, Gemini 2.5 Pro attains the highest score with 86.4% on multiple-choice questions while Gemini 3.0 Pro achieves 92.0% on constructed-response questions.

</details>


### [85] [AgentEval: Generative Agents as Reliable Proxies for Human Evaluation of AI-Generated Content](https://arxiv.org/abs/2512.08273)
*Thanh Vu,Richi Nayak,Thiru Balasubramaniam*

Main category: cs.AI

TL;DR: 这篇论文介绍了一种使用生成式Agent来评估LLM生成内容的方法，以解决传统人工评估的成本和效率问题。


<details>
  <summary>Details</summary>
Motivation: 现代企业在内容生成和评估方面面临时间和成本挑战，需要一种高效、自动化的解决方案来评估LLM生成内容的质量。

Method: 该研究引入了生成式Agent，通过模拟人类判断来评估AI生成内容的一致性、趣味性、清晰度、公平性和相关性。

Result: 通过使用生成式Agent，企业可以简化内容生成过程，确保高质量输出，同时最大程度地减少对昂贵人工评估的依赖。

Conclusion: 该研究为提升LLM生成符合商业目标的高质量内容提供了重要见解，并在自动化内容生成和评估方面取得了显著进展。

Abstract: Modern businesses are increasingly challenged by the time and expense required to generate and assess high-quality content. Human writers face time constraints, and extrinsic evaluations can be costly. While Large Language Models (LLMs) offer potential in content creation, concerns about the quality of AI-generated content persist. Traditional evaluation methods, like human surveys, further add operational costs, highlighting the need for efficient, automated solutions. This research introduces Generative Agents as a means to tackle these challenges. These agents can rapidly and cost-effectively evaluate AI-generated content, simulating human judgment by rating aspects such as coherence, interestingness, clarity, fairness, and relevance. By incorporating these agents, businesses can streamline content generation and ensure consistent, high-quality output while minimizing reliance on costly human evaluations. The study provides critical insights into enhancing LLMs for producing business-aligned, high-quality content, offering significant advancements in automated content generation and evaluation.

</details>


### [86] [Predicting California Bearing Ratio with Ensemble and Neural Network Models: A Case Study from Türkiye](https://arxiv.org/abs/2512.08340)
*Abdullah Hulusi Kökçam,Uğur Dağdeviren,Talas Fikret Kurnaz,Alparslan Serhat Demir,Caner Erden*

Main category: cs.AI

TL;DR: 本文提出了一种利用机器学习模型预测土体CBR值的方法。


<details>
  <summary>Details</summary>
Motivation: 传统的CBR测试耗时、昂贵且不切实际，尤其是在大规模或多样化的土壤剖面中。

Method: 收集了来自土耳其不同地理气候区域的382个土壤样本的数据集，包括物理化学土壤特性。测试了决策树、随机森林、额外树、梯度提升、Xgboost、K-最近邻、支持向量回归、多层感知器、Adaboost、Bagging、Voting和Stacking回归器12种机器学习算法。

Result: 随机森林回归器表现最佳，在训练、验证和测试中分别获得了0.95、0.76和0.83的R2分数。

Conclusion: 这项研究支持将智能的、以数据为中心的模型整合到岩土工程中，为传统方法提供了有效的替代方案，并促进了基础设施分析和设计中的数字化转型。

Abstract: The California Bearing Ratio (CBR) is a key geotechnical indicator used to assess the load-bearing capacity of subgrade soils, especially in transportation infrastructure and foundation design. Traditional CBR determination relies on laboratory penetration tests. Despite their accuracy, these tests are often time-consuming, costly, and can be impractical, particularly for large-scale or diverse soil profiles. Recent progress in artificial intelligence, especially machine learning (ML), has enabled data-driven approaches for modeling complex soil behavior with greater speed and precision. This study introduces a comprehensive ML framework for CBR prediction using a dataset of 382 soil samples collected from various geoclimatic regions in Türkiye. The dataset includes physicochemical soil properties relevant to bearing capacity, allowing multidimensional feature representation in a supervised learning context. Twelve ML algorithms were tested, including decision tree, random forest, extra trees, gradient boosting, xgboost, k-nearest neighbors, support vector regression, multi-layer perceptron, adaboost, bagging, voting, and stacking regressors. Each model was trained, validated, and evaluated to assess its generalization and robustness. Among them, the random forest regressor performed the best, achieving strong R2 scores of 0.95 (training), 0.76 (validation), and 0.83 (test). These outcomes highlight the model's powerful nonlinear mapping ability, making it a promising tool for predictive geotechnical tasks. The study supports the integration of intelligent, data-centric models in geotechnical engineering, offering an effective alternative to traditional methods and promoting digital transformation in infrastructure analysis and design.

</details>


### [87] [Soil Compaction Parameters Prediction Based on Automated Machine Learning Approach](https://arxiv.org/abs/2512.08343)
*Caner Erden,Alparslan Serhat Demir,Abdullah Hulusi Kokcam,Talas Fikret Kurnaz,Ugur Dagdeviren*

Main category: cs.AI

TL;DR: 这篇文章探讨了使用自动化机器学习（AutoML）方法预测土壤压实参数（最佳含水量和最大干密度）的问题，并发现XGBoost算法表现最佳，提高了预测精度和泛化能力。


<details>
  <summary>Details</summary>
Motivation: 传统的土壤压实参数测定方法费时费力，且经验回归模型适用性有限。现有的机器学习模型在处理异构土壤数据集时，预测精度和泛化能力不足。

Method: 本研究提出了一种自动化机器学习（AutoML）方法来预测最佳含水量（OMC）和最大干密度（MDD）。AutoML 方法能够自动选择算法并优化超参数。

Result: 通过广泛的实验，研究发现极限梯度提升（XGBoost）算法表现最佳，在独立的测试数据集上，MDD 的R方值为80.4％，OMC 的R方值为89.1％。

Conclusion: AutoML 方法在预测不同类型土壤的压实参数方面表现出有效性，其中XGBoost 算法表现最佳。研究强调了异构数据集对于提高机器学习模型泛化能力和性能的重要性，从而有助于实现更高效和可靠的施工实践。

Abstract: Soil compaction is critical in construction engineering to ensure the stability of structures like road embankments and earth dams. Traditional methods for determining optimum moisture content (OMC) and maximum dry density (MDD) involve labor-intensive laboratory experiments, and empirical regression models have limited applicability and accuracy across diverse soil types. In recent years, artificial intelligence (AI) and machine learning (ML) techniques have emerged as alternatives for predicting these compaction parameters. However, ML models often struggle with prediction accuracy and generalizability, particularly with heterogeneous datasets representing various soil types. This study proposes an automated machine learning (AutoML) approach to predict OMC and MDD. AutoML automates algorithm selection and hyperparameter optimization, potentially improving accuracy and scalability. Through extensive experimentation, the study found that the Extreme Gradient Boosting (XGBoost) algorithm provided the best performance, achieving R-squared values of 80.4% for MDD and 89.1% for OMC on a separate dataset. These results demonstrate the effectiveness of AutoML in predicting compaction parameters across different soil types. The study also highlights the importance of heterogeneous datasets in improving the generalization and performance of ML models. Ultimately, this research contributes to more efficient and reliable construction practices by enhancing the prediction of soil compaction parameters.

</details>


### [88] [DeepFeature: Iterative Context-aware Feature Generation for Wearable Biosignals](https://arxiv.org/abs/2512.08379)
*Kaiwei Liu,Yuting He,Bufang Yang,Mu Yuan,Chun Man Victor Wong,Ho Pong Andrew Sze,Zhenyu Yan,Hongkai Chen*

Main category: cs.AI

TL;DR: 该论文提出了DeepFeature，这是一个由大型语言模型（LLM）驱动的、上下文感知的可穿戴生物信号特征生成框架。


<details>
  <summary>Details</summary>
Motivation: 现有的可穿戴生物信号特征提取方法缺乏任务特定的上下文知识，难以在高维特征空间中识别最优特征提取设置，并且容易出现代码生成和自动化错误。

Method: DeepFeature框架引入了多源特征生成机制，融合了专家知识和任务设置。它采用迭代特征细化过程，利用基于特征评估的反馈进行特征重新选择。此外，DeepFeature利用鲁棒的多层过滤和验证方法，以确保特征到代码的翻译准确无误。

Result: DeepFeature在八项不同的任务中实现了平均4.21%-9.67%的AUROC性能提升。在其中五项任务中，DeepFeature超越了最先进的方法，并在其余任务中保持了相当的性能。

Conclusion: DeepFeature通过LLM赋能、上下文感知和多源集成，显著提升了可穿戴生物信号特征提取的性能和鲁棒性，解决了现有方法的局限性。

Abstract: Biosignals collected from wearable devices are widely utilized in healthcare applications. Machine learning models used in these applications often rely on features extracted from biosignals due to their effectiveness, lower data dimensionality, and wide compatibility across various model architectures. However, existing feature extraction methods often lack task-specific contextual knowledge, struggle to identify optimal feature extraction settings in high-dimensional feature space, and are prone to code generation and automation errors. In this paper, we propose DeepFeature, the first LLM-empowered, context-aware feature generation framework for wearable biosignals. DeepFeature introduces a multi-source feature generation mechanism that integrates expert knowledge with task settings. It also employs an iterative feature refinement process that uses feature assessment-based feedback for feature re-selection. Additionally, DeepFeature utilizes a robust multi-layer filtering and verification approach for robust feature-to-code translation to ensure that the extraction functions run without crashing. Experimental evaluation results show that DeepFeature achieves an average AUROC improvement of 4.21-9.67% across eight diverse tasks compared to baseline methods. It outperforms state-of-the-art approaches on five tasks while maintaining comparable performance on the remaining tasks.

</details>


### [89] [Using reinforcement learning to probe the role of feedback in skill acquisition](https://arxiv.org/abs/2512.08463)
*Antonio Terpin,Raffaello D'Andrea*

Main category: cs.AI

TL;DR: 本文探讨了在没有外部反馈的情况下，通过强化学习智能体在物理系统中进行技能习得的过程。


<details>
  <summary>Details</summary>
Motivation: 研究在完全受控条件下获取技能的过程，特别是当外部反馈有限或缺失时。

Method: 将通用强化学习智能体与桌面循环水槽中的旋转圆柱体连接，以最大化或最小化阻力。

Result: 通过高维流动反馈，智能体能够在几分钟内发现高性能的阻力控制策略。执行这些策略时不需要反馈。在没有流动反馈的情况下，智能体在阻力最大化方面失败，但在阻力最小化方面仍然成功，尽管速度较慢且可靠性较低。

Conclusion: 高性能技能的学习可能需要比执行更丰富的信息。学习条件可以是“友善”或“险恶”的，这仅取决于目标，而不是动力学或策略的复杂性。

Abstract: Many high-performance human activities are executed with little or no external feedback: think of a figure skater landing a triple jump, a pitcher throwing a curveball for a strike, or a barista pouring latte art. To study the process of skill acquisition under fully controlled conditions, we bypass human subjects. Instead, we directly interface a generalist reinforcement learning agent with a spinning cylinder in a tabletop circulating water channel to maximize or minimize drag. This setup has several desirable properties. First, it is a physical system, with the rich interactions and complex dynamics that only the physical world has: the flow is highly chaotic and extremely difficult, if not impossible, to model or simulate accurately. Second, the objective -- drag minimization or maximization -- is easy to state and can be captured directly in the reward, yet good strategies are not obvious beforehand. Third, decades-old experimental studies provide recipes for simple, high-performance open-loop policies. Finally, the setup is inexpensive and far easier to reproduce than human studies. In our experiments we find that high-dimensional flow feedback lets the agent discover high-performance drag-control strategies with only minutes of real-world interaction. When we later replay the same action sequences without any feedback, we obtain almost identical performance. This shows that feedback, and in particular flow feedback, is not needed to execute the learned policy. Surprisingly, without flow feedback during training the agent fails to discover any well-performing policy in drag maximization, but still succeeds in drag minimization, albeit more slowly and less reliably. Our studies show that learning a high-performance skill can require richer information than executing it, and learning conditions can be kind or wicked depending solely on the goal, not on dynamics or policy complexity.

</details>


### [90] [Principles2Plan: LLM-Guided System for Operationalising Ethical Principles into Plans](https://arxiv.org/abs/2512.08536)
*Tammy Zhong,Yang Song,Maurice Pagnucco*

Main category: cs.AI

TL;DR: 为了让机器人在人类环境中运行，道德意识至关重要，但现有工具支持不足，而手动指定规则劳动密集且需高度的特定上下文。Principles2Plan 是一个交互式研究原型，展示了人类和 大型语言模型（LLM）如何合作生成上下文敏感的道德规则，并指导自动化规划。


<details>
  <summary>Details</summary>
Motivation: 自动化规划工具对机器人伦理操作的支持不足；手动指定伦理规则劳动密集且高度依赖上下文。

Method: Principles2Plan 允许领域专家提供规划域、问题细节和高级原则（如仁慈和隐私）。系统生成与这些原则一致的可操作伦理规则，用户可以审查、优先排序并提供给规划器，以生成符合伦理的计划。

Result: 据我们所知，以前没有系统支持用户为经典规划环境生成基于原则的规则。Principles2Plan 展示了人机协作在使伦理自动化规划更实用和可行方面的潜力。

Conclusion: Principles2Plan 通过人与大型语言模型（LLM）的协作，使为机器人创建基于原则的、上下文敏感的伦理规则成为可能，从而推动了伦理自动化规划的实用性和可行性。

Abstract: Ethical awareness is critical for robots operating in human environments, yet existing automated planning tools provide little support. Manually specifying ethical rules is labour-intensive and highly context-specific. We present Principles2Plan, an interactive research prototype demonstrating how a human and a Large Language Model (LLM) can collaborate to produce context-sensitive ethical rules and guide automated planning. A domain expert provides the planning domain, problem details, and relevant high-level principles such as beneficence and privacy. The system generates operationalisable ethical rules consistent with these principles, which the user can review, prioritise, and supply to a planner to produce ethically-informed plans. To our knowledge, no prior system supports users in generating principle-grounded rules for classical planning contexts. Principles2Plan showcases the potential of human-LLM collaboration for making ethical automated planning more practical and feasible.

</details>


### [91] [CogMCTS: A Novel Cognitive-Guided Monte Carlo Tree Search Framework for Iterative Heuristic Evolution with Large Language Models](https://arxiv.org/abs/2512.08609)
*Hui Wang,Yang Liu,Xiaoyu Zhang,Chaoxu Mu*

Main category: cs.AI

TL;DR: 本文提出了一种新颖的认知引导蒙特卡洛树搜索框架 (CogMCTS)，它将大型语言模型的认知引导机制与蒙特卡罗树搜索紧密结合，以实现高效的自动化启发式优化。


<details>
  <summary>Details</summary>
Motivation: 现有的基于LLM的演化方法依赖于群体策略，容易陷入局部最优。将LLM与蒙特卡罗树搜索（MCTS）相结合，可以改善探索和利用之间的权衡，但多轮认知整合仍然有限，搜索多样性受到限制。

Method: CogMCTS框架采用多轮认知反馈来结合历史经验、节点信息和负面结果，动态改进启发式生成。双轨节点扩展与精英启发式管理相结合，平衡了不同启发式方法的探索和高质量经验的利用。此外，策略性变异修改启发式形式和参数，以进一步增强解决方案的多样性和整体优化性能。

Result: 实验结果表明，CogMCTS在稳定性、效率和解决方案质量方面优于现有的基于LLM的AHD方法。

Conclusion: CogMCTS框架通过紧密集成LLMs的认知引导机制与MCTS，并引入多轮认知反馈、双轨节点扩展和策略性变异等机制，有效克服了现有方法的局限性，在实现高效自动化启发式优化方面取得了显著的性能提升。

Abstract: Automatic Heuristic Design (AHD) is an effective1 framework for solving complex optimization prob-2 lems. The development of large language mod-3 els (LLMs) enables the automated generation of4 heuristics. Existing LLM-based evolutionary meth-5 ods rely on population strategies and are prone6 to local optima. Integrating LLMs with Monte7 Carlo Tree Search (MCTS) improves the trade-off8 between exploration and exploitation, but multi-9 round cognitive integration remains limited and10 search diversity is constrained. To overcome these11 limitations, this paper proposes a novel cognitive-12 guided MCTS framework (CogMCTS). CogMCTS13 tightly integrates the cognitive guidance mecha-14 nism of LLMs with MCTS to achieve efficient au-15 tomated heuristic optimization. The framework16 employs multi-round cognitive feedback to incor-17 porate historical experience, node information, and18 negative outcomes, dynamically improving heuris-19 tic generation. Dual-track node expansion com-20 bined with elite heuristic management balances the21 exploration of diverse heuristics and the exploita-22 tion of high-quality experience. In addition, strate-23 gic mutation modifies the heuristic forms and pa-24 rameters to further enhance the diversity of the so-25 lution and the overall optimization performance.26 The experimental results indicate that CogMCTS27 outperforms existing LLM-based AHD methods in28 stability, efficiency, and solution quality.

</details>


### [92] [Protein Secondary Structure Prediction Using Transformers](https://arxiv.org/abs/2512.08613)
*Manzi Kevin Maxime*

Main category: cs.AI

TL;DR: 这篇论文提出了一个基于Transformer的模型，利用滑动窗口数据增强技术预测蛋白质二级结构。


<details>
  <summary>Details</summary>
Motivation: 蛋白质二级结构（如阿尔法螺旋、贝塔折叠和卷曲）的预测对理解蛋白质功能至关重要。

Method: 本研究提出了一个基于Transformer的模型，该模型将注意力机制应用于蛋白质序列数据以预测结构基序。模型利用滑动窗口数据增强技术在CB513数据集上扩展训练样本。

Result: Transformer模型在可变长度序列上表现出强大的泛化能力，并能有效捕捉局部和长程残基相互作用。

Conclusion: 该模型成功地利用注意力机制和数据增强，对蛋白质二级结构进行了有效的预测，并在可变长度序列和残基相互作用方面表现出色。

Abstract: Predicting protein secondary structures such as alpha helices, beta sheets, and coils from amino acid sequences is essential for understanding protein function. This work presents a transformer-based model that applies attention mechanisms to protein sequence data to predict structural motifs. A sliding-window data augmentation technique is used on the CB513 dataset to expand the training samples. The transformer shows strong ability to generalize across variable-length sequences while effectively capturing both local and long-range residue interactions.

</details>


### [93] [A Practical Guide for Designing, Developing, and Deploying Production-Grade Agentic AI Workflows](https://arxiv.org/abs/2512.08769)
*Eranga Bandara,Ross Gore,Peter Foytik,Sachin Shetty,Ravi Mukkamala,Abdul Rahman,Xueping Liang,Safdar H. Bouk,Amin Hass,Sachini Rajapakse,Ng Wee Keong,Kasun De Zoysa,Aruna Withanage,Nilaan Loganathan*

Main category: cs.AI

TL;DR: 这篇论文提供了一份关于设计、开发和部署生产级智能AI工作流的端到端指南，涵盖了工程生命周期、多智能体设计模式、工具集成、编排、负责任AI考虑以及部署策略，并提出了九项核心最佳实践，通过一个多模态新闻分析和媒体生成工作流案例进行了演示。


<details>
  <summary>Details</summary>
Motivation: 智能AI标志着自主系统在推理、规划和执行多步任务方面的重大转变。然而，行业和研究领域面临着如何设计、工程化和操作可靠、可观察、可维护且符合安全与治理要求的生产级智能AI工作流的挑战。

Method: 本文提出了一种结构化的工程生命周期，包括工作流分解、多智能体设计模式、模型上下文协议（MCP）和工具集成、确定性编排、负责任AI考虑以及环境感知部署策略。此外，本文还提出了九项核心最佳实践，例如工具优先设计（而非MCP）、纯函数调用、单一工具和单一职责智能体、外部化提示管理、负责任AI对齐的模型联盟设计、工作流逻辑与MCP服务器的清晰分离、用于可扩展操作的容器化部署以及遵循KISS原则。

Result: 通过结合架构指导、操作模式和实际实现见解，本文为构建强大的、可扩展的生产就绪型智能AI工作流提供了基础性参考。为了演示这些原则，文中提出了一个多模态新闻分析和媒体生成工作流的综合案例研究。

Conclusion: 本文为解决生产级智能AI工作流的设计、工程和操作挑战提供了全面的指南和实践框架，强调了可靠性、可观察性、可维护性、安全性和治理的重要性，并通过具体案例展示了所提出原则的有效性。

Abstract: Agentic AI marks a major shift in how autonomous systems reason, plan, and execute multi-step tasks. Unlike traditional single model prompting, agentic workflows integrate multiple specialized agents with different Large Language Models(LLMs), tool-augmented capabilities, orchestration logic, and external system interactions to form dynamic pipelines capable of autonomous decision-making and action. As adoption accelerates across industry and research, organizations face a central challenge: how to design, engineer, and operate production-grade agentic AI workflows that are reliable, observable, maintainable, and aligned with safety and governance requirements. This paper provides a practical, end-to-end guide for designing, developing, and deploying production-quality agentic AI systems. We introduce a structured engineering lifecycle encompassing workflow decomposition, multi-agent design patterns, Model Context Protocol(MCP), and tool integration, deterministic orchestration, Responsible-AI considerations, and environment-aware deployment strategies. We then present nine core best practices for engineering production-grade agentic AI workflows, including tool-first design over MCP, pure-function invocation, single-tool and single-responsibility agents, externalized prompt management, Responsible-AI-aligned model-consortium design, clean separation between workflow logic and MCP servers, containerized deployment for scalable operations, and adherence to the Keep it Simple, Stupid (KISS) principle to maintain simplicity and robustness. To demonstrate these principles in practice, we present a comprehensive case study: a multimodal news-analysis and media-generation workflow. By combining architectural guidance, operational patterns, and practical implementation insights, this paper offers a foundational reference to build robust, extensible, and production-ready agentic AI workflows.

</details>


### [94] [CARLoS: Retrieval via Concise Assessment Representation of LoRAs at Scale](https://arxiv.org/abs/2512.08826)
*Shahar Sarfaty,Adi Haviv,Uri Hacohen,Niva Elkin-Koren,Roi Livni,Amit H. Bermano*

Main category: cs.AI

TL;DR: CARLoS是一个大规模框架，用于表征LoRA组件，无需额外元数据，并通过CLIP嵌入和三部分表示（方向、强度、一致性）来评估其行为。


<details>
  <summary>Details</summary>
Motivation: 现有LoRA发现方法依赖不可靠的用户描述或有偏见的人气指标，影响可用性。

Method: CARLoS通过在不同提示和种子下使用LoRA生成图像，并利用CLIP嵌入及其与基础模型生成的差异，定义了LoRA行为的三部分表示：方向（语义偏移）、强度（效果显著性）和一致性（效果稳定性）。

Result: CARLoS开发了一个高效的检索框架，可以语义匹配文本查询与相关LoRA，并过滤掉过强或不稳定的LoRA，在自动化和人工评估中优于文本基线。

Conclusion: CARLoS不仅能有效检索LoRA，其表示方法还能支持分析LoRA的强度和一致性与版权中“实质性”和“意图”等法律概念的关联，证明了其在LoRA分析中的广泛实用性。

Abstract: The rapid proliferation of generative components, such as LoRAs, has created a vast but unstructured ecosystem. Existing discovery methods depend on unreliable user descriptions or biased popularity metrics, hindering usability. We present CARLoS, a large-scale framework for characterizing LoRAs without requiring additional metadata. Analyzing over 650 LoRAs, we employ them in image generation over a variety of prompts and seeds, as a credible way to assess their behavior. Using CLIP embeddings and their difference to a base-model generation, we concisely define a three-part representation: Directions, defining semantic shift; Strength, quantifying the significance of the effect; and Consistency, quantifying how stable the effect is. Using these representations, we develop an efficient retrieval framework that semantically matches textual queries to relevant LoRAs while filtering overly strong or unstable ones, outperforming textual baselines in automated and human evaluations. While retrieval is our primary focus, the same representation also supports analyses linking Strength and Consistency to legal notions of substantiality and volition, key considerations in copyright, positioning CARLoS as a practical system with broader relevance for LoRA analysis.

</details>


### [95] [Interpolation in Knowledge Representation](https://arxiv.org/abs/2512.08833)
*Jean Christoph Jung,Patrick Koopmann,Matthias Knorr*

Main category: cs.AI

TL;DR: 本文讨论了描述逻辑和逻辑编程中Craig插值和均匀插值的理论结果和实践方法。


<details>
  <summary>Details</summary>
Motivation: Craig插值和均匀插值在知识表示中具有广泛应用，但许多知识表示形式通常不具备这两种插值，且计算插值在实践中具有挑战性。

Method: 本文深入研究了描述逻辑和逻辑编程这两种突出的知识表示形式，并讨论了计算插值的理论结果和实践方法。

Result: 本文讨论了描述逻辑和逻辑编程中Craig插值和均匀插值的理论结果和实践方法。

Conclusion: 更好地理解和计算插值有助于推动知识表示及其相关领域（如可解释性、遗忘和模块化）的进一步发展。

Abstract: Craig interpolation and uniform interpolation have many applications in knowledge representation, including explainability, forgetting, modularization and reuse, and even learning. At the same time, many relevant knowledge representation formalisms do in general not have Craig or uniform interpolation, and computing interpolants in practice is challenging. We have a closer look at two prominent knowledge representation formalisms, description logics and logic programming, and discuss theoretical results and practical methods for computing interpolants.

</details>


### [96] [Same Content, Different Answers: Cross-Modal Inconsistency in MLLMs](https://arxiv.org/abs/2512.08923)
*Angela van Sprang,Laurens Samson,Ana Lucic,Erman Acar,Sennay Ghebreab,Yuki M. Asano*

Main category: cs.AI

TL;DR: 本文介绍了 REST 和 REST+ 基准测试，用于评估多模态大型语言模型（MLLMs）中跨模态不一致性，并发现 MLLMs 在处理不同模态的相同语义信息时存在一致性问题。


<details>
  <summary>Details</summary>
Motivation: 开发新的基准测试（REST和REST+），以系统地评估多模态大型语言模型（MLLMs）中跨模态不一致性，因为MLLMs虽然在同一嵌入空间中表示视觉和语言，但在不同模态中无法执行相同的任务。

Method: 本文构建包含相同语义信息但模态不同的样本（图像、文本、混合），评估了15个MLLMs。研究了文本识别（OCR）问题、视觉特征（文本颜色、分辨率、视觉token数量）对模型性能的影响，并分析了一致性分数与文本和图像之间模态差距的关系。

Result: 最先进的MLLMs无法在不同模态上进行一致推理。模态不一致程度差异显著，且渲染文本为图像或渲染图像为文本都无法解决不一致性。即使OCR正确，视觉特征（文本颜色、分辨率、视觉token数量）也会影响模型性能。一致性得分与文本和图像之间的模态差距相关。

Conclusion: MLLMs在处理跨模态信息时存在严重的不一致性问题，这与文本和图像之间的模态差距有关。未来的研究需要解决这些模态间的不一致性，以提升MLLMs的性能和泛化能力。

Abstract: We introduce two new benchmarks REST and REST+(Render-Equivalence Stress Tests) to enable systematic evaluation of cross-modal inconsistency in multimodal large language models (MLLMs). MLLMs are trained to represent vision and language in the same embedding space, yet they cannot perform the same tasks in both modalities. Our benchmarks contain samples with the same semantic information in three modalities (image, text, mixed) and we show that state-of-the-art MLLMs cannot consistently reason over these different modalities. We evaluate 15 MLLMs and find that the degree of modality inconsistency varies substantially, even when accounting for problems with text recognition (OCR). Neither rendering text as image nor rendering an image as text solves the inconsistency. Even if OCR is correct, we find that visual characteristics (text colour and resolution, but not font) and the number of vision tokens have an impact on model performance. Finally, we find that our consistency score correlates with the modality gap between text and images, highlighting a mechanistic interpretation of cross-modal inconsistent MLLMs.

</details>


<div id='stat.ML'></div>

# stat.ML [[Back]](#toc)

### [97] [Provable Diffusion Posterior Sampling for Bayesian Inversion](https://arxiv.org/abs/2512.08022)
*Jinyuan Chang,Chenguang Duan,Yuling Jiao,Ruoxuan Li,Jerry Zhijian Yang,Cheng Yuan*

Main category: stat.ML

TL;DR: 本文提出了一种基于扩散的后验采样方法，该方法在PnP框架内工作，并利用Langevin动力学和数据学习的评分来近似后验评分，从而捕捉先验分布的结构特征。


<details>
  <summary>Details</summary>
Motivation: 为了解决传统PnP方法中后验评分近似的启发式问题，并提高复杂多模态目标后验分布的收敛性，本文提出了一种新颖的扩散PnP方法。

Method: 该方法通过构建从易于采样的终端分布到目标后验的概率传输，并采用暖启动策略初始化粒子。通过使用Langevin动力学生成粒子，开发了一种蒙特卡罗估计器来近似后验评分，避免了以往工作中常用的启发式近似。Langevin动力学中的评分是从数据中学习的。

Result: 在理论方面，本文提供了非渐近误差界限，表明该方法即使对于复杂的多模态目标后验分布也能收敛。这些界限明确量化了后验评分估计、暖启动初始化和后验采样过程中产生的误差。分析还阐明了先验评分匹配误差和贝叶斯逆问题的条件数如何影响整体性能。数值实验证明了该方法在各种逆问题中的有效性。

Conclusion: 本文提出了一种有效且理论上得到支持的扩散PnP方法，该方法通过改进后验评分估计和处理复杂后验分布的能力，优于现有方法。

Abstract: This paper proposes a novel diffusion-based posterior sampling method within a plug-and-play (PnP) framework. Our approach constructs a probability transport from an easy-to-sample terminal distribution to the target posterior, using a warm-start strategy to initialize the particles. To approximate the posterior score, we develop a Monte Carlo estimator in which particles are generated using Langevin dynamics, avoiding the heuristic approximations commonly used in prior work. The score governing the Langevin dynamics is learned from data, enabling the model to capture rich structural features of the underlying prior distribution. On the theoretical side, we provide non-asymptotic error bounds, showing that the method converges even for complex, multi-modal target posterior distributions. These bounds explicitly quantify the errors arising from posterior score estimation, the warm-start initialization, and the posterior sampling procedure. Our analysis further clarifies how the prior score-matching error and the condition number of the Bayesian inverse problem influence overall performance. Finally, we present numerical experiments demonstrating the effectiveness of the proposed method across a range of inverse problems.

</details>


### [98] [Heuristics for Combinatorial Optimization via Value-based Reinforcement Learning: A Unified Framework and Analysis](https://arxiv.org/abs/2512.08601)
*Orit Davidovich,Shimrit Shtern,Segev Wasserkrug,Nimrod Megiddo*

Main category: stat.ML

TL;DR: 这篇论文介绍了一个统一的框架，用于通过马尔可夫决策过程（MDP）对组合优化（CO）问题进行建模，并使用强化学习（RL）技术解决它们。


<details>
  <summary>Details</summary>
Motivation: 组合优化（CO）问题需要专家设计启发式算法，这篇论文旨在通过强化学习（RL）来自动化这个过程。

Method: 通过马尔可夫决策过程（MDP）对组合优化问题进行建模，并使用强化学习技术进行求解。文章提出了一些易于测试的假设，在此假设下，CO问题可以被表述为等价的无折扣MDP，从而为原始CO问题提供最优解。此外，文章还建立了一些条件，在这些条件下，基于价值的RL技术可以收敛到CO问题的近似解，并能保证相关的最优性差距。

Result: 收敛性分析提供了：(1) 在每次RL迭代中批量大小和投影梯度下降步骤的充分增长率；(2) 根据问题参数和目标RL精度得出的最优性差距；以及 (3) 状态空间嵌入选择的重要性。

Conclusion: 本文的分析阐明了深度Q学习算法在该问题背景下的成功之处（和局限性）。

Abstract: Since the 1990s, considerable empirical work has been carried out to train statistical models, such as neural networks (NNs), as learned heuristics for combinatorial optimization (CO) problems. When successful, such an approach eliminates the need for experts to design heuristics per problem type. Due to their structure, many hard CO problems are amenable to treatment through reinforcement learning (RL). Indeed, we find a wealth of literature training NNs using value-based, policy gradient, or actor-critic approaches, with promising results, both in terms of empirical optimality gaps and inference runtimes. Nevertheless, there has been a paucity of theoretical work undergirding the use of RL for CO problems. To this end, we introduce a unified framework to model CO problems through Markov decision processes (MDPs) and solve them using RL techniques. We provide easy-to-test assumptions under which CO problems can be formulated as equivalent undiscounted MDPs that provide optimal solutions to the original CO problems. Moreover, we establish conditions under which value-based RL techniques converge to approximate solutions of the CO problem with a guarantee on the associated optimality gap. Our convergence analysis provides: (1) a sufficient rate of increase in batch size and projected gradient descent steps at each RL iteration; (2) the resulting optimality gap in terms of problem parameters and targeted RL accuracy; and (3) the importance of a choice of state-space embedding. Together, our analysis illuminates the success (and limitations) of the celebrated deep Q-learning algorithm in this problem context.

</details>


<div id='cs.MA'></div>

# cs.MA [[Back]](#toc)

### [99] [CrowdLLM: Building LLM-Based Digital Populations Augmented with Generative Models](https://arxiv.org/abs/2512.07890)
*Ryan Feng Lin,Keyu Tian,Hanming Zheng,Congjing Zhang,Li Zeng,Shuai Huang*

Main category: cs.MA

TL;DR: CrowdLLM利用预训练LLM和生成模型提供具有成本效益、多样性和可扩展的数字人群，可以匹配真实人群的质量，并在准确性和分布保真度方面对标人类数据。


<details>
  <summary>Details</summary>
Motivation: 现有的数字人群研究主要依赖大型语言模型（LLMs），未能充分捕捉真实人类人群的准确性和多样性。

Method: 我们提出了CrowdLLM，它集成了预训练的LLMs和生成模型，以增强数字人群的多样性和保真度。

Result: CrownLLM在多个领域（如众包、投票、用户评分）和仿真研究中表现出色，在准确性和与人类数据的分布保真度方面均取得了可喜的成果。

Conclusion: CrowdLLM是一个有前途的框架，可以创建具有成本效益、足够代表性、可扩展的数字人群。

Abstract: The emergence of large language models (LLMs) has sparked much interest in creating LLM-based digital populations that can be applied to many applications such as social simulation, crowdsourcing, marketing, and recommendation systems. A digital population can reduce the cost of recruiting human participants and alleviate many concerns related to human subject study. However, research has found that most of the existing works rely solely on LLMs and could not sufficiently capture the accuracy and diversity of a real human population. To address this limitation, we propose CrowdLLM that integrates pretrained LLMs and generative models to enhance the diversity and fidelity of the digital population. We conduct theoretical analysis of CrowdLLM regarding its great potential in creating cost-effective, sufficiently representative, scalable digital populations that can match the quality of a real crowd. Comprehensive experiments are also conducted across multiple domains (e.g., crowdsourcing, voting, user rating) and simulation studies which demonstrate that CrowdLLM achieves promising performance in both accuracy and distributional fidelity to human data.

</details>


### [100] [MARINE: Theoretical Optimization and Design for Multi-Agent Recursive IN-context Enhancement](https://arxiv.org/abs/2512.07898)
*Hongwei Zhang,Ji Lu,Yongsheng Du,Yanqin Gao,Lingjun Huang,Baoli Wang,Fang Tan,Peng Zou*

Main category: cs.MA

TL;DR: MARINE（Multi-Agent Recursive IN-context Enhancement）是一个多智能体递归上下文增强框架，它将测试时推理看作是对持久性参考轨迹的迭代细化。它能将基础模型的pass@N能力转化为接近最优的pass@1性能，并在多项任务中实现最先进的结果，大幅提升了参数效率并提高了样本质量。


<details>
  <summary>Details</summary>
Motivation: 尽管大语言模型（LLM）的智能体展现出卓越的推理能力，但实际应用中的输出通常局限于单一响应，未能充分发挥其潜在性能。

Method: 本文提出了MARINE框架，理论上将测试时推理重构为对持久参考轨迹的迭代细化。MARINE的细化操作能将基础模型的pass@N能力系统地转化为接近最优的pass@1性能。理论分析表明，在固定的调用预算下，最小可行批次能够最大化预期性能增益，而对数增长的批次调度确保了在没有计算限制的情况下持续改进。

Result: 在BrowserComp-ZH基准测试中，685亿参数的实现达到了46.0%的pass@1准确率。一个80亿参数的模型在MARINE的增强下，性能达到了独立10000亿参数智能体的水平，将参数需求降低了一个数量级以上。在固定计算预算内，MARINE比传统的采样和排序策略提供了更高质量的样本。

Conclusion: MARINE框架通过迭代细化和多智能体协调，显著提升了LLM智能体的推理能力和参数效率，为训练后效率的提升提供了巨大潜力。

Abstract: Large Language Model (LLM)-based agents demonstrate advanced reasoning capabilities, yet practical constraints frequently limit outputs to single responses, leaving significant performance potential unrealized. This paper introduces MARINE (Multi-Agent Recursive IN-context Enhancement), a theoretically grounded framework that reconceptualizes test-time reasoning as iterative refinement of a persistent reference trajectory, fundamentally departing from conventional one-shot or multi-sample paradigms. The MARINE refinement operator systematically converts a base model's pass@N capabilities into near-optimal pass@1 performance. Rigorous theoretical analysis establishes that minimal feasible batches maximize expected performance gains under fixed invocation budgets, while logarithmically growing batch schedules ensure continuous improvement without computational constraints. Comprehensive evaluation on the BrowserComp-ZH benchmark demonstrates state-of-the-art results, with a 685B-parameter implementation achieving 46.0% pass@1 accuracy. Meanwhile, MARINE establishes a new paradigm for parameter-efficient reasoning: an 80B-parameter model augmented with MARINE matches the performance of standalone 1000B-parameter agents, reducing parameter requirements by over an order of magnitude. Notably, within a fixed computational budget, the proposed MARINE delivers higher-quality samples to alignment and optimization processes than traditional sampling-and-ranking strategies. Consequently, it has great potential to boost post-training efficiency.

</details>


### [101] [Probabilistic Multi-Agent Aircraft Landing Time Prediction](https://arxiv.org/abs/2512.08281)
*Kyungmin Kim,Seokbin Yoon,Keumjin Lee*

Main category: cs.MA

TL;DR: 这篇论文提出了一个概率多智能体飞机着陆时间预测框架，该框架能够以分布的形式预测多次飞机的着陆时间。


<details>
  <summary>Details</summary>
Motivation: 目前模型在预测的准确性和可信度方面存在挑战，同时需要考虑空中交通管制干预下的多智能体交互。

Method: 通过建立概率多智能体飞机着陆时间预测框架，考虑了飞机轨迹的不确定性和多智能体交互。

Result: 提出的模型比基线模型具有更高的预测精度，并且量化了预测结果的不确定性。此外，该模型通过注意力分数揭示了空中交通管制的潜在模式，增强了可解释性。

Conclusion: 该框架能够有效提高飞机着陆时间预测的准确性和可信度，为航空交通管理提供更好的支持。

Abstract: Accurate and reliable aircraft landing time prediction is essential for effective resource allocation in air traffic management. However, the inherent uncertainty of aircraft trajectories and traffic flows poses significant challenges to both prediction accuracy and trustworthiness. Therefore, prediction models should not only provide point estimates of aircraft landing times but also the uncertainties associated with these predictions. Furthermore, aircraft trajectories are frequently influenced by the presence of nearby aircraft through air traffic control interventions such as radar vectoring. Consequently, landing time prediction models must account for multi-agent interactions in the airspace. In this work, we propose a probabilistic multi-agent aircraft landing time prediction framework that provides the landing times of multiple aircraft as distributions. We evaluate the proposed framework using an air traffic surveillance dataset collected from the terminal airspace of the Incheon International Airport in South Korea. The results demonstrate that the proposed model achieves higher prediction accuracy than the baselines and quantifies the associated uncertainties of its outcomes. In addition, the model uncovered underlying patterns in air traffic control through its attention scores, thereby enhancing explainability.

</details>


<div id='cs.GT'></div>

# cs.GT [[Back]](#toc)

### [102] [The Theory of Strategic Evolution: Games with Endogenous Players and Strategic Replicators](https://arxiv.org/abs/2512.07901)
*Kevin Vallier*

Main category: cs.GT

TL;DR: 本文提出了战略演化理论，这是一个关于参与者、策略和制度规则共同演化的系统模型。该理论扩展了复制器动力学，涵盖了内生参与者、多层次选择、创新、宪法变革和元治理。


<details>
  <summary>Details</summary>
Motivation: 作者旨在开发一个通用模型，以解释参与者、策略和制度规则共同演化的系统。

Method: 该理论的中心数学对象是Poiesis堆栈：一个通过跨层增益矩阵连接的战略层级结构。在小增益条件下，系统存在一个全局Lyapunov函数，并在每个有限深度满足选择、跟踪和随机稳定性结果。

Result: 作者证明了该理论是封闭的，可以进行块扩展、创新事件、异构效用、连续策略空间和宪法演化。封闭定理表明，在更高层次不会出现新的动力学，并且不受限制的自我修改无法保留Lyapunov结构。

Conclusion: 该理论统一了演化博弈论、制度设计、创新动力学和宪法政治经济学的研究成果，为长期战略适应提供了一个通用的数学模型。

Abstract: This paper develops the Theory of Strategic Evolution, a general model for systems in which the population of players, strategies, and institutional rules evolve together. The theory extends replicator dynamics to settings with endogenous players, multi level selection, innovation, constitutional change, and meta governance. The central mathematical object is a Poiesis stack: a hierarchy of strategic layers linked by cross level gain matrices. Under small gain conditions, the system admits a global Lyapunov function and satisfies selection, tracking, and stochastic stability results at every finite depth. We prove that the class is closed under block extension, innovation events, heterogeneous utilities, continuous strategy spaces, and constitutional evolution. The closure theorem shows that no new dynamics arise at higher levels and that unrestricted self modification cannot preserve Lyapunov structure. The theory unifies results from evolutionary game theory, institutional design, innovation dynamics, and constitutional political economy, providing a general mathematical model of long run strategic adaptation.

</details>


### [103] [Selling Privacy in Blockchain Transactions](https://arxiv.org/abs/2512.08096)
*Georgios Chionas,Olga Gorelkina,Piotr Krysta,Rida Laraki*

Main category: cs.GT

TL;DR: 这篇文章从经济学角度研究了区块链交易中增强隐私的方法，特别是在订单流和荷兰式拍卖中，并提出了一个保证社会福利的隐私市场机制。


<details>
  <summary>Details</summary>
Motivation: 研究区块链交易中增强隐私的方法，特别关注隐私感知用户，他们的效用不仅取决于交易结果，还受到经济偏好暴露的负面影响。

Method: 1. 分析订单流拍卖：研究隐私程度如何影响拍卖收益和隐私感知用户的净效用，并描述了最优密封投标拍卖。2. 分析荷兰式拍卖的变体：用户逐渐降低价格和隐私程度直到交易成功，并将其收益与最优拍卖进行比较。3. 引入双边市场（隐私市场）：提出一种针对多用户和多搜索者的后定价机制，该机制在保证激励兼容性和预算平衡的同时，能实现最优社会福利的恒定近似。

Result: 1. 确定了订单流拍卖中的最优拍卖是密封投标拍卖。2. 比较了荷兰式拍卖变体与最优拍卖的收益。3. 提出了一个在隐私市场中能近似最优社会福利的后定价机制，并保证了激励兼容性和预算平衡。

Conclusion: 本文通过在订单流拍卖和荷兰式拍卖的变体中考虑隐私感知用户，并设计了一个多功能的隐私市场机制，为区块链交易中的隐私增强提供了经济学视角下的新方法。该研究将密码学原语应用于经济机制，以提高其性能。

Abstract: We study methods to enhance privacy in blockchain transactions from an economic angle. We consider mechanisms for privacy-aware users whose utility depends not only on the outcome of the mechanism but also negatively on the exposure of their economic preferences. Specifically, we study two auction-theoretic settings with privacy-aware users. First, we analyze an order flow auction, where a user auctions off to specialized agents, called searchers, the right to execute her transaction while maintaining a degree of privacy. We examine how the degree of privacy affects the revenue of the auction and, broadly, the net utility of the privacy-aware user. In this new setting, we describe the optimal auction, which is a sealed-bid auction. Subsequently, we analyze a variant of a Dutch auction in which the user gradually decreases the price and the degree of privacy until the transaction is sold. We compare the revenue of this auction to that of the optimal one as a function of the number of communication rounds. Then, we introduce a two-sided market - a privacy marketplace - with multiple users selling their transactions under their privacy preferences to multiple searchers. We propose a posted-price mechanism for the two-sided market that guarantees constant approximation of the optimal social welfare while maintaining incentive compatibility (from both sides of the market) and budget balance. This work builds on the emerging line of research that attempts to improve the performance of economic mechanisms by appending cryptographic primitives to them.

</details>


### [104] [Multi-agent learning under uncertainty: Recurrence vs. concentration](https://arxiv.org/abs/2512.08132)
*Kyriakos Lotidis,Panayotis Mertikopoulos,Nicholas Bambos,Jose Blanchet*

Main category: cs.GT

TL;DR: 本文分析了不确定性下多智能体学习的收敛性，揭示了在持续随机性和不确定性下正则化学习的局限性。


<details>
  <summary>Details</summary>
Motivation: 在连续博弈的随机正则化学习模型中，分析多智能体学习在不确定性下的收敛性。

Method: 本文分析了两种连续博弈中正则化学习的随机模型（连续时间和离散时间），以表征所产生的行为序列的长期表现。

Result: 在强单调博弈中，正则化学习的动态可能会无限次偏离均衡，但总能在有限时间内返回其附近，并且其长期分布 Sharply 集中在该邻域。如果底层博弈不是强单调的，这些有利的特性可能会失效。

Conclusion: 正则化学习在存在持续随机性和不确定性的情况下，其收敛性会受到限制，特别是在非强单调博弈中。

Abstract: In this paper, we examine the convergence landscape of multi-agent learning under uncertainty. Specifically, we analyze two stochastic models of regularized learning in continuous games -- one in continuous and one in discrete time with the aim of characterizing the long-run behavior of the induced sequence of play. In stark contrast to deterministic, full-information models of learning (or models with a vanishing learning rate), we show that the resulting dynamics do not converge in general. In lieu of this, we ask instead which actions are played more often in the long run, and by how much. We show that, in strongly monotone games, the dynamics of regularized learning may wander away from equilibrium infinitely often, but they always return to its vicinity in finite time (which we estimate), and their long-run distribution is sharply concentrated around a neighborhood thereof. We quantify the degree of this concentration, and we show that these favorable properties may all break down if the underlying game is not strongly monotone -- underscoring in this way the limits of regularized learning in the presence of persistent randomness and uncertainty.

</details>


### [105] [Robust equilibria in continuous games: From strategic to dynamic robustness](https://arxiv.org/abs/2512.08138)
*Kyriakos Lotidis,Panayotis Mertikopoulos,Nicholas Bambos,Jose Blanchet*

Main category: cs.GT

TL;DR: 本文分析了连续对策中纳什均衡在战略和动态不确定性下的鲁棒性。


<details>
  <summary>Details</summary>
Motivation: 研究纳什均衡在连续对策中面对战略和动态不确定性时的鲁棒性。

Method: 1. 引入“鲁棒均衡”概念，并给出几何刻画。
2. 探讨在随机性和不确定性下，哪些均衡可以作为“跟随正则化领导者”（FTRL）动态的稳定极限点。
3. 分析两种鲁棒性（战略鲁棒性和动态鲁棒性）之间的结构对应关系。
4. 研究收敛到鲁棒均衡的速度。

Result: 1. 提出了鲁棒均衡的概念，并对其进行了明确的几何表征。
2. 建立了战略鲁棒性和动态鲁棒性之间的结构对应关系：战略鲁棒性意味着动态鲁棒性，反之亦然。
3. 证明了在仿射约束行动空间的游戏中，熵正则化学习以几何速率收敛到鲁棒均衡。

Conclusion: 战略鲁棒性和动态鲁棒性之间存在紧密的联系，且战略鲁棒性是维持动态鲁棒性的必要条件。

Abstract: In this paper, we examine the robustness of Nash equilibria in continuous games, under both strategic and dynamic uncertainty. Starting with the former, we introduce the notion of a robust equilibrium as those equilibria that remain invariant to small -- but otherwise arbitrary -- perturbations to the game's payoff structure, and we provide a crisp geometric characterization thereof. Subsequently, we turn to the question of dynamic robustness, and we examine which equilibria may arise as stable limit points of the dynamics of "follow the regularized leader" (FTRL) in the presence of randomness and uncertainty. Despite their very distinct origins, we establish a structural correspondence between these two notions of robustness: strategic robustness implies dynamic robustness, and, conversely, the requirement of strategic robustness cannot be relaxed if dynamic robustness is to be maintained. Finally, we examine the rate of convergence to robust equilibria as a function of the underlying regularizer, and we show that entropically regularized learning converges at a geometric rate in games with affinely constrained action spaces.

</details>


<div id='cs.IT'></div>

# cs.IT [[Back]](#toc)

### [106] [Structure Theorems (and Fast Algorithms) for List Recovery of Subspace-Design Codes](https://arxiv.org/abs/2512.08017)
*Rohan Goyal,Venkatesan Guruswami*

Main category: cs.IT

TL;DR: 本文提出了一种改进的列表恢复算法，用于处理折叠Reed-Solomon码和单变量乘码中的大列表，解决了现有算法在处理大列表时效率低下的问题。


<details>
  <summary>Details</summary>
Motivation: 现有的折叠Reed-Solomon码和线性码的列表恢复算法在错误率接近容量时，列表大小呈指数增长，导致恢复时间复杂度高，限制了其实际应用。

Method: 本文扩展了Ashvinkumar、Habib和Srivastava在列表解码方面的算法进展，证明即使在列表恢复中列表很大，它们也是高度结构化的。我们提出了一种紧凑的描述方法，可以将相关列表的大小控制在 \(\ell^{O((\log \ell)/\varepsilon)}\)，并在 \(1/\varepsilon\) 的多项式时间内运行。

Result: 我们提出的算法在保持低时间复杂度的同时，有效地减小了列表的描述大小。相较于Guruswami和Wang先前的紧凑描述，我们将列表大小从 \(\approx n^{\ell/\varepsilon}\) 降低到 \(\ell^{O((\log \ell)/\varepsilon)}\)。

Conclusion: 本文提出的改进算法显著提升了折叠Reed-Solomon码和单变量乘码的列表恢复效率，为编码理论和理论计算机科学中的相关应用提供了更有效的方法。

Abstract: List recovery of error-correcting codes has emerged as a fundamental notion with broad applications across coding theory and theoretical computer science. Folded Reed-Solomon (FRS) and univariate multiplicity codes are explicit constructions which can be efficiently list-recovered up to capacity, namely a fraction of errors approaching $1-R$ where $R$ is the code rate.
  Chen and Zhang and related works showed that folded Reed-Solomon codes and linear codes must have list sizes exponential in $1/ε$ for list-recovering from an error-fraction $1-R-ε$. These results suggest that one cannot list-recover FRS codes in time that is also polynomial in $1/ε$. In contrast to such limitations, we show, extending algorithmic advances of Ashvinkumar, Habib, and Srivastava for list decoding, that even if the lists in the case of list-recovery are large, they are highly structured. In particular, we can output a compact description of a set of size only $\ell^{O((\log \ell)/ε)}$ which contains the relevant list, while running in time only polynomial in $1/ε$ (the previously known compact description due to Guruswami and Wang had size $\approx n^{\ell/ε}$). We also improve on the state-of-the-art algorithmic results for the task of list-recovery.

</details>


### [107] [Expectations in Expectation Propagation](https://arxiv.org/abs/2512.08034)
*Zilu Zhao,Fangqing Xiao,Dirk Slock*

Main category: cs.IT

TL;DR: 该论文研究了期望传播（EP）算法在线性模型中的应用，特别关注了无限积分值的消息问题，并提出了避免此类问题的策略。


<details>
  <summary>Details</summary>
Motivation: 期望传播（EP）算法是一种广泛使用的消息传递算法，但其在处理具有无限积分值的消息时可能遇到困难，尤其是在高斯投影EP中，这些消息可能表现出“负”方差，从而阻碍算法的进展。

Method: 本文通过分析线性模型中EP及其对应信念之间的关系，提出了两种方法来防止算法被具有无限积分值的消息阻塞：非持久性方法和持久性方法。此外，通过检查线性模型中EP消息之间的关系，开发了一种避免无限积分值消息出现的额外方法。

Result: 通过对EP在信念之间关系的分析，提出了非持久性和持久性方法，有效阻止了无限积分值消息阻塞算法进展。此外，还开发了一种避免无限积分值消息出现的额外方法。

Conclusion: 本文提出了有效策略来解决期望传播（EP）算法在线性模型中遇到的无限积分值消息问题，从而提高了算法的稳定性和性能。

Abstract: Expectation Propagation (EP) is a widely used message-passing algorithm that decomposes a global inference problem into multiple local ones. It approximates marginal distributions (beliefs) using intermediate functions (messages). While beliefs must be proper probability distributions that integrate to one, messages may have infinite integral values. In Gaussian-projected EP, such messages take a Gaussian form and appear as if they have "negative" variances. Although allowed within the EP framework, these negative-variance messages can impede algorithmic progress.
  In this paper, we investigate EP in linear models and analyze the relationship between the corresponding beliefs. Based on the analysis, we propose both non-persistent and persistent approaches that prevent the algorithm from being blocked by messages with infinite integral values.
  Furthermore, by examining the relationship between the EP messages in linear models, we develop an additional approach that avoids the occurrence of messages with infinite integral values.

</details>


### [108] [Adaptive Matched Filtering for Sensing With Communication Signals in Cluttered Environments](https://arxiv.org/abs/2512.08157)
*Lei Xie,Hengtao He,Yifeng Xiong,Fan Liu,Shi Jin*

Main category: cs.IT

TL;DR: 本文提出了一种新的优化目标，即在所有可能的数据负载下最大化平均信号-杂波-噪声比（SCNR），并通过随机矩阵理论（RMT）推导出了平均SCNR的渐近近似。研究结果表明，对于固定的调制基，PSK比QAM和纯高斯星座具有更好的平均SCNR，并且OFDM比SC和AFDM具有更高的平均SCNR。本文还提出了两种导频设计方案（DPD和DPI）来提高系统性能，并开发了相应的优化算法。


<details>
  <summary>Details</summary>
Motivation: 在杂波环境中，特别是在叠加信号下运行时，自适应匹配滤波（AMF）的性能受到瞬时信号-杂波-噪声比（SCNR）的限制。由于瞬时SCNR是一个依赖于数据负载的随机变量，直接将其用作设计目标会带来巨大的计算负担和信令开销。因此，需要寻找一种更有效的方法来优化AMF的性能。

Method: 本文将优化目标从瞬时SCNR转移到统计度量，即最大化所有数据负载下的平均SCNR。为了解决分析上的棘手性，本文利用随机矩阵理论（RMT）推导了平均SCNR的渐近近似。在此基础上，本文提出了两种导频设计方案：数据负载相关（DPD）方案和数据负载无关（DPI）方案。DPD方案最大化每次传输的瞬时SCNR，而DPI方案优化平均SCNR。本文为DPD问题采用了分数优化和KKT条件来推导闭式解，为DPI问题采用了流形优化方法来处理固有的秩一约束。

Result: 理论分析结果表明，对于固定的调制基，PSK比QAM和纯高斯星座具有更好的平均SCNR。此外，对于任何给定的星座，OFDM比SC和AFDM具有更高的平均SCNR。仿真结果验证了理论分析的准确性，并证明了所提方法的有效性。

Conclusion: 本文通过将优化目标从瞬时SCNR转移到统计度量，并利用随机矩阵理论（RMT）推导出平均SCNR的渐近近似，有效地解决了在杂波环境中AMF性能优化的挑战。提出的DPD和DPI导频设计方案及其相应的优化算法，为提高系统性能提供了有效的途径，并在理论和仿真上都得到了验证。未来的工作可以进一步探索在更复杂场景下这些方法的性能和适用性。

Abstract: This paper investigates the performance of the adaptive matched filtering (AMF) in cluttered environments, particularly when operating with superimposed signals. Since the instantaneous signal-to-clutter-plus-noise ratio (SCNR) is a random variable dependent on the data payload, using it directly as a design objective poses severe practical challenges, such as prohibitive computational burdens and signaling overhead. To address this, we propose shifting the optimization objective from an instantaneous to a statistical metric, which focuses on maximizing the average SCNR over all possible payloads. Due to its analytical intractability, we leverage tools from random matrix theory (RMT) to derive an asymptotic approximation for the average SCNR, which remains accurate even in moderate-dimensional regimes. A key finding from our theoretical analysis is that, for a fixed modulation basis, the PSK achieves a superior average SCNR compared to QAM and the pure Gaussian constellation. Furthermore, for any given constellation, the OFDM achieves a higher average SCNR than SC and AFDM. Then, we propose two pilot design schemes to enhance system performance: a Data-Payload-Dependent (DPD) scheme and a Data-Payload-Independent (DPI) scheme. The DPD approach maximizes the instantaneous SCNR for each transmission. Conversely, the DPI scheme optimizes the average SCNR, offering a flexible trade-off between sensing performance and implementation complexity. Then, we develop two dedicated optimization algorithms for DPD and DPI schemes. In particular, for the DPD problem, we employ fractional optimization and the KKT conditions to derive a closed-form solution. For the DPI problem, we adopt a manifold optimization approach to handle the inherent rank-one constraint efficiently. Simulation results validate the accuracy of our theoretical analysis and demonstrate the effectiveness of the proposed methods.

</details>


### [109] [On the Fundamental Tradeoff of Joint Communication and QCD: The Monostatic Case](https://arxiv.org/abs/2512.08332)
*Sung Hoon Lim,Daewon Seo*

Main category: cs.IT

TL;DR: 本文探讨了ISAC系统中通信与快速变化检测的权衡。


<details>
  <summary>Details</summary>
Motivation: 研究ISAC系统中通信与快速变化检测之间的权衡，并提出一种新的联合通信和最快变化子块编码策略（JCCS）。

Method: 引入了一种新颖的联合通信和最快变化子块编码策略（JCCS），该策略利用反馈根据实时状态估计动态调整编码。使用状态相关的互信息和KL散度来表征可达到的速率-延迟区域，并提供了一个部分逆定理，证明了所提出的检测算法在JCCS框架内的渐近最优性。

Result: 通过对二进制和MIMO高斯信道的分析，揭示了在ISAC系统设计中实现最佳权衡的见解。

Conclusion: JCCS策略能够根据实时状态估计动态调整编码，实现了通信性能和检测延迟之间的最佳权衡。

Abstract: This paper investigates the fundamental tradeoff between communication and quickest change detection (QCD) in integrated sensing and communication (ISAC) systems under a monostatic setup. We introduce a novel Joint Communication and quickest Change subblock coding Strategy (JCCS) that leverages feedback to adapt coding dynamically based on real-time state estimation. The achievable rate-delay region is characterized using state-dependent mutual information and KL divergence, providing a comprehensive framework for analyzing the interplay between communication performance and detection delay. Moreover, we provide a partial converse demonstrating the asymptotic optimality of the proposed detection algorithm within the JCCS framework. To illustrate the practical implications, we analyze binary and MIMO Gaussian channels, revealing insights into achieving optimal tradeoffs in ISAC system design.

</details>


### [110] [On Discrete Ambiguity Functions of Random Communication Waveforms](https://arxiv.org/abs/2512.08352)
*Ying Zhang,Fan Liu,Yifeng Xiong,Weijie Yuan,Shuangyang Li,Le Zheng,Tony Xiao Han,Christos Masouros,Shi Jin*

Main category: cs.IT

TL;DR: 本文分析了随机通信波形的离散模糊函数（AFs），推导了两种AFs（DP-AF和FST-AF）的预期旁瓣水平（ESL）和预期综合旁瓣水平（EISL）的精确闭合表达式，并从理论上证明了在紧凑二维区域内任何波形都无法达到最小ESL，并为FST-AF分析了星座图的影响。


<details>
  <summary>Details</summary>
Motivation: 表征随机通信波形的离散模糊函数是未来ISAC应用中评估延迟-多普勒传感性能的关键指标，因此需要对其进行深入研究。

Method: 本文首先为DP-AF和FST-AF这两种AFs开发了一个统一的分析框架。通过分析平方AFs的期望，推导了ESL和EISL的精确闭合表达式。引入基于有限Weyl-Heisenberg (WH) 群的矩阵表示以及WH群的视角来获得结构洞察。

Result: DP-AF的归一化EISL对于所有正交波形都是相同的。WH群的视角表明，最小ESL只能沿一维切割或在一组广泛分散的延迟-多普勒箱中出现，这意味着在DP-AF框架下没有波形可以在任何紧凑的二维区域内达到最小ESL。对于FST-AF，闭合形式的ESL和EISL表达式揭示了一个由其峰度决定的依赖于星座图的机制：OFDM调制实现了亚高斯星座图的最小ESL，而OTFS波形对于超高斯星座图来说是最佳的。

Conclusion: 本文在理论上证明了在紧凑二维区域内任何波形都无法达到最小ESL（这也被称为“no-go”结果），并为FST-AF分析了星座图的影响，证明了OFDM和OTFS在不同星座图下的最优性。未来可以进一步研究其他类型的ISAC波形，并将其应用于实际系统中。

Abstract: This paper provides a fundamental characterization of the discrete ambiguity functions (AFs) of random communication waveforms under arbitrary orthonormal modulation with random constellation symbols, which serve as a key metric for evaluating the delay-Doppler sensing performance in future ISAC applications. A unified analytical framework is developed for two types of AFs, namely the discrete periodic AF (DP-AF) and the fast-slow time AF (FST-AF), where the latter may be seen as a small-Doppler approximation of the DP-AF. By analyzing the expectation of squared AFs, we derive exact closed-form expressions for both the expected sidelobe level (ESL) and the expected integrated sidelobe level (EISL) under the DP-AF and FST-AF formulations. For the DP-AF, we prove that the normalized EISL is identical for all orthogonal waveforms. To gain structural insights, we introduce a matrix representation based on the finite Weyl-Heisenberg (WH) group, where each delay-Doppler shift corresponds to a WH operator acting on the ISAC signal. This WH-group viewpoint yields sharp geometric constraints on the lowest sidelobes: The minimum ESL can only occur along a one-dimensional cut or over a set of widely dispersed delay-Doppler bins. Consequently, no waveform can attain the minimum ESL over any compact two-dimensional region, leading to a no-optimality (no-go) result under the DP-AF framework. For the FST-AF, the closed-form ESL and EISL expressions reveal a constellation-dependent regime governed by its kurtosis: The OFDM modulation achieves the minimum ESL for sub-Gaussian constellations, whereas the OTFS waveform becomes optimal for super-Gaussian constellations. Finally, four representative waveforms, namely, SC, OFDM, OTFS, and AFDM, are examined under both frameworks, and all theoretical results are verified through numerical examples.

</details>


### [111] [Skew polynomial representations of matrix algebras and applications to coding theory](https://arxiv.org/abs/2512.08602)
*Alessandro Neri,Paolo Santonastaso*

Main category: cs.IT

TL;DR: 本文扩展了矩阵代数的斜多项式表示，并利用该表示构造了新的最大和秩距离（MSRD）码族。


<details>
  <summary>Details</summary>
Motivation: 为了寻找一种能够有效表示矩阵代数并能捕获和秩距离的数学工具，进而构造出性能优越的纠错码。

Method: 通过将矩阵代数表示为除环上矩阵空间的直和的斜多项式，并定义了基于多项式次数和最大公右除数的权重函数来捕获和秩距离。利用这种表示方法，构造了在有限域、无限域和除环上的新型MSRD码族。

Result: 构造的MSRD码族推广了许多已知的MSRD码以及在秩和汉明度量下的最优码。在有限域情况下，还得到了长度接近于域大小的子域上的线性MDS码新族。

Conclusion: 斜多项式表示法提供了一种强大的工具，不仅统一并推广了现有的编码方案，还为构造新型高效纠错码开辟了道路。

Abstract: We extend the existing skew polynomial representations of matrix algebras which are direct sum of matrix spaces over division rings. In this representation, the sum-rank distance between two tuples of matrices is captured by a weight function on their associated skew polynomials, defined through degrees and greatest common right divisors with the polynomial that defines the representation. We exploit this representation to construct new families of maximum sum-rank distance (MSRD) codes over finite and infinite fields, and over division rings. These constructions generalize many of the known existing constructions of MSRD codes as well as of optimal codes in the rank and in the Hamming metric. As a byproduct, in the case of finite fields we obtain new families of MDS codes which are linear over a subfield and whose length is close to the field size.

</details>


<div id='cs.SI'></div>

# cs.SI [[Back]](#toc)

### [112] [Finding core subgraphs of directed graphs via discrete Ricci curvature flow](https://arxiv.org/abs/2512.07899)
*Juan Zhao,Jicheng Ma,Yunyan Yang,Liang Zhao*

Main category: cs.SI

TL;DR: 这篇论文介绍了一种针对有向图的Ricci曲率及其伴随的曲率流，能够从弱连接有向图中检测强连接子图，并且在核心评估方面优于传统方法。


<details>
  <summary>Details</summary>
Motivation: 现有研究主要关注无向图的Ricci曲率应用，而对有向图的关注相对较少。

Method: 本文引入了有向图的Ricci曲率定义和曲率流。对于弱连接有向图，通过添加具有非常大人工权重的边将其转换为强连接图，然后应用Ricci曲率流来检测强连接子图。这些添加的边在曲率流的最后迭代中会自动丢弃。

Result: 在核心评估方面，该方法始终优于传统方法，在至少三个关键指标中的两个上取得了更好的结果。

Conclusion: 所提出的针对有向图的Ricci曲率流可以有效地检测弱连接有向图中的强连接子图，并且在性能上优于现有方法。

Abstract: Ricci curvature and its associated flow offer powerful geometric methods for analyzing complex networks. While existing research heavily focuses on applications for undirected graphs such as community detection and core extraction, there have been relatively less attention on directed graphs.
  In this paper, we introduce a definition of Ricci curvature and an accompanying curvature flow for directed graphs. Crucially, for strongly connected directed graphs, this flow admits a unique global solution. We then apply this flow to detect strongly connected subgraphs from weakly connected directed graphs. (A weakly connected graph is connected overall but not necessarily strongly connected). Unlike prior work requiring graphs to be strongly connected, our method loosens this requirement. We transform a weakly connected graph into a strongly connected one by adding edges with very large artificial weights. This modification does not compromise our core subgraph detection. Due to their extreme weight, these added edges are automatically discarded during the final iteration of the Ricci curvature flow.
  For core evaluation, our approach consistently surpasses traditional methods, achieving better results on at least two out of three key metrics. The implementation code is publicly available at https://github.com/12tangze12/Finding-core-subgraphs-on-directed-graphs.

</details>


### [113] [Fairness-aware PageRank via Edge Reweighting](https://arxiv.org/abs/2512.08055)
*Honglian Wang,Haoyun Chen,Aristides Gionis*

Main category: cs.SI

TL;DR: 本文提出了一种通过重新加权转移矩阵中的转移概率，将群体公平性纳入PageRank算法的新方法。


<details>
  <summary>Details</summary>
Motivation: 随着负责任人工智能的重要性日益增加，链接分析算法中的公平性问题受到关注，特别是PageRank算法。

Method: 本文通过最小化公平性损失（原始群体PageRank分布与目标PageRank分布之间的差异）来公式化实现公平PageRank的问题。通过考虑有群体偏向的随机游走来定义群体适应性公平性概念。提出了一种有效的投影梯度下降方法来计算局部最优的边缘权重，该方法不增加新的边，不调整重启向量，只修改现有边的相对重要性。

Result: 本文提出的方法与现有基线相比，通过对转移矩阵进行微小修改，显著提高了PageRank算法的公平性。

Conclusion: 本文提出了一种有效的方法，通过调整现有边的权重，将群体公平性融入到PageRank算法中，为解决链接分析算法中的公平性问题提供了新的思路。

Abstract: Link-analysis algorithms, such as PageRank, are instrumental in understanding the structural dynamics of networks by evaluating the importance of individual vertices based on their connectivity. Recently, with the rising importance of responsible AI, the question of fairness in link-analysis algorithms has gained traction. In this paper, we present a new approach for incorporating group fairness into the PageRank algorithm by reweighting the transition probabilities in the underlying transition matrix. We formulate the problem of achieving fair PageRank by seeking to minimize the fairness loss, which is the difference between the original group-wise PageRank distribution and a target PageRank distribution. We further define a group-adapted fairness notion, which accounts for group homophily by considering random walks with group-biased restart for each group. Since the fairness loss is non-convex, we propose an efficient projected gradient-descent method for computing locally-optimal edge weights. Unlike earlier approaches, we do not recommend adding new edges to the network, nor do we adjust the restart vector. Instead, we keep the topology of the underlying network unchanged and only modify the relative importance of existing edges. We empirically compare our approach with state-of-the-art baselines and demonstrate the efficacy of our method, where very small changes in the transition matrix lead to significant improvement in the fairness of the PageRank algorithm.

</details>


### [114] [Framing Climate Change on YouTube: North-South Divides in Narratives and Public Engagement](https://arxiv.org/abs/2512.08183)
*Sanika Damle,Radhika Krishnan*

Main category: cs.SI

TL;DR: 该研究分析了 YouTube 上气候变化辩论，通过主题建模和情感分析，揭示了全球北方和全球南方在叙事和公众反应方面的差异，并强调了弥合这些差距以促进更具包容性的气候行动的重要性。


<details>
  <summary>Details</summary>
Motivation: 探索 YouTube 上气候变化辩论中的全球北方-南方分歧，揭示其在叙事和公众参与方面的差异与共性。

Method: 收集并分析了758个气候相关视频及其评论区数据，应用主题建模和情感分析识别讨论模式。

Result: 研究发现，全球北方的视频强调碳减排政策，而全球南方的视频侧重于发展优先事项。双方都认同减排和国际协议的重要性。然而，全球北方的评论区充斥着批评、阴谋论和气候疲劳，而全球南方的评论区则更具支持性、建设性和知识导向。

Conclusion: YouTube 既反映又重塑了全球气候政治，揭示了精心策划的叙述与公众情绪之间的 G鸿沟。弥合这些分歧有助于推动更具包容性和合作性的气候行动。

Abstract: Climate change debates have gained increasing visibility on social media, with YouTube emerging as one of the most influential platforms for political communication. Reaching billions of users worldwide, it functions both as a news outlet and as a space for public discourse. While existing studies of climate discourse on YouTube often adopt a global perspective, this study examines the platform through the lens of the Global North-South divide. We analyse a dataset of 758 climate-related videos and their comment sections, applying topic modelling and sentiment analysis to identify recurring discursive patterns. Through these patterns, we recognise parallels with respect to debates in international climate negotiations. The findings reveal notable differences. Videos from the Global North and Global South reflect real-world divides, with the North emphasising the need for policies to curb carbon emissions, while the South highlights developmental priorities. A key area of convergence between the regions lies in the shared recognition of the importance of emissions reduction and international agreements. Audience responses, however, diverge more sharply: comment sections under Global North videos are dominated by criticism, conspiracy, and climate fatigue, whereas those under Global South videos are generally more supportive, constructive, and knowledge-oriented. Overall, the study demonstrates how YouTube reflects and reshapes global climate politics, while also revealing the gap between curated narratives and public sentiment. Bridging these divides may contribute to more inclusive and cooperative approaches to climate action.

</details>
