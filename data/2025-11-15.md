<div id=toc></div>

# Table of Contents

- [cs.CL](#cs.CL) [Total: 5]
- [cs.AI](#cs.AI) [Total: 1]
- [cs.LG](#cs.LG) [Total: 1]


<div id='cs.CL'></div>

# cs.CL [[Back]](#toc)

### [1] [Know Your Limits: Entropy Estimation Modeling for Compression and Generalization](https://arxiv.org/abs/2511.10618)
*Benjamin L. Badger,Matthew Neligeorge*

Main category: cs.CL

TL;DR: 本文介绍了一种编码器增强的因果解码器模型架构，其训练效率更高，压缩率优于因果Transformer，并能更好地进行泛化。


<details>
  <summary>Details</summary>
Motivation: 目前的语言预测模型在计算上难以准确估计语言熵并进行高效压缩。

Method: 引入编码器增强的因果解码器模型架构，并通过在每token基础上获取熵估计来训练模型。

Result: 编码器增强的因果解码器模型在适度硬件上也能实现更高的压缩率，并且其泛化能力优于未考虑熵训练的模型。

Conclusion: 通过将模型训练至接近而非超越估计的每token熵，可以提高模型的泛化能力。

Abstract: Language prediction is constrained by informational entropy intrinsic to language, such that there exists a limit to how accurate any language model can become and equivalently a lower bound to language compression. The most efficient language compression algorithms today are causal (next token prediction) large language models, but the use of these models to form accurate estimates of language entropy is currently computationally infeasible. We introduce encoder-augmented causal decoder model architectures that exhibit superior training efficiency characteristics and achieve higher compression than causal transformers even when trained on modest hardware. We demonstrate how entropy estimates can be obtained on a per-token basis, and show that the generalization of models trained to approach the entropy of their training data necessarily exceeds the generalization of models trained to minimize loss beyond this value. We show empirically that causal models trained to approach but not exceed estimated per-token entropies exhibit greater generalization than models trained without taking entropy into account.

</details>


### [2] [SSR: Socratic Self-Refine for Large Language Model Reasoning](https://arxiv.org/abs/2511.10621)
*Haizhou Shi,Ye Liu,Bo Pang,Zeyu Leo Liu,Hao Wang,Silvio Savarese,Caiming Xiong,Yingbo Zhou,Semih Yavuz*

Main category: cs.CL

TL;DR: Socratic Self-Refine (SSR) 是一种新颖的框架，它将大型语言模型（LLMs）的响应分解为可验证的子问题和子答案对，通过受控的重新解决和自洽性检查来实现步级置信度估计。通过识别不可靠的步骤并迭代地完善它们，SSR能产生更准确和可解释的推理链。


<details>
  <summary>Details</summary>
Motivation: 现有的测试时框架通常依赖粗粒度的自我验证和自我修正，这限制了它们在复杂任务上的有效性。

Method: Socratic Self-Refine (SSR) 框架将模型响应分解为可验证的（子问题，子答案）对，通过受控的重新解决和自洽性检查实现步级置信度估计。它能识别不可靠的步骤并迭代地完善它们。

Result: 在五个推理基准和三个大型语言模型上的实证结果表明，SSR持续优于最先进的迭代自我完善基线。

Conclusion: SSR 提供了一种原则性的黑盒方法，用于评估和理解大型语言模型的内部推理过程，不仅提升了性能，还增强了可解释性。

Abstract: Large Language Models (LLMs) have demonstrated remarkable reasoning abilities, yet existing test-time frameworks often rely on coarse self-verification and self-correction, limiting their effectiveness on complex tasks. In this paper, we propose Socratic Self-Refine (SSR), a novel framework for fine-grained evaluation and precise refinement of LLM reasoning. Our proposed SSR decomposes model responses into verifiable (sub-question, sub-answer) pairs, enabling step-level confidence estimation through controlled re-solving and self-consistency checks. By pinpointing unreliable steps and iteratively refining them, SSR produces more accurate and interpretable reasoning chains. Empirical results across five reasoning benchmarks and three LLMs show that SSR consistently outperforms state-of-the-art iterative self-refinement baselines. Beyond performance gains, SSR provides a principled black-box approach for evaluating and understanding the internal reasoning processes of LLMs. Code is available at https://github.com/SalesforceAIResearch/socratic-self-refine-reasoning.

</details>


### [3] [Instella: Fully Open Language Models with Stellar Performance](https://arxiv.org/abs/2511.10628)
*Jiang Liu,Jialian Wu,Xiaodong Yu,Yusheng Su,Prakamya Mishra,Gowtham Ramesh,Sudhanshu Ranjan,Chaitanya Manem,Ximeng Sun,Ze Wang,Pratik Prabhanjan Brahma,Zicheng Liu,Emad Barsoum*

Main category: cs.CL

TL;DR: Instella是一个完全开源的30亿参数语言模型家族，在公开数据和代码库上训练，由AMD Instinct MI300X GPU支持，并通过大规模预训练、通用指令调优和与人类偏好对齐开发。 它在全开源模型中实现了最先进的结果，并推出了长文本（128K tokens）和数学任务的专业版本。


<details>
  <summary>Details</summary>
Motivation: 目前高性能的大型语言模型大多是闭源或部分开源的，这限制了透明度和可复现性。

Method: Instella模型通过大规模预训练、通用指令调优，并与人类偏好对齐开发。 他们还推出了两个专业版本：Instella-Long，能够处理长达128K tokens的上下文长度；Instella-Math，一个通过在数学任务上进行监督微调和强化学习增强的推理模型。

Result: Instella在全开源模型中实现了最先进的结果，并与同等规模的领先开源模型竞争。 它拥有能够处理长达128K tokens上下文长度的Instella-Long模型，以及一个在数学任务上表现出色的Instella-Math模型。

Conclusion: Instella作为一个透明、高性能和多功能的替代方案，促进了开放和可复现的语言建模研究。

Abstract: Large language models (LLMs) have demonstrated remarkable performance across a wide range of tasks, yet the majority of high-performing models remain closed-source or partially open, limiting transparency and reproducibility. In this work, we introduce Instella, a family of fully open three billion parameter language models trained entirely on openly available data and codebase. Powered by AMD Instinct MI300X GPUs, Instella is developed through large-scale pre-training, general-purpose instruction tuning, and alignment with human preferences. Despite using substantially fewer pre-training tokens than many contemporaries, Instella achieves state-of-the-art results among fully open models and is competitive with leading open-weight models of comparable size. We further release two specialized variants: Instella-Long, capable of handling context lengths up to 128K tokens, and Instella-Math, a reasoning-focused model enhanced through supervised fine-tuning and reinforcement learning on mathematical tasks. Together, these contributions establish Instella as a transparent, performant, and versatile alternative for the community, advancing the goal of open and reproducible language modeling research.

</details>


### [4] [Black-Box On-Policy Distillation of Large Language Models](https://arxiv.org/abs/2511.10643)
*Tianzhu Ye,Li Dong,Zewen Chi,Xun Wu,Shaohan Huang,Furu Wei*

Main category: cs.CL

TL;DR: GAD：一种黑盒蒸馏新范式，使得学生模型在LMSYS-Chat上媲美GPT-5-Chat。


<details>
  <summary>Details</summary>
Motivation: 开发一种在没有内部信息的情况下，从专有教师模型中学习大型语言模型（LLM）的黑盒蒸馏方法，以创建学生LLM。

Method: 生成对抗蒸馏（GAD），将学生LLM作为生成器，训练判别器区分其与教师LLM的响应，形成最小最大博弈。判别器作为同策略奖励模型，与学生一同演进，提供稳定自适应的反馈。

Result: GAD显著超越了常用的序列级知识蒸馏，特别是，使用GAD训练的Qwen2.5-14B-Instruct（学生）在LMSYS-Chat自动评估中与教师模型GPT-5-Chat表现相当。

Conclusion: GAD是一种有前景且有效的黑盒LLM蒸馏范式。

Abstract: Black-box distillation creates student large language models (LLMs) by learning from a proprietary teacher model's text outputs alone, without access to its internal logits or parameters. In this work, we introduce Generative Adversarial Distillation (GAD), which enables on-policy and black-box distillation. GAD frames the student LLM as a generator and trains a discriminator to distinguish its responses from the teacher LLM's, creating a minimax game. The discriminator acts as an on-policy reward model that co-evolves with the student, providing stable, adaptive feedback. Experimental results show that GAD consistently surpasses the commonly used sequence-level knowledge distillation. In particular, Qwen2.5-14B-Instruct (student) trained with GAD becomes comparable to its teacher, GPT-5-Chat, on the LMSYS-Chat automatic evaluation. The results establish GAD as a promising and effective paradigm for black-box LLM distillation.

</details>


### [5] [ParoQuant: Pairwise Rotation Quantization for Efficient Reasoning LLM Inference](https://arxiv.org/abs/2511.10645)
*Yesheng Liang,Haisheng Chen,Song Han,Zhijian Liu*

Main category: cs.CL

TL;DR: ParoQuant是一种权重量化方法，它结合了Givens旋转和通道级缩放，并协同设计了推理内核，使其在推理任务上的准确性比现有方法提高了2.4%以上，开辟了LLM部署的新途径。


<details>
  <summary>Details</summary>
Motivation: 现有的权重量化方法在推理大型语言模型时，由于权值和激活值中存在异常值，导致量化误差大，精度严重下降。这些方法未能充分抑制异常值，或者在推理过程中引入了显著的开销。

Method: ParoQuant通过结合硬件高效和可优化的独立Givens旋转与通道级缩放，来平衡跨通道的幅度并缩小每个量化组内的动态范围。该方法还协同设计了推理核，以充分利用GPU并行性，并在运行时保持旋转和缩放的轻量化。

Result: ParoQuant在推理任务上比AWQ平均提高了2.4%的准确性，且开销低于10%。

Conclusion: ParoQuant为推理大型语言模型提供了一种更高效和准确的部署方法。

Abstract: Weight-only post-training quantization (PTQ) compresses the weights of Large Language Models (LLMs) into low-precision representations to reduce memory footprint and accelerate inference. However, the presence of outliers in weights and activations often leads to large quantization errors and severe accuracy degradation, especially in recent reasoning LLMs where errors accumulate across long chains of thought. Existing PTQ methods either fail to sufficiently suppress outliers or introduce significant overhead during inference. In this paper, we propose Pairwise Rotation Quantization (ParoQuant), a weight-only PTQ method that combines hardware-efficient and optimizable independent Givens rotations with channel-wise scaling to even out the magnitude across channels and narrow the dynamic range within each quantization group. We further co-design the inference kernel to fully exploit GPU parallelism and keep the rotations and scaling lightweight at runtime. ParoQuant achieves an average 2.4% accuracy improvement over AWQ on reasoning tasks with less than 10% overhead. This paves the way for more efficient and accurate deployment of reasoning LLMs.

</details>


<div id='cs.AI'></div>

# cs.AI [[Back]](#toc)

### [6] [Querying Labeled Time Series Data with Scenario Programs](https://arxiv.org/abs/2511.10627)
*Edward Kim,Devan Shanker,Varun Bharadwaj,Hongbeen Park,Jinkyu Kim,Hazem Torfah,Daniel J Fremont,Sanjit A Seshia*

Main category: cs.AI

TL;DR: 这篇论文提出了一种通过匹配抽象场景来验证模拟故障场景的方法，该方法在准确性和速度上优于现有商业模型。


<details>
  <summary>Details</summary>
Motivation: 模拟测试对确保信息物理系统（CPS）的安全至关重要，但模拟中发现的故障场景是否能在现实世界中重现仍是一个关键问题。

Method: 通过Scenic概率编程语言将抽象场景表示为场景程序，并提出了一种查询算法来识别与指定场景匹配的数据子集。

Result: 实验结果表明，该算法比最先进的商业视觉大型语言模型在查询场景方面更准确，速度快几个数量级，并且能够随着查询时间序列数据的持续时间进行扩展。

Conclusion: 该研究提供了一种有效的方法来弥补模拟与现实之间的差距，通过在真实世界数据集中验证模拟故障场景的出现，有助于确保信息物理系统的安全。

Abstract: Simulation-based testing has become a crucial complement to road testing for ensuring the safety of cyber physical systems (CPS). As a result, significant research efforts have been directed toward identifying failure scenarios within simulation environments. However, a critical question remains. Are the AV failure scenarios discovered in simulation reproducible on actual systems in the real world? The sim-to-real gap caused by differences between simulated and real sensor data means that failure scenarios identified in simulation might either be artifacts of synthetic sensor data or actual issues that also occur with real sensor data. To address this, an effective approach to validating simulated failure scenarios is to locate occurrences of these scenarios within real-world datasets and verify whether the failure persists on the datasets. To this end, we introduce a formal definition of how labeled time series sensor data can match an abstract scenario, represented as a scenario program using the Scenic probabilistic programming language. We present a querying algorithm that, given a scenario program and a labeled dataset, identifies the subset of data that matches the specified scenario. Our experiment shows that our algorithm is more accurate and orders of magnitude faster in querying scenarios than the state-of-the-art commercial vision large language models, and can scale with the duration of queried time series data.

</details>


<div id='cs.LG'></div>

# cs.LG [[Back]](#toc)

### [7] [Algorithm Design and Stronger Guarantees for the Improving Multi-Armed Bandits Problem](https://arxiv.org/abs/2511.10619)
*Avrim Blum,Marten Garicano,Kavya Ravichandran,Dravyansh Sharma*

Main category: cs.LG

TL;DR: 这篇论文提出了一种新的多臂老虎机算法，该算法在臂奖励曲线满足额外属性时，能够实现更好的臂选择保证，并且样本复杂度更低。


<details>
  <summary>Details</summary>
Motivation: 尽管在改进型多臂老虎机问题上已经有一些算法，但它们的性能保证比较悲观，甚至存在较强的理论下限。

Method: 我们提出了两种新的参数化多臂老虎机算法族。我们还展示了如何利用离线数据，学习到每个族中接近最优的算法的样本复杂度。

Result: 我们提出的第一种算法族包含了之前工作中提出的最优随机算法。当臂奖励曲线满足与凹度强度相关的额外属性时，该族中一个选择得当的算法可以达到更强的保证，并且在k上具有最优的依赖性。我们的第二种算法族包含的算法，既能保证在表现良好的情况下识别出最优臂，又能退化到表现不佳情况下的最坏情况保证。

Conclusion: 我们无需验证假设是否满足，就可以在多臂老虎机奖励优化问题上通过统计学习的视角，获得更强的数据依赖保证。

Abstract: The improving multi-armed bandits problem is a formal model for allocating effort under uncertainty, motivated by scenarios such as investing research effort into new technologies, performing clinical trials, and hyperparameter selection from learning curves. Each pull of an arm provides reward that increases monotonically with diminishing returns. A growing line of work has designed algorithms for improving bandits, albeit with somewhat pessimistic worst-case guarantees. Indeed, strong lower bounds of $Ω(k)$ and $Ω(\sqrt{k})$ multiplicative approximation factors are known for both deterministic and randomized algorithms (respectively) relative to the optimal arm, where $k$ is the number of bandit arms. In this work, we propose two new parameterized families of bandit algorithms and bound the sample complexity of learning the near-optimal algorithm from each family using offline data. The first family we define includes the optimal randomized algorithm from prior work. We show that an appropriately chosen algorithm from this family can achieve stronger guarantees, with optimal dependence on $k$, when the arm reward curves satisfy additional properties related to the strength of concavity. Our second family contains algorithms that both guarantee best-arm identification on well-behaved instances and revert to worst case guarantees on poorly-behaved instances. Taking a statistical learning perspective on the bandit rewards optimization problem, we achieve stronger data-dependent guarantees without the need for actually verifying whether the assumptions are satisfied.

</details>
