<div id=toc></div>

# Table of Contents

- [cs.CL](#cs.CL) [Total: 5]
- [cs.AI](#cs.AI) [Total: 3]
- [cs.SI](#cs.SI) [Total: 1]
- [cs.GT](#cs.GT) [Total: 1]
- [cs.LG](#cs.LG) [Total: 4]


<div id='cs.CL'></div>

# cs.CL [[Back]](#toc)

### [1] [ToolOrchestra: Elevating Intelligence via Efficient Model and Tool Orchestration](https://arxiv.org/abs/2511.21689)
*Hongjin Su,Shizhe Diao,Ximing Lu,Mingjie Liu,Jiacheng Xu,Xin Dong,Yonggan Fu,Peter Belcak,Hanrong Ye,Hongxu Yin,Yi Dong,Evelina Bakhturina,Tao Yu,Yejin Choi,Jan Kautz,Pavlo Molchanov*

Main category: cs.CL

TL;DR: ToolOrchestra 是一种通过强化学习训练小型编排器来协调智能工具的方法，它在解决复杂问题方面比现有方法更高效和有效。


<details>
  <summary>Details</summary>
Motivation: 大型语言模型在解决深度复杂问题时面临概念性和计算上的挑战，因此需要一种方法来提升效率和智能上限。

Method: 本文介绍了 ToolOrchestra，它利用强化学习训练小型编排器来管理其他模型和工具。该方法考虑了结果、效率和用户偏好等方面的奖励。

Result: 通过 ToolOrchestra，本文训练了一个 8B 的 Orchestrator 模型。该模型在 HLE 上的得分达到 37.1%，超过了 GPT-5 (35.1%)，同时效率提升了 2.5 倍。在 tau2-Bench 和 FRAMES 上，Orchestrator 也大幅超越 GPT-5，而成本仅为其 30%。

Conclusion: ToolOrchestra 的结果表明，通过轻量级编排模型组合不同的工具，比现有方法更高效和有效，为实用且可扩展的工具增强推理系统铺平了道路。

Abstract: Large language models are powerful generalists, yet solving deep and complex problems such as those of the Humanity's Last Exam (HLE) remains both conceptually challenging and computationally expensive. We show that small orchestrators managing other models and a variety of tools can both push the upper bound of intelligence and improve efficiency in solving difficult agentic tasks. We introduce ToolOrchestra, a method for training small orchestrators that coordinate intelligent tools. ToolOrchestra explicitly uses reinforcement learning with outcome-, efficiency-, and user-preference-aware rewards. Using ToolOrchestra, we produce Orchestrator, an 8B model that achieves higher accuracy at lower cost than previous tool-use agents while aligning with user preferences on which tools are to be used for a given query. On HLE, Orchestrator achieves a score of 37.1%, outperforming GPT-5 (35.1%) while being 2.5x more efficient. On tau2-Bench and FRAMES, Orchestrator surpasses GPT-5 by a wide margin while using only about 30% of the cost. Extensive analysis shows that Orchestrator achieves the best trade-off between performance and cost under multiple metrics, and generalizes robustly to unseen tools. These results demonstrate that composing diverse tools with a lightweight orchestration model is both more efficient and more effective than existing methods, paving the way for practical and scalable tool-augmented reasoning systems.

</details>


### [2] [Auxiliary Metrics Help Decoding Skill Neurons in the Wild](https://arxiv.org/abs/2511.21610)
*Yixiu Zhao,Xiaozhi Wang,Zijun Yao,Lei Hou,Juanzi Li*

Main category: cs.CL

TL;DR: 这篇论文介绍了一种识别大型语言模型（LLM）中编码特定技能的神经元的方法，该方法通过关联神经元激活与外部标签和模型置信度等辅助指标，揭示可解释和任务特定的行为，并在开放式文本生成和自然语言推断任务上得到验证。


<details>
  <summary>Details</summary>
Motivation: 大型语言模型（LLM）能力强大，但其内部机制不透明。

Method: 本文提出了一种简单、轻量级且广泛适用的方法，专注于分离编码特定技能的神经元。该方法建立在通过对分类任务进行软提示训练来识别“技能神经元”的先前工作之上，并将分析扩展到涉及多种技能的复杂场景。研究人员将神经元激活与辅助指标（如外部标签和模型的置信度分数）关联起来，从而揭示了可解释和任务特定的行为，而无需手动进行。

Result: 在开放式文本生成和自然语言推断任务上验证了该方法的有效性，证明了它不仅能检测驱动已知技能的神经元，还能发现BigBench算术推理中以前未识别的捷径。

Conclusion: 该方法能够有效识别LLMs中的特定技能神经元，揭示模型的可解释行为和潜在捷径。

Abstract: Large language models (LLMs) exhibit remarkable capabilities across a wide range of tasks, yet their internal mechanisms remain largely opaque. In this paper, we introduce a simple, lightweight, and broadly applicable method with a focus on isolating neurons that encode specific skills. Building upon prior work that identified "skill neurons" via soft prompt training on classification tasks, our approach extends the analysis to complex scenarios involving multiple skills. We correlate neuron activations with auxiliary metrics -- such as external labels and the model's own confidence score -- thereby uncovering interpretable and task-specific behaviors without the need for manual token aggregation. We empirically validate our method on tasks spanning open-ended text generation and natural language inference, demonstrating its ability to detect neurons that not only drive known skills but also reveal previously unidentified shortcuts in arithmetic reasoning on BigBench.

</details>


### [3] [Beyond URLs: Metadata Diversity and Position for Efficient LLM Pretraining](https://arxiv.org/abs/2511.21613)
*Dongyang Fan,Diba Hashemi,Sai Praneeth Karimireddy,Martin Jaggi*

Main category: cs.CL

TL;DR: 这篇论文探讨了将元数据纳入大型语言模型（LLM）预训练以加速训练的方法。


<details>
  <summary>Details</summary>
Motivation: 以往的工作只关注了URL这一个元数据信号，这篇研究旨在探索其他类型的元数据是否能带来更大的益处。

Method: 作者研究了更广泛的元数据类型，并引入了元数据追加（metadata appending）的方法，将预测适当元数据作为辅助任务，以提高训练效率。此外，还使用了通过掩码损失训练的可学习元标记（learnable meta-tokens）来恢复部分加速效果。

Result: 作者发现，诸如文档质量的细粒度指标等其他类型的元数据也能在预训练时加速模型训练，并识别出有效元数据的共同特征：它们以更细的粒度编码信息。通过探究，作者分析了潜在表示，以理解元数据如何影响学习。

Conclusion: 这些结果为整合元数据以提高LLM预训练的效率和有效性提供了实用的指导。

Abstract: Incorporating metadata in Large Language Models (LLMs) pretraining has recently emerged as a promising approach to accelerate training. However prior work highlighted only one useful signal-URLs, leaving open the question of whether other forms of metadata could yield greater benefits. In this study, we investigate a wider range of metadata types and find other types of metadata, such as fine-grained indicators of document quality that can also accelerate pretraining when prepended. We identify a common feature among effective metadata: they encode information at a finer granularity. We further introduce metadata appending as a means of improving training efficiency, where predicting an appropriate metadata as auxiliary task can help speed up pretraining. In addition, learnable meta-tokens trained with masked loss can recover part of the speedup by inducing quality-aware latent structure. Using probing, we analyze latent representations to understand how metadata shapes learning. Together, these results yield practical guidelines for integrating metadata to improve both the efficiency and effectiveness of LLM pretraining.

</details>


### [4] [The author is dead, but what if they never lived? A reception experiment on Czech AI- and human-authored poetry](https://arxiv.org/abs/2511.21629)
*Anna Marklová,Ondřej Vinš,Martina Vokáčová,Jiří Milička*

Main category: cs.CL

TL;DR: 该研究探讨了捷克语使用者对AI生成诗歌的感知，发现人们在识别作者方面表现不佳，并且存在对AI作品的偏见。


<details>
  <summary>Details</summary>
Motivation: 探究大型语言模型在非英语、低资源语言（如捷克语）中生成诗歌的能力，以及人类对其的感知和评价。

Method: 参与者对AI和人类创作的捷克语诗歌进行作者识别和审美判断。通过逻辑回归模型分析人们对诗歌的喜好与作者识别准确性之间的关系，并考察了诗歌熟悉度和文学背景的影响。

Result: 参与者在识别作者方面的准确率接近随机水平（平均45.8%），表明AI生成的捷克语诗歌与人类创作的诗歌基本无法区分。审美评价显示出强烈的作者偏见：当参与者认为一首诗是AI生成时，即使AI诗歌在平均水平上与人类诗歌相当或更受欢迎，他们也会更不喜欢AI诗歌。喜欢一首诗的人越少，他们准确识别作者的可能性就越低。对诗歌的熟悉度或文学背景对识别准确性没有影响。

Conclusion: AI即使在形态复杂、低资源的斯拉夫语（如捷克语）中也能令人信服地创作诗歌。读者的作者信念和诗歌审美评价之间存在相互关联。

Abstract: Large language models are increasingly capable of producing creative texts, yet most studies on AI-generated poetry focus on English -- a language that dominates training data. In this paper, we examine the perception of AI- and human-written Czech poetry. We ask if Czech native speakers are able to identify it and how they aesthetically judge it. Participants performed at chance level when guessing authorship (45.8\% correct on average), indicating that Czech AI-generated poems were largely indistinguishable from human-written ones. Aesthetic evaluations revealed a strong authorship bias: when participants believed a poem was AI-generated, they rated it as less favorably, even though AI poems were in fact rated equally or more favorably than human ones on average. The logistic regression model uncovered that the more the people liked a poem, the less probable was that they accurately assign the authorship. Familiarity with poetry or literary background had no effect on recognition accuracy. Our findings show that AI can convincingly produce poetry even in a morphologically complex, low-resource (with respect of the training data of AI models) Slavic language such as Czech. The results suggest that readers' beliefs about authorship and the aesthetic evaluation of the poem are interconnected.

</details>


### [5] [Revisiting Generalization Across Difficulty Levels: It's Not So Easy](https://arxiv.org/abs/2511.21692)
*Yeganeh Kordi,Nihal V. Nayak,Max Zuo,Ilana Nguyen,Stephen H. Bach*

Main category: cs.CL

TL;DR: 本文研究大型语言模型（LLM）在不同任务难度上的泛化能力，发现跨难度泛化能力有限，训练或评估数据中都应包含不同难度范围的示例以避免风险。


<details>
  <summary>Details</summary>
Motivation: 探讨LLM如何跨不同任务难度进行泛化，以有效进行数据整理和评估。现有研究对训练数据难度如何影响LLM表现及其在不同难度测试数据上的泛化能力存在争议，本研究旨在系统性地解决此问题。

Method: 通过对模型、数据集和细粒度示例难度组进行系统评估来解决问题。本文使用数千个不同LLM的输出和项目反应理论（IRT）对六个数据集中的示例进行难度排序，这种难度评分完全由LLM的能力决定，排除了人类的主观判断。

Result: 研究发现跨难度泛化能力通常是有限的。无论是在简单数据还是困难数据上进行训练，都无法在所有难度范围内实现一致的性能提升。

Conclusion: LLM的训练和评估数据中都应包含不同难度的示例。在处理数据难度时采取捷径存在风险。

Abstract: We investigate how well large language models (LLMs) generalize across different task difficulties, a key question for effective data curation and evaluation. Existing research is mixed regarding whether training on easier or harder data leads to better results, and whether those gains come on easier or harder test data. We address this question by conducting a systematic evaluation of LLMs' generalization across models, datasets, and fine-grained groups of example difficulty. We rank examples in six datasets using the outputs of thousands of different LLMs and Item Response Theory (IRT), a well-established difficulty metric in educational testing. Unlike prior work, our difficulty ratings are therefore determined solely by the abilities of many different LLMs, excluding human opinions of difficulty. With a more objective, larger-scale, and finer-grained analysis, we show that cross-difficulty generalization is often limited; training on either easy or hard data cannot achieve consistent improvements across the full range of difficulties. These results show the importance of having a range of difficulties in both training and evaluation data for LLMs, and that taking shortcuts with respect to difficulty is risky.

</details>


<div id='cs.AI'></div>

# cs.AI [[Back]](#toc)

### [6] [Bridging the Unavoidable A Priori: A Framework for Comparative Causal Modeling](https://arxiv.org/abs/2511.21636)
*Peter S. Hovmand,Kari O'Donnell,Callie Ogland-Hand,Brian Biroscak,Douglas D. Gunzler*

Main category: cs.AI

TL;DR: 本文探讨了AI/ML模型中人类偏见的放大效应，并提出将系统动力学与结构方程模型相结合，以期弥合不同学科方法之间的鸿沟，为数据科学和AI/ML应用提供统一的数学框架和认知基础。


<details>
  <summary>Details</summary>
Motivation: AI/ML模型在解决问题的同时，也无意中放大了人类偏见。负责任的AI/ML倡导者试图利用系统动力学更丰富的因果模型来指导负责任的AI/ML开发。然而，不同学科方法（如系统动力学和结构方程模型）之间潜在假设的差异阻碍了这项工作的进展。

Method: 本文将系统动力学和结构方程模型整合到一个共同的数学框架中。

Result: 这个共同的数学框架可以用于从分布中生成系统、开发方法和比较结果。

Conclusion: 本文旨在通过结合系统动力学和结构方程模型，为数据科学和AI/ML应用提供一个统一的数学框架，来弥合不同方法之间的鸿沟，并为系统动力学应用于数据科学和AI/ML的认知基础提供信息。

Abstract: AI/ML models have rapidly gained prominence as innovations for solving previously unsolved problems and their unintended consequences from amplifying human biases. Advocates for responsible AI/ML have sought ways to draw on the richer causal models of system dynamics to better inform the development of responsible AI/ML. However, a major barrier to advancing this work is the difficulty of bringing together methods rooted in different underlying assumptions (i.e., Dana Meadow's "the unavoidable a priori"). This paper brings system dynamics and structural equation modeling together into a common mathematical framework that can be used to generate systems from distributions, develop methods, and compare results to inform the underlying epistemology of system dynamics for data science and AI/ML applications.

</details>


### [7] [On the Limits of Innate Planning in Large Language Models](https://arxiv.org/abs/2511.21591)
*Charles Schepanowski,Charles Ling*

Main category: cs.AI

TL;DR: 研究了大型语言模型（LLMs）在不使用工具的情况下进行规划和状态推理的能力。通过8-puzzle任务评估，发现LLMs在此类任务中存在脆性内部状态表示和弱启发式规划等显著局限性。


<details>
  <summary>Details</summary>
Motivation: 大型语言模型在许多基准测试中表现出色，但其规划和有状态推理能力尚不明确。因此，本文旨在直接研究LLMs的这些能力，不依赖代码执行或其他外部工具。

Method: 本文使用8-puzzle（八数码问题）作为测试任务，该任务要求跟踪状态和进行目标导向的规划，并允许精确、逐步的评估。在常见的提示条件（Zero-Shot, Chain-of-Thought, Algorithm-of-Thought）以及分层纠正反馈下测试了四种模型。此外，还引入了一个外部移动验证器，只提供有效移动，以进一步评估模型的能力。

Result: 纠正反馈在某些模型-提示组合下提高了成功率，但许多成功的运行是漫长、计算昂贵且间接的。即使在外部移动验证器提供帮助的情况下，所有模型都未能解决任何谜题。定性分析揭示了所有模型普遍存在的两个主要缺陷：1）脆性内部状态表示，导致频繁的无效移动；2）弱启发式规划，模型经常陷入循环或选择不能缩短与目标状态距离的行动。

Conclusion: 在没有外部工具（如代码解释器）的情况下，当前LLMs在规划方面存在显著局限性。未来的进展可能需要维护显式状态和执行结构化搜索的机制。

Abstract: Large language models (LLMs) achieve impressive results on many benchmarks, yet their capacity for planning and stateful reasoning remains unclear. We study these abilities directly, without code execution or other tools, using the 8-puzzle: a classic task that requires state tracking and goal-directed planning while allowing precise, step-by-step evaluation. Four models are tested under common prompting conditions (Zero-Shot, Chain-of-Thought, Algorithm-of-Thought) and with tiered corrective feedback. Feedback improves success rates for some model-prompt combinations, but many successful runs are long, computationally expensive, and indirect. We then examine the models with an external move validator that provides only valid moves. Despite this level of assistance, none of the models solve any puzzles in this setting. Qualitative analysis reveals two dominant deficits across all models: (1) brittle internal state representations, leading to frequent invalid moves, and (2) weak heuristic planning, with models entering loops or selecting actions that do not reduce the distance to the goal state. These findings indicate that, in the absence of external tools such as code interpreters, current LLMs have substantial limitations in planning and that further progress may require mechanisms for maintaining explicit state and performing structured search.

</details>


### [8] [Agentic Learner with Grow-and-Refine Multimodal Semantic Memory](https://arxiv.org/abs/2511.21678)
*Weihao Bo,Shan Zhang,Yanpeng Sun,Jingjing Wu,Qunyi Xie,Xiao Tan,Kunbin Chen,Wei He,Xiaofan Li,Na Zhao,Jingdong Wang,Zechao Li*

Main category: cs.AI

TL;DR: ViLoMem是一个双流记忆框架，可以帮助MLLM避免重复犯错，并在多模态基准测试中持续提高表现。


<details>
  <summary>Details</summary>
Motivation: 现有的多模态大模型（MLLMs）在解决独立查询时表现出色，但它们通常从头开始解决每个问题，容易重复犯错。现有的记忆增强智能体主要存储过去的轨迹以供重用，但这种记忆存在简洁性偏差，并且无法保存视觉注意力和逻辑推理如何共同促进解决方案。这与人类认知不符，人类的语义记忆是多模态和整合的。

Method: 我们引入了ViLoMem，这是一个双流记忆框架，它构建了紧凑的、基于模式的记忆。它分别编码视觉分散模式和逻辑推理错误，使MLLM能够从成功和失败的经验中学习。遵循“增长和完善”的原则，系统逐步积累和更新多模态语义知识，以保持稳定、可推广的策略，同时避免灾难性遗忘。

Result: 在六个多模态基准测试中，ViLoMem持续提高了pass@1准确性，并显著减少了重复的视觉和逻辑错误。消融实验证实了具有明确分散-幻觉分离的双流记忆的必要性。

Conclusion: ViLoMem是一个有效的双模态记忆框架，可以帮助MLLM从错误中学习并在多模态任务中取得更好的表现。这种错误感知多模态记忆对于终身和跨领域智能学习具有重要价值。

Abstract: MLLMs exhibit strong reasoning on isolated queries, yet they operate de novo -- solving each problem independently and often repeating the same mistakes. Existing memory-augmented agents mainly store past trajectories for reuse. However, trajectory-based memory suffers from brevity bias, gradually losing essential domain knowledge. More critically, even in truly multimodal problem-solving settings, it records only a single-modality trace of past behavior, failing to preserve how visual attention and logical reasoning jointly contributed to the solution. This is fundamentally misaligned with human cognition: semantic memory is both multimodal and integrated, preserving visual and abstract knowledge through coordinated but distinct representational streams. We thus introduce ViLoMem, a dual-stream memory framework that constructs compact, schema-based memory. It separately encodes visual distraction patterns and logical reasoning errors, enabling MLLMs to learn from their successful and failed experiences. Following a grow-and-refine principle, the system incrementally accumulates and updates multimodal semantic knowledge -- preserving stable, generalizable strategies while avoiding catastrophic forgetting. Across six multimodal benchmarks, ViLoMem consistently improves pass@1 accuracy and substantially reduces repeated visual and logical errors. Ablations confirm the necessity of dual-stream memory with explicit distraction--hallucination separation, demonstrating the value of error-aware multimodal memory for lifelong and cross-domain agentic learning. Our project page will be available at https://weihao-bo.github.io/ViLoMeo-page.

</details>


<div id='cs.SI'></div>

# cs.SI [[Back]](#toc)

### [9] [TAGFN: A Text-Attributed Graph Dataset for Fake News Detection in the Age of LLMs](https://arxiv.org/abs/2511.21624)
*Kay Liu,Yuwei Han,Haoyan Xu,Henry Peng Zou,Yue Zhao,Philip S. Yu*

Main category: cs.SI

TL;DR: 本文介绍了TAGFN，一个用于异常值检测（特别是假新闻检测）的大规模真实世界文本属性图数据集，旨在解决该领域缺乏可靠基准数据集的问题。该数据集支持传统和基于LLM的图异常值检测方法的评估，并促进通过微调LLM来开发错误信息检测能力。


<details>
  <summary>Details</summary>
Motivation: 大型语言模型（LLMs）彻底改变了文本属性图上的机器学习，但它们在图异常值检测，特别是在假新闻检测中的应用尚未得到充分探索。主要挑战在于缺乏大规模、真实且标注良好的数据集，这些数据集可以作为异常值检测的可靠基准。

Method: 本文通过引入TAGFN数据集来解决数据稀缺问题，该数据集是一个大规模、真实世界的文本属性图数据集，专为异常值（尤其是假新闻）检测而设计。

Result: TAGFN数据集的引入，使得能够对传统和基于LLM的图异常值检测方法进行严格评估，并通过微调LLM来促进错误信息检测能力的发展。

Conclusion: TAGFN将成为社区的宝贵资源，推动鲁棒的图基异常值检测和可信任人工智能的进展。

Abstract: Large Language Models (LLMs) have recently revolutionized machine learning on text-attributed graphs, but the application of LLMs to graph outlier detection, particularly in the context of fake news detection, remains significantly underexplored. One of the key challenges is the scarcity of large-scale, realistic, and well-annotated datasets that can serve as reliable benchmarks for outlier detection. To bridge this gap, we introduce TAGFN, a large-scale, real-world text-attributed graph dataset for outlier detection, specifically fake news detection. TAGFN enables rigorous evaluation of both traditional and LLM-based graph outlier detection methods. Furthermore, it facilitates the development of misinformation detection capabilities in LLMs through fine-tuning. We anticipate that TAGFN will be a valuable resource for the community, fostering progress in robust graph-based outlier detection and trustworthy AI. The dataset is publicly available at https://huggingface.co/datasets/kayzliu/TAGFN and our code is available at https://github.com/kayzliu/tagfn.

</details>


<div id='cs.GT'></div>

# cs.GT [[Back]](#toc)

### [10] [Arctic Auctions, Linear Fisher Markets, and Rational Convex Programs](https://arxiv.org/abs/2511.21637)
*Vijay V. Vazirani*

Main category: cs.GT

TL;DR: 本文探讨了北极拍卖和线性费雪市场这两种经济学和算法博弈论中的基础结构在差异化商品高效配置中的统一应用。


<details>
  <summary>Details</summary>
Motivation: 在复杂市场中，传统方法难以有效配置差异化商品，因此需要一种新的理论框架来解决这个问题。

Method: 本文通过证明北极拍卖的均衡可以通过有理凸规划来捕捉，并推导出了第一个组合多项式时间算法来计算北极拍卖均衡，从而将北极拍卖和线性费雪市场进行统一。

Result: 北极拍卖的均衡能够被有理凸规划所捕捉；获得了第一个用于计算北极拍卖均衡的组合多项式时间算法。

Conclusion: 本文成功地将北极拍卖和线性费雪市场这两种理论结构统一起来，并为差异化商品在复杂市场中的高效配置提供了新的理论工具和算法。

Abstract: This paper unifies two foundational constructs from economics and algorithmic game theory, the Arctic Auction and the linear Fisher market, to address the efficient allocation of differentiated goods in complex markets. Our main contributions are showing that an equilibrium for the Arctic Auction is captured by a Rational Convex Program, and deriving the first combinatorial polynomial-time algorithm for computing Arctic Auction equilibria.

</details>


<div id='cs.LG'></div>

# cs.LG [[Back]](#toc)

### [11] [Visualizing LLM Latent Space Geometry Through Dimensionality Reduction](https://arxiv.org/abs/2511.21594)
*Alex Ning,Vainateya Rangaraju*

Main category: cs.LG

TL;DR: 这篇论文通过降维方法（PCA和UMAP）提取、处理和可视化了大型语言模型（LLMs）的内部机制，特别是Transformer模型中的潜在状态几何。


<details>
  <summary>Details</summary>
Motivation: 尽管大型语言模型在自然语言任务中取得了最先进的结果，但其内部机制难以解释。

Method: 本文在Transformer块内的多个点捕获逐层激活，并通过主成分分析（PCA）和均匀流形逼近（UMAP）进行系统分析。

Result: 研究发现：1. 在中间层中，注意力机制和多层感知器（MLP）组件的输出之间存在明显分离。2. 初始序列位置的潜在状态规范较高。3. 潜在状态的逐层演变可视化。4. GPT-2的位置嵌入具有高维螺旋结构。5. LLaMa模型中存在序列几何模式。6. 对重复的token序列进行了实验。

Conclusion: 本研究旨在支持对Transformer内部机制的系统分析，以期推动可复现的解释性研究。

Abstract: Large language models (LLMs) achieve state-of-the-art results across many natural language tasks, but their internal mechanisms remain difficult to interpret. In this work, we extract, process, and visualize latent state geometries in Transformer-based language models through dimensionality reduction. We capture layerwise activations at multiple points within Transformer blocks and enable systematic analysis through Principal Component Analysis (PCA) and Uniform Manifold Approximation (UMAP). We demonstrate experiments on GPT-2 and LLaMa models, where we uncover interesting geometric patterns in latent space. Notably, we identify a clear separation between attention and MLP component outputs across intermediate layers, a pattern not documented in prior work to our knowledge. We also characterize the high norm of latent states at the initial sequence position and visualize the layerwise evolution of latent states. Additionally, we demonstrate the high-dimensional helical structure of GPT-2's positional embeddings, the sequence-wise geometric patterns in LLaMa, and experiment with repeating token sequences. We aim to support systematic analysis of Transformer internals with the goal of enabling further reproducible interpretability research. We make our code available at https://github.com/Vainateya/Feature_Geometry_Visualization.

</details>


### [12] [On the Origin of Algorithmic Progress in AI](https://arxiv.org/abs/2511.21622)
*Hans Gundlach,Alex Fogelson,Jayson Lynch,Ana Trisovic,Jonathan Rosenfeld,Anmol Sandhu,Neil Thompson*

Main category: cs.LG

TL;DR: 该文章分析了2012年至2023年间AI训练FLOP效率的提高，发现其中大部分效率提升来自算法的规模依赖性效率改进，特别是从LSTM到Transformer的转变，并指出小型模型的算法进展比之前假设的要慢。


<details>
  <summary>Details</summary>
Motivation: 研究2012年至2023年AI训练FLOP效率显著提升的原因，特别是传统消融实验未能完全解释这些增长，从而引入了规模依赖性效率改进的视角。

Method: 作者首先进行了小规模消融实验，以量化关键创新带来的效率增益。随后，他们进行了尺度实验，比较了LSTM和Transformer等算法在不同计算规模下的效率表现。最后，结合实验外推和文献估计，量化了规模依赖性效率改进的总贡献。

Result: 传统消融实验和文献调研只能解释22,000倍效率提升中的不到100倍。通过尺度实验，作者发现算法的效率增益与计算规模密切相关，特别是LSTM到Transformer的转变带来了显著的规模依赖性效率提升。最终，他们解释了同期6,930倍的效率提升，其中大部分归因于规模依赖性的LSTM到Transformer的转变。

Conclusion: 算法效率的提高与计算规模密切相关，特别是对于大型模型而言，算法创新带来了巨大的效率提升。相比之下，小型模型的算法进展相对较慢，并且算法效率的衡量与参照系密切相关。

Abstract: Algorithms have been estimated to increase AI training FLOP efficiency by a factor of 22,000 between 2012 and 2023 [Ho et al., 2024]. Running small-scale ablation experiments on key innovations from this time period, we are able to account for less than 10x of these gains. Surveying the broader literature, we estimate that additional innovations not included in our ablations account for less than 10x, yielding a total under 100x. This leads us to conduct scaling experiments, which reveal that much of this efficiency gap can be explained by algorithms with scale-dependent efficiency improvements. In particular, we conduct scaling experiments between LSTMs and Transformers, finding exponent differences in their compute-optimal scaling law while finding little scaling difference for many other innovations. These experiments demonstrate that - contrary to standard assumptions - an algorithm's efficiency gains are tied to compute scale. Using experimental extrapolation and literature estimates, we account for 6,930x efficiency gains over the same time period, with the scale-dependent LSTM-to-Transformer transition accounting for the majority of gains. Our results indicate that algorithmic progress for small models has been far slower than previously assumed, and that measures of algorithmic efficiency are strongly reference-dependent.

</details>


### [13] [Mechanisms of Non-Monotonic Scaling in Vision Transformers](https://arxiv.org/abs/2511.21635)
*Anantha Padmanaban Krishna Kumar*

Main category: cs.LG

TL;DR: 本文分析了Vision Transformer（ViT）中深度增加有时反而导致性能下降的现象，并提出了“Cliff-Plateau-Climb”三阶段模式来描述表征随深度的演变。


<details>
  <summary>Details</summary>
Motivation: 探索为什么更深的Vision Transformers在性能上不如更浅的 counterparts，这与传统的缩放假设相悖。

Method: 通过对ViT-S、ViT-B和ViT-L在ImageNet上的系统实证分析，本文发现深层ViT中[CLS] token的作用逐渐被边缘化，并引入“信息混淆指数”来量化信息混合模式。

Result: 发现性能的提升与[CLS] token逐渐边缘化，转而由patch tokens之间的分布式共识相关。在ViT-L中，信息-任务权衡比ViT-B晚了大约10层出现，这些额外的层与信息扩散的增加而非任务性能的提升相关。

Conclusion: 深层Transformer架构可能受益于精心校准的深度，以实现清晰的阶段性转换，而不仅仅是增加参数数量。信息混淆指数为现有模型提供了一种有用的诊断方法，并为未来的架构设计提供了潜在目标。

Abstract: Deeper Vision Transformers often perform worse than shallower ones, which challenges common scaling assumptions. Through a systematic empirical analysis of ViT-S, ViT-B, and ViT-L on ImageNet, we identify a consistent three-phase Cliff-Plateau-Climb pattern that governs how representations evolve with depth. We observe that better performance is associated with progressive marginalization of the [CLS] token, originally designed as a global aggregation hub, in favor of distributed consensus among patch tokens. We quantify patterns of information mixing with an Information Scrambling Index, and show that in ViT-L the information-task tradeoff emerges roughly 10 layers later than in ViT-B, and that these additional layers correlate with increased information diffusion rather than improved task performance. Taken together, these results suggest that transformer architectures in this regime may benefit more from carefully calibrated depth that executes clean phase transitions than from simply increasing parameter count. The Information Scrambling Index provides a useful diagnostic for existing models and suggests a potential design target for future architectures. All code is available at: https://github.com/AnanthaPadmanaban-KrishnaKumar/Cliff-Plateau-Climb.

</details>


### [14] [Through the telecom lens: Are all training samples important?](https://arxiv.org/abs/2511.21668)
*Shruti Bothe,Illyyne Saffar,Aurelie Boisbunon,Hasan Farooq,Julien Forgeat,Md Moin Uddin Chowdhury*

Main category: cs.LG

TL;DR: 该论文旨在通过选择性地优先处理有影响力的样本来优化电信数据训练，从而降低计算成本，同时保持或提高模型性能。这有助于实现电信领域AI的可持续发展。


<details>
  <summary>Details</summary>
Motivation: 传统的AI训练工作流程假设所有训练样本都同等重要，但电信数据通常嘈杂、高维且处理成本高昂。该研究旨在挑战这一假设，并通过优化样本选择来提高AI模型的效率、准确性和可持续性，以适应下一代系统的需求。

Method: 论文通过对不同训练周期的样本级别梯度进行分析，识别出模型学习过程中的影响模式和冗余。在此基础上，提出了一种样本重要性框架，该框架能够选择性地优先考虑有影响力的样本数据，从而减少计算量。

Result: 在三个真实的电信数据集上进行的实验表明，该方法在保持性能的同时，有效减少了数据需求和计算开销。

Conclusion: 该研究提出了一种新的样本重要性框架，通过关注样本对模型学习的影响，提高了电信领域AI训练的效率和可持续性，减少了计算资源消耗，同时保持了模型性能。

Abstract: The rise of AI in telecommunications, from optimizing Radio Access Networks to managing user experience, has sharply increased data volumes and training demands. Telecom data is often noisy, high-dimensional, costly to store, process, and label. Despite Ai's critical role, standard workflows still assume all training samples contribute equally. On the other hand, next generation systems require AI models that are accurate, efficient, and sustainable.The paper questions the assumptions of equal importance by focusing on applying and analyzing the roles of individual samples in telecom training and assessing whether the proposed model optimizes computation and energy use. we perform sample-level gradient analysis across epochs to identify patterns of influence and redundancy in model learning. Based on this, we propose a sample importance framework thats electively prioritizes impactful data and reduces computation without compromising accuracy. Experiments on three real-world telecom datasets show that our method [reserves performance while reducing data needs and computational overhead while advancing the goals of sustainable AI in telecommunications.

</details>
