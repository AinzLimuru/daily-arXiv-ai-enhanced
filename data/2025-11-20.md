<div id=toc></div>

# Table of Contents

- [cs.CL](#cs.CL) [Total: 22]
- [cs.IT](#cs.IT) [Total: 5]
- [stat.ML](#stat.ML) [Total: 8]
- [cs.LG](#cs.LG) [Total: 34]
- [cs.AI](#cs.AI) [Total: 17]
- [cs.GT](#cs.GT) [Total: 2]


<div id='cs.CL'></div>

# cs.CL [[Back]](#toc)

### [1] [Test-time Scaling of LLMs: A Survey from A Subproblem Structure Perspective](https://arxiv.org/abs/2511.14772)
*Zhuoyi Yang,Xu Guo,Tong Zhang,Huijuan Xu,Boyang Li*

Main category: cs.CL

TL;DR: 这篇论文旨在通过在推理时分配额外的计算来提高预训练大型语言模型（LLM）的预测准确性。


<details>
  <summary>Details</summary>
Motivation: 调研并分类在推理时通过分配额外计算来提高预训练大型语言模型预测准确性的技术，重点关注问题分解和子问题拓扑结构。

Method: 通过问题分解为子问题（无论是顺序、并行还是树状结构），统一并分析了像思维链（Chain-of-Thought）、分支-解决-合并（Branch-Solve-Merge）和思维树（Tree-of-Thought）等多种方法。

Result: 统一了多种测试时扩展方法，并分析了它们的优缺点。

Conclusion: 总结了现有技术的优缺点，并指出了未来研究的潜在方向。

Abstract: With this paper, we survey techniques for improving the predictive accuracy of pretrained large language models by allocating additional compute at inference time. In categorizing test-time scaling methods, we place special emphasis on how a problem is decomposed into subproblems and on the topological organization of these subproblems whether sequential, parallel, or tree-structured. This perspective allows us to unify diverse approaches such as Chain-of-Thought, Branch-Solve-Merge, and Tree-of-Thought under a common lens. We further synthesize existing analyses of these techniques, highlighting their respective strengths and weaknesses, and conclude by outlining promising directions for future research

</details>


### [2] [Temporal Predictors of Outcome in Reasoning Language Models](https://arxiv.org/abs/2511.14773)
*Joey David*

Main category: cs.CL

TL;DR: 该论文通过探测大型语言模型LoLM的内部状态，揭示在CoT范式中，模型很早就确定了最终结果，这对可解释性和推理时间控制具有重要意义。


<details>
  <summary>Details</summary>
Motivation: LoLM在CoT范式中，模型很早就确定了最终准确性，我们旨在通过训练线性分类器来探索其内部机制。

Method: 在推理的前t个tokens之后，我们在隐藏状态上训练线性分类器，并分析最终正确性预测的变化。

Result: 最终的正确性在仅有少量tokens之后就具有高度可预测性，即使需要更长的输出才能得出明确的答案。对于较难的问题，预测准确性的下降突出了一种选择性偏差：较难的项目在长CoT中占的比例过高。

Conclusion: 对于推理模型，内部的成功自我评估在仅有少量tokens之后就会出现，这对可解释性和推理时间控制具有重要意义。

Abstract: The chain-of-thought (CoT) paradigm uses the elicitation of step-by-step rationales as a proxy for reasoning, gradually refining the model's latent representation of a solution. However, it remains unclear just how early a Large Language Model (LLM) internally commits to an eventual outcome. We probe this by training linear classifiers on hidden states after the first t reasoning tokens, showing that eventual correctness is highly predictable after only a few tokens, even when longer outputs are needed to reach a definite answer. We show that, for harder questions, a drop in predictive accuracy highlights a selection artifact: hard items are disproportionately represented in long CoTs. Overall, our results imply that for reasoning models, internal self-assessment of success tends to emerge after only a few tokens, with implications for interpretability and for inference-time control.

</details>


### [3] [LiveCLKTBench: Towards Reliable Evaluation of Cross-Lingual Knowledge Transfer in Multilingual LLMs](https://arxiv.org/abs/2511.14774)
*Pei-Fu Guo,Yun-Da Tsai,Chun-Chia Hsu,Kai-Xin Chen,Ya-An Tsai,Kai-Wei Chang,Nanyun Peng,Mi-Yen Yeh,Shou-De Lin*

Main category: cs.CL

TL;DR: LiveCLKTBench是一个自动化生成管道，用于隔离和测量大型语言模型中的跨语言知识转移，可以识别独立、时间敏感的知识实体，并生成多语言问题来评估模型的跨语言转移能力。


<details>
  <summary>Details</summary>
Motivation: 评估大型语言模型中的跨语言知识转移是一项挑战，因为目标语言中的正确答案可能来自真实的转移，也可能来自预训练期间的先验暴露。

Method: 该管道识别来自现实世界的独立、时间敏感的知识实体，根据时间出现过滤它们，并根据模型的知识验证它们。然后，这些有效实体的文档用于生成事实问题，这些问题被翻译成多种语言，以评估跨语言边界的可转移性。

Result: 研究人员使用LiveCLKTBench评估了五种语言的几个大型语言模型，观察到跨语言转移受到语言距离的强烈影响，并且在不同语言方向上通常是不对称的。虽然较大的模型可以改善转移，但这种增益会随着规模的扩大而减小，并且在不同领域之间会发生变化。

Conclusion: 这些发现为多语言转移提供了新的见解，并证明了LiveCLKTBench作为未来研究可靠基准的价值。

Abstract: Evaluating cross-lingual knowledge transfer in large language models is challenging, as correct answers in a target language may arise either from genuine transfer or from prior exposure during pre-training. We present LiveCLKTBench, an automated generation pipeline specifically designed to isolate and measure cross-lingual knowledge transfer. Our pipeline identifies self-contained, time-sensitive knowledge entities from real-world domains, filters them based on temporal occurrence, and verifies them against the model's knowledge. The documents of these valid entities are then used to generate factual questions, which are translated into multiple languages to evaluate transferability across linguistic boundaries. Using LiveCLKTBench, we evaluate several LLMs across five languages and observe that cross-lingual transfer is strongly influenced by linguistic distance and often asymmetric across language directions. While larger models improve transfer, the gains diminish with scale and vary across domains. These findings provide new insights into multilingual transfer and demonstrate the value of LiveCLKTBench as a reliable benchmark for future research.

</details>


### [4] [COMPASS: Context-Modulated PID Attention Steering System for Hallucination Mitigation](https://arxiv.org/abs/2511.14776)
*Snigdha Pandya,Rohan Nagale,Kenji Sahay,Anna Lin,Shikhar Shiromani,Kevin Zhu,Dev Sunishchal*

Main category: cs.CL

TL;DR: COMPASS是一个轻量级、可解释的控制框架，它通过引入上下文依赖分数（CRS）来量化上下文依赖性，并通过PID控制器动态调整注意力头，从而在解码过程中减少大型语言模型的幻觉。


<details>
  <summary>Details</summary>
Motivation: 大型语言模型（LLMs）在生成流畅内容时常常出现事实性错误，这源于它们在上下文知识和参数知识之间分配注意力的机制。因此，理解和引导这种内部行为对于LLM的可靠部署和科学解释性至关重要。

Method: 我们引入了COMPASS框架，这是一个轻量级、可解释的控制系统，它在解码过程中直接嵌入了一个基于模型的反馈回路。COMPASS通过透明的上下文依赖分数（CRS）来量化上下文依赖性，CRS作为一个在线探针，用于评估注意力头如何将生成内容与证据对齐。然后，一个PID控制器利用这个可解释的信号动态调整注意力头，以在不进行重新训练或多遍解码的情况下保持事实一致性。

Result: 在多个基准测试（HotpotQA、XSum、HaluEval、RAGTruth）中，COMPASS持续降低了上下文幻觉率（绝对值降低2.8%至5.8%），并揭示了不同注意力头如何促进证据对齐。

Conclusion: 这些结果表明，反馈驱动的可解释性是理解大型语言模型行为的科学途径。

Abstract: Large language models (LLMs) often generate fluent but factually incorrect statements despite having access to relevant evidence, a failure mode rooted in how they allocate attention between contextual and parametric knowledge. Understanding and steering this internal behavior is key both for trustworthy deployment and for scientific interpretability of model mechanisms. We introduce COMPASS (Context-Modulated PID Attention Steering System), a lightweight, interpretable control framework that embeds a model-based feedback loop directly within decoding. COMPASS quantifies context reliance via a transparent metric, the Context Reliance Score (CRS), which serves as an online probe of how attention heads ground generation in evidence. Using this interpretable signal, a PID controller dynamically modulates attention heads to maintain factual consistency without retraining or multi-pass decoding. Across benchmarks (HotpotQA, XSum, HaluEval, RAGTruth), COMPASS consistently reduces contextual hallucination rates (2.8 to 5.8 percent absolute) while revealing how distinct attention heads contribute to evidence alignment. These results highlight feedback-driven interpretability as a pathway toward scientific understanding of LLM behavior.

</details>


### [5] [The Impact of Prosodic Segmentation on Speech Synthesis of Spontaneous Speech](https://arxiv.org/abs/2511.14779)
*Julio Cesar Galdino,Sidney Evaldo Leal,Leticia Gabriella De Souza,Rodrigo de Freitas Lima,Antonio Nelson Fornari Mendes Moreira,Arnaldo Candido Junior,Miguel Oliveira,Edresson Casanova,Sandra M. Aluísio*

Main category: cs.CL

TL;DR: 这篇论文研究了在巴西葡萄牙语中使用手动和自动韵律切分标注对非自回归模型FastSpeech 2语音合成质量的影响。


<details>
  <summary>Details</summary>
Motivation: 尽管语音合成系统在生成自然语音方面取得了显著进展，但显式韵律切分数据集的构建及其对自发语音合成的影响仍未被充分探索。

Method: 本文评估了在巴西葡萄牙语中，手动和自动韵律切分标注对非自回归模型FastSpeech 2合成语音质量的影响。

Result: 实验结果表明，使用韵律切分进行训练能产生稍微更清晰和声学上更自然的语音。虽然自动切分倾向于创建更规则的语音段，但手动韵律切分引入了更大的变异性，这有助于产生更自然的韵律。对中性陈述性话语的分析表明，两种训练方法都重现了预期的核重音模式，但韵律模型与自然的韵核前轮廓更密切契合。

Conclusion: 手动韵律切分引入了更大的变异性，有助于生成更自然的韵律。

Abstract: Spontaneous speech presents several challenges for speech synthesis, particularly in capturing the natural flow of conversation, including turn-taking, pauses, and disfluencies. Although speech synthesis systems have made significant progress in generating natural and intelligible speech, primarily through architectures that implicitly model prosodic features such as pitch, intensity, and duration, the construction of datasets with explicit prosodic segmentation and their impact on spontaneous speech synthesis remains largely unexplored. This paper evaluates the effects of manual and automatic prosodic segmentation annotations in Brazilian Portuguese on the quality of speech synthesized by a non-autoregressive model, FastSpeech 2. Experimental results show that training with prosodic segmentation produced slightly more intelligible and acoustically natural speech. While automatic segmentation tends to create more regular segments, manual prosodic segmentation introduces greater variability, which contributes to more natural prosody. Analysis of neutral declarative utterances showed that both training approaches reproduced the expected nuclear accent pattern, but the prosodic model aligned more closely with natural pre-nuclear contours. To support reproducibility and future research, all datasets, source codes, and trained models are publicly available under the CC BY-NC-ND 4.0 license.

</details>


### [6] [NAMeGEn: Creative Name Generation via A Novel Agent-based Multiple Personalized Goal Enhancement Framework](https://arxiv.org/abs/2511.15408)
*Shanlin Zhou,Xinpeng Wang,Jianxun Lian,Zhenghao Liu,Laks V. S. Lakshmanan,Xiaoyuan Yi,Yongtao Hao*

Main category: cs.CL

TL;DR: 本文提出了NAMeGEn，一个新颖的多智能体优化框架，旨在解决创意自然语言生成（CNLG）中的多目标灵活性和解释复杂性挑战，并通过在中国婴儿命名任务上的出色表现和详尽的解释证明了其有效性。


<details>
  <summary>Details</summary>
Motivation: 目前的创意自然语言生成（CNLG）方法在满足用户个性化、细致、多元化的需求以及理解和解释隐含意义方面存在显著局限性，尤其是在短文本生成方面。

Method: 提出NAMeGEn，一个多智能体优化框架，迭代地进行目标提取、名称生成和评估。构建了一个包含1.7万多首诗的古典中文诗歌语料库，以增强美学。引入了CBNames，一个带有定制度量的新基准。

Result: NAMeGEn能够有效生成满足多样化个性化需求的创意名称，并提供有意义的解释。在没有进行任何训练的情况下，其性能优于六种基于不同大型语言模型骨干的基线方法。

Conclusion: NAMeGEn框架成功解决了CNLG中文本生成和意义解释的挑战，特别是在中国婴儿命名任务中表现出卓越的性能和灵活性，为个性化创意文本生成提供了一个强大的解决方案。

Abstract: Trained on diverse human-authored texts, Large Language Models (LLMs) unlocked the potential for Creative Natural Language Generation (CNLG), benefiting various applications like advertising and storytelling. Nevertheless, CNLG still remains difficult due to two main challenges. (1) Multi-objective flexibility: user requirements are often personalized, fine-grained, and pluralistic, which LLMs struggle to satisfy simultaneously; (2) Interpretive complexity: beyond generation, creativity also involves understanding and interpreting implicit meaning to enhance users' perception. These challenges significantly limit current methods, especially in short-form text generation, in generating creative and insightful content. To address this, we focus on Chinese baby naming, a representative short-form CNLG task requiring adherence to explicit user constraints (e.g., length, semantics, anthroponymy) while offering meaningful aesthetic explanations. We propose NAMeGEn, a novel multi-agent optimization framework that iteratively alternates between objective extraction, name generation, and evaluation to meet diverse requirements and generate accurate explanations. To support this task, we further construct a classical Chinese poetry corpus with 17k+ poems to enhance aesthetics, and introduce CBNames, a new benchmark with tailored metrics. Extensive experiments demonstrate that NAMeGEn effectively generates creative names that meet diverse, personalized requirements while providing meaningful explanations, outperforming six baseline methods spanning various LLM backbones without any training.

</details>


### [7] [Hierarchical Token Prepending: Enhancing Information Flow in Decoder-based LLM Embeddings](https://arxiv.org/abs/2511.14868)
*Xueying Ding,Xingyue Huang,Mingxuan Ju,Liam Collins,Yozen Liu,Leman Akoglu,Neil Shah,Tong Zhao*

Main category: cs.CL

TL;DR: 该论文提出了一种名为分层令牌前置（HTP）的方法，通过引入分块摘要令牌和均值池化来改善大型语言模型在长文本处理中的表示质量。


<details>
  <summary>Details</summary>
Motivation: 大型语言模型中的因果注意力机制限制了信息从后续词元流向前序词元，从而降低了表示质量，尤其是在长文档中。现有方法通过在文本前添加一个摘要令牌来解决此问题，但这种方法会过度压缩信息，反而损害了长文档的处理性能。

Method: 论文提出了分层令牌前置（HTP）方法，该方法通过两种方式解决关键瓶颈：1. 为了减轻注意力层面的压缩，HTP 将输入分成块，并将块级摘要令牌前置到后续块中，为反向信息流创建了多个路径。2. 为了解决读取层面的过度挤压，HTP 用均值池化取代了最后一个令牌池化。

Result: HTP 在 11 个检索数据集和 30 个通用嵌入基准测试中取得了持续的性能提升，尤其是在长上下文设置中表现突出。

Conclusion: HTP 是一种简单且与架构无关的方法，可以增强零样本模型和微调模型，为获得更优质的长文档嵌入提供了一个可扩展的方案。

Abstract: Large language models produce powerful text embeddings, but their causal attention mechanism restricts the flow of information from later to earlier tokens, degrading representation quality. While recent methods attempt to solve this by prepending a single summary token, they over-compress information, hence harming performance on long documents. We propose Hierarchical Token Prepending (HTP), a method that resolves two critical bottlenecks. To mitigate attention-level compression, HTP partitions the input into blocks and prepends block-level summary tokens to subsequent blocks, creating multiple pathways for backward information flow. To address readout-level over-squashing, we replace last-token pooling with mean-pooling, a choice supported by theoretical analysis. HTP achieves consistent performance gains across 11 retrieval datasets and 30 general embedding benchmarks, especially in long-context settings. As a simple, architecture-agnostic method, HTP enhances both zero-shot and finetuned models, offering a scalable route to superior long-document embeddings.

</details>


### [8] [Mathematical Analysis of Hallucination Dynamics in Large Language Models: Uncertainty Quantification, Advanced Decoding, and Principled Mitigation](https://arxiv.org/abs/2511.15005)
*Moses Kiprono*

Main category: cs.CL

TL;DR: 本文提出一个数学框架来理解、测量和减轻大型语言模型（LLM）中的幻觉问题。


<details>
  <summary>Details</summary>
Motivation: 大型语言模型（LLM）容易产生幻觉，即听起来合理但事实不正确或无法证实的结果。

Method: 该方法利用概率建模、信息论、三角信号分析和贝叶斯不确定性估计，分析错误如何自回归地复合，并提出了改进的不确定性度量，包括语义和相位感知变体。此外，本文还开发了包括对比解码、检索增强接地、事实对齐和弃权在内的原则性缓解策略。

Result: 本文提供了一个统一的视角，将校准、检索和对齐方面的最新进展联系起来，以支持更安全、更可靠的LLMs。

Conclusion: 本文提出了一个数学框架来理解、测量和减轻LLM中的幻觉问题，并通过新颖的缓解策略提高了LLM的可靠性和安全性。

Abstract: Large Language Models (LLMs) are powerful linguistic engines but remain susceptible to hallucinations: plausible-sounding outputs that are factually incorrect or unsupported. In this work, we present a mathematically grounded framework to understand, measure, and mitigate these hallucinations. Drawing on probabilistic modeling, information theory, trigonometric signal analysis, and Bayesian uncertainty estimation, we analyze how errors compound autoregressively, propose refined uncertainty metrics, including semantic and phase-aware variants, and develop principled mitigation strategies such as contrastive decoding, retrieval-augmented grounding, factual alignment, and abstention. This unified lens connects recent advances in calibration, retrieval, and alignment to support safer and more reliable LLMs.

</details>


### [9] [Teaching According to Students' Aptitude: Personalized Mathematics Tutoring via Persona-, Memory-, and Forgetting-Aware LLMs](https://arxiv.org/abs/2511.15163)
*Yang Wu,Rujing Yao,Tong Zhang,Yufei Shi,Zhuoren Jiang,Zhushan Li,Xiaozhong Liu*

Main category: cs.CL

TL;DR: 该论文提出了一个名为TASA的智能辅导框架，通过整合学生画像、记忆和遗忘动态来提供个性化的数学学习，解决了现有大语言模型辅导系统未能捕捉学生知识动态演变的问题。


<details>
  <summary>Details</summary>
Motivation: 现有的将大型语言模型集成到智能辅导系统中的方法未能捕捉学生知识（熟练度、概念差距和遗忘模式）的动态演变，尤其是在需要根据学生掌握水平和认知记忆提供精细化支架的数学辅导中。

Method: TASA框架通过维护结构化的学生画像（捕捉熟练度Aptitude）和事件记忆（记录先前的学习互动）来整合学生画像（persona）、记忆（memory）和遗忘（forgetting）动态。它结合了连续遗忘曲线和知识追踪，动态更新学生的掌握状态，并生成符合语境、难度校准的问题和解释。

Result: 实验结果表明，与代表性基线相比，TASA在学习成果和自适应辅导行为方面取得了优越的表现。

Conclusion: TASA框架通过建模时间遗忘和学习者档案，提高了基于大语言模型的辅导系统的有效性，强调了在智能辅导系统中考虑这些动态因素的重要性。

Abstract: Large Language Models (LLMs) are increasingly integrated into intelligent tutoring systems to provide human-like and adaptive instruction. However, most existing approaches fail to capture how students' knowledge evolves dynamically across their proficiencies, conceptual gaps, and forgetting patterns. This challenge is particularly acute in mathematics tutoring, where effective instruction requires fine-grained scaffolding precisely calibrated to each student's mastery level and cognitive retention. To address this issue, we propose TASA (Teaching According to Students' Aptitude), a student-aware tutoring framework that integrates persona, memory, and forgetting dynamics for personalized mathematics learning. Specifically, TASA maintains a structured student persona capturing proficiency profiles and an event memory recording prior learning interactions. By incorporating a continuous forgetting curve with knowledge tracing, TASA dynamically updates each student's mastery state and generates contextually appropriate, difficulty-calibrated questions and explanations. Empirical results demonstrate that TASA achieves superior learning outcomes and more adaptive tutoring behavior compared to representative baselines, underscoring the importance of modeling temporal forgetting and learner profiles in LLM-based tutoring systems.

</details>


### [10] [HinTel-AlignBench: A Framework and Benchmark for Hindi-Telugu with English-Aligned Samples](https://arxiv.org/abs/2511.15183)
*Rishikant Chigrupaatii,Ponnada Sai Tulasi Kanishka,Lalit Chandra Routhu,Martin Patel Sama Supratheek Reddy,Divyam Gupta,Dasari Srikar,Krishna Teja Kuchimanchi,Rajiv Misra,Rohun Tripathi*

Main category: cs.CL

TL;DR: 本文提出一个可扩展的框架来评估印度语言的VLM，并创建了HinTel-AlignBench基准，旨在解决现有评估方法的局限性，并对最先进的VLM进行了详细的性能分析。


<details>
  <summary>Details</summary>
Motivation: 目前的跨语言VLM评估存在四个主要限制：依赖未经证实的自动翻译、任务/领域覆盖范围狭窄、样本量有限以及缺乏文化和本地化的问答。

Method: 本文提出了一个可扩展的框架来评估印度语言的VLM，并创建了HinTel-AlignBench基准。该基准通过半自动数据集创建框架，结合反向翻译、过滤和人工验证。它包含了印地语和泰卢固语的多种来源，包括适配后的英语数据集（VQAv2, RealWorldQA, CLEVR-Math）和本地化的新型印度数据集（JEE for STEM, VAANI for cultural grounding），每种语言约有4,000个问答对。

Result: 在对各种最先进的开源和闭源VLM进行详细性能分析后，本文发现在五分之四的任务中，印度语言的表现相对于英语任务有所下降，印地语平均下降8.3分，泰卢固语平均下降5.5分。

Conclusion: 本文提出了一个可扩展的框架和基准，以解决现有跨语言VLM评估的局限性，并揭示了模型在印度语言任务中相对于英语任务的性能下降，明确了多语言多模态理解中需要改进的具体领域。

Abstract: With nearly 1.5 billion people and more than 120 major languages, India represents one of the most diverse regions in the world. As multilingual Vision-Language Models (VLMs) gain prominence, robust evaluation methodologies are essential to drive progress toward equitable AI for low-resource languages. Current multilingual VLM evaluations suffer from four major limitations: reliance on unverified auto-translations, narrow task/domain coverage, limited sample sizes, and lack of cultural and natively sourced Question-Answering (QA). To address these gaps, we present a scalable framework to evaluate VLMs in Indian languages and compare it with performance in English. Using the framework, we generate HinTel-AlignBench, a benchmark that draws from diverse sources in Hindi and Telugu with English-aligned samples. Our contributions are threefold: (1) a semi-automated dataset creation framework combining back-translation, filtering, and human verification; (2) the most comprehensive vision-language benchmark for Hindi and and Telugu, including adapted English datasets (VQAv2, RealWorldQA, CLEVR-Math) and native novel Indic datasets (JEE for STEM, VAANI for cultural grounding) with approximately 4,000 QA pairs per language; and (3) a detailed performance analysis of various State-of-the-Art (SOTA) open-weight and closed-source VLMs. We find a regression in performance for tasks in English versus in Indian languages for 4 out of 5 tasks across all the models, with an average regression of 8.3 points in Hindi and 5.5 points for Telugu. We categorize common failure modes to highlight concrete areas of improvement in multilingual multimodal understanding.

</details>


### [11] [Unveiling Intrinsic Dimension of Texts: from Academic Abstract to Creative Story](https://arxiv.org/abs/2511.15210)
*Vladislav Pedashenko,Laida Kushnareva,Yana Khassan Nibal,Eduard Tulchinskii,Kristian Kuznetsov,Vladislav Zharchinskii,Yury Maximov,Irina Piontkovskaya*

Main category: cs.CL

TL;DR: 本文对大语言模型（LLM）分析中的内在维度（ID）进行了全面的研究，揭示了其文本决定因素。


<details>
  <summary>Details</summary>
Motivation: 尽管内在维度在LLM分析中是一个重要工具，但其文本决定因素仍未被充分探索。

Method: 本文通过交叉编码器分析、语言特征和稀疏自编码器（SAE）等方法，首次全面地将ID与可解释的文本属性相结合进行研究。

Result: 研究取得了三个主要发现：
1. ID与基于熵的指标是互补的，在控制了文本长度后，两者不相关，ID捕捉了几何复杂性而非预测质量。
2. ID呈现出稳健的文体分层：科学散文的ID较低（约8），百科内容中等（约9），创意/评论写作的ID较高（约10.5），这表明当代LLM认为科学文本在表示上“简单”。
3. 通过SAE，研究确定了因果特征：科学信号（正式语调、报告模板、统计数据）降低ID；人性化信号（个性化、情感、叙述）增加ID。转向实验证实了这些因果效应。

Conclusion: 对于当代模型而言，科学写作相对“容易”，而小说、观点和情感增加了表示的自由度。本研究为正确使用ID和合理解释基于ID的结果提供了实践指导。

Abstract: Intrinsic dimension (ID) is an important tool in modern LLM analysis, informing studies of training dynamics, scaling behavior, and dataset structure, yet its textual determinants remain underexplored. We provide the first comprehensive study grounding ID in interpretable text properties through cross-encoder analysis, linguistic features, and sparse autoencoders (SAEs). In this work, we establish three key findings. First, ID is complementary to entropy-based metrics: after controlling for length, the two are uncorrelated, with ID capturing geometric complexity orthogonal to prediction quality. Second, ID exhibits robust genre stratification: scientific prose shows low ID (~8), encyclopedic content medium ID (~9), and creative/opinion writing high ID (~10.5) across all models tested. This reveals that contemporary LLMs find scientific text "representationally simple" while fiction requires additional degrees of freedom. Third, using SAEs, we identify causal features: scientific signals (formal tone, report templates, statistics) reduce ID; humanized signals (personalization, emotion, narrative) increase it. Steering experiments confirm these effects are causal. Thus, for contemporary models, scientific writing appears comparatively "easy", whereas fiction, opinion, and affect add representational degrees of freedom. Our multi-faceted analysis provides practical guidance for the proper use of ID and the sound interpretation of ID-based results.

</details>


### [12] [OEMA: Ontology-Enhanced Multi-Agent Collaboration Framework for Zero-Shot Clinical Named Entity Recognition](https://arxiv.org/abs/2511.15211)
*Xinli Tao,Xin Dong,Xuezhong Zhou*

Main category: cs.CL

TL;DR: OEMA是一个零样本临床命名实体识别框架，它通过多智能体协作实现，包括自标注器、筛选器和预测器，在MTSamples和VAERS数据集上表现出色，甚至在相关匹配方面能与监督模型媲美。


<details>
  <summary>Details</summary>
Motivation: 临床命名实体识别对于从电子健康记录中提取信息至关重要，但监督模型需要大量标注数据，而现有的零样本方法在示例选择细粒度和提示与自我改进的集成方面存在不足。

Method: OEMA框架包含三个组件：一个自标注器用于生成示例，一个判别器通过SNOMED CT过滤这些示例，以及一个预测器利用实体描述进行准确推断。

Result: 在MTSamples和VAERS数据集上，OEMA在精确匹配方面达到了最先进的性能。在相关匹配方面，它与监督模型BioClinicalBERT持平并超越了CRF。

Conclusion: OEMA通过本体引导推理和多智能体协作解决了零样本NER的关键挑战，实现了接近监督模型的性能，并为临床自然语言处理应用展现了前景。

Abstract: Clinical named entity recognition (NER) is crucial for extracting information from electronic health records (EHRs), but supervised models like CRF and BioClinicalBERT require costly annotated data. While zero-shot NER with large language models (LLMs) reduces this dependency, it struggles with example selection granularity and integrating prompts with self-improvement. To address this, we propose OEMA, a zero-shot clinical NER framework using multi-agent collaboration. OEMA's three components are: a self-annotator generating examples, a discriminator filtering them via SNOMED CT, and a predictor using entity descriptions for accurate inference. On MTSamples and VAERS datasets, OEMA achieves state-of-the-art exact-match performance. Under related-match, it matches supervised BioClinicalBERT and surpasses CRF. OEMA addresses key zero-shot NER challenges through ontology-guided reasoning and multi-agent collaboration, achieving near-supervised performance and showing promise for clinical NLP applications.

</details>


### [13] [Context Cascade Compression: Exploring the Upper Limits of Text Compression](https://arxiv.org/abs/2511.15244)
*Fanfan Liu,Haibo Qiu*

Main category: cs.CL

TL;DR: C3 (Context Cascade Compression) 提出了一种利用大小不同LLM级联进行文本压缩与解码的方法，以应对长上下文任务中百万级token输入带来的挑战。C3在20倍压缩率下达到98%的解码准确率，远超DeepSeek-OCR的60%；在40倍压缩率下仍能保持93%准确率，这表明C3在上下文压缩方面具有卓越的性能和可行性。


<details>
  <summary>Details</summary>
Motivation: 长上下文任务中百万级的token输入对大型语言模型（LLM）构成显著的计算和内存挑战。DeepSeek-OCR在上下文光学压缩方面的初步研究激发了本文探索文本压缩极限的灵感。

Method: C3方法级联使用两个不同大小的LLM来处理压缩和解码任务。首先，一个作为第一阶段的小型LLM将长上下文压缩成一组潜在token（例如，32或64个），实现高文本token与潜在token比例的压缩。然后，作为第二阶段的大型LLM对这个压缩后的上下文执行解码任务。

Result: 在20倍压缩比下，C3模型的解码准确率达到98%，远高于DeepSeek-OCR的约60%。当压缩比进一步提高到40倍时，准确率仍能保持在93%左右。这表明C3在上下文压缩领域显示出优越的性能和可行性。

Conclusion: C3在上下文压缩方面表现出卓越的性能和可行性，显著超越了光学字符压缩方法。C3的纯文本处理流程忽略了布局、颜色和视觉编码器造成的信息丢失等因素，这也为未来光学字符压缩、OCR及相关领域的压缩比设定了潜在的上限。

Abstract: Million-level token inputs in long-context tasks pose significant computational and memory challenges for Large Language Models (LLMs). Recently, DeepSeek-OCR conducted research into the feasibility of Contexts Optical Compression and achieved preliminary results. Inspired by this, we introduce Context Cascade Compression C3 to explore the upper limits of text compression. Our method cascades two LLMs of different sizes to handle the compression and decoding tasks. Specifically, a small LLM, acting as the first stage, performs text compression by condensing a long context into a set of latent tokens (e.g., 32 or 64 in length), achieving a high ratio of text tokens to latent tokens. A large LLM, as the second stage, then executes the decoding task on this compressed context. Experiments show that at a 20x compression ratio (where the number of text tokens is 20 times the number of latent tokens), our model achieves 98% decoding accuracy, compared to approximately 60% for DeepSeek-OCR. When we further increase the compression ratio to 40x, the accuracy is maintained at around 93%. This indicates that in the domain of context compression, C3 Compression demonstrates superior performance and feasibility over optical character compression. C3 uses a simpler, pure-text pipeline that ignores factors like layout, color, and information loss from a visual encoder. This also suggests a potential upper bound for compression ratios in future work on optical character compression, OCR, and related fields. Codes and model weights are publicly accessible at https://github.com/liufanfanlff/C3-Context-Cascade-Compression

</details>


### [14] [IndicGEC: Powerful Models, or a Measurement Mirage?](https://arxiv.org/abs/2511.15260)
*Sowmya Vajjala*

Main category: cs.CL

TL;DR: TeamNRC团队在BHASHA-Task 1语法纠错共享任务中取得了不错的成绩，尤其在泰卢固语和印地语方面。他们发现小型语言模型在处理印度语言语法纠错方面潜力巨大，但也指出了数据集质量和评估标准方面存在的问题。


<details>
  <summary>Details</summary>
Motivation: 探索不同大小语言模型在印度语言语法纠错任务中的表现，并评估数据集质量和评估指标的适用性。

Method: 采用零样本/少样本提示方法，使用4B到大型专有语言模型，参与了BHASHA-Task 1语法纠错共享任务。

Result: 在泰卢固语和印地语中分别获得第四名和第二名，GLEU分数分别为83.78和84.31。通过实验证明小型语言模型具有潜力，但也提出数据质量和评估指标的担忧。

Conclusion: 小型语言模型在印度语言语法纠错任务中表现出巨大潜力，但需要关注高质量数据集的构建和更适合印度语言的评估指标。

Abstract: In this paper, we report the results of the TeamNRC's participation in the BHASHA-Task 1 Grammatical Error Correction shared task https://github.com/BHASHA-Workshop/IndicGEC2025/ for 5 Indian languages. Our approach, focusing on zero/few-shot prompting of language models of varying sizes (4B to large proprietary models) achieved a Rank 4 in Telugu and Rank 2 in Hindi with GLEU scores of 83.78 and 84.31 respectively. In this paper, we extend the experiments to the other three languages of the shared task - Tamil, Malayalam and Bangla, and take a closer look at the data quality and evaluation metric used. Our results primarily highlight the potential of small language models, and summarize the concerns related to creating good quality datasets and appropriate metrics for this task that are suitable for Indian language scripts.

</details>


### [15] [MAPROC at AHaSIS Shared Task: Few-Shot and Sentence Transformer for Sentiment Analysis of Arabic Hotel Reviews](https://arxiv.org/abs/2511.15291)
*Randa Zarnoufi*

Main category: cs.CL

TL;DR: 这篇论文介绍了一种使用SetFit框架对阿拉伯方言（摩洛哥和沙特方言）酒店评论进行情感分析的方法，在AHaSIS共享任务中取得了73%的F1分数。


<details>
  <summary>Details</summary>
Motivation: 阿拉伯方言的情感分析面临语言多样性和标注数据稀缺的挑战，尤其是在酒店评论这样的特定领域。

Method: 本文采用了SetFit（Sentence Transformer Fine-tuning）框架，这是一种数据高效的少样本学习技术。

Result: 在官方评估集中，该系统获得了73%的F1分数，在26个参与者中排名第12位。

Conclusion: 这项工作突出了少样本学习在处理特定领域（如酒店评论）中细微的阿拉伯方言文本时，解决数据稀缺问题的潜力。

Abstract: Sentiment analysis of Arabic dialects presents significant challenges due to linguistic diversity and the scarcity of annotated data. This paper describes our approach to the AHaSIS shared task, which focuses on sentiment analysis on Arabic dialects in the hospitality domain. The dataset comprises hotel reviews written in Moroccan and Saudi dialects, and the objective is to classify the reviewers sentiment as positive, negative, or neutral. We employed the SetFit (Sentence Transformer Fine-tuning) framework, a data-efficient few-shot learning technique. On the official evaluation set, our system achieved an F1 of 73%, ranking 12th among 26 participants. This work highlights the potential of few-shot learning to address data scarcity in processing nuanced dialectal Arabic text within specialized domains like hotel reviews.

</details>


### [16] [HEAD-QA v2: Expanding a Healthcare Benchmark for Reasoning](https://arxiv.org/abs/2511.15355)
*Alexis Correa-Guillén,Carlos Gómez-Rodríguez,David Vilares*

Main category: cs.CL

TL;DR: HEAD-QA v2是一个扩展的西班牙语/英语医疗保健选择题推理数据集，包含超过12,000个问题，并对开源大型语言模型进行了基准测试。


<details>
  <summary>Details</summary>
Motivation: 构建一个高质量的数据集，捕捉医疗保健推理的语言和概念复杂性。

Method: 通过扩展HEAD-QA数据集，增加来自西班牙专业考试的问题，并使用提示、RAG和基于概率的答案选择对多种开源大型语言模型进行基准测试。

Result: 模型性能主要受模型规模和内在推理能力驱动，复杂的推理策略带来的收益有限。

Conclusion: HEAD-QA v2是一个可靠的资源，可用于推进生物医学推理和模型改进研究。

Abstract: We introduce HEAD-QA v2, an expanded and updated version of a Spanish/English healthcare multiple-choice reasoning dataset originally released by Vilares and Gómez-Rodríguez (2019). The update responds to the growing need for high-quality datasets that capture the linguistic and conceptual complexity of healthcare reasoning. We extend the dataset to over 12,000 questions from ten years of Spanish professional exams, benchmark several open-source LLMs using prompting, RAG, and probability-based answer selection, and provide additional multilingual versions to support future work. Results indicate that performance is mainly driven by model scale and intrinsic reasoning ability, with complex inference strategies obtaining limited gains. Together, these results establish HEAD-QA v2 as a reliable resource for advancing research on biomedical reasoning and model improvement.

</details>


### [17] [A Compliance-Preserving Retrieval System for Aircraft MRO Task Search](https://arxiv.org/abs/2511.15383)
*Byungho Jo*

Main category: cs.CL

TL;DR: 该研究介绍了一种飞机维修技术员查询系统，旨在提高维修操作中的效率和遵守合规性，通过结合大型语言模型（LLM）重排序和语义搜索，取得了90%以上的检索准确率，并将查询时间从6-15分钟缩短到18秒。


<details>
  <summary>Details</summary>
Motivation: 在MRO（维护、修理和大修）操作中，飞机维修技术员（AMTs）将30%的工作时间用于查阅手册，这是一个效率瓶颈。此外，所有操作程序都必须可追溯到经过认证的来源，因此需要一个既能提高效率又能保持合规性的检索系统。

Method: 该系统通过以下方式实现合规性检索： 1.构建修订鲁棒的嵌入，这些嵌入源自ATA章节的层次结构。 2.利用视觉语言解析技术来组织经过认证的内容。 3.系统与现有的经过认证的遗留查看器协同工作，而不是取代它们。 4.该系统允许技术员预览排好序的任务，并访问现有查看器中经过验证的程序。

Result: 1.在4.9万次合成查询中，检索准确率达到90%以上。 2.在有10名持证AMT参与的双语对照研究中，前10名的成功率达到90.9%。 3.查询时间减少了95%，从每个任务6-15分钟缩短到18秒。

Conclusion: 该研究提供了具体的证据，证明语义检索系统可以在严格的监管约束下运行，并能显著减少实际多语言MRO工作流程中的操作工作量，尤其是在提高查找效率和保持合规性方面。

Abstract: Aircraft Maintenance Technicians (AMTs) spend up to 30% of work time searching manuals, a documented efficiency bottleneck in MRO operations where every procedure must be traceable to certified sources. We present a compliance-preserving retrieval system that adapts LLM reranking and semantic search to aviation MRO environments by operating alongside, rather than replacing, certified legacy viewers. The system constructs revision-robust embeddings from ATA chapter hierarchies and uses vision-language parsing to structure certified content, allowing technicians to preview ranked tasks and access verified procedures in existing viewers. Evaluation on 49k synthetic queries achieves >90% retrieval accuracy, while bilingual controlled studies with 10 licensed AMTs demonstrate 90.9% top-10 success rate and 95% reduction in lookup time, from 6-15 minutes to 18 seconds per task. These gains provide concrete evidence that semantic retrieval can operate within strict regulatory constraints and meaningfully reduce operational workload in real-world multilingual MRO workflows.

</details>


### [18] [Building Robust and Scalable Multilingual ASR for Indian Languages](https://arxiv.org/abs/2511.15418)
*Arjun Gangwar,Kaousheik Jayakumar,S. Umesh*

Main category: cs.CL

TL;DR: 本文介绍了SPRING Lab为ASRU MADASR 2.0挑战开发的系统，旨在改进ASR系统在预测8种语言和33种方言的语言和方言方面的能力。


<details>
  <summary>Details</summary>
Motivation: 开发针对ASRU MADASR 2.0挑战的ASR系统，以提高语言和方言识别的准确性，尤其是在资源受限（不允许额外数据）和从头开发多语言系统的情况下。

Method: 采用多解码器架构，使用音素通用标签集（CLS）作为中间表示，并提出了新颖的训练方法。同时探讨了将音素空间中的增益转换回字素表示的方法。

Result: 在Track 2的3种语言中，系统在词错误率（WER）/字符错误率（CER）方面优于基线，并在所有参与团队中取得了最高的语言ID和方言ID准确率。

Conclusion: SPRING Lab开发的系统在ASRU MADASR 2.0挑战中表现出色，通过创新的多解码器架构和音素CLS中间表示，显著提升了ASR系统在多语言多方言环境下的性能。

Abstract: This paper describes the systems developed by SPRING Lab, Indian Institute of Technology Madras, for the ASRU MADASR 2.0 challenge. The systems developed focuses on adapting ASR systems to improve in predicting the language and dialect of the utterance among 8 languages across 33 dialects. We participated in Track 1 and Track 2, which restricts the use of additional data and develop from-the-scratch multilingual systems. We presented a novel training approach using Multi-Decoder architecture with phonemic Common Label Set (CLS) as intermediate representation. It improved the performance over the baseline (in the CLS space). We also discuss various methods used to retain the gain obtained in the phonemic space while converting them back to the corresponding grapheme representations. Our systems beat the baseline in 3 languages (Track 2) in terms of WER/CER and achieved the highest language ID and dialect ID accuracy among all participating teams (Track 2).

</details>


### [19] [LLM-MemCluster: Empowering Large Language Models with Dynamic Memory for Text Clustering](https://arxiv.org/abs/2511.15424)
*Yuanjie Zhu,Liangwei Yang,Ke Xu,Weizhi Zhang,Zihe Song,Jindong Wang,Philip S. Yu*

Main category: cs.CL

TL;DR: LLM-MemCluster是一个新颖的端到端LLM文本聚类框架，它通过动态内存和双提示策略解决了现有LLM在文本聚类中缺乏状态记忆和难以管理聚类粒度的问题。


<details>
  <summary>Details</summary>
Motivation: 现有的大型语言模型（LLM）在文本聚类方面存在局限性，主要表现在缺乏迭代改进的状态记忆以及难以管理聚类粒度。这导致现有方法通常依赖于带有外部模块的复杂管道，牺牲了端到端的方法。

Method: 我们引入了LLM-MemCluster框架，它将聚类重新概念化为完全LLM原生的任务。该框架利用动态内存（Dynamic Memory）来赋予LLM状态感知能力，并采用双提示策略（Dual-Prompt Strategy）使模型能够推理并确定聚类的数量。

Result: LLM-MemCluster在多个基准数据集上进行了评估，其免调优框架显著且持续地优于强劲的基线模型。

Conclusion: LLM-MemCluster为基于LLM的文本聚类提供了一个有效、可解释且真正的端到端范式。

Abstract: Large Language Models (LLMs) are reshaping unsupervised learning by offering an unprecedented ability to perform text clustering based on their deep semantic understanding. However, their direct application is fundamentally limited by a lack of stateful memory for iterative refinement and the difficulty of managing cluster granularity. As a result, existing methods often rely on complex pipelines with external modules, sacrificing a truly end-to-end approach. We introduce LLM-MemCluster, a novel framework that reconceptualizes clustering as a fully LLM-native task. It leverages a Dynamic Memory to instill state awareness and a Dual-Prompt Strategy to enable the model to reason about and determine the number of clusters. Evaluated on several benchmark datasets, our tuning-free framework significantly and consistently outperforms strong baselines. LLM-MemCluster presents an effective, interpretable, and truly end-to-end paradigm for LLM-based text clustering.

</details>


### [20] [Standardising the NLP Workflow: A Framework for Reproducible Linguistic Analysis](https://arxiv.org/abs/2511.15512)
*Yves Pauli,Jan-Bernard Marsman,Finn Rabe,Victoria Edkins,Roya Hüppi,Silvia Ciampelli,Akhil Ratan Misra,Nils Lang,Wolfram Hinzen,Iris Sommer,Philipp Homan*

Main category: cs.CL

TL;DR: 该文章介绍了语言处理数据结构（LPDS）和pelican nlp，旨在解决语言数据处理中标准化和可重复性不足的问题。


<details>
  <summary>Details</summary>
Motivation: 目前AI语言处理领域缺乏管理和共享语言数据的标准化方法，并且处理方法的可重复性不足。

Method: 文章提出了两种方案：1. 语言处理数据结构（LPDS）：一种受脑图像数据结构（BIDS）启发的标准化数据结构，用于组织语言研究的文件夹结构和文件命名规范。 2. pelican nlp：一个模块化的Python包，用于简化语言处理流程，包括数据清洗、预处理以及语言和声学特征提取。该工具可通过单个配置文件在LPDS格式的数据上执行。

Result: LPDS和pelican nlp共同提供了一个端到端的语言数据处理流程，能够生成预处理的语言数据、标准化的语言和声学特征提取以及相应的结果聚合。

Conclusion: LPDS和pelican nlp的结合能够提高语言数据处理工作的透明度和可重复性，促进未来的标准化。

Abstract: The introduction of large language models and other influential developments in AI-based language processing have led to an evolution in the methods available to quantitatively analyse language data. With the resultant growth of attention on language processing, significant challenges have emerged, including the lack of standardisation in organising and sharing linguistic data and the absence of standardised and reproducible processing methodologies. Striving for future standardisation, we first propose the Language Processing Data Structure (LPDS), a data structure inspired by the Brain Imaging Data Structure (BIDS), a widely adopted standard for handling neuroscience data. It provides a folder structure and file naming conventions for linguistic research. Second, we introduce pelican nlp, a modular and extensible Python package designed to enable streamlined language processing, from initial data cleaning and task-specific preprocessing to the extraction of sophisticated linguistic and acoustic features, such as semantic embeddings and prosodic metrics. The entire processing workflow can be specified within a single, shareable configuration file, which pelican nlp then executes on LPDS-formatted data. Depending on the specifications, the reproducible output can consist of preprocessed language data or standardised extraction of both linguistic and acoustic features and corresponding result aggregations. LPDS and pelican nlp collectively offer an end-to-end processing pipeline for linguistic data, designed to ensure methodological transparency and enhance reproducibility.

</details>


### [21] [Multimodal Evaluation of Russian-language Architectures](https://arxiv.org/abs/2511.15552)
*Artem Chervyakov,Ulyana Isaeva,Anton Emelyanov,Artem Safin,Maria Tikhonova,Alexander Kharitonov,Yulia Lyakh,Petr Surovtsev,Denis Shevelev Vildan Saburov,Vasily Konovalov,Elisei Rykov,Ivan Sviridov,Amina Miftakhova,Ilseyar Alimova,Alexander Panchenko,Alexander Kapitanov,Alena Fenogenova*

Main category: cs.CL

TL;DR: 本文介绍了Mera Multi，一个针对俄语多模态大模型的评估框架。


<details>
  <summary>Details</summary>
Motivation: 目前，多模态大模型（MLLMs）在功能和规模上取得了快速进展，但其智能水平、局限性及潜在风险尚未被充分理解，尤其是在俄语领域，尚缺乏相应的多模态基准。

Method: Mera Multi是一个指令型多模态评估框架，涵盖文本、图像、音频和视频等模态。它包含18个专门为通用模型和特定模态架构（图像转文本、视频转文本、音频转文本）设计的新评估任务。本文还提出了一种通用的多模态能力分类方法，并从头构建了18个数据集，这些数据集考虑了俄罗斯的文化和语言特性，并统一了提示和评估指标。此外，论文还提出了一种防止基准泄露的方法，包括水印和私有数据集许可。

Result: 虽然论文没有直接给出具体的实验结果，但它提到了为闭源和开源模型提供了基线测试结果。同时，它表示该基准提供了一种可复制的方法，用于构建类型多样的语言，特别是斯拉夫语族的多模态基准。

Conclusion: Mera Multi填补了俄语多模态基准的空白，促进了对多模态大模型在俄语语境下智能、局限性和风险的理解。该框架及其方法对于构建其他语言的多模态基准也具有重要的借鉴意义。

Abstract: Multimodal large language models (MLLMs) are currently at the center of research attention, showing rapid progress in scale and capabilities, yet their intelligence, limitations, and risks remain insufficiently understood. To address these issues, particularly in the context of the Russian language, where no multimodal benchmarks currently exist, we introduce Mera Multi, an open multimodal evaluation framework for Russian-spoken architectures. The benchmark is instruction-based and encompasses default text, image, audio, and video modalities, comprising 18 newly constructed evaluation tasks for both general-purpose models and modality-specific architectures (image-to-text, video-to-text, and audio-to-text). Our contributions include: (i) a universal taxonomy of multimodal abilities; (ii) 18 datasets created entirely from scratch with attention to Russian cultural and linguistic specificity, unified prompts, and metrics; (iii) baseline results for both closed-source and open-source models; (iv) a methodology for preventing benchmark leakage, including watermarking and licenses for private sets. While our current focus is on Russian, the proposed benchmark provides a replicable methodology for constructing multimodal benchmarks in typologically diverse languages, particularly within the Slavic language family.

</details>


### [22] [HSKBenchmark: Modeling and Benchmarking Chinese Second Language Acquisition in Large Language Models through Curriculum Tuning](https://arxiv.org/abs/2511.15574)
*Qihao Yang,Xuelin Wang,Jiale Chen,Xuelian Dong,Yuxin Hao,Tianyong Hao*

Main category: cs.CL

TL;DR: HSKBenchmark是首个用于中文二语习得（SLA）中大型语言模型（LLMs）分阶段建模和写作评估的基准，它模拟了人类学习轨迹并提供了一个全面的评估系统。


<details>
  <summary>Details</summary>
Motivation: 在人类学习者中进行语言输入控制实验在伦理上和实践上都不可行，这给语言习得建模的验证性和可扩展性带来了挑战，尤其是在中文二语习得（SLA）领域。

Method: 本文提出了HSKBenchmark，一个涵盖HSK 3到6级的基准，包含676万token的真实教材，1.6万合成指令样本，30个测试主题和一个基于语言学的评估系统。引入了课程调整框架来模拟人类学习轨迹，并创建了一个评估系统来检查基于水平的语法覆盖率、写作错误、词汇和句法复杂性以及整体评分。此外，还构建了HSKAgent，它在1万篇学习者作文上进行了微调。

Result: HSKBenchmark能够有效地对中文二语习得进行建模，并作为LLMs动态写作评估的可靠基准。微调后的LLMs的写作表现与高级人类学习者相当，并表现出类似人类的习得特征。

Conclusion: HSKBenchmark、HSKAgent和检查点是基础工具和资源，有望为未来语言习得建模和LLMs可解释性研究铺平道路。

Abstract: Language acquisition is vital to revealing the nature of human language intelligence and has recently emerged as a promising perspective for improving the interpretability of large language models (LLMs). However, it is ethically and practically infeasible to conduct experiments that require controlling human learners' language inputs. This poses challenges for the verifiability and scalability of language acquisition modeling, particularly in Chinese second language acquisition (SLA). While LLMs provide a controllable and reproducible alternative, a systematic benchmark to support phase-wise modeling and assessment is still lacking. In this paper, we present HSKBenchmark, the first benchmark for staged modeling and writing assessment of LLMs in Chinese SLA. It covers HSK levels 3 to 6 and includes authentic textbooks with 6.76 million tokens, 16K synthetic instruction samples, 30 test topics, and a linguistically grounded evaluation system. To simulate human learning trajectories, we introduce a curriculum-tuning framework that trains models from beginner to advanced levels. An evaluation system is created to examine level-based grammar coverage, writing errors, lexical and syntactic complexity, and holistic scoring. We also build HSKAgent, fine-tuned on 10K learner compositions. Extensive experimental results demonstrate that HSKBenchmark not only models Chinese SLA effectively, but also serves as a reliable benchmark for dynamic writing assessment in LLMs. Our fine-tuned LLMs have writing performance on par with advanced human learners and exhibit human-like acquisition characteristics. The HSKBenchmark, HSKAgent, and checkpoints serve as foundational tools and resources, with the potential to pave the way for future research on language acquisition modeling and LLMs interpretability. Code and data are publicly available at: https://github.com/CharlesYang030/HSKB.

</details>


<div id='cs.IT'></div>

# cs.IT [[Back]](#toc)

### [23] [Channel Coding for Gaussian Channels with Multifaceted Power Constraints](https://arxiv.org/abs/2511.14849)
*Adeel Mahmood,Aaron B. Wagner*

Main category: cs.IT

TL;DR: 研究了高阶编码性能的功率依赖性，特别是在高斯信道下，通过引入多功能功率模型，对最小平均错误概率进行了精确的数学表征。


<details>
  <summary>Details</summary>
Motivation: 作者希望通过引入新的多功能功率模型，更精细地研究高阶编码性能对平均功率以及输入功率更精细统计信息的依赖性。

Method: 本文引入了一个多功能功率模型，该模型限制了归一化平均功率的任意数量任意函数的期望。这种方法概括了现有模型，包括标准的最大功率和预期功率约束以及最近的均值和方差约束。在函数满足某些增长和连续性假设的条件下，作者给出了高斯信道下作为一阶和二阶编码速率函数的最小平均错误概率的精确数学表征。

Result: 通过这种多功能功率模型，作者精确地刻画了高斯信道下最小平均错误概率与一阶和二阶编码速率之间的关系。

Conclusion: 这项研究为理解和优化高斯信道下的高阶编码性能提供了新的理论工具和精确的数学表征，有助于更细致地分析功率约束对通信系统性能的影响。

Abstract: Motivated by refined asymptotic results based on the normal approximation, we study how higher-order coding performance depends on the mean power $Γ$ as well as on finer statistics of the input power. We introduce a multifaceted power model in which the expectation of an arbitrary number of arbitrary functions of the normalized average power is constrained. The framework generalizes existing models, recovering the standard maximal and expected power constraints and the recent mean and variance constraint as special cases. Under certain growth and continuity assumptions on the functions, our main theorem gives an exact characterization of the minimum average error probability for Gaussian channels as a function of the first- and second-order coding rates. The converse proof reduces the code design problem to minimization over a compact (under the Prokhorov metric) set of probability distributions, characterizes the extreme points of this set and invokes the Bauer's maximization principle.

</details>


### [24] [Beyond the "G" Frontier: A Time Traveler's Century-Long Vision for Wireless Intelligence](https://arxiv.org/abs/2511.14906)
*Yasser Al Eryani*

Main category: cs.IT

TL;DR: 本文提出一个跨越未来一百年的信息-曲率效率定律（ICEL）分析框架，预测无线通信的演进将超越传统代际划分，融合电磁学、生物学、热力学和认知科学，形成一个自我感知的信息流全球生态系统，其中几何学与通信的融合将共同维系技术与生物生命。


<details>
  <summary>Details</summary>
Motivation: 为了预测未来一百年（2025-2125）无线通信的演进路径，并提出超越传统G代划分的创新视角。

Method: 通过信息-曲率效率定律（ICEL）的分析视角，将无线通信的演进与电磁学、生物学、热力学和认知科学进行跨学科融合。

Result: 无线通信的未来演进将不再是简单的代际升级（如6G, 7G），而是通过曲率管理的模式，实现多学科的深度融合。最终将形成一个全球性的、自我感知的信息流生态系统。

Conclusion: 几何学和通信的深度融合将成为未来技术和生物生命可持续发展的关键，预示着一个全新的、跨越式发展的无线通信时代。

Abstract: This article travels one century into the future--from 2025 to 2125--through the analytical lens of the Information--Curvature Efficiency Law (ICEL). It contends that wireless evolution will not proceed through incremental generations such as 6G or 7G, but through a curvature-managed integration of electromagnetics, biology, thermodynamics, and cognition. The resulting infrastructure will constitute a global ecology of self-aware information flow, where geometry and communication converge to sustain both technological and biological life.

</details>


### [25] [Mutual Information Bounds in the Shuffle Model](https://arxiv.org/abs/2511.15051)
*Pengcheng Su,Haibo Cheng,Ping Wang*

Main category: cs.IT

TL;DR: 本文从信息论的角度对单消息混洗模型进行了首次系统研究，将混洗模型分为仅混洗设置和混洗-DP设置，并在这两种设置下对隐私泄露进行量化，建立了混洗差分隐私与基于互信息隐私之间的联系。


<details>
  <summary>Details</summary>
Motivation: 为了解决混洗模型中用户报告匿名化的问题，本文旨在从信息论的角度深入理解单消息混洗模型的隐私保护能力，并量化不同设置下的信息泄露。

Method: 本文首先将混洗模型分为两种设置：仅混洗设置（shuffle-only）和混洗-DP设置（shuffle-DP）。在仅混洗设置下，对于一个可处理但富有表现力的基本配置，推导了渐进互信息表达式来量化隐私泄露。在混洗-DP设置下，本文建立了信息论上的总信息泄露上限。

Result: 在仅混洗设置下，推导了当用户数量趋于无穷时，关于目标用户信息和用户位置信息的互信息渐近表达式，并表明该分析框架可以推广到异构用户分布设置。在混洗-DP设置下，证明了当每个用户应用一个$\varepsilon_0$-差分隐私机制时，总泄露满足 $I(K; \boldsymbol{Z}) \le 2\varepsilon_0$ 和 $I(X_1; \boldsymbol{Z}\mid (X_i)_{i=2}^n) \le (e^{\varepsilon_0}-1)/(2n) + O(n^{-3/2})$。

Conclusion: 本文首次从信息论角度系统研究了单消息混洗模型，量化了混洗设置和混洗-DP设置下的隐私泄露，并建立了混洗差分隐私与基于互信息的隐私之间的联系。

Abstract: The shuffle model enhances privacy by anonymizing users' reports through random permutation. This paper presents the first systematic study of the single-message shuffle model from an information-theoretic perspective. We analyze two regimes: the shuffle-only setting, where each user directly submits its message ($Y_i=X_i$), and the shuffle-DP setting, where each user first applies a local $\varepsilon_0$-differentially private mechanism before shuffling ($Y_i=\mathcal{R}(X_i)$). Let $\boldsymbol{Z} = (Y_{σ(i)})_i$ denote the shuffled sequence produced by a uniformly random permutation $σ$, and let $K = σ^{-1}(1)$ represent the position of user 1's message after shuffling.
  For the shuffle-only setting, we focus on a tractable yet expressive \emph{basic configuration}, where the target user's message follows $Y_1 \sim P$ and the remaining users' messages are i.i.d.\ samples from $Q$, i.e., $Y_2,\dots,Y_n \sim Q$. We derive asymptotic expressions for the mutual information quantities $I(Y_1;\boldsymbol{Z})$ and $I(K;\boldsymbol{Z})$ as $n \to \infty$, and demonstrate how this analytical framework naturally extends to settings with heterogeneous user distributions.
  For the shuffle-DP setting, we establish information-theoretic upper bounds on total information leakage. When each user applies an $\varepsilon_0$-DP mechanism, the overall leakage satisfies $I(K; \boldsymbol{Z}) \le 2\varepsilon_0$ and $I(X_1; \boldsymbol{Z}\mid (X_i)_{i=2}^n) \le (e^{\varepsilon_0}-1)/(2n) + O(n^{-3/2})$. These results bridge shuffle differential privacy and mutual-information-based privacy.

</details>


### [26] [Generalized Repetition Codes and Their Application to HARQ](https://arxiv.org/abs/2511.15207)
*Chaofeng Guan,Gaojun Luo,Lan Luo,Yangyang Fei,Hong Wang*

Main category: cs.IT

TL;DR: 本文提出两类广义重复码（GRCs），旨在解决重复通信模型中的错误纠正问题，通过优化多个最小距离，提高纠错能力，并探讨了其构造方法。


<details>
  <summary>Details</summary>
Motivation: 通信信道的内在不确定性导致编码方案存在错误纠正失败的概率，因此需要重传机制。传统的双层冗余框架通过纠错码和循环冗余校验来确保消息的可靠性和完整性，但核心挑战是在有限的传输轮数内最大化正确消息解码的概率。

Method: 本文提出两类广义重复码（GRCs），分别对应两种重复通信模型（Type-I和Type-II）。与经典理论不同，GRCs被视为在多个度量下具有多个最小距离的纠错码，从而实现多轮纠错。针对GRCs的特殊结构，文章分别研究了Type-I和Type-II GRCs的界限和构造方法，并获得了许多最优的GRCs。

Result: 通过将GRCs视为在多个度量下具有多个最小距离的纠错码，实现了比经典纠错码更强的错误纠正能力。文章成功获得了许多最优的Type-I和Type-II GRCs。

Conclusion: GRCs通过在多个度量下优化最小距离，显著提升了重复通信系统中的错误纠正能力。未来的研究可以进一步探索更高效的GRCs构造方法，并将其应用于实际通信系统中。

Abstract: The inherent uncertainty of communication channels implies that any coding scheme has a non-zero probability of failing to correct errors, making retransmission mechanisms essential. To ensure message reliability and integrity, a dual-layer redundancy framework is typically employed: error correction codes mitigate noise-induced impairments at the physical layer, while cyclic redundancy checks verify message integrity after decoding. Retransmission is initiated if verification fails. This operational model can be categorized into two types of repeated communication models: Type-I systems repeatedly transmit identical codewords, whereas Type-II systems transmit distinct coded representations of the same message. The core challenge lies in maximizing the probability of correct message decoding within a limited number of transmission rounds through verification-based feedback mechanisms.
  In this paper, we consider a scenario where the same error-correcting code is used for repeated transmissions, and we specifically propose two classes of generalized repetition codes (GRCs), corresponding to the two repeated communication models. In contrast to classical theory, we regard GRCs as error-correcting codes under multiple metrics--that is, GRCs possess multiple minimum distances. This design enables GRCs to perform multi-round error correction under different metrics, achieving stronger error-correction capabilities than classical error-correcting codes. However, the special structure of GRCs makes their construction more challenging, as it requires simultaneously optimizing multiple minimum distances. To address this, we separately investigate the bounds and constructions for Type-I and Type-II GRCs, and obtain numerous optimal Type-I and Type-II GRCs.

</details>


### [27] [The Rate-Distortion-Perception Trade-Off with Algorithmic Realism](https://arxiv.org/abs/2511.15255)
*Yassine Hamdi,Aaron B. Wagner,Deniz Gündüz*

Main category: cs.IT

TL;DR: 本文探讨了有损压缩中感知质量约束下的率失真理论，指出在现实约束下，如果批量大小不切实际地大，理论上大量的公共随机性资源对于实现真实感是必要的，但在实践中并未发现其效用。


<details>
  <summary>Details</summary>
Motivation: 在有损压缩领域，特别是图像压缩中，真实感约束受到了广泛关注。理论研究表明，压缩器和解压缩器之间的高速率公共随机性是实现真实感的宝贵资源。然而，在实践中，大量公共随机性的效用并未得到证实，这引发了一个理论与实践之间的矛盾。

Method: 本文通过引入一个“通用评论家”来检查单个压缩重建或其批次的实现，从而考虑了新的真实感约束，并以此解释了理论与实践之间的差异。

Result: 在所提出的真实感约束下，本文刻画了最优的率失真权衡，并证明了除非批量大小不切实际地大，否则在没有任何公共随机性的情况下，这种权衡是可以渐近实现的。

Conclusion: 本文通过引入通用评论家模型，解决了有损压缩中真实感约束下公共随机性理论与实践不符的问题，并证明了在合理批量大小下，无需公共随机性即可实现最优率失真权衡，为有损压缩的实际应用提供了新思路。

Abstract: Realism constraints (or constraints on perceptual quality) have received considerable recent attention within the context of lossy compression, particularly of images. Theoretical studies of lossy compression indicate that high-rate common randomness between the compressor and the decompressor is a valuable resource for achieving realism. On the other hand, the utility of significant amounts of common randomness has not been noted in practice. We offer an explanation for this discrepancy by considering a realism constraint that requires satisfying a universal critic that inspects realizations of individual compressed reconstructions, or batches thereof. We characterize the optimal rate-distortion trade-off under such a realism constraint, and show that it is asymptotically achievable without any common randomness, unless the batch size is impractically large.

</details>


<div id='stat.ML'></div>

# stat.ML [[Back]](#toc)

### [28] [Convex Clustering Redefined: Robust Learning with the Median of Means Estimator](https://arxiv.org/abs/2511.14784)
*Sourav De,Koustav Chowdhury,Bibhabasu Mandal,Sagar Ghosh,Swagatam Das,Debolina Paul,Saptarshi Chakraborty*

Main category: stat.ML

TL;DR: 该文章介绍了一种结合了凸聚类和均值中的中位数（MoM）估计器的新型聚类方法，旨在解决传统聚类方法在处理高维数据和存在噪声、异常值时的局限性，并且无需预先指定聚类数量。


<details>
  <summary>Details</summary>
Motivation: 传统的聚类方法（如k-means）需要预先指定聚类数量k，并且对初始化敏感。凸聚类虽然能提供唯一的全局解，但在处理高维数据、噪声和异常值时面临挑战。强融合正则化也会阻碍有效的聚类形成。

Method: 本文提出了一种将凸聚类与均值中的中位数（MoM）估计器相结合的鲁棒方法。该方法利用MoM的鲁棒性以及凸聚类的稳定性，能够有效地抵抗异常值，并且不需要预先知道聚类的数量。

Result: 该方法在处理大规模数据集时，能够提升性能和效率。理论分析表明在特定条件下具有弱一致性。在合成数据集和真实世界数据集上的实验证明，该方法的性能优于现有方法。

Conclusion: 本文提出了一种结合凸聚类与MoM估计器的鲁棒聚类框架，解决了传统聚类方法在处理高维数据、噪声和异常值时的痛点，并且无需预设聚类数量，在性能和效率上都有显著提升。

Abstract: Clustering approaches that utilize convex loss functions have recently attracted growing interest in the formation of compact data clusters. Although classical methods like k-means and its wide family of variants are still widely used, all of them require the number of clusters k to be supplied as input, and many are notably sensitive to initialization. Convex clustering provides a more stable alternative by formulating the clustering task as a convex optimization problem, ensuring a unique global solution. However, it faces challenges in handling high-dimensional data, especially in the presence of noise and outliers. Additionally, strong fusion regularization, controlled by the tuning parameter, can hinder effective cluster formation within a convex clustering framework. To overcome these challenges, we introduce a robust approach that integrates convex clustering with the Median of Means (MoM) estimator, thus developing an outlier-resistant and efficient clustering framework that does not necessitate prior knowledge of the number of clusters. By leveraging the robustness of MoM alongside the stability of convex clustering, our method enhances both performance and efficiency, especially on large-scale datasets. Theoretical analysis demonstrates weak consistency under specific conditions, while experiments on synthetic and real-world datasets validate the method's superior performance compared to existing approaches.

</details>


### [29] [Implicit Bias of the JKO Scheme](https://arxiv.org/abs/2511.14827)
*Peter Halmos,Boris Hanin*

Main category: stat.ML

TL;DR: 本文探讨了Wasserstein梯度流的离散化方法——Jordan-Kinderlehrer-Otto（JKO）方案的二阶隐式偏差。JKO方案不仅能一阶近似梯度流，还具有能量耗散保持和无条件稳定性等特性。研究发现，JKO方案在二阶上等价于在一个修正能量泛函$J^η$上的梯度流，该泛函是通过从原始泛函$J$中减去与$J$的度量曲率相关的项得到的。这意味着JKO方案会在$J$的度量曲率快速变化的T方向上引入减速。文章还通过实例分析了这种隐式偏差在不同泛函下的表现及其对优化的影响。


<details>
  <summary>Details</summary>
Motivation: Wasserstein梯度流在概率测度空间上的能量泛函最小化问题中具有广泛应用。Jordan-Kinderlehrer-Otto（JKO）方案作为其规范的时间离散化方法，展现出优于其他一阶积分器的特性，例如保持能量耗散和在特定条件下无条件稳定。然而，为了更深入地理解JKO方案的工作机制及其对优化过程的影响，有必要对其高阶偏差进行细致的分析。特别是，了解其在二阶上的隐式偏差可以揭示其在优化动力学中的独特行为。

Method: 本文通过数学推导，将JKO方案的离散化序列$ρ_k^η$在二阶精度$η^2$下近似为在修正能量泛函$J^η(ρ) = J(ρ) - \fracη{4}\int_M \Big\lVert \nabla_g \frac{δJ}{δρ} (ρ) \Big\rVert_{2}^{2} \,ρ(dx)$上的Wasserstein梯度流。这个修正泛函是通过从原始泛函$J$中减去与$J$的度量曲率平方乘以$η/4$的项而得到的。这种修正项的引入揭示了JKO方案在二阶上引入的“减速”效应。此外，文章还通过具体的数值例子，如Bures-Wasserstein空间上的Langevin动力学和一维四次势中的Langevin采样，来研究和比较在$J$和$J^η$上最小化的差异，以深入理解JKO-Flow（在$J^η$上的Wasserstein梯度流）的行为。

Result: 研究结果表明，JKO方案在二阶精度$η^2$下表现出一种隐式偏差，等价于在一个修正能量泛函$J^η$上进行Wasserstein梯度流。修正泛函$J^η$通过从原始泛函$J$中减去与$J$的度量曲率平方乘以$η/4$的项来引入。这意味着JKO方案会在$J$的度量曲率快速变化的方向引入“减速”效应。对于常见泛函，这种隐式偏差具有特定的形式：对于熵是Fisher信息，对于KL散度是Fisher-Hyvärinen散度，对于黎曼梯度下降是度量$g$中的动能。数值实验（如在Bures-Wasserstein空间上的Langevin动力学和一维四次势中的Langevin采样）展示了在原始泛函$J$和修正泛函$J^η$上最小化的区别，从而提供了对JKO方案独特动态行为的深入理解。

Conclusion: 本文揭示了Jordan-Kinderlehrer-Otto（JKO）方案在Wasserstein梯度流时间离散化中的二阶隐式偏差。我们证明，JKO方案在二阶上等效于在一个经过修正的能量泛函$J^η$上进行梯度流，该泛函通过减去与原始泛函$J$的度量曲率相关的项来减缓优化过程。这一发现不仅为理解JKO方案为何具有优异的稳定性和能量耗散保持特性提供了理论基础，也揭示了其在不同泛函下的具体行为。通过数值实例，我们进一步验证了这种隐式偏差对优化动力学的影响。未来研究可以基于此，更深入地探索JKO方案在高维复杂问题中的应用，并开发利用其隐式偏差的优化策略。

Abstract: Wasserstein gradient flow provides a general framework for minimizing an energy functional $J$ over the space of probability measures on a Riemannian manifold $(M,g)$. Its canonical time-discretization, the Jordan-Kinderlehrer-Otto (JKO) scheme, produces for any step size $η>0$ a sequence of probability distributions $ρ_k^η$ that approximate to first order in $η$ Wasserstein gradient flow on $J$. But the JKO scheme also has many other remarkable properties not shared by other first order integrators, e.g. it preserves energy dissipation and exhibits unconditional stability for $λ$-geodesically convex functionals $J$. To better understand the JKO scheme we characterize its implicit bias at second order in $η$. We show that $ρ_k^η$ are approximated to order $η^2$ by Wasserstein gradient flow on a \emph{modified} energy \[ J^η(ρ) = J(ρ) - \fracη{4}\int_M \Big\lVert \nabla_g \frac{δJ}{δρ} (ρ) \Big\rVert_{2}^{2} \,ρ(dx), \] obtained by subtracting from $J$ the squared metric curvature of $J$ times $η/4$. The JKO scheme therefore adds at second order in $η$ a \textit{deceleration} in directions where the metric curvature of $J$ is rapidly changing. This corresponds to canonical implicit biases for common functionals: for entropy the implicit bias is the Fisher information, for KL-divergence it is the Fisher-Hyv{ä}rinen divergence, and for Riemannian gradient descent it is the kinetic energy in the metric $g$. To understand the differences between minimizing $J$ and $J^η$ we study \emph{JKO-Flow}, Wasserstein gradient flow on $J^η$, in several simple numerical examples. These include exactly solvable Langevin dynamics on the Bures-Wasserstein space and Langevin sampling from a quartic potential in 1D.

</details>


### [30] [Latent space analysis and generalization to out-of-distribution data](https://arxiv.org/abs/2511.15010)
*Katie Rainey,Erin Hausmann,Donald Waagen,David Gray,Donald Hulsey*

Main category: stat.ML

TL;DR: 这篇论文研究了深度学习系统中潜在决策空间中数据点之间关系的重要性，并探讨了OOD检测与模型分类精度之间的联系。


<details>
  <summary>Details</summary>
Motivation: 理解深度学习系统所推导出的潜在决策空间中数据点之间的关系对于评估和解释系统在实际数据上的性能至关重要。检测深度学习系统的“分布外”（OOD）数据仍然是一个活跃的研究课题。

Method: 本文使用开源的模拟和测量的合成孔径雷达（SAR）数据集进行研究。

Result: 经验证明OOD检测不能作为模型性能的替代衡量标准。

Conclusion: 这篇论文希望激发更多关于潜在空间几何特性的研究，从而为深度学习的鲁棒性和泛化能力提供新的见解。

Abstract: Understanding the relationships between data points in the latent decision space derived by the deep learning system is critical to evaluating and interpreting the performance of the system on real world data. Detecting \textit{out-of-distribution} (OOD) data for deep learning systems continues to be an active research topic. We investigate the connection between latent space OOD detection and classification accuracy of the model. Using open source simulated and measured Synthetic Aperture RADAR (SAR) datasets, we empirically demonstrate that the OOD detection cannot be used as a proxy measure for model performance. We hope to inspire additional research into the geometric properties of the latent space that may yield future insights into deep learning robustness and generalizability.

</details>


### [31] [Beyond Uncertainty Sets: Leveraging Optimal Transport to Extend Conformal Predictive Distribution to Multivariate Settings](https://arxiv.org/abs/2511.15146)
*Eugene Ndiaye*

Main category: stat.ML

TL;DR: 本文提出了一种基于最优传输（OT）的一致性预测（CP）方法，用于处理高维数据，并首次构建了具有有限样本校准的多元一致性预测分布（CPD）。


<details>
  <summary>Details</summary>
Motivation: 传统的一致性预测（CP）方法在处理高维数据时存在局限性，因为其程序只在分数是标量值时才直接适用，或者需要对高维数据进行临时降维。虽然最优传输（OT）提供了定义向量排序和多变量分位数区域的原则性方法，但通常只提供渐近覆盖保证。

Method: 本文通过对向量值最优传输（OT）分位数区域进行一致化处理，恢复了有限样本、无分布的覆盖保证。候选的排序是通过一个传输图定义的，该传输图是针对校准分数以及该候选分数计算的。由此产生了一个OT问题连续体，并证明了最优分配在分数空间的固定多面体划分上是分段常数的。此外，本文还首次构建了具有有限样本校准的多元一致性预测分布（CPDs），包括保守版本和精确随机版本。

Result: 本文的方法能够有效处理高维数据，并提供有限样本覆盖保证的预测集。首次构建的多元一致性预测分布（CPDs）能够弥补传统预测集只能指示结果是否合理，但不能指示相对可能性的缺陷。精确随机版本是经典Dempster-Hill程序的多元泛化。

Conclusion: 本文成功地将一致性预测（CP）扩展到高维向量值分数，并通过最优传输（OT）提供了有限样本、无分布的覆盖保证。此外，本文首次提出了具有有限样本校准的多元一致性预测分布（CPDs），为解决高维预测分布问题提供了有效途径。

Abstract: Conformal prediction (CP) constructs uncertainty sets for model outputs with finite-sample coverage guarantees. A candidate output is included in the prediction set if its non-conformity score is not considered extreme relative to the scores observed on a set of calibration examples. However, this procedure is only straightforward when scores are scalar-valued, which has limited CP to real-valued scores or ad-hoc reductions to one dimension. The problem of ordering vectors has been studied via optimal transport (OT), which provides a principled method for defining vector-ranks and multivariate quantile regions, though typically with only asymptotic coverage guarantees. We restore finite-sample, distribution-free coverage by conformalizing the vector-valued OT quantile region. Here, a candidate's rank is defined via a transport map computed for the calibration scores augmented with that candidate's score. This defines a continuum of OT problems for which we prove that the resulting optimal assignment is piecewise-constant across a fixed polyhedral partition of the score space. This allows us to characterize the entire prediction set tractably, and provides the machinery to address a deeper limitation of prediction sets: that they only indicate which outcomes are plausible, but not their relative likelihood. In one dimension, conformal predictive distributions (CPDs) fill this gap by producing a predictive distribution with finite-sample calibration. Extending CPDs beyond one dimension remained an open problem. We construct, to our knowledge, the first multivariate CPDs with finite-sample calibration, i.e., they define a valid multivariate distribution where any derived uncertainty region automatically has guaranteed coverage. We present both conservative and exact randomized versions, the latter resulting in a multivariate generalization of the classical Dempster-Hill procedure.

</details>


### [32] [Robust Bayesian Optimisation with Unbounded Corruptions](https://arxiv.org/abs/2511.15315)
*Abdelhamid Ezzerg,Ilija Bogunovic,Jeremias Knoblauch*

Main category: stat.ML

TL;DR: RCGP-UCB 是一种针对贝叶斯优化的鲁棒算法，能够抵御极端异常值，并针对异常值的频率而非幅度进行预算限制。该算法在存在异常值的情况下实现了次线性憾事，并且在没有异常值的情况下与标准 GP-UCB 的性能相当。


<details>
  <summary>Details</summary>
Motivation: 现有的贝叶斯优化方法容易受到极端异常值的影响，尤其是在面对单个高强度异常值时。本文旨在提出一种新的方法来解决这个问题，即在异常值可能具有无限大的情况下，其预算仅在频率上受限。

Method: 本文提出了一种名为 RCGP-UCB 的算法。该算法结合了著名的 UCB 方法和鲁棒共轭高斯过程（RCGP）。RCGP-UCB 提供了稳定和自适应的版本。

Result: RCGP-UCB 在存在多达 $O(T^{1/2})$ 和 $O(T^{1/3})$ 次可能无限大的异常值的情况下，实现了次线性憾事。在没有异常值的情况下，RCGP-UCB 的憾事界限与标准 GP-UCB 算法的憾事界限相匹配。

Conclusion: RCGP-UCB 是一种有效的鲁棒贝叶斯优化算法，能够以接近零的成本处理极端异常值，并且在没有异常值的情况下，其性能与现有最佳算法相当。

Abstract: Bayesian Optimization is critically vulnerable to extreme outliers. Existing provably robust methods typically assume a bounded cumulative corruption budget, which makes them defenseless against even a single corruption of sufficient magnitude. To address this, we introduce a new adversary whose budget is only bounded in the frequency of corruptions, not in their magnitude. We then derive RCGP-UCB, an algorithm coupling the famous upper confidence bound (UCB) approach with a Robust Conjugate Gaussian Process (RCGP). We present stable and adaptive versions of RCGP-UCB, and prove that they achieve sublinear regret in the presence of up to $O(T^{1/2})$ and $O(T^{1/3})$ corruptions with possibly infinite magnitude. This robustness comes at near zero cost: without outliers, RCGP-UCB's regret bounds match those of the standard GP-UCB algorithm.

</details>


### [33] [Exponential Lasso: robust sparse penalization under heavy-tailed noise and outliers with exponential-type loss](https://arxiv.org/abs/2511.15332)
*The Tien Mai*

Main category: stat.ML

TL;DR: 该文章介绍了一种名为Exponential Lasso的新型鲁棒方法，用于在高维统计中进行变量选择和参数估计，以解决传统Lasso对异常值敏感的问题。


<details>
  <summary>Details</summary>
Motivation: 传统的Lasso方法依赖于平方损失函数，这使得它对异常值和重尾噪声高度敏感，可能导致不可靠的模型选择和有偏估计。因此，作者旨在开发一种更鲁棒的方法来解决这一限制。

Method: Exponential Lasso方法将指数型损失函数集成到Lasso框架中。这种损失函数旨在实现在高斯噪声下的统计效率和数据污染下的鲁棒性之间的平滑权衡。与截断大残差影响的其他方法不同，指数损失函数平滑地下降，有效地降低了极端异常值的影响，同时对小误差保持了近似二次行为。该估计器通过Majorization-Minimization (MM) 算法进行高效优化，该算法迭代地解决一系列加权Lasso子问题。

Result: 理论上，Exponential Lasso实现了强大的统计收敛速度，在理想条件下与经典Lasso匹配，同时在存在重尾污染的情况下保持其鲁棒性。数值实验表明，该方法具有很强的竞争力，在受污染的环境中优于经典Lasso，即使在高斯噪声下也能保持强大的性能。

Conclusion: Exponential Lasso通过引入指数型损失函数，有效解决了传统Lasso对异常值敏感的问题，并在理论和实践中都展现出优越的性能和鲁棒性。

Abstract: In high-dimensional statistics, the Lasso is a cornerstone method for simultaneous variable selection and parameter estimation. However, its reliance on the squared loss function renders it highly sensitive to outliers and heavy-tailed noise, potentially leading to unreliable model selection and biased estimates. To address this limitation, we introduce the Exponential Lasso, a novel robust method that integrates an exponential-type loss function within the Lasso framework. This loss function is designed to achieve a smooth trade-off between statistical efficiency under Gaussian noise and robustness against data contamination. Unlike other methods that cap the influence of large residuals, the exponential loss smoothly redescends, effectively downweighting the impact of extreme outliers while preserving near-quadratic behavior for small errors. We establish theoretical guarantees showing that the Exponential Lasso achieves strong statistical convergence rates, matching the classical Lasso under ideal conditions while maintaining its robustness in the presence of heavy-tailed contamination. Computationally, the estimator is optimized efficiently via a Majorization-Minimization (MM) algorithm that iteratively solves a series of weighted Lasso subproblems. Numerical experiments demonstrate that the proposed method is highly competitive, outperforming the classical Lasso in contaminated settings and maintaining strong performance even under Gaussian noise.
  Our method is implemented in the \texttt{R} package \texttt{heavylasso} available on Github: https://github.com/tienmt/heavylasso

</details>


### [34] [Gini Score under Ties and Case Weights](https://arxiv.org/abs/2511.15446)
*Alexej Brauer,Mario V. Wüthrich*

Main category: stat.ML

TL;DR: 本文探讨了Gini分数在统计建模和机器学习中的应用，特别是在处理风险排名中的联系和案例权重时的扩展


<details>
  <summary>Details</summary>
Motivation: Gini分数是统计建模和机器学习中用于模型验证和选择的流行工具，但其在处理风险排名中的联系（ties）和案例权重（case weights）方面仍有待探讨。

Method: 本文讨论了如何将Gini分数应用于风险排名中存在联系的情况，并将其调整以适应精算中常见的案例权重情况。

Result: Gini分数可以扩展应用于风险排名中存在联系的情况，并且可以根据案例权重进行调整，以适应精算领域的特定需求。

Conclusion: 本文提出了Gini分数在处理风险排名中的联系和案例权重时的应用方法，这为Gini分数在更广泛的场景中应用提供了可能性。

Abstract: The Gini score is a popular tool in statistical modeling and machine learning for model validation and model selection. It is a purely rank based score that allows one to assess risk rankings. The Gini score for statistical modeling has mainly been used in a binary context, in which it has many equivalent reformulations such as the receiver operating characteristic (ROC) or the area under the curve (AUC). In the actuarial literature, this rank based score for binary responses has been extended to general real-valued random variables using Lorenz curves and concentration curves. While these initial concepts assume that the risk ranking is generated by a continuous distribution function, we discuss in this paper how the Gini score can be used in the case of ties in the risk ranking. Moreover, we adapt the Gini score to the common actuarial situation of having case weights.

</details>


### [35] [A Physics Informed Machine Learning Framework for Optimal Sensor Placement and Parameter Estimation](https://arxiv.org/abs/2511.15543)
*Georgios Venianakis,Constantinos Theodoropoulos,Michail Kavousanakis*

Main category: stat.ML

TL;DR: 本文提出了一种基于PINN的框架，用于同时解决参数估计和传感器优化问题。


<details>
  <summary>Details</summary>
Motivation: 在工程领域中，参数估计是一个具有挑战性的任务，因为数据采集成本高、有限或容易出现不准确（噪声、不确定性）。因此，本文旨在确定能够提供关于未知参数最大信息量的传感器配置，特别是对于空间变化很重要的分布式参数系统。

Method: 本文提出了一种PINN框架，同时解决参数估计和传感器优化问题。具体来说，我们训练了一个PINN模型，其中感兴趣的参数作为额外的输入。通过自动微分计算敏感性函数，并利用D最优性准则确定最佳传感器位置。

Result: 通过在两个分布式参数反应-扩散-对流问题上进行验证，结果显示，与直观或随机选择的传感器位置相比，我们提出的基于PINN的方法能够持续获得更高的准确性。

Conclusion: 本文提出的PINN框架能够有效地进行参数估计，并且在传感器优化方面表现出色，为工程领域中的参数估计和传感器优化问题提供了一种有前途的解决方案。

Abstract: Parameter estimation remains a challenging task across many areas of engineering. Because data acquisition can often be costly, limited, or prone to inaccuracies (noise, uncertainty) it is crucial to identify sensor configurations that provide the maximum amount of information about the unknown parameters, in particular for the case of distributed-parameter systems, where spatial variations are important. Physics-Informed Neural Networks (PINNs) have recently emerged as a powerful machine-learning (ML) tool for parameter estimation, particularly in cases with sparse or noisy measurements, overcoming some of the limitations of traditional optimization-based and Bayesian approaches. Despite the widespread use of PINNs for solving inverse problems, relatively little attention has been given to how their performance depends on sensor placement. This study addresses this gap by introducing a comprehensive PINN-based framework that simultaneously tackles optimal sensor placement and parameter estimation. Our approach involves training a PINN model in which the parameters of interest are included as additional inputs. This enables the efficient computation of sensitivity functions through automatic differentiation, which are then used to determine optimal sensor locations exploiting the D-optimality criterion. The framework is validated on two illustrative distributed-parameter reaction-diffusion-advection problems of increasing complexity. The results demonstrate that our PINNs-based methodology consistently achieves higher accuracy compared to parameter values estimated from intuitively or randomly selected sensor positions.

</details>


<div id='cs.LG'></div>

# cs.LG [[Back]](#toc)

### [36] [Transformer Injectivity & Geometric Robustness - Analytic Margins and Bi-Lipschitz Uniformity of Sequence-Level Hidden States](https://arxiv.org/abs/2511.14808)
*Mikael von Strauss*

Main category: cs.LG

TL;DR: 这篇论文探讨了在解码器专用Transformer模型中，从离散提示到最后一词隐藏状态的映射的单射性。在实际分析假设下，研究发现这种映射在有限提示集上是泛型单射的。


<details>
  <summary>Details</summary>
Motivation: 在解码器专用Transformer模型中，理解离散提示到隐藏状态映射的单射性，并识别导致非单射性的碰撞判别式，对于深入理解模型的工作原理至关重要。

Method: 本文定义了每一层的碰撞判别式$Δ^ℓ$和单射层$U^ℓ = Θ\setminus Δ^ℓ$，并证明了一个二分法：模型要么在这个集合上处处非单射，要么$U^ℓ$是开集且稠密，并且每个$F^ℓ_θ$都是单射的。在优化器温和的非奇异假设和绝对连续初始化的条件下，泛型单射性在任何固定时间范围内的平滑训练轨迹中持续存在。研究还考虑了对称群$G$，表明判别式和单射层会下降到商集$Θ/G$，因此单射性自然是函数等价类的一个属性。同时，论文还通过对大规模提示集上最近邻统计的估计，定义了提示空间和最后一词表示空间之间的分离裕度与共Lipschitz（下限Lipschitz）常数，并对预训练的LLaMA-3和Qwen模型进行了实证研究。

Result: 在全精度或8位量化下，采样的提示没有发生碰撞；而4位量化会导致少量碰撞，并显著缩小共Lipschitz估计。对于从头开始训练的小型GPT-2模型，归一化指标在训练过程中保持稳定。

Conclusion: Transformer表示在连续参数理想化中是泛型且持久的单射，其在实践中的可逆性可以通过简单的几何诊断方法进行探测。

Abstract: Under real-analytic assumptions on decoder-only Transformers, recent work shows that the map from discrete prompts to last-token hidden states is generically injective on finite prompt sets. We refine this picture: for each layer $\ell$ we define a collision discriminant $Δ^\ell \subset Θ$ and injective stratum $U^\ell = Θ\setminus Δ^\ell$, and prove a dichotomy -- either the model is nowhere injective on the set, or $U^\ell$ is open and dense and every $F^\ell_θ$ is injective. Under mild non-singularity assumptions on the optimizer and an absolutely continuous initialization, generic injectivity persists along smooth training trajectories over any fixed horizon. We also treat symmetry groups $G$, showing that discriminants and injective strata descend to the quotient $Θ/G$, so injectivity is naturally a property of functional equivalence classes.
  We complement these results with an empirical study of layerwise geometric diagnostics. We define a separation margin and a co-Lipschitz (lower Lipschitz) constant between prompt space and last-token representation space, estimated via nearest-neighbor statistics on large prompt sets. Applying these diagnostics to pretrained LLaMA-3 and Qwen models, we study behavior across layers, sequence lengths, model scales, and 8- and 4-bit activation quantization. On our sampled prompts we see no collisions in full precision or at 8 bits, while 4-bit quantization induces a small number of collisions and markedly shrinks co-Lipschitz estimates. For a small GPT-2 trained from scratch, normalized metrics remain stable over training. Overall, the results suggest that Transformer representations are generically and persistently injective in the continuous-parameter idealization, while their practical invertibility can be probed using simple geometric diagnostics.

</details>


### [37] [DEVAL: A Framework for Evaluating and Improving the Derivation Capability of Large Language Models](https://arxiv.org/abs/2511.14813)
*Yifan Li,Qin Li,Min Zhang,Min Zhang,Peixin Wang*

Main category: cs.LG

TL;DR: Derivation Relation (DR)和Derivation Capability (DC)被提出用于评估大型语言模型（LLMs）的推理能力，并通过DEVAL框架对LLMs进行了评估。同时，提出了一种名为Derivation Prompting (DP)的新型提示工程方法，显著提高了LLMs的DC。


<details>
  <summary>Details</summary>
Motivation: 目前对大型语言模型（LLMs）的数据推理能力评估仍是一个突出的研究问题。人类推理可以根据输入的变化相应地修改输出，这种依赖于抽象规则的推理模式在LLMs中尚未得到全面描述或评估。

Method: 本文正式将这种推理模式定义为派生关系（Derivation Relation, DR），并引入了派生能力（Derivation Capability, DC）的概念。为了评估DC，我们提出了一个名为DEVAL的系统化评估框架，并用它评估了五种流行的LLMs和一种大型推理模型在七项主流任务中的表现。此外，我们提出了一种名为Derivation Prompting (DP)的新型提示工程方法来提高DC。

Result: 评估结果显示，主流LLMs（如GPT-4o和Claude3.5）展现出中等的DR识别能力，但在解决问题时应用DR的有效性显著下降。Derivation Prompting (DP)方法将所有测试LLMs的DC平均提高了15.2%，优于常用的提示工程技术。

Conclusion: 本文定义了DR和DC，并提出了DEVAL框架来评估LLMs的派生能力。研究发现当前LLMs在有效应用DR方面存在不足，而Derivation Prompting (DP)是一种有效的提示工程方法，可以显著提升LLMs的派生能力。

Abstract: Assessing the reasoning ability of Large Language Models (LLMs) over data remains an open and pressing research question. Compared with LLMs, human reasoning can derive corresponding modifications to the output based on certain kinds of changes to the input. This reasoning pattern, which relies on abstract rules that govern relationships between changes of data, has not been comprehensively described or evaluated in LLMs. In this paper, we formally define this reasoning pattern as the Derivation Relation (DR) and introduce the concept of Derivation Capability (DC), i.e. applying DR by making the corresponding modification to the output whenever the input takes certain changes. To assess DC, a systematically constructed evaluation framework named DEVAL is proposed and used to evaluate five popular LLMs and one Large Reasoning Model in seven mainstream tasks. The evaluation results show that mainstream LLMs, such as GPT-4o and Claude3.5, exhibit moderate DR recognition capabilities but reveal significant drop-offs on applying DR effectively in problem-solving scenarios. To improve this, we propose a novel prompt engineering approach called Derivation Prompting (DP). It achieves an average improvement of 15.2% in DC for all tested LLMs, outperforming commonly used prompt engineering techniques.

</details>


### [38] [Dynamic Nested Hierarchies: Pioneering Self-Evolution in Machine Learning Architectures for Lifelong Intelligence](https://arxiv.org/abs/2511.14823)
*Akbar Anbar Jafari,Cagri Ozcinar,Gholamreza Anbarjafari*

Main category: cs.LG

TL;DR: 这篇论文提出了动态嵌套层次结构，这是一种新的机器学习范式，旨在通过允许模型自主调整优化级别、嵌套结构和更新频率来解决传统模型在非平稳环境中适应性差的问题，从而实现终身学习和对分布变化的持续适应。


<details>
  <summary>Details</summary>
Motivation: 目前机器学习模型（包括大型语言模型）在静态任务中表现出色，但在非平稳环境中适应性不佳，因为其固定的架构阻碍了持续适应和终身学习。

Method: 本文在嵌套学习范式的基础上，提出动态嵌套层次结构。它允许模型在训练或推理过程中自主调整优化级别、嵌套结构和更新频率，灵感来源于神经可塑性，以实现无预定义约束的自我演进。

Result: 通过严格的数学公式、收敛性的理论证明、表达能力界限以及在不同机制下的次线性遗憾，以及在语言建模、持续学习和长上下文推理方面的卓越性能的实证 H 演示，动态嵌套层次结构为实现自适应的通用智能奠定了基础。

Conclusion: 动态嵌套层次结构通过动态压缩上下文流和适应分布变化，解决了现有模型中的顺行性遗忘问题，促进了真正的终身学习，并为自适应通用智能奠定了基础。

Abstract: Contemporary machine learning models, including large language models, exhibit remarkable capabilities in static tasks yet falter in non-stationary environments due to rigid architectures that hinder continual adaptation and lifelong learning. Building upon the nested learning paradigm, which decomposes models into multi-level optimization problems with fixed update frequencies, this work proposes dynamic nested hierarchies as the next evolutionary step in advancing artificial intelligence and machine learning. Dynamic nested hierarchies empower models to autonomously adjust the number of optimization levels, their nesting structures, and update frequencies during training or inference, inspired by neuroplasticity to enable self-evolution without predefined constraints. This innovation addresses the anterograde amnesia in existing models, facilitating true lifelong learning by dynamically compressing context flows and adapting to distribution shifts. Through rigorous mathematical formulations, theoretical proofs of convergence, expressivity bounds, and sublinear regret in varying regimes, alongside empirical demonstrations of superior performance in language modeling, continual learning, and long-context reasoning, dynamic nested hierarchies establish a foundational advancement toward adaptive, general-purpose intelligence.

</details>


### [39] [Empowering Multi-Turn Tool-Integrated Reasoning with Group Turn Policy Optimization](https://arxiv.org/abs/2511.14846)
*Yifeng Ding,Hung Le,Songyang Han,Kangrui Ruan,Zhenghui Jin,Varun Kumar,Zijian Wang,Anoop Deoras*

Main category: cs.LG

TL;DR: GTPO 通过引入对每个 turn 更细致的奖励、基于回报的优势估计以及自我监督的奖励塑造来解决现有强化学习方法在训练大型语言模型处理多轮工具集成推理时面临的挑战，从而在各种推理基准测试中超越 GRPO。


<details>
  <summary>Details</summary>
Motivation: 现有的强化学习方法在训练大型语言模型处理多轮工具集成推理时，由于奖励粒度过粗，导致学习信号不足，训练停滞不前。

Method: 提出了一种新颖的强化学习算法 Group Turn Policy Optimization (GTPO)，该算法包含三项关键创新：1）turn 级别奖励分配，为每个 turn 提供细粒度反馈；2）基于回报的优势估计，将归一化折扣回报计算为优势；3）自我监督奖励塑造，利用生成的代码中的自我监督信号来强化稀疏的基于二元结果的奖励。

Result: GTPO 在各种推理基准测试中平均比 GRPO 高出 3.0%。

Conclusion: GTPO 算法有效地推进了大型语言模型在复杂数学推理方面的能力。

Abstract: Training Large Language Models (LLMs) for multi-turn Tool-Integrated Reasoning (TIR) - where models iteratively reason, generate code, and verify through execution - remains challenging for existing reinforcement learning (RL) approaches. Current RL methods, exemplified by Group Relative Policy Optimization (GRPO), suffer from coarse-grained, trajectory-level rewards that provide insufficient learning signals for complex multi-turn interactions, leading to training stagnation. To address this issue, we propose Group Turn Policy Optimization (GTPO), a novel RL algorithm specifically designed for training LLMs on multi-turn TIR tasks. GTPO introduces three key innovations: (1) turn-level reward assignment that provides fine-grained feedback for individual turns, (2) return-based advantage estimation where normalized discounted returns are calculated as advantages, and (3) self-supervised reward shaping that exploits self-supervision signals from generated code to densify sparse binary outcome-based rewards. Our comprehensive evaluation demonstrates that GTPO outperforms GRPO by 3.0% on average across diverse reasoning benchmarks, establishing its effectiveness for advancing complex mathematical reasoning in the real world.

</details>


### [40] [FinTRec: Transformer Based Unified Contextual Ads Targeting and Personalization for Financial Applications](https://arxiv.org/abs/2511.14865)
*Dwipam Katariya,Snehita Varma,Akshat Shreemali,Benjamin Wu,Kalanand Mishra,Pranab Mohanty*

Main category: cs.LG

TL;DR: 这篇论文提出了 FinTRec，一个基于 Transformer 的金融服务推荐系统，它解决了实时推荐的挑战，并在性能上优于传统的树形模型。


<details>
  <summary>Details</summary>
Motivation: Transformer 架构在金融服务实时推荐中面临用户交互长、多产品关联以及业务目标冲突等挑战，需要一个能够解决这些问题的框架。

Method: 本文提出了 FinTRec，一个基于 Transformer 的框架，旨在解决金融服务中实时推荐的挑战和操作目标，并通过历史模拟和在线 A/B 测试相关性进行评估。

Result: FinTRec 在所有产品上都持续优于生产级别的树型基线模型，并通过产品适应性的微调实现了跨产品信号共享，降低了训练成本和技术债务，同时提高了离线性能。

Conclusion: FinTRec 是金融服务领域统一序列推荐建模的首次全面研究，它在技术和业务方面都展现出优越性，证明了 Transformer 架构在金融服务中的可行性和有效性。

Abstract: Transformer-based architectures are widely adopted in sequential recommendation systems, yet their application in Financial Services (FS) presents distinct practical and modeling challenges for real-time recommendation. These include:a) long-range user interactions (implicit and explicit) spanning both digital and physical channels generating temporally heterogeneous context, b) the presence of multiple interrelated products require coordinated models to support varied ad placements and personalized feeds, while balancing competing business goals. We propose FinTRec, a transformer-based framework that addresses these challenges and its operational objectives in FS. While tree-based models have traditionally been preferred in FS due to their explainability and alignment with regulatory requirements, our study demonstrate that FinTRec offers a viable and effective shift toward transformer-based architectures. Through historic simulation and live A/B test correlations, we show FinTRec consistently outperforms the production-grade tree-based baseline. The unified architecture, when fine-tuned for product adaptation, enables cross-product signal sharing, reduces training cost and technical debt, while improving offline performance across all products. To our knowledge, this is the first comprehensive study of unified sequential recommendation modeling in FS that addresses both technical and business considerations.

</details>


### [41] [Transformer-Guided Deep Reinforcement Learning for Optimal Takeoff Trajectory Design of an eVTOL Drone](https://arxiv.org/abs/2511.14887)
*Nathan M. Roberts,Xiaosong Du*

Main category: cs.LG

TL;DR: 这篇论文提出了一种Transformer引导的深度强化学习（DRL）方法，用于优化电动垂直起降（eVTOL）飞行器的起飞轨迹，以实现最小能耗。


<details>
  <summary>Details</summary>
Motivation: 电动垂直起降（eVTOL）飞行器的快速发展为缓解城市交通拥堵提供了有益的机会。因此，开发最小能耗的最优起飞轨迹对于更广泛的eVTOL飞行器应用至关重要。

Method: 本文提出了一种Transformer引导的DRL方法，通过在每个时间步使用Transformer探索现实状态空间来缓解训练难度。

Result: Transformer引导的DRL智能体以4.57 \times 10^6的时间步长学会了起飞，这相当于普通DRL智能体所需19.79 \times 10^6时间步长的25%。此外，在最佳能耗方面，Transformer引导的DRL达到了97.2%的准确率，而普通DRL达到了96.3%的准确率。

Conclusion: 所提出的Transformer引导的DRL在训练效率和优化设计验证方面均优于普通DRL。

Abstract: The rapid advancement of electric vertical take-off and landing (eVTOL) aircraft offers a promising opportunity to alleviate urban traffic congestion. Thus, developing optimal takeoff trajectories for minimum energy consumption becomes essential for broader eVTOL aircraft applications. Conventional optimal control methods (such as dynamic programming and linear quadratic regulator) provide highly efficient and well-established solutions but are limited by problem dimensionality and complexity. Deep reinforcement learning (DRL) emerges as a special type of artificial intelligence tackling complex, nonlinear systems; however, the training difficulty is a key bottleneck that limits DRL applications. To address these challenges, we propose the transformer-guided DRL to alleviate the training difficulty by exploring a realistic state space at each time step using a transformer. The proposed transformer-guided DRL was demonstrated on an optimal takeoff trajectory design of an eVTOL drone for minimal energy consumption while meeting takeoff conditions (i.e., minimum vertical displacement and minimum horizontal velocity) by varying control variables (i.e., power and wing angle to the vertical). Results presented that the transformer-guided DRL agent learned to take off with $4.57\times10^6$ time steps, representing 25% of the $19.79\times10^6$ time steps needed by a vanilla DRL agent. In addition, the transformer-guided DRL achieved 97.2% accuracy on the optimal energy consumption compared against the simulation-based optimal reference while the vanilla DRL achieved 96.3% accuracy. Therefore, the proposed transformer-guided DRL outperformed vanilla DRL in terms of both training efficiency as well as optimal design verification.

</details>


### [42] [It's LIT! Reliability-Optimized LLMs with Inspectable Tools](https://arxiv.org/abs/2511.14903)
*Ruixin Zhang,Jon Donnelly,Zhicheng Guo,Ghazal Khalighinejad,Haiyang Huang,Alina Jade Barnett,Cynthia Rudin*

Main category: cs.LG

TL;DR: 该论文提出了一个名为LIT的框架，旨在通过强制大型语言模型（LLMs）在解决问题时优先使用外部、更可靠的工具，从而提高LLMs在复杂任务中的可靠性和可解释性。


<details>
  <summary>Details</summary>
Motivation: 大型语言模型（LLMs）在处理实际任务时展现出强大的能力，但其不透明的推理过程限制了它们在高风险领域中的应用，因为这些领域的解决方案需要高度可信。LLMs可能会选择不可靠且难以排查的解决方案，即使存在更好的选择。

Method: LIT框架基于现有LLMs的工具调用能力，引导LLMs选择最可靠且易于排查的解决方案路径，该路径可能涉及多个连续的工具调用。为了支持LIT，引入了一个包含1,300个问题的新基准数据集和一个可定制的可靠性成本函数集。这些成本函数评估了工具的可靠性和易于排查性。

Result: 研究表明，通过使用LIT框架，LLMs在保持任务性能的同时，能够实现更可靠和更明智的问题解决。

Conclusion: LIT框架通过结合外部可靠工具，有效提升了LLMs在复杂问题解决中的可靠性和可解释性，为LLMs在高风险领域的应用提供了新的途径。

Abstract: Large language models (LLMs) have exhibited remarkable capabilities across various domains. The ability to call external tools further expands their capability to handle real-world tasks. However, LLMs often follow an opaque reasoning process, which limits their usefulness in high-stakes domains where solutions need to be trustworthy to end users. LLMs can choose solutions that are unreliable and difficult to troubleshoot, even if better options are available. We address this issue by forcing LLMs to use external -- more reliable -- tools to solve problems when possible. We present a framework built on the tool-calling capabilities of existing LLMs to enable them to select the most reliable and easy-to-troubleshoot solution path, which may involve multiple sequential tool calls. We refer to this framework as LIT (LLMs with Inspectable Tools). In order to support LIT, we introduce a new and challenging benchmark dataset of 1,300 questions and a customizable set of reliability cost functions associated with a collection of specialized tools. These cost functions summarize how reliable each tool is and how easy it is to troubleshoot. For instance, a calculator is reliable across domains, whereas a linear prediction model is not reliable if there is distribution shift, but it is easy to troubleshoot. A tool that constructs a random forest is neither reliable nor easy to troubleshoot. These tools interact with the Harvard USPTO Patent Dataset and a new dataset of NeurIPS 2023 papers to solve mathematical, coding, and modeling problems of varying difficulty levels. We demonstrate that LLMs can achieve more reliable and informed problem-solving while maintaining task performance using our framework.

</details>


### [43] [Integrating Causal Inference with Graph Neural Networks for Alzheimer's Disease Analysis](https://arxiv.org/abs/2511.14922)
*Pranay Kumar Peddi,Dhrubajyoti Ghosh*

Main category: cs.LG

TL;DR: Causal-GCN: 一种基于干预图卷积的框架，集成do-calculus反门调整来识别对阿尔茨海默病（AD）进展施加稳定因果影响的大脑区域。


<details>
  <summary>Details</summary>
Motivation: 以往的深度图学习模型在AD分类中多为相关性，混淆了人口统计学和遗传因素与疾病特异性特征。因此，需要一个能够识别病理因果关系的模型。

Method: Causal-GCN将每个受试者的MRI表示为结构连接组。通过主成分分析总结年龄、性别和APOE4基因型等混杂因素，并将其纳入因果调整集。模型通过干预模拟单个区域对疾病概率的平均因果影响。

Result: Causal-GCN在ADNI队列的484名受试者中，实现了与基线GNN相当的性能。

Conclusion: Causal-GCN提供了可解释的因果效应排名，突出了与已建立的AD神经病理学一致性，例如在后部、扣带和岛叶中枢。

Abstract: Deep graph learning has advanced Alzheimer's (AD) disease classification from MRI, but most models remain correlational, confounding demographic and genetic factors with disease specific features. We present Causal-GCN, an interventional graph convolutional framework that integrates do-calculus-based back-door adjustment to identify brain regions exerting stable causal influence on AD progression. Each subject's MRI is represented as a structural connectome where nodes denote cortical and subcortical regions and edges encode anatomical connectivity. Confounders such as age, sec, and APOE4 genotype are summarized via principal components and included in the causal adjustment set. After training, interventions on individual regions are simulated by serving their incoming edges and altering node features to estimate average causal effects on disease probability. Applied to 484 subjects from the ADNI cohort, Causal-GCN achieves performance comparable to baseline GNNs while providing interpretable causal effect rankings that highlight posterior, cingulate, and insular hubs consistent with established AD neuropathology.

</details>


### [44] [How to Train Private Clinical Language Models: A Comparative Study of Privacy-Preserving Pipelines for ICD-9 Coding](https://arxiv.org/abs/2511.14936)
*Mathieu Dufour,Andrew Duncan*

Main category: cs.LG

TL;DR: 这篇论文比较了在临床文本上训练的大型语言模型中，四种保护患者数据隐私的方法，以找出在保护隐私的同时保持诊断准确性的最佳策略。


<details>
  <summary>Details</summary>
Motivation: 大型语言模型在临床文本上的训练存在暴露患者敏感信息的风险，而差分隐私（DP）方法通常会严重降低部署所需的诊断准确性。目前尚不清楚哪种隐私保护策略最适合临床语言任务。

Method: 我们对医院出院摘要自动诊断编码的四种训练流程进行了首次系统的头对头比较。所有流程都使用相同的10亿参数模型和匹配的隐私预算来预测ICD-9代码。

Result: 在中等和宽松的隐私预算（ε ∈ {4, 6}）下，来自DP训练教师的知识蒸馏优于直接DP-SGD和DP合成数据训练，恢复了高达63%的非私有性能，同时保持了强大的经验隐私（成员推断AUC ≈ 0.5）。

Conclusion: 这些发现揭示了不同架构在隐私-效用权衡方面的巨大差异，并指出知识蒸馏是实现隐私保护临床自然语言处理最实用的途径。

Abstract: Large language models trained on clinical text risk exposing sensitive patient information, yet differential privacy (DP) methods often severely degrade the diagnostic accuracy needed for deployment. Despite rapid progress in DP optimisation and text generation, it remains unclear which privacy-preserving strategy actually works best for clinical language tasks. We present the first systematic head-to-head comparison of four training pipelines for automated diagnostic coding from hospital discharge summaries. All pipelines use identical 1B-parameter models and matched privacy budgets to predict ICD-9 codes. At moderate and relaxed privacy budgets ($\varepsilon \in \{4, 6\}$), knowledge distillation from DP-trained teachers outperforms both direct DP-SGD and DP-synthetic data training, recovering up to 63\% of the non-private performance whilst maintaining strong empirical privacy (membership-inference AUC $\approx$ 0.5). These findings expose large differences in the privacy-utility trade-off across architectures and identify knowledge distillation as the most practical route to privacy-preserving clinical NLP.

</details>


### [45] [Knowledge Graphs as Structured Memory for Embedding Spaces: From Training Clusters to Explainable Inference](https://arxiv.org/abs/2511.14961)
*Artur A. Oliveira,Mateus Espadoto,Roberto M. Cesar,Roberto Hirata*

Main category: cs.LG

TL;DR: 该论文介绍了一种名为图记忆（Graph Memory, GM）的结构化非参数框架，它通过区域级原型，以紧凑的关系记忆增强了基于嵌入的推理。


<details>
  <summary>Details</summary>
Motivation: GM的动机在于解决传统方法中，独立处理每个训练实例的局限性，并通过引入原型节点及其关系来概括嵌入空间，从而统一实例检索、基于原型的推理和基于图的标签传播。

Method: GM通过将嵌入空间总结为原型节点来实现，这些节点标注了可靠性指标并通过编码几何和上下文关系边连接起来。该模型在一个统一的归纳框架中支持高效推理和忠实解释。

Result: 在乳腺组织病理学（IDC）等合成和真实数据集上的实验表明，GM在准确性上与kNN和Label Spreading具有竞争力，同时提供了更好的校准和平滑的决策边界，并且所需的样本量减少了一个数量级。

Conclusion: 通过明确地建模可靠性和关系结构，GM在非参数学习中为局部证据和全局一致性之间提供了一个原则性的桥梁。

Abstract: We introduce Graph Memory (GM), a structured non-parametric framework that augments embedding-based inference with a compact, relational memory over region-level prototypes. Rather than treating each training instance in isolation, GM summarizes the embedding space into prototype nodes annotated with reliability indicators and connected by edges that encode geometric and contextual relations. This design unifies instance retrieval, prototype-based reasoning, and graph-based label propagation within a single inductive model that supports both efficient inference and faithful explanation. Experiments on synthetic and real datasets including breast histopathology (IDC) show that GM achieves accuracy competitive with $k$NN and Label Spreading while offering substantially better calibration and smoother decision boundaries, all with an order of magnitude fewer samples. By explicitly modeling reliability and relational structure, GM provides a principled bridge between local evidence and global consistency in non-parametric learning.

</details>


### [46] [Simulated Human Learning in a Dynamic, Partially-Observed, Time-Series Environment](https://arxiv.org/abs/2511.15032)
*Jeffrey Jiang,Kevin Hong,Emily Kuczynski,Gregory Pottie*

Main category: cs.LG

TL;DR: 这篇论文开发了一种动态、时间序列的环境来模拟课堂环境，并提出了一种强化学习智能辅导系统，该系统结合了个体学生状态学习和基于探索性干预的人群信息利用。


<details>
  <summary>Details</summary>
Motivation: 智能辅导系统（ITSs）可以利用以前学生的信息来实现个性化教学，但每个新学生都是独特的，且学习过程只能部分观测。

Method: 我们开发了一个动态、时间序列的环境来模拟课堂环境，其中包括师生干预（辅导课程、讲座和考试）。我们设计了模拟环境，允许不同程度的探索性干预，以收集更多信息。然后，我们开发了强化学习ITSs，结合学习学生个体状态，并通过探索性干预利用人群信息。

Result: 我们发现标准RL算法与贪婪的基于规则的启发式方法提供了不同的解决方案，但结果相似。我们还强调了随着隐藏信息水平的增加，问题的难度会增加，以及如果允许探测干预所获得的好处。我们展示了启发式和RL策略在改变学生群体分布方面的灵活性，发现两者都具有灵活性，但RL策略在帮助更难的班级时会遇到困难。最后，我们测试了不同课程结构下的非探索性策略，发现我们的策略能够比纯期末考试结构更能提高小测和期中考试结构的表现。

Conclusion: 探索性干预可以降低学生评估的难度，但也会引入成本效益决策，需要在充分探索以获得准确估计和过度探索之间找到平衡，从而避免干扰学生。同时，额外的干预信息有助于提高小测和期中等课程结构的表现。

Abstract: While intelligent tutoring systems (ITSs) can use information from past students to personalize instruction, each new student is unique. Moreover, the education problem is inherently difficult because the learning process is only partially observable. We therefore develop a dynamic, time-series environment to simulate a classroom setting, with student-teacher interventions - including tutoring sessions, lectures, and exams. In particular, we design the simulated environment to allow for varying levels of probing interventions that can gather more information. Then, we develop reinforcement learning ITSs that combine learning the individual state of students while pulling from population information through the use of probing interventions. These interventions can reduce the difficulty of student estimation, but also introduce a cost-benefit decision to find a balance between probing enough to get accurate estimates and probing so often that it becomes disruptive to the student. We compare the efficacy of standard RL algorithms with several greedy rules-based heuristic approaches to find that they provide different solutions, but with similar results. We also highlight the difficulty of the problem with increasing levels of hidden information, and the boost that we get if we allow for probing interventions. We show the flexibility of both heuristic and RL policies with regards to changing student population distributions, finding that both are flexible, but RL policies struggle to help harder classes. Finally, we test different course structures with non-probing policies and we find that our policies are able to boost the performance of quiz and midterm structures more than we can in a finals-only structure, highlighting the benefit of having additional information.

</details>


### [47] [Interpretable temporal fusion network of multi- and multi-class arrhythmia classification](https://arxiv.org/abs/2511.15062)
*Yun Kwan Kim*

Main category: cs.LG

TL;DR: 该研究提出了一种新的心律失常分类框架，该框架能有效地融合局部和全局信息，并在两个标准数据库上取得了优于现有方法的性能。


<details>
  <summary>Details</summary>
Motivation: 传统的临床决策支持系统在心律失常分类方面面临挑战，因为心律失常的持续时间不一，且现有方法未能充分考虑心律失常的起始时间变化。

Method: 本研究提出了一种包含（i）局部和全局信息提取以及（ii）带有注意力的局部-全局信息融合的框架，以在受限的输入长度下实现心律失常的检测和分类。

Result: 在MITDB和AFDB数据库上，该框架在持续时间、事件和Dice分数方面的F1得分分别达到了96.45%、82.05%和96.31%（MITDB）以及97.57%、98.31%和97.45%（AFDB）。与基准模型相比，性能统计学上更优。在泛化能力测试中也表现出优越性。

Conclusion: 该方法能够有效地捕获局部和全局信息以及动态，且没有显著信息损失，从而能够更准确地检测心律失常并精确确定其发生时间，这有助于临床领域制定更准确的治疗方案。

Abstract: Clinical decision support systems (CDSSs) have been widely utilized to support the decisions made by cardiologists when detecting and classifying arrhythmia from electrocardiograms. However, forming a CDSS for the arrhythmia classification task is challenging due to the varying lengths of arrhythmias. Although the onset time of arrhythmia varies, previously developed methods have not considered such conditions. Thus, we propose a framework that consists of (i) local and global extraction and (ii) local-global information fusion with attention to enable arrhythmia detection and classification within a constrained input length. The framework's performance was evaluated in terms of 10-class and 4-class arrhythmia detection, focusing on identifying the onset and ending point of arrhythmia episodes and their duration using the MIT-BIH arrhythmia database (MITDB) and the MIT-BIH atrial fibrillation database (AFDB). Duration, episode, and Dice score performances resulted in overall F1-scores of 96.45%, 82.05%, and 96.31% on the MITDB and 97.57%, 98.31%, and 97.45% on the AFDB, respectively. The results demonstrated statistically superior performance compared to those of the benchmark models. To assess the generalization capability of the proposed method, an MITDB-trained model and MIT-BIH malignant ventricular arrhythmia database-trained model were tested AFDB and MITDB, respectively. Superior performance was attained compared with that of a state-of-the-art model. The proposed method effectively captures both local and global information and dynamics without significant information loss. Consequently, arrhythmias can be detected with greater accuracy, and their occurrence times can be precisely determined, enabling the clinical field to develop more accurate treatment plans based on the proposed method.

</details>


### [48] [Deep Pathomic Learning Defines Prognostic Subtypes and Molecular Drivers in Colorectal Cancer](https://arxiv.org/abs/2511.15067)
*Zisong Wang,Xuanyu Wang,Hang Chen,Haizhou Wang,Yuxin Chen,Yihang Xu,Yunhe Yuan,Lihuan Luo,Xitong Ling,Xiaoping Liu*

Main category: cs.LG

TL;DR: 本研究开发并验证了一个名为TDAM-CRC的多实例学习模型，用于结直肠癌的预后预测，并揭示了其潜在的分子机制。


<details>
  <summary>Details</summary>
Motivation: 结直肠癌（CRC）的高异质性使得精确的预后分层成为主要的临床挑战。传统的TNM分期系统不足以实现个性化医疗。

Method: 本研究在TCGA发现队列（n=581）中训练了TDAM-CRC模型，并在一个独立的外部队列（n=1031）中进行了验证。同时，整合了多组学数据以提高模型可解释性并识别新的预后生物标志物。

Result: TDAM-CRC模型在两个队列中均实现了稳健的风险分层，其预测性能显著优于传统的临床分期系统和多个现有模型。TDAM-CRC风险评分被证实为多变量分析中的独立预后因素。多组学分析显示高风险亚型与代谢重编程和免疫抑制性肿瘤微环境密切相关。MRPL37被确定为连接深度病理特征和临床预后的关键枢纽基因，其高表达是良好预后的独立生物标志物。

Conclusion: TDAM-CRC模型为结直肠癌的风险分层提供了一个强大的工具，揭示了新的分子靶点，并促进了个性化临床决策。

Abstract: Precise prognostic stratification of colorectal cancer (CRC) remains a major clinical challenge due to its high heterogeneity. The conventional TNM staging system is inadequate for personalized medicine. We aimed to develop and validate a novel multiple instance learning model TDAM-CRC using histopathological whole-slide images for accurate prognostic prediction and to uncover its underlying molecular mechanisms. We trained the model on the TCGA discovery cohort (n=581), validated it in an independent external cohort (n=1031), and further we integrated multi-omics data to improve model interpretability and identify novel prognostic biomarkers. The results demonstrated that the TDAM-CRC achieved robust risk stratification in both cohorts. Its predictive performance significantly outperformed the conventional clinical staging system and multiple state-of-the-art models. The TDAM-CRC risk score was confirmed as an independent prognostic factor in multivariable analysis. Multi-omics analysis revealed that the high-risk subtype is closely associated with metabolic reprogramming and an immunosuppressive tumor microenvironment. Through interaction network analysis, we identified and validated Mitochondrial Ribosomal Protein L37 (MRPL37) as a key hub gene linking deep pathomic features to clinical prognosis. We found that high expression of MRPL37, driven by promoter hypomethylation, serves as an independent biomarker of favorable prognosis. Finally, we constructed a nomogram incorporating the TDAM-CRC risk score and clinical factors to provide a precise and interpretable clinical decision-making tool for CRC patients. Our AI-driven pathological model TDAM-CRC provides a robust tool for improved CRC risk stratification, reveals new molecular targets, and facilitates personalized clinical decision-making.

</details>


### [49] [Sample-Adaptivity Tradeoff in On-Demand Sampling](https://arxiv.org/abs/2511.15507)
*Nika Haghtalab,Omar Montasser,Mingda Qiao*

Main category: cs.LG

TL;DR: 本文研究了按需采样中样本复杂度与轮数复杂度之间的权衡，并提出了一个新的框架OODS。


<details>
  <summary>Details</summary>
Motivation: 在Multi-Distribution Learning (MDL) 的可实现设置中，研究算法按需采样时样本复杂度与轮数复杂度之间的权衡。

Method: 对于可实现设置，展示了r轮算法的最优样本复杂度近似为dk^(Θ(1/r)) / ε。对于ทั่วไป的不可知情况，提出了一种算法，在O(k)轮内实现了接近最优的样本复杂度O((d + k) / ε^2)。引入了一个新的框架：Optimization via On-Demand Sampling (OODS)。

Result: OODS框架抽象了样本适应性权衡，并捕获了大多数现有MDL算法。建立了OODS设置中轮数复杂度的紧密界限。

Conclusion: 上界直接产生了不可知MDL的O(k)轮算法，而下界意味着，实现亚多项式轮复杂度将需要从根本上绕过OODS固有难度的新技术。

Abstract: We study the tradeoff between sample complexity and round complexity in on-demand sampling, where the learning algorithm adaptively samples from $k$ distributions over a limited number of rounds. In the realizable setting of Multi-Distribution Learning (MDL), we show that the optimal sample complexity of an $r$-round algorithm scales approximately as $dk^{Θ(1/r)} / ε$. For the general agnostic case, we present an algorithm that achieves near-optimal sample complexity of $\widetilde O((d + k) / ε^2)$ within $\widetilde O(\sqrt{k})$ rounds. Of independent interest, we introduce a new framework, Optimization via On-Demand Sampling (OODS), which abstracts the sample-adaptivity tradeoff and captures most existing MDL algorithms. We establish nearly tight bounds on the round complexity in the OODS setting. The upper bounds directly yield the $\widetilde O(\sqrt{k})$-round algorithm for agnostic MDL, while the lower bounds imply that achieving sub-polynomial round complexity would require fundamentally new techniques that bypass the inherent hardness of OODS.

</details>


### [50] [Novel sparse matrix algorithm expands the feasible size of a self-organizing map of the knowledge indexed by a database of peer-reviewed medical literature](https://arxiv.org/abs/2511.15136)
*Andrew Amos,Joanne Lee,Tarun Sen Gupta,Bunmi S. Malau-Aduli*

Main category: cs.LG

TL;DR: 本文提出了一种新颖的稀疏矩阵乘法算法，首次将自组织映射应用于整个Medline数据集，从而能更全面地展现医学知识图谱。


<details>
  <summary>Details</summary>
Motivation: 过去由于现有算法的内存和处理能力限制，Medline数据库的映射工作仅限于部分数据，无法提供完整的医学知识图谱。

Method: 设计了一种新颖的稀疏矩阵乘法算法，并将其应用于自组织映射，进而处理整个Medline数据集。

Result: 成功将自组织映射应用于整个Medline数据集，得到了更完整的医学知识图谱。该算法也使得随着时间推移，对自组织映射进行调整以适应数据集变化成为可能。

Conclusion: 本文提出的新算法克服了现有方法的局限性，实现了对整个Medline数据库的全面映射，显著提升了医学知识图谱的完整性，并为未来动态更新提供了可能性。

Abstract: Past efforts to map the Medline database have been limited to small subsets of the available data because of the exponentially increasing memory and processing demands of existing algorithms. We designed a novel algorithm for sparse matrix multiplication that allowed us to apply a self-organizing map to the entire Medline dataset, allowing for a more complete map of existing medical knowledge. The algorithm also increases the feasibility of refining the self-organizing map to account for changes in the dataset over time.

</details>


### [51] [Cross-Modal Consistency-Guided Active Learning for Affective BCI Systems](https://arxiv.org/abs/2511.15138)
*Hyo-Jeong Jang,Hye-Bin Shin,Kang Yin*

Main category: cs.LG

TL;DR: 该文章提出了一种不确定性感知的主动学习框架，旨在解决脑电图（EEG）情感识别中标签稀缺和噪声的问题。该框架通过结合模型不确定性和跨模态一致性来增强对标签噪声的鲁棒性。


<details>
  <summary>Details</summary>
Motivation: 传统的深度学习模型在脑电图（EEG）情感识别中存在挑战，因为高质量、大量的标签难以获取。EEG信号易受伪影和个体差异影响，而情感标签往往主观且不一致。

Method: 该方法不单纯依赖基于EEG的不确定性估计，而是评估跨模态对齐，以判断不确定性是源于认知模糊还是传感器噪声。通过一个表征对齐模块，将EEG和面部特征嵌入共享潜在空间，强制实现模态间的语义连贯性。残余差异被视为噪声引起的不一致，并选择性地查询这些样本，以在主动学习期间获得预言机反馈。

Result: 该方法在ASCERTAIN数据集上进行了实验，验证了其效率和鲁棒性。

Conclusion: 所提出的框架在EEG情感识别中具有数据高效性和噪声容忍性，有望应用于脑机接口系统。

Abstract: Deep learning models perform best with abundant, high-quality labels, yet such conditions are rarely achievable in EEG-based emotion recognition. Electroencephalogram (EEG) signals are easily corrupted by artifacts and individual variability, while emotional labels often stem from subjective and inconsistent reports-making robust affective decoding particularly difficult. We propose an uncertainty-aware active learning framework that enhances robustness to label noise by jointly leveraging model uncertainty and cross-modal consistency. Instead of relying solely on EEG-based uncertainty estimates, the method evaluates cross-modal alignment to determine whether uncertainty originates from cognitive ambiguity or sensor noise. A representation alignment module embeds EEG and face features into a shared latent space, enforcing semantic coherence between modalities. Residual discrepancies are treated as noise-induced inconsistencies, and these samples are selectively queried for oracle feedback during active learning. This feedback-driven process guides the network toward reliable, informative samples and reduces the impact of noisy labels. Experiments on the ASCERTAIN dataset examine the efficiency and robustness of ours, highlighting its potential as a data-efficient and noise-tolerant approach for EEG-based affective decoding in brain-computer interface systems.

</details>


### [52] [Complex variational autoencoders admit Kähler structure](https://arxiv.org/abs/2511.15172)
*Andrew Gracyk*

Main category: cs.LG

TL;DR: 本文探讨了复数变分自编码器（VAEs）中的Kähler几何结构，提出了一种高效计算Fisher信息度量的方法，并通过实验证明其能产生更平滑的表示和更少的语义异常值。


<details>
  <summary>Details</summary>
Motivation: 开发一种适用于复杂变分自编码器（VAEs）的几何结构分析方法，并解决传统方法中计算Fisher信息度量的效率问题。

Method: 将黎曼几何结构适应到具有复数潜在阶段的复数VAE；推导潜在复数高斯正则化下的复数Fisher信息度量；提出并利用Kähler电位导数来近似Fisher信息度量；通过解码器几何正则化潜在空间。

Result: 复数VAE展现出Kähler几何结构；Kähler电位导数能有效地计算度量，且作为多重次调和函数（PSH）有效；潜在空间可以通过解码器几何进行正则化；能根据加权复体积元素进行采样；这些策略在牺牲样本变化的情况下，能产生更平滑的表示和更少的语义异常值。

Conclusion: 复数VAES展现出Kähler几何结构，通过利用Kähler电位导数，可以高效地计算Fisher信息度量，并改善潜在空间的正则化和样本质量。

Abstract: It has been discovered that latent-Euclidean variational autoencoders (VAEs) admit, in various capacities, Riemannian structure. We adapt these arguments but for complex VAEs with a complex latent stage. We show that complex VAEs reveal to some level Kähler geometric structure. Our methods will be tailored for decoder geometry. We derive the Fisher information metric in the complex case under a latent complex Gaussian regularization with trivial relation matrix. It is well known from statistical information theory that the Fisher information coincides with the Hessian of the Kullback-Leibler (KL) divergence. Thus, the metric Kähler potential relation is exactly achieved under relative entropy. We propose a Kähler potential derivative of complex Gaussian mixtures that has rough equivalence to the Fisher information metric while still being faithful to the underlying Kähler geometry. Computation of the metric via this potential is efficient, and through our potential, valid as a plurisubharmonic (PSH) function, large scale computational burden of automatic differentiation is displaced to small scale. We show that we can regularize the latent space with decoder geometry, and that we can sample in accordance with a weighted complex volume element. We demonstrate these strategies, at the exchange of sample variation, yield consistently smoother representations and fewer semantic outliers.

</details>


### [53] [FaultDiffusion: Few-Shot Fault Time Series Generation with Diffusion Model](https://arxiv.org/abs/2511.15174)
*Yi Xu,Zhigang Chen,Rui Wang,Yangfan Li,Fengxiao Tang,Ming Zhao,Jiaqi Liu*

Main category: cs.LG

TL;DR: 这篇论文提出了一种基于扩散模型的少样本故障时间序列生成框架，以解决工业设备故障诊断中故障数据稀缺的问题。


<details>
  <summary>Details</summary>
Motivation: 在工业设备监控中，故障诊断对于确保系统可靠性和实现预测性维护至关重要，但故障事件的稀有性和数据标注的高成本导致故障数据稀缺，这严重阻碍了数据驱动方法的发展。现有针对大量正常数据优化的时间序列生成模型，难以在少样本场景中捕获故障分布。

Method: 本文提出了一种基于扩散模型的少样本故障时间序列生成框架。该框架采用正负差异适配器，利用预训练的正常数据分布来建模正常域和故障域之间的差异，从而实现准确的故障合成。此外，引入了多样性损失，通过样本间差异正则化来鼓励生成多样的故障样本，以防止模式崩溃。

Result: 实验结果表明，该模型在真实性和多样性方面显著优于传统方法，并在关键基准测试中达到了最先进的性能。

Conclusion: 本文提出的基于扩散模型的少样本故障时间序列生成框架，有效解决了故障数据稀缺的问题，提高了故障诊断的准确性和多样性。

Abstract: In industrial equipment monitoring, fault diagnosis is critical for ensuring system reliability and enabling predictive maintenance. However, the scarcity of fault data, due to the rarity of fault events and the high cost of data annotation, significantly hinders data-driven approaches. Existing time-series generation models, optimized for abundant normal data, struggle to capture fault distributions in few-shot scenarios, producing samples that lack authenticity and diversity due to the large domain gap and high intra-class variability of faults. To address this, we propose a novel few-shot fault time-series generation framework based on diffusion models. Our approach employs a positive-negative difference adapter, leveraging pre-trained normal data distributions to model the discrepancies between normal and fault domains for accurate fault synthesis. Additionally, a diversity loss is introduced to prevent mode collapse, encouraging the generation of diverse fault samples through inter-sample difference regularization. Experimental results demonstrate that our model significantly outperforms traditional methods in authenticity and diversity, achieving state-of-the-art performance on key benchmarks.

</details>


### [54] [Masked Auto-Regressive Variational Acceleration: Fast Inference Makes Practical Reinforcement Learning](https://arxiv.org/abs/2511.15190)
*Yuxuan Gu,Weimin Bai,Yifei Wang,Weijian Luo,He Sun*

Main category: cs.LG

TL;DR: 本文提出了MARVAL，一个基于蒸馏的框架，它将扩散链压缩到单个AR生成步骤中，同时保留了灵活的自回归unmasking顺序。


<details>
  <summary>Details</summary>
Motivation: 解决现有MAR模型推理速度慢的问题，并使其能更好地应用于强化学习（RL）后训练。

Method: MARVAL通过一个新颖的基于分数的变分目标，将掩码自回归扩散模型蒸馏成一个单生成步骤模型。此外，还提出了MARVAL-RL，一个高效的强化学习框架。

Result: 在ImageNet 256*256数据集上，MARVAL-Huge实现了2.00的FID分数，比MAR-diffusion加速了30倍以上。MARVAL-RL在ImageNet数据集上的CLIP和图像奖励分数方面也取得了显著提升。

Conclusion: MARVAL为掩码自回归扩散模型的蒸馏和强化学习提供了一条实用的路径，实现了快速采样和更好的偏好对齐。

Abstract: Masked auto-regressive diffusion models (MAR) benefit from the expressive modeling ability of diffusion models and the flexibility of masked auto-regressive ordering. However, vanilla MAR suffers from slow inference due to its hierarchical inference mechanism: an outer AR unmasking loop and an inner diffusion denoising chain. Such decoupled structure not only harm the generation efficiency but also hinder the practical use of MAR for reinforcement learning (RL), an increasingly critical paradigm for generative model post-training.To address this fundamental issue, we introduce MARVAL (Masked Auto-regressive Variational Acceleration), a distillation-based framework that compresses the diffusion chain into a single AR generation step while preserving the flexible auto-regressive unmasking order. Such a distillation with MARVAL not only yields substantial inference acceleration but, crucially, makes RL post-training with verifiable rewards practical, resulting in scalable yet human-preferred fast generative models. Our contributions are twofold: (1) a novel score-based variational objective for distilling masked auto-regressive diffusion models into a single generation step without sacrificing sample quality; and (2) an efficient RL framework for masked auto-regressive models via MARVAL-RL. On ImageNet 256*256, MARVAL-Huge achieves an FID of 2.00 with more than 30 times speedup compared with MAR-diffusion, and MARVAL-RL yields consistent improvements in CLIP and image-reward scores on ImageNet datasets with entity names. In conclusion, MARVAL demonstrates the first practical path to distillation and RL of masked auto-regressive diffusion models, enabling fast sampling and better preference alignments.

</details>


### [55] [Reasoning in Diffusion Large Language Models is Concentrated in Dynamic Confusion Zones](https://arxiv.org/abs/2511.15208)
*Ranfei Chen,Ming Chen,Kaifei Wang*

Main category: cs.LG

TL;DR: 该论文介绍了一种名为自适应轨迹策略优化（ATPO）的轻量级步长选择策略，旨在通过动态重新分配梯度更新来提升扩散大语言模型（dLLMs）的推理准确性和训练稳定性。


<details>
  <summary>Details</summary>
Motivation: 现有的轨迹强化学习方法在去噪步骤中均匀分配策略梯度，忽略了不同步骤的重要性。作者通过分析轨迹中的不确定性区域，发现模型在某些步骤中存在“混淆区”，这些区域对最终的成功或失败有强烈的预测性。

Method: 作者提出了ATPO方法，该方法利用混合的RoEC+CM规则动态地将梯度更新重新分配到高影响力的步骤，而不改变强化学习目标、奖励或计算预算。通过熵基不确定性、置信度-裕度（CM）不确定性和熵变率（RoEC）等步长级指标分析轨迹，揭示了“混淆区”。

Result: ATPO方法在多个基准测试中显著提高了推理准确性和训练稳定性。

Conclusion: 利用轨迹动态对于推进扩散大语言模型（dLLM）的强化学习至关重要。

Abstract: Diffusion Large Language Models (dLLMs) are rapidly emerging alongside autoregressive models as a powerful paradigm for complex reasoning, with reinforcement learning increasingly used for downstream alignment. Existing trajectory-based RL methods uniformly allocate policy gradients across denoising steps, implicitly treating all steps as equally important. We challenge this assumption by analyzing trajectories with several step-level metrics: entropy-based uncertainty, Confidence-Margin (CM) uncertainty, and Rate of Entropy Change (RoEC). These reveal structured "zones of confusion": transient spikes in uncertainty and instability that strongly predict final success or failure, while most steps remain stable. We propose Adaptive Trajectory Policy Optimization (ATPO), a lightweight step-selection strategy that dynamically reallocates gradient updates to these high-leverage steps without changing the RL objective, rewards, or compute budget. Using a hybrid RoEC+CM rule, ATPO delivers substantial gains in reasoning accuracy and training stability across benchmarks, showing that exploiting trajectory dynamics is key to advancing dLLM RL.

</details>


### [56] [EntroPIC: Towards Stable Long-Term Training of LLMs via Entropy Stabilization with Proportional-Integral Control](https://arxiv.org/abs/2511.15248)
*Kai Yang,Xin Xu,Yangkun Chen,Weijie Liu,Jiafei Lyu,Zichuan Lin,Deheng Ye,Saiyong Yang*

Main category: cs.LG

TL;DR: 本文提出EntroPIC，一种通过动态调整损失系数来稳定熵的方法，以在大型语言模型的长期训练中保持有效的探索和稳定的进度。


<details>
  <summary>Details</summary>
Motivation: 大型语言模型的长期训练需要稳定的探索，以防止模型陷入次优行为。熵在这个过程中至关重要，因为它控制探索并有助于避免过早收敛到次优解。然而，现有的强化学习方法难以在训练过程中保持适当的熵水平，因为训练过程涉及正负样本的混合，每个样本以不同的方式影响熵。

Method: 我们提出了EntroPIC（通过比例积分控制进行熵稳定），这是一种新颖的方法，通过动态调整正负样本的损失系数来自适应地调整它们的影响。这种方法在整个训练过程中稳定熵。

Result: EntroPIC在大型语言模型训练中有效控制熵。我们的方法成功地保持了所需的熵水平，从而实现了LLM稳定和最优的强化学习训练。

Conclusion: EntroPIC是一种稳定熵的有效方法，可确保大型语言模型训练中的高效探索和稳定进度。

Abstract: Long-term training of large language models (LLMs) requires maintaining stable exploration to prevent the model from collapsing into sub-optimal behaviors. Entropy is crucial in this context, as it controls exploration and helps avoid premature convergence to sub-optimal solutions. However, existing reinforcement learning methods struggle to maintain an appropriate level of entropy, as the training process involves a mix of positive and negative samples, each affecting entropy in different ways across steps. To address this, we propose Entropy stablilization via Proportional-Integral Control (EntroPIC), a novel method that adaptively adjusts the influence of positive and negative samples by dynamically tuning their loss coefficients. This approach stabilizes entropy throughout training, ensuring efficient exploration and steady progress. We provide a comprehensive theoretical analysis for both on-policy and off-policy learning settings, demonstrating that EntroPIC is effective at controlling entropy in large-scale LLM training. Experimental results show that our method successfully maintains desired entropy levels, enabling stable and optimal RL training for LLMs.

</details>


### [57] [GRPO-RM: Fine-Tuning Representation Models via GRPO-Driven Reinforcement Learning](https://arxiv.org/abs/2511.15256)
*Yanchen Xu,Ziheng Jiao,Hongyuan Zhang,Xuelong Li*

Main category: cs.LG

TL;DR: 该论文提出了GRPO-RM，一种将Group Relative Policy Optimization（GRPO）应用于表征学习模型的方法，旨在通过引入预定义输出集和专门设计的奖励函数来验证GRPO类策略在表征模型后训练中的性能。


<details>
  <summary>Details</summary>
Motivation: GRPO在微调大型语言模型（LLMs）方面表现出有效性，但其能否推广到表征学习模型仍是一个问题。

Method: 本文提出了Group Relative Policy Optimization for Representation Model（GRPO-RM）。该方法通过建立预设的输出集来替代LLM中的token序列采样，以生成输出组，并设计了一个专门的奖励函数来适应表征模型的特性。

Result: 通过在各种真实世界数据集上进行大量的实验，验证了所提出方法的有效性。

Conclusion: GRGRPO-RM成功地将GRPO-like策略应用于表征模型，并通过实验证明了其有效性。

Abstract: The Group Relative Policy Optimization (GRPO), a reinforcement learning method used to fine-tune large language models (LLMs), has proved its effectiveness in practical applications such as DeepSeek-R1. It raises a question whether GRPO can be generalized to representation learning models. In this paper, we propose Group Relative Policy Optimization for Representation Model (GRPO-RM), and investigate the performance of GRPO-like policy in post-training representation models. Specifically, our method establishes a predefined output set to functionally replace token sequence sampling in LLMs, thereby generating an output group, which is essential for the probability-driven optimization of GRPO. In addition, a specialized reward function is designed to accommodate the properties of representation models. Extensive experiments are conducted on various real-world datasets to validate the effectiveness of our proposed method.

</details>


### [58] [SNAP: Low-Latency Test-Time Adaptation with Sparse Updates](https://arxiv.org/abs/2511.15276)
*Hyeongheon Cha,Dong Min Kim,Hye Won Chung,Taesik Gong,Sung-Ju Lee*

Main category: cs.LG

TL;DR: SNAP是一种稀疏测试时间适应（TTA）框架，通过减少适应频率和数据使用，同时保持高精度，使其适用于资源受限的边缘环境。


<details>
  <summary>Details</summary>
Motivation: 现有的测试时间适应（TTA）方法依赖于频繁适应和高计算成本，不适用于资源受限的边缘环境。

Method: SNAP提出了两个关键组件：1. 类别和领域代表性记忆（CnDRM），用于识别和存储少量具有代表性的样本，以支持有限数据的高效适应。2. 仅推理批次感知记忆归一化（IoBMN），在推理时利用这些代表性样本动态调整归一化统计数据，从而有效对齐目标域。

Result: SNAP在仅使用1%的传入数据流进行适应时，仍能保持与现有先进TTA算法相当的精度，甚至在适应率低至1%到高至50%的情况下，将延迟降低高达93.12%，同时将精度下降保持在3.3%以下。

Conclusion: SNAP显著降低了TTA的计算成本和数据使用，同时保持了高精度，使其非常适合资源受限和对延迟敏感的边缘设备应用。

Abstract: Test-Time Adaptation (TTA) adjusts models using unlabeled test data to handle dynamic distribution shifts. However, existing methods rely on frequent adaptation and high computational cost, making them unsuitable for resource-constrained edge environments. To address this, we propose SNAP, a sparse TTA framework that reduces adaptation frequency and data usage while preserving accuracy. SNAP maintains competitive accuracy even when adapting based on only 1% of the incoming data stream, demonstrating its robustness under infrequent updates. Our method introduces two key components: (i) Class and Domain Representative Memory (CnDRM), which identifies and stores a small set of samples that are representative of both class and domain characteristics to support efficient adaptation with limited data; and (ii) Inference-only Batch-aware Memory Normalization (IoBMN), which dynamically adjusts normalization statistics at inference time by leveraging these representative samples, enabling efficient alignment to shifting target domains. Integrated with five state-of-the-art TTA algorithms, SNAP reduces latency by up to 93.12%, while keeping the accuracy drop below 3.3%, even across adaptation rates ranging from 1% to 50%. This demonstrates its strong potential for practical use on edge devices serving latency-sensitive applications. The source code is available at https://github.com/chahh9808/SNAP.

</details>


### [59] [Quant-Trim in Practice: Improved Cross-Platform Low-Bit Deployment on Edge NPUs](https://arxiv.org/abs/2511.15300)
*Rayen Dhahri,Steffen Urban*

Main category: cs.LG

TL;DR: Quant-Trim是一种训练阶段的方法，能够生成对后端和精度选择都具有鲁棒性的硬件中立检查点，从而解决边缘加速器在低位量化中遇到的不一致性问题。


<details>
  <summary>Details</summary>
Motivation: 现有的边缘加速器在低位量化方面表现不佳，因为不同的供应商编译器在缩放、裁剪和内核支持方面存在差异，导致浮点检查点在不同后端上产生不一致的精度。这使得开发人员不得不调整参数或重构模型以适应供应商友好的操作子集。

Method: Quant-Trim结合了渐进式伪量化（使训练与部署的整数网格对齐）和反向剪枝（在保留可学习性的同时，抑制异常值导致的尺度膨胀）。该方法对量化方案（对称/非对称、逐张量/逐通道、INT8/INT4）不可知，并且不需要对供应商特定的图进行更改。

Result: Quant-Trim在各种模型和任务中缩小了浮点与低位之间的差距，减少了对编译器启发式/校准的依赖，并避免了每个后端的重新训练。

Conclusion: Quant-Trim通过在训练阶段生成硬件中立的检查点，显著提高了边缘加速器在低位量化场景下的精度和鲁棒性，减少了对特定后端或编译器的依赖，从而简化了开发流程。

Abstract: Specialized edge accelerators rely on low-bit quantization, but vendor compilers differ in scaling, clipping, and kernel support, often as black boxes. The same floating-point (FP) checkpoint can therefore yield inconsistent accuracy across backends, forcing practitioners to tweak flags or refactor models to vendor-friendly operator subsets. We introduce Quant-Trim, a training-phase method that produces a hardware-neutral checkpoint robust to backend and precision choices. It combines progressive fake quantization to align training with the deployed integer grid and reverse pruning to tame outlier-driven scale inflation while preserving learnability. Quant-Trim is agnostic to quantization schemes (symmetric/asymmetric,per-tensor/per-channel, INT8/INT4) and requires no vendor-specific graph changes.Across models and tasks, it narrows the FP,low-bit gap, reduces dependence on compiler heuristics/calibration, and avoids per-backend retraining. We report accuracy and edge metrics latency, throughput, energy/inference, and cost under static/dynamic activation scaling and varying operator coverage.

</details>


### [60] [On the Internal Semantics of Time-Series Foundation Models](https://arxiv.org/abs/2511.15324)
*Atharva Pandey,Abhilash Neog,Gautam Jajoo*

Main category: cs.LG

TL;DR: 本文探讨了时间序列基础模型（TSFMs）中概念的可解释性，揭示了不同层级如何编码时间序列概念，以及这些模型如何处理概念组合。


<details>
  <summary>Details</summary>
Motivation: 时间序列基础模型（TSFMs）尽管取得了经验上的成功，但其内部机制，特别是模型如何表示基本时间序列概念，仍未被充分理解。

Method: 通过分层分析、线性可恢复性测试和表示相似性度量，系统地探究了：（i）哪些层编码哪些概念，（ii）概念参数是否可线性恢复，（iii）概念解缠和抽象在模型深度上的演变，以及（iv）模型如何处理概念组合。

Result: 早期层主要捕获局部及时域模式（如AR(1)、水平偏移、趋势），而更深层编码离散度和变化时间信号。谱因子和扭曲因子最难线性恢复。在概念组合设置中，探测性能下降，揭示概念间存在干扰。

Conclusion: 原子概念在TSFMs中可靠地局部化，但概念组合仍然是一个挑战，这突显了当前TSFMs在表示交互时间现象方面的关键局限性。

Abstract: Time-series Foundation Models (TSFMs) have recently emerged as a universal paradigm for learning across diverse temporal domains. However, despite their empirical success, the internal mechanisms by which these models represent fundamental time-series concepts remain poorly understood. In this work, we undertake a systematic investigation of concept interpretability in TSFMs. Specifically, we examine: (i) which layers encode which concepts, (ii) whether concept parameters are linearly recoverable, (iii) how representations evolve in terms of concept disentanglement and abstraction across model depth, and (iv) how models process compositions of concepts. We systematically probe these questions using layer-wise analyses, linear recoverability tests, and representation similarity measures, providing a structured account of TSFM semantics. The resulting insights show that early layers mainly capture local, time-domain patterns (e.g., AR(1), level shifts, trends), while deeper layers encode dispersion and change-time signals, with spectral and warping factors remaining the hardest to recover linearly. In compositional settings, however, probe performance degrades, revealing interference between concepts. This highlights that while atomic concepts are reliably localized, composition remains a challenge, underscoring a key limitation in current TSFMs' ability to represent interacting temporal phenomena.

</details>


### [61] [LaguerreNet: Advancing a Unified Solution for Heterophily and Over-smoothing with Adaptive Continuous Polynomials](https://arxiv.org/abs/2511.15328)
*Huseyin Goksu*

Main category: cs.LG

TL;DR: LaguerreNet基于连续Laguerre多项式，通过学习滤波器alpha参数的频谱形状，解决了现有GNN滤波器在异配图上的性能不佳和高多项式度下的过平滑问题，并在多个基准测试中取得了最先进的成果。


<details>
  <summary>Details</summary>
Motivation: 现有GNN滤波器（如ChebyNet）在异配图上性能不佳，并且在高多项式度（K值）下容易出现过平滑问题，这主要是由于标准滤波器静态、低通的特性。

Method: LaguerreNet是一种基于连续Laguerre多项式的新型GNN滤波器。它通过使核心alpha参数可训练来学习滤波器的频谱形状，从而实现自适应多项式方法。为了解决无界多项式严重的O(k^2)数值不稳定性，LaguerreNet采用了基于LayerNorm的稳定技术。

Result: LaguerreNet在具有挑战性的异配基准测试中取得了最先进的结果，并且对过平滑具有极强的鲁棒性，在K=10时性能达到峰值，这比ChebyNet出现崩溃的程度高出一个数量级。

Conclusion: LaguerreNet通过引入可训练的alpha参数和LayerNorm稳定技术，在连续域中成功地将自适应多项式方法应用于GNN滤波器，解决了异配图性能和过平滑问题，并显著提升了GNN的性能和鲁棒性。

Abstract: Spectral Graph Neural Networks (GNNs) suffer from two critical limitations: poor performance on "heterophilic" graphs and performance collapse at high polynomial degrees (K), known as over-smoothing. Both issues stem from the static, low-pass nature of standard filters (e.g., ChebyNet). While adaptive polynomial filters, such as the discrete MeixnerNet, have emerged as a potential unified solution, their extension to the continuous domain and stability with unbounded coefficients remain open questions. In this work, we propose `LaguerreNet`, a novel GNN filter based on continuous Laguerre polynomials. `LaguerreNet` learns the filter's spectral shape by making its core alpha parameter trainable, thereby advancing the adaptive polynomial approach. We solve the severe O(k^2) numerical instability of these unbounded polynomials using a `LayerNorm`-based stabilization technique. We demonstrate experimentally that this approach is highly effective: 1) `LaguerreNet` achieves state-of-the-art results on challenging heterophilic benchmarks. 2) It is exceptionally robust to over-smoothing, with performance peaking at K=10, an order of magnitude beyond where ChebyNet collapses.

</details>


### [62] [Multi-layer Stack Ensembles for Time Series Forecasting](https://arxiv.org/abs/2511.15350)
*Nathanael Bosch,Oleksandr Shchur,Nick Erickson,Michael Bohlke-Schneider,Caner Türkmen*

Main category: cs.LG

TL;DR: 本文探讨了时间序列预测中的集成方法，特别是堆叠技术。作者评估了33种集成模型，并提出了一个多层堆叠框架，以提高预测准确性。


<details>
  <summary>Details</summary>
Motivation: 在时间序列预测中，集成方法（尤其是堆叠）的使用不足，当前最先进的方法仍是简单的线性组合。研究旨在系统地探索和改进时间序列预测中的集成策略。

Method: 本文评估了33种现有及新颖的集成模型，涵盖了50个真实世界的数据集。作者提出并验证了一个多层堆叠框架，该框架结合了不同堆叠模型的优势。

Result: 研究结果表明，堆叠技术能持续提高预测准确性，但没有单一的堆叠模型在所有任务中都表现最佳。提出的多层堆叠框架在各种预测场景中均能提供卓越的准确性。

Conclusion: 堆叠方法，尤其是多层堆叠框架，在时间序列预测中具有显著潜力，可以有效提升自动化机器学习系统的性能。

Abstract: Ensembling is a powerful technique for improving the accuracy of machine learning models, with methods like stacking achieving strong results in tabular tasks. In time series forecasting, however, ensemble methods remain underutilized, with simple linear combinations still considered state-of-the-art. In this paper, we systematically explore ensembling strategies for time series forecasting. We evaluate 33 ensemble models -- both existing and novel -- across 50 real-world datasets. Our results show that stacking consistently improves accuracy, though no single stacker performs best across all tasks. To address this, we propose a multi-layer stacking framework for time series forecasting, an approach that combines the strengths of different stacker models. We demonstrate that this method consistently provides superior accuracy across diverse forecasting scenarios. Our findings highlight the potential of stacking-based methods to improve AutoML systems for time series forecasting.

</details>


### [63] [CID: Measuring Feature Importance Through Counterfactual Distributions](https://arxiv.org/abs/2511.15371)
*Eddie Conti,Álvaro Parafita,Axel Brando*

Main category: cs.LG

TL;DR: 这篇论文介绍了一种名为“反事实重要性分布（CID）”的局部特征重要性方法。


<details>
  <summary>Details</summary>
Motivation: 尽管存在许多评估机器学习模型特征重要性的方法，但由于缺乏明确的对比标准，因此需要替代的、有充分依据的衡量标准。

Method: 生成两组正负反事实，使用核密度估计对其分布进行建模，并根据分布差异度量对特征进行排序。

Result: 与现有方法相比，CID方法在忠实度指标（包括全面性和充分性）上表现更好，提供了更忠实的系统解释。

Conclusion: CID方法为模型分析提供了一个有价值的工具，并为现有方法提供了补充视角。

Abstract: Assessing the importance of individual features in Machine Learning is critical to understand the model's decision-making process. While numerous methods exist, the lack of a definitive ground truth for comparison highlights the need for alternative, well-founded measures. This paper introduces a novel post-hoc local feature importance method called Counterfactual Importance Distribution (CID). We generate two sets of positive and negative counterfactuals, model their distributions using Kernel Density Estimation, and rank features based on a distributional dissimilarity measure. This measure, grounded in a rigorous mathematical framework, satisfies key properties required to function as a valid metric. We showcase the effectiveness of our method by comparing with well-established local feature importance explainers. Our method not only offers complementary perspectives to existing approaches, but also improves performance on faithfulness metrics (both for comprehensiveness and sufficiency), resulting in more faithful explanations of the system. These results highlight its potential as a valuable tool for model analysis.

</details>


### [64] [Parameter Importance-Driven Continual Learning for Foundation Models](https://arxiv.org/abs/2511.15375)
*Lingxiang Wang,Hainan Zhang,Zhiming Zheng*

Main category: cs.LG

TL;DR: 本文提出了一种名为PIECE的参数重要性估计持续增强方法，旨在解决基础模型在领域特定后训练中出现的灾难性遗忘问题，并平衡通用能力维护与领域知识学习。


<details>
  <summary>Details</summary>
Motivation: 传统的持续学习方法在下游任务性能、历史数据依赖或额外参数开销方面存在不足。现有的参数高效微调方法也因参数选择和更新策略而受到限制，无法很好地解决灾难性遗忘和通用能力与领域知识学习的平衡问题。

Method: PIECE方法通过选择性地更新仅占0.1%的核心参数来学习领域知识，这些参数由两种重要性估计器指导：基于Fisher信息的PIECE-F和基于结合梯度与曲率信息的二阶归一化的PIECE-S。该方法无需访问先前的训练数据，也不增加模型参数。

Result: 在三种语言模型和两种多模态模型上的实验表明，PIECE在保持通用能力的同时，在多种下游任务上实现了最先进的持续学习性能。

Conclusion: PIECE为实现可扩展、领域自适应的基础模型提供了一条实用途径，有效避免了灾难性遗忘。

Abstract: Domain-specific post-training often causes catastrophic forgetting, making foundation models lose their general reasoning ability and limiting their adaptability to dynamic real-world environments. Preserving general capabilities while acquiring downstream domain knowledge is a central challenge for large language and multimodal models. Traditional continual learning methods, such as regularization, replay and architectural isolation, suffer from poor downstream performance, reliance on inaccessible historical data, or additional parameter overhead. While recent parameter-efficient tuning (PET) methods can alleviate forgetting, their effectiveness strongly depends on the choice of parameters and update strategies. In this paper, we introduce PIECE, a Parameter Importance Estimation-based Continual Enhancement method that preserves general ability while efficiently learning domain knowledge without accessing prior training data or increasing model parameters. PIECE selectively updates only 0.1% of core parameters most relevant to new tasks, guided by two importance estimators: PIECE-F based on Fisher Information, and PIECE-S based on a second-order normalization that combines gradient and curvature information. Experiments across three language models and two multimodal models show that PIECE maintains general capabilities and achieves state-of-the-art continual learning performance across diverse downstream tasks. Our results highlight a practical path to scalable, domain-adaptive foundation models without catastrophic forgetting.

</details>


### [65] [Proximal Approximate Inference in State-Space Models](https://arxiv.org/abs/2511.15409)
*Hany Abdulsamad,Ángel F. García-Fernández,Simo Särkkä*

Main category: cs.LG

TL;DR: 该文章提出了一种用于非线性、非高斯状态空间模型中状态估计的变分拉格朗日算法。


<details>
  <summary>Details</summary>
Motivation: 在非线性、非高斯状态空间模型中进行准确的状态估计，是许多实际应用中的关键问题。传统的滤波方法往往难以处理这类模型的复杂性。

Method: 文章提出了一种基于变分拉格朗日公式的算法，将贝叶斯推断转换为一系列受动态约束的熵信任区域更新。该框架产生了一系列前向-后向算法，其结构由变分后验的选择分解决定。文章通过关注高斯-马尔可夫近似，导出了具有良好计算复杂度的递归方案。对于一般的非线性、非高斯模型，则使用广义统计线性回归和傅里叶-埃尔米特矩匹配来完成递归。

Result: 该方法产生了一系列前向-后向算法，并通过高斯-马尔可夫近似导出了具有良好计算复杂度的递归方案。对于通用模型，通过广义统计线性回归和傅里叶-埃尔米特矩匹配完成递归。

Conclusion: 该研究为非线性、非高斯状态空间模型的状态估计提供了一种有效的变分拉格朗日算法，具有低计算复杂度的递归方案，并能处理通用模型。

Abstract: We present a class of algorithms for state estimation in nonlinear, non-Gaussian state-space models. Our approach is based on a variational Lagrangian formulation that casts Bayesian inference as a sequence of entropic trust-region updates subject to dynamic constraints. This framework gives rise to a family of forward-backward algorithms, whose structure is determined by the chosen factorization of the variational posterior. By focusing on Gauss--Markov approximations, we derive recursive schemes with favorable computational complexity. For general nonlinear, non-Gaussian models we close the recursions using generalized statistical linear regression and Fourier--Hermite moment matching.

</details>


### [66] [Towards Understanding Layer Contributions in Tabular In-Context Learning Models](https://arxiv.org/abs/2511.15432)
*Amir Rezaei Balef,Mykhailo Koshil,Katharina Eggensperger*

Main category: cs.LG

TL;DR: 本文探讨了表格上下文化学习（ICL）模型中各层如何影响表格预测，识别了冗余层，并通过“层即画家”的视角分析了TabPFN和TabICL，发现只有部分层共享相同的表征语言，表明存在结构冗余，为模型压缩和可解释性提升提供了机会。


<details>
  <summary>Details</summary>
Motivation: 尽管表格ICL模型和大型语言模型（LLM）在架构上相似，但目前对于ICL模型中各个层如何对表格预测做出贡献知之甚少。

Method: 本文通过“层即画家”视角，调查了表格ICL模型中潜在空间在不同层之间的演变，识别了潜在的冗余层，并将这些动态与在LLM中观察到的动态进行了比较。具体分析了TabPFN和TabICL。

Result: 研究发现，只有部分层共享共同的表征语言，这表明存在结构冗余。

Conclusion: 结构冗余的存在为模型压缩和提高可解释性提供了可能性和机会。

Abstract: Despite the architectural similarities between tabular in-context learning (ICL) models and large language models (LLMs), little is known about how individual layers contribute to tabular prediction. In this paper, we investigate how the latent spaces evolve across layers in tabular ICL models, identify potential redundant layers, and compare these dynamics with those observed in LLMs. We analyze TabPFN and TabICL through the "layers as painters" perspective, finding that only subsets of layers share a common representational language, suggesting structural redundancy and offering opportunities for model compression and improved interpretability.

</details>


### [67] [TSFM in-context learning for time-series classification of bearing-health status](https://arxiv.org/abs/2511.15447)
*Michel Tokic,Slobodan Djukanović,Anja von Beuningen,Cheng Feng*

Main category: cs.LG

TL;DR: 本文提出了一种利用时间序列基础模型（TSFM）进行语境学习的分类方法，无需微调即可对新数据进行分类。


<details>
  <summary>Details</summary>
Motivation: 为了解决传统AI解决方案在故障诊断中对长尾和不常见故障模式识别能力不足的问题，本文旨在开发一种利用时间序列基础模型（TSFM）进行语境学习的分类方法，以便在不重新训练模型的情况下对模型训练数据语料库之外的数据进行分类，从而实现更广泛、更具适应性的AI驱动维护系统。

Method: 该方法将频率域参考信号转换为伪时间序列模式，生成对齐的协变量和目标信号，并使用时间序列基础模型（TSFM）通过语境学习预测分类数据与预定义标签对应的概率。它通过在模型提示中表示目标（类别ID）和协变量（数据矩阵）形式的示例，实现对未知协变量数据模式的分类。

Result: 该方法在伺服压机电机轴承健康状态评估的振动数据上进行了应用，并展示了其在不同运行条件下的有效性。

Conclusion: 所提出的方法利用预训练模型的扩展性，在无需微调的情况下对时间序列数据进行分类，在超越定制窄AI解决方案、迈向更广泛的AI驱动维护系统方面取得了显著进展。

Abstract: This paper introduces a classification method using in-context learning in time-series foundation models (TSFM). We show how data, which was not part of the TSFM training data corpus, can be classified without the need of finetuning the model. Examples are represented in the form of targets (class id) and covariates (data matrix) within the prompt of the model, which enables to classify an unknown covariate data pattern alongside the forecast axis through in-context learning. We apply this method to vibration data for assessing the health state of a bearing within a servo-press motor. The method transforms frequency domain reference signals into pseudo time-series patterns, generates aligned covariate and target signals, and uses the TSFM to predict probabilities how classified data corresponds to predefined labels. Leveraging the scalability of pre-trained models this method demonstrates efficacy across varied operational conditions. This marks significant progress beyond custom narrow AI solutions towards broader, AI-driven maintenance systems.

</details>


### [68] [FairEnergy: Contribution-Based Fairness meets Energy Efficiency in Federated Learning](https://arxiv.org/abs/2511.15454)
*Ouiame Marnissi,Hajar EL Hammouti,El Houcine Bergou*

Main category: cs.LG

TL;DR: FairEnergy是一种为联邦学习设计的框架，它通过优化设备选择、带宽分配和压缩级别，在保证模型精度的前提下，显著降低能耗，同时考虑客户端贡献的公平性。


<details>
  <summary>Details</summary>
Motivation: 在无线边缘系统中，联邦学习面临着如何在保证模型准确性、平衡能源效率和公平参与之间取得平衡的挑战，这主要源于异构资源、不平等的客户端贡献和有限的通信容量。

Method: FairEnergy框架通过将一个捕获更新大小和压缩率的贡献分数整合到设备选择、带宽分配和压缩级别的联合优化中。它将由此产生的混合整数非凸问题通过松弛二元选择变量和应用拉格朗日分解来解决全局带宽耦合问题，随后进行每个设备的子问题优化。

Result: 在非独立同分布（non-IID）数据上的实验表明，与基线策略相比，FairEnergy在实现更高准确性的同时，能耗降低高达79%。

Conclusion: FairEnergy框架通过其创新的联合优化方法，成功地解决了联邦学习在无线边缘系统中能源效率、公平性和模型准确性之间的平衡问题，为未来的联邦学习部署提供了有前景的解决方案。

Abstract: Federated learning (FL) enables collaborative model training across distributed devices while preserving data privacy. However, balancing energy efficiency and fair participation while ensuring high model accuracy remains challenging in wireless edge systems due to heterogeneous resources, unequal client contributions, and limited communication capacity. To address these challenges, we propose FairEnergy, a fairness-aware energy minimization framework that integrates a contribution score capturing both the magnitude of updates and their compression ratio into the joint optimization of device selection, bandwidth allocation, and compression level. The resulting mixed-integer non-convex problem is solved by relaxing binary selection variables and applying Lagrangian decomposition to handle global bandwidth coupling, followed by per-device subproblem optimization. Experiments on non-IID data show that FairEnergy achieves higher accuracy while reducing energy consumption by up to 79\% compared to baseline strategies.

</details>


### [69] [NTK-Guided Implicit Neural Teaching](https://arxiv.org/abs/2511.15487)
*Chen Zhang,Wei Zuo,Bingyang Cheng,Yikun Wang,Wei-Bin Kou,Yik Chung WU,Ngai Wong*

Main category: cs.LG

TL;DR: NINT通过动态选择能够最大化全局函数更新的坐标点来加速INR的训练，从而将训练时间缩短了近一半，同时保持或提高了表示质量。


<details>
  <summary>Details</summary>
Motivation: 隐式神经表示（INRs）在处理高分辨率信号时，需要优化数百万个坐标，导致计算成本过高。

Method: 提出NTK引导的隐式神经教学（NINT）方法，通过神经正切核（NTK）对坐标进行评分，该评分结合了拟合误差和异构杠杆（自影响和跨坐标耦合），从而动态选择能够最大化全局函数更新的坐标进行训练。

Result: NINT方法在保持或提高表示质量的同时，将训练时间缩短了近一半，并在现有基于采样的加速策略中达到了最先进的水平。

Conclusion: NINT通过创新的坐标选择策略，显著提升了隐式神经表示的训练效率，为高分辨率信号的建模提供了更高效的解决方案。

Abstract: Implicit Neural Representations (INRs) parameterize continuous signals via multilayer perceptrons (MLPs), enabling compact, resolution-independent modeling for tasks like image, audio, and 3D reconstruction. However, fitting high-resolution signals demands optimizing over millions of coordinates, incurring prohibitive computational costs. To address it, we propose NTK-Guided Implicit Neural Teaching (NINT), which accelerates training by dynamically selecting coordinates that maximize global functional updates. Leveraging the Neural Tangent Kernel (NTK), NINT scores examples by the norm of their NTK-augmented loss gradients, capturing both fitting errors and heterogeneous leverage (self-influence and cross-coordinate coupling). This dual consideration enables faster convergence compared to existing methods. Through extensive experiments, we demonstrate that NINT significantly reduces training time by nearly half while maintaining or improving representation quality, establishing state-of-the-art acceleration among recent sampling-based strategies.

</details>


<div id='cs.AI'></div>

# cs.AI [[Back]](#toc)

### [70] [Learning Interestingness in Automated Mathematical Theory Formation](https://arxiv.org/abs/2511.14778)
*George Tsoukalas,Rahul Saha,Amitayush Thakur,Sabrina Reguyal,Swarat Chaudhuri*

Main category: cs.AI

TL;DR: 该论文介绍了FERMAT，一个用于数学理论发现的强化学习环境，并探讨了使用进化算法，特别是基于LLM的进化算法，来自动评估数学对象趣味性，这在发现初等数论和有限域方面取得了显著改进。


<details>
  <summary>Details</summary>
Motivation: 作者旨在通过引入FERMAT强化学习环境，并探索自动评估数学对象趣味性，从而在自动化新数学理论的开放式发现方面迈出关键性步伐，以应对人工智能领域的重大挑战。

Method: 1. 引入FERMAT强化学习环境，该环境使用一组符号动作模拟概念发现和定理证明，从而为理论发现提出了一系列相关的强化学习问题。
2. 通过FERMAT环境，作者探索了自动评估数学对象“趣味性”这一具体问题。
3. 研究了用于合成非平凡趣味性度量的进化算法。
4. 特别是，引入了一种基于大型语言模型（LLM）的进化算法，该算法引入了函数抽象。

Result: 1. 成功创建了FERMAT强化学习环境，为数学理论发现提供了新的研究平台。
2. 发现基于LLM的进化算法在发现初等数论和有限域方面，相比硬编码的基线方法，取得了显著的改进。

Conclusion: FERMAT环境和基于LLM的进化算法在自动化数学理论发现方面显示出巨大潜力，尤其是在评估数学对象趣味性方面取得了突破，为未来的研究奠定了基础。

Abstract: We take two key steps in automating the open-ended discovery of new mathematical theories, a grand challenge in artificial intelligence. First, we introduce $\emph{FERMAT}$, a reinforcement learning (RL) environment that models concept discovery and theorem-proving using a set of symbolic actions, opening up a range of RL problems relevant to theory discovery. Second, we explore a specific problem through $\emph{FERMAT}$: automatically scoring the $\emph{interestingness}$ of mathematical objects. We investigate evolutionary algorithms for synthesizing nontrivial interestingness measures. In particular, we introduce an LLM-based evolutionary algorithm that features function abstraction, leading to notable improvements in discovering elementary number theory and finite fields over hard-coded baselines. We open-source the $\emph{FERMAT}$ environment at this URL(https://github.com/trishullab/Fermat).

</details>


### [71] [Ask WhAI:Probing Belief Formation in Role-Primed LLM Agents](https://arxiv.org/abs/2511.14780)
*Keith Moore,Jun W. Kim,David Lyu,Jeffrey Heo,Ehsan Adeli*

Main category: cs.AI

TL;DR: Ask WhAI是一个系统级框架，用于检查和扰动多智能体交互中的信念状态。它记录并回放智能体交互，支持对每个智能体的信念和理由进行带外查询，并能够注入反事实证据来测试信念结构如何响应新信息。该框架应用于一个医学案例模拟器，该模拟器具有多智能体共享内存和一个在明确查询时才揭示真实实验室结果的预言机智能体。该系统在对一名患有急性起病神经精神疾病的儿童进行多专业诊断的过程中进行了压力测试。


<details>
  <summary>Details</summary>
Motivation: 开发一个系统级框架，用于检查和扰动多智能体交互中的信念状态，以研究信念形成和知识孤岛。

Method: 提出了Ask WhAI框架，该框架记录并重放智能体交互，支持对智能体信念和理由的带外查询，并能够注入反事实证据。该框架应用于一个医学案例模拟器，该模拟器具有多智能体共享内存（时间戳电子病历）和一个预言机智能体。通过大型语言模型智能体模拟多专业诊断过程，这些智能体具有特定的角色先验，并写入共享病历。在关键诊断时刻设置断点，以进行事件前和事件后的信念查询。

Result: Ask WhAI框架成功应用于医学案例模拟器。模拟结果显示，智能体的信念往往反映了现实世界中的学科立场，包括过度依赖经典研究和抵制反证据。这些信念可以被追踪和审问，这是人类专家无法做到的。

Conclusion: Ask WhAI提供了一种可重现的方法来研究多智能体科学推理中的信念形成和知识孤岛，使这些动态变得可见和可测试。

Abstract: We present Ask WhAI, a systems-level framework for inspecting and perturbing belief states in multi-agent interactions. The framework records and replays agent interactions, supports out-of-band queries into each agent's beliefs and rationale, and enables counterfactual evidence injection to test how belief structures respond to new information. We apply the framework to a medical case simulator notable for its multi-agent shared memory (a time-stamped electronic medical record, or EMR) and an oracle agent (the LabAgent) that holds ground truth lab results revealed only when explicitly queried. We stress-test the system on a multi-specialty diagnostic journey for a child with an abrupt-onset neuropsychiatric presentation. Large language model agents, each primed with strong role-specific priors ("act like a neurologist", "act like an infectious disease specialist"), write to a shared medical record and interact with a moderator across sequential or parallel encounters. Breakpoints at key diagnostic moments enable pre- and post-event belief queries, allowing us to distinguish entrenched priors from reasoning or evidence-integration effects. The simulation reveals that agent beliefs often mirror real-world disciplinary stances, including overreliance on canonical studies and resistance to counterevidence, and that these beliefs can be traced and interrogated in ways not possible with human experts. By making such dynamics visible and testable, Ask WhAI offers a reproducible way to study belief formation and epistemic silos in multi-agent scientific reasoning.

</details>


### [72] [Subnational Geocoding of Global Disasters Using Large Language Models](https://arxiv.org/abs/2511.14788)
*Michele Ronco,Damien Delforge,Wiebke S. Jäger,Christina Corbane*

Main category: cs.AI

TL;DR: 该论文提出了一个全自动的LLM辅助工作流程，用于处理灾害事件的文本位置信息，并将其与地理空间数据集集成，生成带有可靠性评分的次国家级地理几何数据。


<details>
  <summary>Details</summary>
Motivation: 现有的灾害数据库（如EM-DAT）中的地点信息通常以非结构化文本形式存在，粒度或拼写不一致，难以与空间数据集集成，这使得风险评估和灾害风险减轻面临挑战。

Method: 该方法利用GPT-4o处理和清理文本位置信息，并通过交叉核对GADM、OpenStreetMap和Wikidata三个独立的地理信息库来分配地理几何。根据这些来源的一致性和可用性，为每个地点分配一个可靠性评分，并生成次国家级地理几何。

Result: 将此工作流程应用于2000年至2024年的EM-DAT数据集，成功地对14,215个事件的17,948个独立地点进行了地理编码。

Conclusion: 该方法无需人工干预，覆盖所有灾害类型，支持多源交叉验证，并允许灵活地重新映射到首选框架。此外，还展示了LLM从非结构化文本中提取和结构化地理信息的潜力，为相关分析提供了一种可扩展且可靠的方法。

Abstract: Subnational location data of disaster events are critical for risk assessment and disaster risk reduction. Disaster databases such as EM-DAT often report locations in unstructured textual form, with inconsistent granularity or spelling, that make it difficult to integrate with spatial datasets. We present a fully automated LLM-assisted workflow that processes and cleans textual location information using GPT-4o, and assigns geometries by cross-checking three independent geoinformation repositories: GADM, OpenStreetMap and Wikidata. Based on the agreement and availability of these sources, we assign a reliability score to each location while generating subnational geometries. Applied to the EM-DAT dataset from 2000 to 2024, the workflow geocodes 14,215 events across 17,948 unique locations. Unlike previous methods, our approach requires no manual intervention, covers all disaster types, enables cross-verification across multiple sources, and allows flexible remapping to preferred frameworks. Beyond the dataset, we demonstrate the potential of LLMs to extract and structure geographic information from unstructured text, offering a scalable and reliable method for related analyses.

</details>


### [73] [Project Rachel: Can an AI Become a Scholarly Author?](https://arxiv.org/abs/2511.14819)
*Martin Monperrus,Benoit Baudry,Clément Vidal*

Main category: cs.AI

TL;DR: 该研究探讨了AI作者身份，通过创建一个名为Rachel So的AI学术身份，发表论文，并观察学术界的回应。


<details>
  <summary>Details</summary>
Motivation: 探索学术生态系统对AI作者身份的反应，并为未来关于超人类AI系统学术交流的辩论提供实证行动研究数据。

Method: 本项目通过创建一个名为Rachel So的AI学术身份，在2025年3月至10月期间发表了10余篇AI生成的论文，并跟踪其被引用和收到同行评审邀请的情况。

Result: AI学术身份Rachel So成功发表了多篇论文，被引用，并获得了同行评审邀请。

Conclusion: AI作者身份对出版商、研究人员和整个科学系统都产生了影响，引发了关于超人类AI系统进行学术交流的讨论。

Abstract: This paper documents Project Rachel, an action research study that created and tracked a complete AI academic identity named Rachel So. Through careful publication of AI-generated research papers, we investigate how the scholarly ecosystem responds to AI authorship. Rachel So published 10+ papers between March and October 2025, was cited, and received a peer review invitation. We discuss the implications of AI authorship on publishers, researchers, and the scientific system at large. This work contributes empirical action research data to the necessary debate about the future of scholarly communication with super human, hyper capable AI systems.

</details>


### [74] [Uncertainty-Aware Measurement of Scenario Suite Representativeness for Autonomous Systems](https://arxiv.org/abs/2511.14853)
*Robab Aghazadeh Chakherlou,Siddartha Khastgir,Xingyu Zhao,Jerein Jeyachandran,Shufeng Chen*

Main category: cs.AI

TL;DR: 该论文关注人工智能系统数据集的代表性，并提出一种概率方法来量化场景数据与目标操作域之间的代表性。


<details>
  <summary>Details</summary>
Motivation: 确保AI系统（如自动驾驶汽车）的可信赖性和安全性，需要数据集具备良好的数据相关安全属性，特别是代表性。

Method: 提出一种概率方法，通过比较场景套件编码的特征统计分布与代表TOD的相应特征分布来量化代表性。使用不精确贝叶斯方法处理有限数据和不确定的先验，生成区间值的、不确定性感知的代表性估计。

Result: 通过一个数值示例，比较了不同操作类别（天气、道路类型、一天中的时间等）下场景套件分布和推断的TOD分布，并估计了局部和全局的区间代表性。

Conclusion: 该研究提出了一种量化数据集代表性的不精确贝叶斯方法，为评估AI系统训练和测试数据集的质量提供了一种新的视角，尤其在数据有限和先验不确定的情况下具有实用价值。

Abstract: Assuring the trustworthiness and safety of AI systems, e.g., autonomous vehicles (AV), depends critically on the data-related safety properties, e.g., representativeness, completeness, etc., of the datasets used for their training and testing. Among these properties, this paper focuses on representativeness-the extent to which the scenario-based data used for training and testing, reflect the operational conditions that the system is designed to operate safely in, i.e., Operational Design Domain (ODD) or expected to encounter, i.e., Target Operational Domain (TOD). We propose a probabilistic method that quantifies representativeness by comparing the statistical distribution of features encoded by the scenario suites with the corresponding distribution of features representing the TOD, acknowledging that the true TOD distribution is unknown, as it can only be inferred from limited data.
  We apply an imprecise Bayesian method to handle limited data and uncertain priors. The imprecise Bayesian formulation produces interval-valued, uncertainty-aware estimates of representativeness, rather than a single value. We present a numerical example comparing the distributions of the scenario suite and the inferred TOD across operational categories-weather, road type, time of day, etc., under dependencies and prior uncertainty. We estimate representativeness locally (between categories) and globally as an interval.

</details>


### [75] [Learning Human-Like RL Agents Through Trajectory Optimization With Action Quantization](https://arxiv.org/abs/2511.15055)
*Jian-Ting Guo,Yu-Cheng Chen,Ping-Chun Hsieh,Kuo-Hao Ho,Po-Wei Huang,Ti-Rong Wu,I-Chen Wu*

Main category: cs.AI

TL;DR: 本文提出了Macro Action Quantization (MAQ)框架，通过向量量化VAE将人类演示蒸馏成宏观动作，以实现类人强化学习。


<details>
  <summary>Details</summary>
Motivation: 尽管强化学习在许多领域取得了超人的表现，但很少关注设计类人强化学习智能体。许多奖励驱动的强化学习智能体表现出与人类不符的非自然行为，这引起了对可解释性和可信度的担忧。

Method: 本文将类人性定义为轨迹优化问题，目标是寻找与人类行为高度契合且能最大化奖励的动作序列。为此，本文将经典的“预测控制”算法应用于类人学习，作为一种可行且高效的实现。提出了Macro Action Quantization (MAQ)框架，通过向量量化VAE将人类演示蒸馏成宏观动作。

Result: 在D4RL Adroit基准测试中的实验结果表明，MAQ显著提高了类人性，增加了轨迹相似性分数，并在人类评估研究中，在所有强化学习智能体中获得了最高的类人性排名。MAQ可以很容易地集成到各种现成的强化学习算法中。

Conclusion: MAQ提供了一个有前途的方向，用于学习类人强化学习智能体。

Abstract: Human-like agents have long been one of the goals in pursuing artificial intelligence. Although reinforcement learning (RL) has achieved superhuman performance in many domains, relatively little attention has been focused on designing human-like RL agents. As a result, many reward-driven RL agents often exhibit unnatural behaviors compared to humans, raising concerns for both interpretability and trustworthiness. To achieve human-like behavior in RL, this paper first formulates human-likeness as trajectory optimization, where the objective is to find an action sequence that closely aligns with human behavior while also maximizing rewards, and adapts the classic receding-horizon control to human-like learning as a tractable and efficient implementation. To achieve this, we introduce Macro Action Quantization (MAQ), a human-like RL framework that distills human demonstrations into macro actions via Vector-Quantized VAE. Experiments on D4RL Adroit benchmarks show that MAQ significantly improves human-likeness, increasing trajectory similarity scores, and achieving the highest human-likeness rankings among all RL agents in the human evaluation study. Our results also demonstrate that MAQ can be easily integrated into various off-the-shelf RL algorithms, opening a promising direction for learning human-like RL agents. Our code is available at https://rlg.iis.sinica.edu.tw/papers/MAQ.

</details>


### [76] [Beyond GeneGPT: A Multi-Agent Architecture with Open-Source LLMs for Enhanced Genomic Question Answering](https://arxiv.org/abs/2511.15061)
*Haodong Chen,Guido Zuccon,Teerapong Leelanupab*

Main category: cs.AI

TL;DR: OpenBioLLM是一个开源多智能体框架，利用小型开源模型和模块化设计，在基因组问答任务上超越了GeneGPT，同时显著降低了延迟。


<details>
  <summary>Details</summary>
Motivation: GeneGPT在基因组问答方面存在可扩展性、成本、数据隐私和泛化方面的局限性，因为它依赖于专有的大型语言模型。本研究旨在通过使用开源模型以及更高效的架构来解决这些问题。

Method: 本研究首先使用Llama 3.1、Qwen2.5和Qwen2.5 Coder等开源模型在单一架构下重新实现并复现了GeneGPT，以识别这种方法的局限性。在此基础上，研究人员开发了OpenBioLLM，这是一个模块化的多智能体框架，通过引入工具路由、查询生成和响应验证的智能体专业化来扩展GeneGPT。

Result: OpenBioLLM在超过90%的基准任务上与GeneGPT持平或表现更好，在Gene-Turing上平均得分为0.849，在GeneHop上平均得分为0.830。此外，OpenBioLLM的模块化多智能体设计将基准任务的延迟降低了40-50%。

Conclusion: 开源多智能体系统在基因组问答方面具有巨大潜力。OpenBioLLM通过使用小型开源模型和模块化多智能体设计，不仅在性能上超越了专有模型，还显著提高了效率，解决了专有模型在可扩展性、成本和隐私方面的局限性。

Abstract: Genomic question answering often requires complex reasoning and integration across diverse biomedical sources. GeneGPT addressed this challenge by combining domain-specific APIs with OpenAI's code-davinci-002 large language model to enable natural language interaction with genomic databases. However, its reliance on a proprietary model limits scalability, increases operational costs, and raises concerns about data privacy and generalization.
  In this work, we revisit and reproduce GeneGPT in a pilot study using open source models, including Llama 3.1, Qwen2.5, and Qwen2.5 Coder, within a monolithic architecture; this allows us to identify the limitations of this approach. Building on this foundation, we then develop OpenBioLLM, a modular multi-agent framework that extends GeneGPT by introducing agent specialization for tool routing, query generation, and response validation. This enables coordinated reasoning and role-based task execution.
  OpenBioLLM matches or outperforms GeneGPT on over 90% of the benchmark tasks, achieving average scores of 0.849 on Gene-Turing and 0.830 on GeneHop, while using smaller open-source models without additional fine-tuning or tool-specific pretraining. OpenBioLLM's modular multi-agent design reduces latency by 40-50% across benchmark tasks, significantly improving efficiency without compromising model capability. The results of our comprehensive evaluation highlight the potential of open-source multi-agent systems for genomic question answering. Code and resources are available at https://github.com/ielab/OpenBioLLM.

</details>


### [77] [ProRAC: A Neuro-symbolic Method for Reasoning about Actions with LLM-based Progression](https://arxiv.org/abs/2511.15069)
*Haoyong Wu,Yongmei Liu*

Main category: cs.AI

TL;DR: 本文提出了ProRAC，一个神经符号框架，用于解决RAC问题。


<details>
  <summary>Details</summary>
Motivation: 解决RAC问题。

Method: ProRAC从问题中提取RAC元素（动作和问题），逐步执行每个动作以得出最终状态，然后根据进展状态评估查询以得到答案。

Result: ProRAC在多个RAC基准测试中取得了强大的性能。

Conclusion: ProRAC在不同基准、领域、LLM骨干和RAC任务类型上都表现出色。

Abstract: In this paper, we propose ProRAC (Progression-based Reasoning about Actions and Change), a neuro-symbolic framework that leverages LLMs to tackle RAC problems. ProRAC extracts fundamental RAC elements including actions and questions from the problem, progressively executes each action to derive the final state, and then evaluates the query against the progressed state to arrive at an answer. We evaluate ProRAC on several RAC benchmarks, and the results demonstrate that our approach achieves strong performance across different benchmarks, domains, LLM backbones, and types of RAC tasks.

</details>


### [78] [Knowledge-Informed Automatic Feature Extraction via Collaborative Large Language Model Agents](https://arxiv.org/abs/2511.15074)
*Henrik Bradland,Morten Goodwin,Vladimir I. Zadorozhny,Per-Arne Andersen*

Main category: cs.AI

TL;DR: 该论文介绍了一个名为Rogue One的新型多智能体框架，利用大型语言模型和外部知识，通过分散的智能体协作，为表格数据自动提取高质量特征，并在性能上超越了现有技术。


<details>
  <summary>Details</summary>
Motivation: 现有的自动化特征提取方法受限于单一的大型语言模型架构、简单的定量反馈以及未能系统地整合外部领域知识，导致在表格数据上的机器学习模型性能受影响。

Method: Rogue One框架通过设立科学家、提取器和测试员三个专门的智能体，形成一个去中心化系统。这些智能体迭代协作，发现、生成和验证预测性特征。该框架引入了丰富的定性反馈机制和“泛洪-修剪”策略，以平衡特征的探索和利用。同时，它通过集成检索增强（RAG）系统主动整合外部知识，以生成具有统计效力、语义意义和可解释性的特征。

Result: Rogue One在19个分类数据集和9个回归数据集上显著优于最先进的方法。此外，系统还发现了新的可测试假设，例如在心肌数据集中识别出潜在的生物标志物。

Conclusion: Rogue One是一个有效的知识驱动型自动特征提取工具，不仅提升了机器学习模型的性能，还能够促进科学发现。

Abstract: The performance of machine learning models on tabular data is critically dependent on high-quality feature engineering. While Large Language Models (LLMs) have shown promise in automating feature extraction (AutoFE), existing methods are often limited by monolithic LLM architectures, simplistic quantitative feedback, and a failure to systematically integrate external domain knowledge. This paper introduces Rogue One, a novel, LLM-based multi-agent framework for knowledge-informed automatic feature extraction. Rogue One operationalizes a decentralized system of three specialized agents-Scientist, Extractor, and Tester-that collaborate iteratively to discover, generate, and validate predictive features. Crucially, the framework moves beyond primitive accuracy scores by introducing a rich, qualitative feedback mechanism and a "flooding-pruning" strategy, allowing it to dynamically balance feature exploration and exploitation. By actively incorporating external knowledge via an integrated retrieval-augmented (RAG) system, Rogue One generates features that are not only statistically powerful but also semantically meaningful and interpretable. We demonstrate that Rogue One significantly outperforms state-of-the-art methods on a comprehensive suite of 19 classification and 9 regression datasets. Furthermore, we show qualitatively that the system surfaces novel, testable hypotheses, such as identifying a new potential biomarker in the myocardial dataset, underscoring its utility as a tool for scientific discovery.

</details>


### [79] [SafeRBench: A Comprehensive Benchmark for Safety Assessment in Large Reasoning Models](https://arxiv.org/abs/2511.15169)
*Xin Gao,Shaohan Yu,Zerui Chen,Yueming Lyu,Weichen Yu,Guanghao Li,Jiyao Liu,Jianxiong Gao,Jian Liang,Ziwei Liu,Chenyang Si*

Main category: cs.AI

TL;DR: SafeRBench是首个端到端评估大语言模型（LRMs）安全性的基准。其创新性地引入了风险类别和级别进行输入设计，提出了一种微思维分块机制来细化输出分析，并通过人类安全对齐验证了评估的有效性，最终对19个LRM进行了评估，证明了其能够提供多维度的安全评估。


<details>
  <summary>Details</summary>
Motivation: 大型推理模型（LRMs）通过明确的思维链提高了答案质量，但这也带来了新的安全风险，即有害内容可能被隐蔽地注入、逐渐浮现或通过误导性理由在推理过程中得到解释。现有的安全评估主要关注输出层面的判断，很少能捕捉到推理过程中这些动态风险。

Method: SafeRBench从输入、中间推理到最终输出，评估LRM的端到端安全性。它包括以下几个方面：1. 输入特征化：引入风险类别和级别到输入设计中，明确考虑受影响的群体和严重程度，建立了一个平衡的提示套件，反映了不同的危害梯度。2. 细粒度输出分析：引入微思维分块机制，将长推理过程分割成语义连贯的单元，从而能够在十个安全维度上进行细粒度评估。3. 人类安全对齐：通过专门设计用于捕捉安全判断的人类标注，验证基于LLM的评估。

Result: 对19个LRM进行的评估表明，SafeRBench能够实现详细的多维度安全评估，从多个角度深入了解风险和保护机制。

Conclusion: SafeRBench是第一个能够端到端评估大型推理模型（LRMs）安全性的基准。它通过创新的输入设计、细粒度的输出分析和人类安全对齐，为LRM的安全评估提供了全面和多维度的视角，有助于识别风险并开发保护机制。

Abstract: Large Reasoning Models (LRMs) improve answer quality through explicit chain-of-thought, yet this very capability introduces new safety risks: harmful content can be subtly injected, surface gradually, or be justified by misleading rationales within the reasoning trace. Existing safety evaluations, however, primarily focus on output-level judgments and rarely capture these dynamic risks along the reasoning process. In this paper, we present SafeRBench, the first benchmark that assesses LRM safety end-to-end -- from inputs and intermediate reasoning to final outputs. (1) Input Characterization: We pioneer the incorporation of risk categories and levels into input design, explicitly accounting for affected groups and severity, and thereby establish a balanced prompt suite reflecting diverse harm gradients. (2) Fine-Grained Output Analysis: We introduce a micro-thought chunking mechanism to segment long reasoning traces into semantically coherent units, enabling fine-grained evaluation across ten safety dimensions. (3) Human Safety Alignment: We validate LLM-based evaluations against human annotations specifically designed to capture safety judgments. Evaluations on 19 LRMs demonstrate that SafeRBench enables detailed, multidimensional safety assessment, offering insights into risks and protective mechanisms from multiple perspectives.

</details>


### [80] [As If We've Met Before: LLMs Exhibit Certainty in Recognizing Seen Files](https://arxiv.org/abs/2511.15192)
*Haodong Li,Jingqi Zhang,Xiao Cheng,Peihua Mai,Haoyu Wang,Yang Pan*

Main category: cs.AI

TL;DR: COPYCHECK是一种新颖的框架，它利用不确定性信号来检测LLMs训练集中是否使用了受版权保护的内容。它可以实现对LLaMA 7b和LLaMA2 7b的检测，平均平衡准确度达到90.1％和91.6％。


<details>
  <summary>Details</summary>
Motivation: 大型语言模型（LLMs）的卓越语言能力源于对海量数据集的广泛训练，其中通常包含受版权保护的材料，这引发了对未经授权使用的严重担忧。

Method: COPYCHECK利用不确定性信号来检测受版权保护的内容是否被用于法学硕士训练集，通过捕捉不确定性模式，可靠地区分“见过”（训练数据）和“未见过”（非训练数据）的内容，从而将法学硕士的过度自信从一个限制转化为一种资产。它进一步实施了两重策略：1）将文件战略性地分割成更小的代码片段，以减少对大规模训练数据的依赖；2）不确定性引导的无监督集群，以消除对经验调整阈值的需求。

Result: COPYCHECK在检测已见文件方面达到了90.1％（LLaMA 7b）和91.6％（LLaMA2 7b）的平均平衡准确率。与SOTA基线相比，COPYCHECK实现了超过90％的相对改进，达到了93.8％的平衡准确率。它在不同架构上表现出强大的泛化能力，在GPT-J 6B上保持了高性能。

Conclusion: 这项工作首次将不确定性应用于LLM的版权检测，为训练数据的透明度提供了实用工具。

Abstract: The remarkable language ability of Large Language Models (LLMs) stems from extensive training on vast datasets, often including copyrighted material, which raises serious concerns about unauthorized use. While Membership Inference Attacks (MIAs) offer potential solutions for detecting such violations, existing approaches face critical limitations and challenges due to LLMs' inherent overconfidence, limited access to ground truth training data, and reliance on empirically determined thresholds.
  We present COPYCHECK, a novel framework that leverages uncertainty signals to detect whether copyrighted content was used in LLM training sets. Our method turns LLM overconfidence from a limitation into an asset by capturing uncertainty patterns that reliably distinguish between ``seen" (training data) and ``unseen" (non-training data) content. COPYCHECK further implements a two-fold strategy: (1) strategic segmentation of files into smaller snippets to reduce dependence on large-scale training data, and (2) uncertainty-guided unsupervised clustering to eliminate the need for empirically tuned thresholds. Experiment results show that COPYCHECK achieves an average balanced accuracy of 90.1% on LLaMA 7b and 91.6% on LLaMA2 7b in detecting seen files. Compared to the SOTA baseline, COPYCHECK achieves over 90% relative improvement, reaching up to 93.8\% balanced accuracy. It further exhibits strong generalizability across architectures, maintaining high performance on GPT-J 6B. This work presents the first application of uncertainty for copyright detection in LLMs, offering practical tools for training data transparency.

</details>


### [81] [Efficiency Will Not Lead to Sustainable Reasoning AI](https://arxiv.org/abs/2511.15259)
*Philipp Wiesner,Daniel W. O'Neill,Francesca Larosa,Odej Kao*

Main category: cs.AI

TL;DR: 人工智能研究正转向复杂问题解决，模型不仅优化模式识别，还优化多步推理。计算的全球能源足迹曾因效率提升和需求自然饱和而稳定。然而，效率提升已接近物理极限，新兴推理AI缺乏可比的饱和点：性能不再受限于训练数据量，而是随训练和推理中指数级的计算投入持续扩展。本文认为，仅靠效率无法实现可持续的推理AI，并讨论了将明确限制嵌入此类系统优化和治理的研究和政策方向。


<details>
  <summary>Details</summary>
Motivation: AI研究正转向复杂问题解决，需要多步推理能力，而非仅仅是模式识别。然而，效率的提升已接近物理极限，而推理AI的性能提升却需要指数级的计算投入，这使得其能源消耗持续增长，对环境造成压力。因此，本文旨在探讨如何在推理AI的优化和治理中嵌入明确的限制，以实现可持续发展。

Method: 本文通过分析当前AI研究的趋势和挑战，特别是推理AI在能源消耗方面的特点，提出了“效率 alone will not lead to sustainable reasoning AI”的论点。基于此，文章进一步讨论了实现可持续推理AI所需的研究方向，旨在寻找将明确限制嵌入系统优化和治理的方法。

Result: 本文的分析表明，仅靠提高效率无法解决推理AI带来的能源消耗问题。推理AI的性能与计算投入呈指数级关系，这意味着其能源足迹将持续增长。

Conclusion: 为了实现可持续的推理AI，必须在优化和治理中嵌入明确的限制。这需要研究人员和政策制定者共同努力，探索新的研究方向并制定相应的政策。

Abstract: AI research is increasingly moving toward complex problem solving, where models are optimized not only for pattern recognition but for multi-step reasoning. Historically, computing's global energy footprint has been stabilized by sustained efficiency gains and natural saturation thresholds in demand. But as efficiency improvements are approaching physical limits, emerging reasoning AI lacks comparable saturation points: performance is no longer limited by the amount of available training data but continues to scale with exponential compute investments in both training and inference. This paper argues that efficiency alone will not lead to sustainable reasoning AI and discusses research and policy directions to embed explicit limits into the optimization and governance of such systems.

</details>


### [82] [Realist and Pluralist Conceptions of Intelligence and Their Implications on AI Research](https://arxiv.org/abs/2511.15282)
*Ninell Oldenburg,Ruchira Dhar,Anders Søgaard*

Main category: cs.AI

TL;DR: 这篇论文探讨了人工智能研究中存在的两种不同的智能观念：智能实在论和智能多元论，并分析了它们如何影响研究方法、对实验证据的解读以及对AI风险的评估。


<details>
  <summary>Details</summary>
Motivation: 目前人工智能研究中存在对智能的不同理解，但这些理解往往是隐性的。本文旨在揭示这些隐性假设，以促进对AI研究分歧的更清晰理解。

Method: 通过分析当前人工智能研究中的辩论，展示智能实在论和智能多元论这两种观念如何影响模型选择、基准设计、实验验证、对相同经验现象的解释以及对AI风险的评估。

Result: 智能实在论者倾向于将智能视为单一、普遍的能力，导致其在方法论上采用特定的模型选择、基准设计和实验验证方法，并在解释能力涌现和系统局限性时采取特定视角。在AI风险方面，他们主要关注超级智能并寻求统一的对齐解决方案。智能多元论者则认为智能是多样化且依赖于上下文的能力，其研究方法和对实验现象的解释与实在论者不同。在AI风险方面，他们看到不同领域的多样化威胁，需要特定于上下文的解决方案。

Conclusion: 明确区分智能实在论和智能多元论这两种观念有助于更好地理解人工智能研究中的分歧，因为它们从根本上塑造了实证证据的解释和研究路径。

Abstract: In this paper, we argue that current AI research operates on a spectrum between two different underlying conceptions of intelligence: Intelligence Realism, which holds that intelligence represents a single, universal capacity measurable across all systems, and Intelligence Pluralism, which views intelligence as diverse, context-dependent capacities that cannot be reduced to a single universal measure. Through an analysis of current debates in AI research, we demonstrate how the conceptions remain largely implicit yet fundamentally shape how empirical evidence gets interpreted across a wide range of areas. These underlying views generate fundamentally different research approaches across three areas. Methodologically, they produce different approaches to model selection, benchmark design, and experimental validation. Interpretively, they lead to contradictory readings of the same empirical phenomena, from capability emergence to system limitations. Regarding AI risk, they generate categorically different assessments: realists view superintelligence as the primary risk and search for unified alignment solutions, while pluralists see diverse threats across different domains requiring context-specific solutions. We argue that making explicit these underlying assumptions can contribute to a clearer understanding of disagreements in AI research.

</details>


### [83] [Octopus: Agentic Multimodal Reasoning with Six-Capability Orchestration](https://arxiv.org/abs/2511.15351)
*Yifu Guo,Zishan Xu,Zhiyuan Yao,Yuquan Lu,Jiaye Lin,Sen Hu,Zhenheng Tang,Yingchao Li,Huacan Wang,Ronghao Chen*

Main category: cs.AI

TL;DR: 该论文介绍了一种名为“章鱼”的新型多模态推理模型，它通过模拟人类的六种核心能力，解决了现有模型在动态任务中适应性差的问题。


<details>
  <summary>Details</summary>
Motivation: 现有模型缺乏自主探索推理路径的能力，难以适应动态变化的真实世界任务，而人类在解决此类任务时能表现出互补的思维能力。

Method: 提出了一种新的多模态智能体推理范式——章鱼（Octopus），该模型定义了六种多模态推理的核心能力，并能自主探索并在推理过程中动态选择最适合当前状态的能力。同时，还组织了一个全面的评估基准——Octopus-Bench。

Result: 章鱼模型在Octopus-Bench的绝大多数任务中都取得了最佳性能。

Conclusion: 能力协调在智能体多模态推理中扮演着关键角色。

Abstract: Existing multimodal reasoning models and frameworks suffer from fundamental architectural limitations: most lack the human-like ability to autonomously explore diverse reasoning pathways-whether in direct inference, tool-driven visual exploration, programmatic visual manipulation, or intrinsic visual imagination. Consequently, they struggle to adapt to dynamically changing capability requirements in real-world tasks. Meanwhile, humans exhibit a complementary set of thinking abilities when addressing such tasks, whereas existing methods typically cover only a subset of these dimensions. Inspired by this, we propose Octopus: Agentic Multimodal Reasoning with Six-Capability Orchestration, a new paradigm for multimodal agentic reasoning. We define six core capabilities essential for multimodal reasoning and organize a comprehensive evaluation benchmark, Octopus-Bench, accordingly. Octopus is capable of autonomously exploring during reasoning and dynamically selecting the most appropriate capability based on the current state. Experimental results show that Octopus achieves the best performance on the vast majority of tasks in Octopus-Bench, highlighting the crucial role of capability coordination in agentic multimodal reasoning.

</details>


### [84] [Terra Nova: A Comprehensive Challenge Environment for Intelligent Agents](https://arxiv.org/abs/2511.15378)
*Trevor McInroe*

Main category: cs.AI

TL;DR: Terra Nova 是一个受 Civilization V 启发的新型强化学习挑战环境，旨在同时解决多个经典的强化学习难题，而非仅仅聚合不相关的任务。


<details>
  <summary>Details</summary>
Motivation: 现有的多任务基准测试主要评估智能体在不相关策略之间切换的能力，而不是在多个相互作用的挑战中进行深度推理的能力。

Method: 本文引入了一个名为 Terra Nova 的综合挑战环境，该环境受到 Civilization V 的启发，旨在同时激发多个经典的强化学习挑战，例如部分可观测性、信用分配、表征学习和巨大的动作空间。

Result: Terra Nova 作为一个强化学习的综合挑战环境，它能同时激发多个经典的强化学习挑战。

Conclusion: Terra Nova 提供了一个单一环境，其中涉及到多个相互作用的挑战，需要智能体进行长时间的理解和深度推理，而不是简单地在不相关的任务之间切换。

Abstract: We introduce Terra Nova, a new comprehensive challenge environment (CCE) for reinforcement learning (RL) research inspired by Civilization V. A CCE is a single environment in which multiple canonical RL challenges (e.g., partial observability, credit assignment, representation learning, enormous action spaces, etc.) arise simultaneously. Mastery therefore demands integrated, long-horizon understanding across many interacting variables. We emphasize that this definition excludes challenges that only aggregate unrelated tasks in independent, parallel streams (e.g., learning to play all Atari games at once). These aggregated multitask benchmarks primarily asses whether an agent can catalog and switch among unrelated policies rather than test an agent's ability to perform deep reasoning across many interacting challenges.

</details>


### [85] [IPR-1: Interactive Physical Reasoner](https://arxiv.org/abs/2511.15407)
*Mingyu Zhang,Lifeng Zhuo,Tianxi Tan,Guocan Xie,Xian Nie,Yan Li,Renjie Zhao,Zizhu He,Ziyu Wang,Jiting Cai,Yong-Lu Li*

Main category: cs.AI

TL;DR: 该论文旨在探讨智能体是否能通过与环境的交互来学习类人推理，并随着经验的增长而不断提高。


<details>
  <summary>Details</summary>
Motivation: 智能体在物理和因果推理方面存在不足，尤其是世界模型倾向于模仿视觉模式而非分析物理和因果，而VLM/VLA智能体在交互环境中缺乏前瞻性。

Method: 本文提出了IPR（Interactive Physical Reasoner）模型，该模型利用世界模型的rollout来评估和强化VLM的策略，并引入了PhysCode，这是一种以物理为中心的行动代码，旨在将语义意图与动力学对齐，为预测和推理提供共享的行动空间。在Game-to-Unseen（G2U）设置中，使用1000多个异构游戏进行训练和评估。

Result: IPR模型在三个评估层次（生存、好奇心、效用）上表现出色，总体上与GPT-4持平，并在“好奇心”层次上超越了GPT-4。研究发现，性能随着训练游戏数量和交互步骤的增加而提高，并且模型能够零样本迁移到未见过的游戏中。

Conclusion: 以物理为中心的交互是稳步提高物理推理能力的有效途径。

Abstract: Humans learn by observing, interacting with environments, and internalizing physics and causality. Here, we aim to ask whether an agent can similarly acquire human-like reasoning from interaction and keep improving with more experience. We study this in a Game-to-Unseen (G2U) setting, curating 1,000+ heterogeneous games with diverse physical and causal mechanisms, and evaluate at three human-like levels: Survival, Curiosity, Utility, from primitive intuition to goal-driven reasoning. Our analysis reveals complementary failures: VLM/VLA agents reason but lack look-ahead in interactive settings, while world models imagine but imitate visual patterns rather than analyze physics and causality. We therefore propose IPR (Interactive Physical Reasoner), using world-model rollouts to score and reinforce a VLM's policy, and introduce PhysCode, a physics-centric action code aligning semantic intent with dynamics to provide a shared action space for prediction and reasoning. Pretrained on 1,000+ games, our IPR performs robustly on three levels, matches GPT-5 overall, and surpasses it on Curiosity. We find that performance improves with more training games and interaction steps, and that the model also zero-shot transfers to unseen games. These results support physics-centric interaction as a path to steadily improving physical reasoning.

</details>


### [86] [Know Your Intent: An Autonomous Multi-Perspective LLM Agent Framework for DeFi User Transaction Intent Mining](https://arxiv.org/abs/2511.15456)
*Qian'ang Mao,Yuxuan Zhang,Jiaman Chen,Wenjun Zhou,Jiaqi Yan*

Main category: cs.AI

TL;DR: 该论文提出了一个名为TIM的框架，用于推断去中心化金融（DeFi）交易中用户的意图，该框架利用DeFi意图分类法和多智能体大型语言模型系统来增强理解。


<details>
  <summary>Details</summary>
Motivation: 理解DeFi交易背后的用户意图至关重要，但由于复杂的智能合约交互、多方面的链上/链下因素和不透明的十六进制日志，这变得具有挑战性，现有方法缺乏深层语义洞察。

Method: 本文提出了交易意图挖掘（TIM）框架。TIM利用基于扎根理论构建的DeFi意图分类法，以及一个多智能体大型语言模型（LLM）系统，以稳健地推断用户意图。元级别规划器动态协调领域专家，将多个特定视角的意图分析分解为可解决的子任务。问题解决者利用多模态链上/链下数据处理任务。认知评估器减少LLM幻觉并确保可验证性。

Result: 实验表明，TIM在性能上显著优于机器学习模型、单一LLM和单一Agent基线。

Conclusion: 这项工作有助于更可靠地理解DeFi中的用户动机，为复杂的区块链活动提供情境感知的解释。

Abstract: As Decentralized Finance (DeFi) develops, understanding user intent behind DeFi transactions is crucial yet challenging due to complex smart contract interactions, multifaceted on-/off-chain factors, and opaque hex logs. Existing methods lack deep semantic insight. To address this, we propose the Transaction Intent Mining (TIM) framework. TIM leverages a DeFi intent taxonomy built on grounded theory and a multi-agent Large Language Model (LLM) system to robustly infer user intents. A Meta-Level Planner dynamically coordinates domain experts to decompose multiple perspective-specific intent analyses into solvable subtasks. Question Solvers handle the tasks with multi-modal on/off-chain data. While a Cognitive Evaluator mitigates LLM hallucinations and ensures verifiability. Experiments show that TIM significantly outperforms machine learning models, single LLMs, and single Agent baselines. We also analyze core challenges in intent inference. This work helps provide a more reliable understanding of user motivations in DeFi, offering context-aware explanations for complex blockchain activity.

</details>


<div id='cs.GT'></div>

# cs.GT [[Back]](#toc)

### [87] [Computing Power Indices in Weighted Majority Games with Formal Power Series](https://arxiv.org/abs/2511.14995)
*Naonori Kakimura,Yoshihiko Terai*

Main category: cs.GT

TL;DR: 这篇论文提出了一种在加权多数博弈中计算权力指数的快速伪多项式时间算法。


<details>
  <summary>Details</summary>
Motivation: 在加权多数博弈中，计算权力指数是一项重要的任务。现有的算法在某些情况下效率不高，因此需要更快的方法。

Method: 本文提出了一种利用形式幂级数高效计算技术的方法。

Result: Banzhaf指数的计算时间复杂o(n+q log(q))，Shapley-Shubik指数的计算时间复杂度为O(nq log(q))。当q=2^o(n)时，该算法比现有算法更快。

Conclusion: 本文提出的算法在特定条件下能够显著提高加权多数博弈中权力指数的计算效率。

Abstract: In this paper, we propose fast pseudo-polynomial-time algorithms for computing power indices in weighted majority games. We show that we can compute the Banzhaf index for all players in $O(n+q\log (q))$ time, where $n$ is the number of players and $q$ is a given quota. Moreover, we prove that the Shapley--Shubik index for all players can be computed in $O(nq\log (q))$ time. Our algorithms are faster than existing algorithms when $q=2^{o(n)}$. Our algorithms exploit efficient computation techniques for formal power series.

</details>


### [88] [Coopetitive Index: a measure of cooperation and competition in coalition formation](https://arxiv.org/abs/2511.15441)
*Michele Aleandri,Marco Dall'Aglio*

Main category: cs.GT

TL;DR: 本文将Aleandri和Dall'Aglio（2025）引入的简单博弈合作指数扩展到更广泛的单调可转移效用（TU）博弈和所有非空联盟，包括单例。


<details>
  <summary>Details</summary>
Motivation: 为了解决简单博弈合作指数的局限性，并使其适用于更广泛的博弈类型。

Method: 1. 将合作指数推广到单调可转移效用（TU）博弈和所有非空联盟，包括单例。 2. 提出了一个绝对合作指数，其通用范围为[-1,1]，以促进联盟之间的有意义比较。 3. 研究了Banzhaf、均匀Shapley和Shapley-Owen合作指数等实例，并推导了合作与经典半值之间的明确公式。 4. 提供了均匀Shapley和Shapley-Owen版本的公理化特征。

Result: 1. 建立了具有通用范围的新型绝对合作指数，可以在联盟之间进行有意义的比较。 2. 发现了合作指数与经典半值之间的明确联系。 3. 证明了均匀Shapley和Shapley-Owen版本合作指数的独特性，它们由线性、纯讨价还价博弈的对称性、外部空玩家中立性和反映其内部分布的收缩公理唯一确定。

Conclusion: 合作指数是量化TU博弈中联盟合作和竞争趋势的多功能工具。

Abstract: We extend the coopetition index introduced by Aleandri and Dall'Aglio (2025) for simple games to the broader class of monotone transferable utility (TU) games and to all non-empty coalitions, including singletons. The new formulation allows us to define an absolute coopetition index with a universal range in [-1,1], facilitating meaningful comparisons across coalitions.
  We study several notable instances of the index, including the Banzhaf, Uniform Shapley, and Shapley-Owen coopetition indices, and we derive explicit formulas that connect coopetition to classical semivalues. Finally, we provide axiomatic characterizations of the Uniform Shapley and Shaple--Owen versions, showing that each is uniquely determined by linearity, symmetry over pure bargaining games, external null player neutrality, and a contraction axiom reflecting its internal distribution. These results position the coopetition index as a versatile tool for quantifying the cooperative and competitive tendencies of coalitions in TU-games.

</details>
