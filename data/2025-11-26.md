<div id=toc></div>

# Table of Contents

- [cs.CL](#cs.CL) [Total: 26]
- [stat.ML](#stat.ML) [Total: 2]
- [cs.IT](#cs.IT) [Total: 7]
- [cs.LG](#cs.LG) [Total: 74]
- [cs.AI](#cs.AI) [Total: 22]
- [cs.GT](#cs.GT) [Total: 4]
- [cs.MA](#cs.MA) [Total: 3]


<div id='cs.CL'></div>

# cs.CL [[Back]](#toc)

### [1] [Efficient Multi-Hop Question Answering over Knowledge Graphs via LLM Planning and Embedding-Guided Search](https://arxiv.org/abs/2511.19648)
*Manil Shrestha,Edward Kim*

Main category: cs.CL

TL;DR: 这篇论文提出了两种混合算法来解决知识图谱多跳问答中的效率和可验证性问题，旨在减少大型语言模型的依赖并提高答案的可靠性。


<details>
  <summary>Details</summary>
Motivation: 现有的知识图谱多跳问答方法在推理路径上存在组合爆炸问题，且高度依赖昂贵的大型语言模型进行实体链接和路径排序，导致实际部署受限。此外，大型语言模型生成的答案往往缺乏可验证的结构化知识基础。

Method: 1. LLM引导的规划（LLM-Guided Planning）：该方法通过一次大型语言模型调用预测关系序列，并通过广度优先搜索执行，实现了接近完美的准确率（micro-F1 > 0.90），并确保所有答案都基于知识图谱。2. 嵌入引导的神经搜索（Embedding-Guided Neural Search）：该方法完全消除了对大型语言模型的调用，通过轻量级的670万参数边缘评分器融合文本和图嵌入，实现了超过100倍的速度提升，同时保持了有竞争力的准确率。此外，论文还通过知识蒸馏将规划能力压缩到一个4B参数的模型中。

Result: 在MetaQA上的评估表明，有根据的推理始终优于无根据的生成。结构化规划比直接答案生成具有更好的可迁移性。压缩后的模型能够以零API成本实现与大型模型相同的性能。

Conclusion: 可验证的多跳推理在推理时不需要大型模型，而是需要结合符号结构和学习表示的正确架构归纳偏差。这两种方法。这两种方法。

Abstract: Multi-hop question answering over knowledge graphs remains computationally challenging due to the combinatorial explosion of possible reasoning paths. Recent approaches rely on expensive Large Language Model (LLM) inference for both entity linking and path ranking, limiting their practical deployment. Additionally, LLM-generated answers often lack verifiable grounding in structured knowledge. We present two complementary hybrid algorithms that address both efficiency and verifiability: (1) LLM-Guided Planning that uses a single LLM call to predict relation sequences executed via breadth-first search, achieving near-perfect accuracy (micro-F1 > 0.90) while ensuring all answers are grounded in the knowledge graph, and (2) Embedding-Guided Neural Search that eliminates LLM calls entirely by fusing text and graph embeddings through a lightweight 6.7M-parameter edge scorer, achieving over 100 times speedup with competitive accuracy. Through knowledge distillation, we compress planning capability into a 4B-parameter model that matches large-model performance at zero API cost. Evaluation on MetaQA demonstrates that grounded reasoning consistently outperforms ungrounded generation, with structured planning proving more transferable than direct answer generation. Our results show that verifiable multi-hop reasoning does not require massive models at inference time, but rather the right architectural inductive biases combining symbolic structure with learned representations.

</details>


### [2] [Can LLMs Faithfully Explain Themselves in Low-Resource Languages? A Case Study on Emotion Detection in Persian](https://arxiv.org/abs/2511.19719)
*Mobina Mehrazar,Mohammad Amin Yousefi,Parisa Abolfath Beygi,Behnam Bahrak*

Main category: cs.CL

TL;DR: 本研究评估了大型语言模型（LLM）在波斯语情感分类中生成自我解释的忠实性，发现LLM的解释与人类判断存在差异，并指出了当前解释方法和指标的局限性。


<details>
  <summary>Details</summary>
Motivation: 大型语言模型（LLM）在生成自我解释的同时进行预测，但在低资源语言中，这些解释的忠实性引发担忧。本研究旨在评估LLM在波斯语情感分类任务中生成解释的忠实性，并找出其与人类判断的差异。

Method: 本研究通过比较模型识别的影响词与人类标注者识别的影响词，评估了LLM在波斯语情感分类中生成解释的忠实性。研究还测试了两种提示策略（先预测后解释和先解释后预测）对解释忠实性的影响，并使用token级别对数概率的置信度分数来评估忠实性。

Result: LLM在分类性能上表现出色，但其生成的解释往往与忠实的推理存在分歧，LLM之间的一致性高于与人类判断的一致性。

Conclusion: 本研究结果强调了当前解释方法和评估指标的局限性，需要更稳健的方法来确保LLM在多语言和低资源环境中的可靠性。

Abstract: Large language models (LLMs) are increasingly used to generate self-explanations alongside their predictions, a practice that raises concerns about the faithfulness of these explanations, especially in low-resource languages. This study evaluates the faithfulness of LLM-generated explanations in the context of emotion classification in Persian, a low-resource language, by comparing the influential words identified by the model against those identified by human annotators. We assess faithfulness using confidence scores derived from token-level log-probabilities. Two prompting strategies, differing in the order of explanation and prediction (Predict-then-Explain and Explain-then-Predict), are tested for their impact on explanation faithfulness. Our results reveal that while LLMs achieve strong classification performance, their generated explanations often diverge from faithful reasoning, showing greater agreement with each other than with human judgments. These results highlight the limitations of current explanation methods and metrics, emphasizing the need for more robust approaches to ensure LLM reliability in multilingual and low-resource contexts.

</details>


### [3] [Comparative Analysis of LoRA-Adapted Embedding Models for Clinical Cardiology Text Representation](https://arxiv.org/abs/2511.19739)
*Richard J. Young,Alice M. Matthews*

Main category: cs.CL

TL;DR: 该研究评估了十种针对心脏病学领域优化的transformer文本嵌入模型，发现编码器架构，特别是BioLinkBERT，在领域特定性能上优于大型解码器模型，且计算资源需求更少。


<details>
  <summary>Details</summary>
Motivation: 尽管领域特定的文本嵌入对临床自然语言处理至关重要，但对不同模型架构进行系统比较的研究仍然有限。

Method: 本研究通过在106,535个源自权威医学教科书的心脏病学文本对上进行低秩适应（LoRA）微调，评估了十种针对心脏病学调整的基于transformer的嵌入模型。

Result: 结果表明，编码器架构，特别是BioLinkBERT，取得了卓越的领域特定性能（分离分数：0.510），相较于较大的基于解码器的模型，计算资源需求显著减少。

Conclusion: 研究结果挑战了大型语言模型必然产生更好领域特定嵌入的假设，并为临床自然语言处理系统的开发提供了实用指导。

Abstract: Domain-specific text embeddings are critical for clinical natural language processing, yet systematic comparisons across model architectures remain limited. This study evaluates ten transformer-based embedding models adapted for cardiology through Low-Rank Adaptation (LoRA) fine-tuning on 106,535 cardiology text pairs derived from authoritative medical textbooks. Results demonstrate that encoder-only architectures, particularly BioLinkBERT, achieve superior domain-specific performance (separation score: 0.510) compared to larger decoder-based models, while requiring significantly fewer computational resources. The findings challenge the assumption that larger language models necessarily produce better domain-specific embeddings and provide practical guidance for clinical NLP system development. All models, training code, and evaluation datasets are publicly available to support reproducible research in medical informatics.

</details>


### [4] [What does it mean to understand language?](https://arxiv.org/abs/2511.19757)
*Colton Casto,Anna Ivanova,Evelina Fedorenko,Nancy Kanwisher*

Main category: cs.CL

TL;DR: 深入理解语言需要将信息从语言系统输出到大脑的其他区域，以便进行更深层次的加工，从而构建丰富的心理模型。


<details>
  <summary>Details</summary>
Motivation: 处理语言时，大脑核心语言系统存在局限性，不能完全理解语言的深层含义，这促使研究者探索语言信息如何与大脑其他区域协同工作以实现深度理解。

Method: 本文主要通过回顾现有证据，并结合认知神经科学的最新进展来论证其假设，即语言理解需要大脑各区域之间的信息输出和整合。

Result: 作者提出，将语言系统的信息输出到负责感知、运动、心理模型构建以及世界知识和自传体记忆存储的大脑区域，对于深度理解语言至关重要。

Conclusion: 语言的深度理解不仅涉及语言输入，还涉及大脑不同区域之间的信息交互和整合，认知神经科学的新方法为直接验证这一机制提供了可能。

Abstract: Language understanding entails not just extracting the surface-level meaning of the linguistic input, but constructing rich mental models of the situation it describes. Here we propose that because processing within the brain's core language system is fundamentally limited, deeply understanding language requires exporting information from the language system to other brain regions that compute perceptual and motor representations, construct mental models, and store our world knowledge and autobiographical memories. We review the existing evidence for this hypothesis, and argue that recent progress in cognitive neuroscience provides both the conceptual foundation and the methods to directly test it, thus opening up a new strategy to reveal what it means, cognitively and neurally, to understand language.

</details>


### [5] [Gender Bias in Emotion Recognition by Large Language Models](https://arxiv.org/abs/2511.19785)
*Maureen Herbert,Katie Sun,Angelica Lim,Yasaman Etesam*

Main category: cs.CL

TL;DR: 该文章探讨了大型语言模型（LLMs）在情感理论理解中是否存在性别偏见，并提出了去偏策略。


<details>
  <summary>Details</summary>
Motivation: 随着LLMs的快速发展及其在日常生活中日益增长的整合，评估和确保其公平性变得至关重要，特别是针对情感理论理解领域的性别偏见。

Method: 通过向LLMs提供人物及其环境的描述，并询问“这个人感觉如何？”，来检查LLMs在情感理论理解中表现出的性别偏见。此外，文章提出并评估了几种去偏策略。

Result: 为了有意义地减少偏见，需要基于训练的干预措施，而不是仅仅依靠推理时基于提示的方法（例如提示工程）。

Conclusion: LLMs在情感理论理解中存在性别偏见，且有效的偏见消除需要从模型训练阶段进行干预。

Abstract: The rapid advancement of large language models (LLMs) and their growing integration into daily life underscore the importance of evaluating and ensuring their fairness. In this work, we examine fairness within the domain of emotional theory of mind, investigating whether LLMs exhibit gender biases when presented with a description of a person and their environment and asked, "How does this person feel?". Furthermore, we propose and evaluate several debiasing strategies, demonstrating that achieving meaningful reductions in bias requires training based interventions rather than relying solely on inference-time prompt-based approaches such as prompt engineering.

</details>


### [6] [Breaking Bad: Norms for Valence, Arousal, and Dominance for over 10k English Multiword Expressions](https://arxiv.org/abs/2511.19816)
*Saif M. Mohammad*

Main category: cs.CL

TL;DR: 该论文介绍了NRC VAD Lexicon v2，这是一个包含10k英语多词表达（MWE）及其组成词的效价、唤醒度和主导度人类评分的词典。它还在v1的基础上增加了单字词的覆盖范围，总计包含10k MWE和25k单词。


<details>
  <summary>Details</summary>
Motivation: 现有的词典（如2018年发布的NRC VAD Lexicon v1）已经包含了单词的VAD关联评级。该论文旨在通过增加多词表达（MWE）和更多单字词的覆盖范围来补充和扩展现有词典，以更好地理解MWE的情感特征。

Method: 通过人类评分的方式，为10k英语多词表达（MWE）及其组成词，以及更多常用单字词，提供效价（Valence）、唤醒度（Arousal）和主导度（Dominance）的关联评级。论文还验证了这些关联评级的高度可靠性。

Result: 创建了NRC VAD Lexicon v2，其中包含10k MWE和25k单词的VAD评分。该词典被用于探究MWE的情感特征，包括其情感强度以及情感组合性。结果表明这些关联性非常可靠。

Conclusion: NRC VAD Lexicon v2的发布极大地扩展了现有情感词典的范围，特别是在多词表达方面。它为自然语言处理、心理学、公共卫生、数字人文和社会科学等领域的广泛研究提供了宝贵的资源，有助于更深入地理解语言中的情感。

Abstract: Factor analysis studies have shown that the primary dimensions of word meaning are Valence (V), Arousal (A), and Dominance (D). Existing lexicons such as the NRC VAD Lexicon, published in 2018, include VAD association ratings for words. Here, we present a complement to it, which has human ratings of valence, arousal, and dominance for 10k English Multiword Expressions (MWEs) and their constituent words. We also increase the coverage of unigrams, especially words that have become more common since 2018. In all, the new NRC VAD Lexicon v2 now has entries for 10k MWEs and 25k words, in addition to the entries in v1. We show that the associations are highly reliable. We use the lexicon to examine emotional characteristics of MWEs, including: 1. The degree to which MWEs (idioms, noun compounds, and verb particle constructions) exhibit strong emotionality; 2. The degree of emotional compositionality in MWEs. The lexicon enables a wide variety of research in NLP, Psychology, Public Health, Digital Humanities, and Social Sciences. The NRC VAD Lexicon v2 is freely available through the project webpage: http://saifmohammad.com/WebPages/nrc-vad.html

</details>


### [7] [Language-Independent Sentiment Labelling with Distant Supervision: A Case Study for English, Sepedi and Setswana](https://arxiv.org/abs/2511.19818)
*Koena Ronny Mabokela,Tim Schlippe,Mpho Raborife,Turgay Celik*

Main category: cs.CL

TL;DR: 该论文提出了一种自动的、语言无关的情感标注方法，利用情感表情符号和词语的信息，解决了非洲低资源语言情感分析中数据标注耗时耗力的问题。


<details>
  <summary>Details</summary>
Motivation: 现有的情感分析系统多针对英语，而许多非洲语言因缺乏标注数据被归类为低资源语言。手动标注文本数据耗时且成本高昂，因此需要自动化和快速的标注流程。

Method: 本研究提出了一种自动的、与语言无关的情感标注方法，该方法利用带有情感的表情符号和词语信息。

Result: 实验在来自SAfriSenti多语言情感语料库的英语、塞佩蒂语和塞茨瓦纳语推文上进行。该方法对英语推文的标注准确率为66%，塞佩蒂语为69%，塞茨瓦纳语为63%。

Conclusion: 这项情感标注方法能够有效地为低资源语言（如非洲语言）自动生成情感标签，平均只有34%的自动生成标签需要人工纠正，大大减少了手动标注工作量，提高了标注效率。

Abstract: Sentiment analysis is a helpful task to automatically analyse opinions and emotions on various topics in areas such as AI for Social Good, AI in Education or marketing. While many of the sentiment analysis systems are developed for English, many African languages are classified as low-resource languages due to the lack of digital language resources like text labelled with corresponding sentiment classes. One reason for that is that manually labelling text data is time-consuming and expensive. Consequently, automatic and rapid processes are needed to reduce the manual effort as much as possible making the labelling process as efficient as possible. In this paper, we present and analyze an automatic language-independent sentiment labelling method that leverages information from sentiment-bearing emojis and words. Our experiments are conducted with tweets in the languages English, Sepedi and Setswana from SAfriSenti, a multilingual sentiment corpus for South African languages. We show that our sentiment labelling approach is able to label the English tweets with an accuracy of 66%, the Sepedi tweets with 69%, and the Setswana tweets with 63%, so that on average only 34% of the automatically generated labels remain to be corrected.

</details>


### [8] [Profile-LLM: Dynamic Profile Optimization for Realistic Personality Expression in LLMs](https://arxiv.org/abs/2511.19852)
*Shi-Wei Dai,Yan-Wei Shie,Tsung-Huan Yang,Lun-Wei Ku,Yung-Hui Li*

Main category: cs.CL

TL;DR: 为了增强大语言模型（LLM）的用户交互体验，本文提出了PersonaPulse框架，旨在通过迭代优化角色扮演提示来最大化LLM的个性表达，并利用情境响应基准进行评估和指导优化过程。


<details>
  <summary>Details</summary>
Motivation: 以往研究未能充分优化提示以最大化LLM的个性表达，因此需要一种方法来有效提升LLM的个性化交互能力。

Method: 本文提出了PersonaPulse框架，该框架利用LLM固有的个性知识，通过迭代优化角色扮演提示，并整合情境响应基准作为评分工具，以确保评估的现实性和上下文相关性，从而指导优化过程。

Result: PersonaPulse生成的提示在定量评估中超越了以往基于心理学描述设计的提示。此外，研究还探讨了模型大小与个性建模之间的关系。最后发现，对于某些个性特质，通过暂停优化过程可以部分控制个性唤起的程度。

Conclusion: 提示优化在塑造LLM中的个性表达方面至关重要，为未来自适应AI交互研究提供了宝贵见解。

Abstract: Personalized Large Language Models (LLMs) have been shown to be an effective way to create more engaging and enjoyable user-AI interactions. While previous studies have explored using prompts to elicit specific personality traits in LLMs, they have not optimized these prompts to maximize personality expression. To address this limitation, we propose PersonaPulse: Dynamic Profile Optimization for Realistic Personality Expression in LLMs, a framework that leverages LLMs' inherent knowledge of personality traits to iteratively enhance role-play prompts while integrating a situational response benchmark as a scoring tool, ensuring a more realistic and contextually grounded evaluation to guide the optimization process. Quantitative evaluations demonstrate that the prompts generated by PersonaPulse outperform those of prior work, which were designed based on personality descriptions from psychological studies. Additionally, we explore the relationship between model size and personality modeling through extensive experiments. Finally, we find that, for certain personality traits, the extent of personality evocation can be partially controlled by pausing the optimization process. These findings underscore the importance of prompt optimization in shaping personality expression within LLMs, offering valuable insights for future research on adaptive AI interactions.

</details>


### [9] [A Systematic Analysis of Large Language Models with RAG-enabled Dynamic Prompting for Medical Error Detection and Correction](https://arxiv.org/abs/2511.19858)
*Farzad Ahmed,Joniel Augustine Jerome,Meliha Yetisgen,Özlem Uzuner*

Main category: cs.CL

TL;DR: 本文探讨了大型语言模型（LLMs）在医学错误检测和纠正方面的表现，比较了零样本、静态随机样本和检索增强动态提示策略，并发现检索增强动态提示在各项任务中表现最佳，能够提高准确性并减少误报。


<details>
  <summary>Details</summary>
Motivation: 临床文档中的错误可能会危及患者安全，LLMs 或许能帮助检测和纠正这些错误，但不同提示策略下 LLMs 的表现尚不明确。

Method: 使用 MEDEC 数据集，评估了九种指令调整的 LLMs（GPT、Claude、Gemini 和 OpenAI o-series 模型）。通过准确率、召回率、误报率（FPR）以及 ROUGE-1、BLEURT 和 BERTScore 的综合得分来衡量性能。同时分析了输出示例以识别故障模式以及 LLM 和临床医生推理之间的差异。

Result: 零样本提示在检测任务中召回率较低，常常遗漏缩写多或不典型的错误。SPR 提高了召回率但增加了 FPR。在所有九个 LLM 中，RDP 将 FPR 降低了约 15%，在错误句子检测中召回率提高了 5% 到 10%，并生成了更符合上下文的纠正。

Conclusion: 在各种 LLM 中，RDP 优于零样本和 SPR 提示。使用检索到的示例可提高检测准确性，减少误报，并增强医学错误校正的可靠性。

Abstract: Objective: Clinical documentation contains factual, diagnostic, and management errors that can compromise patient safety. Large language models (LLMs) may help detect and correct such errors, but their behavior under different prompting strategies remains unclear. We evaluate zero-shot prompting, static prompting with random exemplars (SPR), and retrieval-augmented dynamic prompting (RDP) for three subtasks of medical error processing: error flag detection, error sentence detection, and error correction.
  Methods: Using the MEDEC dataset, we evaluated nine instruction-tuned LLMs (GPT, Claude, Gemini, and OpenAI o-series models). We measured performance using accuracy, recall, false-positive rate (FPR), and an aggregate score of ROUGE-1, BLEURT, and BERTScore for error correction. We also analyzed example outputs to identify failure modes and differences between LLM and clinician reasoning.
  Results: Zero-shot prompting showed low recall in both detection tasks, often missing abbreviation-heavy or atypical errors. SPR improved recall but increased FPR. Across all nine LLMs, RDP reduced FPR by about 15 percent, improved recall by 5 to 10 percent in error sentence detection, and generated more contextually accurate corrections.
  Conclusion: Across diverse LLMs, RDP outperforms zero-shot and SPR prompting. Using retrieved exemplars improves detection accuracy, reduces false positives, and enhances the reliability of medical error correction.

</details>


### [10] [AppSelectBench: Application-Level Tool Selection Benchmark](https://arxiv.org/abs/2511.19957)
*Tianyi Chen,Michael Solodko,Sen Wang,Jongwoo Ko,Junheng Hao,Colby Banbury,Sara Abdali,Saeed Amizadeh,Qing Xiao,Yinheng Li,Tianyu Ding,Kamran Ghasedi Dizaji,Suzhen Zheng,Hao Fan,Justin Wagle,Pashmina Cameron,Kazuhito Koishida*

Main category: cs.CL

TL;DR: AppSelectBench是一个评估CUA中应用程序选择能力的基准测试平台，它包含大量真实的用户任务和桌面应用程序，并揭示了现有模型在该能力上的不足。


<details>
  <summary>Details</summary>
Motivation: 现有基准测试主要评估细粒度API选择，对模型在不同应用程序之间进行推理和选择的能力缺乏深入评估。

Method: 本文介绍了AppSelectBench，一个评估CUA应用程序选择能力的综合基准。它包含一个新颖的用户任务生成管道，可以大规模生成真实、多样且语义连贯的用户意图，并提供统一的评估协议，涵盖随机、启发式、零样本、少样本和检索增强设置。AppSelectBench覆盖了100个广泛使用的桌面应用程序和超过十万个真实、多样且语义连贯的用户任务。

Result: 在闭源和开源大型语言模型上的大量实验揭示了应用程序间推理的系统性优缺点，表明即使是最有能力的模型在做出一致的应用程序选择时仍然存在困难。

Conclusion: AppSelectBench为研究和推进应用程序级别的推理奠定了基础，这是智能CUA一个重要但未被充分探索的能力。

Abstract: Computer Using Agents (CUAs) are increasingly equipped with external tools, enabling them to perform complex and realistic tasks. For CUAs to operate effectively, application selection, which refers to deciding which application to use before invoking fine-grained tools such as APIs, is a fundamental capability. It determines whether the agent initializes the correct environment, avoids orchestration confusion, and efficiently focuses on relevant context. However, existing benchmarks primarily assess fine-grained API selection, offering limited insight into whether models can reason across and choose between different applications. To fill this gap, we introduce AppSelectBench, a comprehensive benchmark for evaluating application selection in CUAs. AppSelectBench contains a novel user task generation pipeline that produces realistic, diverse, and semantically grounded user intents at scale, together with unified evaluation protocols covering random, heuristic, zero-shot, few-shot, and retrieval-augmented-settings. AppSelectBench covers one hundred widely used desktop applications and includes more than one hundred thousand realistic, diverse, and semantically grounded user tasks. Extensive experiments across both closed-source and open-source large language models reveal systematic strengths and weaknesses in inter-application reasoning, showing that even the most capable models still struggle to make consistent application choices. Together, these results establish AppSelectBench as a foundation for studying and advancing application level reasoning, an essential yet underexplored capability of intelligent CUAs. The source is available at https://github.com/microsoft/appselectbench.

</details>


### [11] [$\text{R}^2\text{R}$: A Route-to-Rerank Post-Training Framework for Multi-Domain Decoder-Only Rerankers](https://arxiv.org/abs/2511.19987)
*Xinyu Wang,Hanwei Wu,Qingchen Hu,Zhenghan Tai,Jingrui Tian,Lei Ding,Jijun Chi,Hailin He,Tung Sum Thomas Kwok,Yufei Cui,Sicheng Lyu,Muzhi Li,Mingze Li,Xinyue Yu,Ling Zhou,Peng Lu*

Main category: cs.CL

TL;DR: R2R框架通过动态专家路由和两阶段训练策略EAG，解决了检索增强生成（RAG）中解码器重排序器在特定领域中缺乏细致性和微调导致的过拟合和灾难性遗忘问题。


<details>
  <summary>Details</summary>
Motivation: 解决通用模型在金融和法律等高风险领域中，因缺乏领域特定细微差别而导致的性能不佳，以及简单微调引起的表面形式过拟合和灾难性遗忘问题。

Method: R2R框架结合了动态专家路由和两阶段训练策略EAG。EAG通过掩盖最具预测性的表面线索，强制重排序器学习领域不变的相关性模式，而不是记忆数据集特定的实体。为了高效激活领域专家，R2R使用轻量级潜在语义路由器，探测冻结主干解码器的内部表示，为每个查询选择最优的LoRA专家。

Result: 在不同重排序器主干和多样领域（法律、医疗和金融）的广泛实验表明，R2R持续超越了通用模型和单领域微调的基线模型。

Conclusion: R2R是一种模型不可知且模块化的方法，用于领域专业化，具有强大的跨领域鲁棒性。

Abstract: Decoder-only rerankers are central to Retrieval-Augmented Generation (RAG). However, generalist models miss domain-specific nuances in high-stakes fields like finance and law, and naive fine-tuning causes surface-form overfitting and catastrophic forgetting. To address this challenge, we introduce R2R, a domain-aware framework that combines dynamic expert routing with a two-stage training strategy, Entity Abstraction for Generalization (EAG). EAG introduces a counter-shortcut mechanism by masking the most predictive surface cues, forcing the reranker to learn domain-invariant relevance patterns rather than memorizing dataset-specific entities. To efficiently activate domain experts, R2R employs a lightweight Latent Semantic Router that probes internal representations from the frozen backbone decoder to select the optimal LoRA expert per query. Extensive experiments across different reranker backbones and diverse domains (legal, medical, and financial) demonstrate that R2R consistently surpasses generalist and single-domain fine-tuned baselines. Our results confirm that R2R is a model-agnostic and modular approach to domain specialization with strong cross-domain robustness.

</details>


### [12] [Directional Optimization Asymmetry in Transformers: A Synthetic Stress Test](https://arxiv.org/abs/2511.19997)
*Mihir Sahasrabudhe*

Main category: cs.CL

TL;DR: 尽管Transformer在理论上具有逆转不变性，但在语言任务中却表现出“逆转诅咒”。本文通过一个合成的、熵控的基准测试，揭示了这种方向性学习障碍并非源于语言统计，而是模型架构本身固有的问题。


<details>
  <summary>Details</summary>
Motivation: Transformer在处理自然语言时普遍存在“逆转诅咒”现象，即对从左到右的映射学习能力优于从右到左。研究旨在探究这种方向性学习的失败是源于语言统计特性还是模型架构本身。同时，也有研究指出，大型语言模型中的时间不对称性表明真实世界的语料库带有其自身的时间方向性。因此，需要一个明确的实验来区分这些因素。

Method: 本文使用了一个合成的、熵控的基准测试，该测试设计为一个“洁净室”压力测试，用于评估方向性学习能力。通过具有可调分支因子K的随机字符串映射，构建了条件熵为零的正向任务和具有分析确定熵底限的逆向任务。通过比较模型在这些任务上的损失与熵底限，来量化方向性学习的差距。实验中使用了从头开始训练的GPT-2模型、在相同数据上训练的MLP，并考察了预训练初始化和LoRA对结果的影响。

Result: 实验结果表明，即使是从头开始训练的GPT-2模型也表现出强烈的、可复现的方向性优化差距（例如，K=5时为1.16纳特），这个差距远大于在相同数据上训练的MLP模型。预训练的初始化虽然改变了优化行为，但未能消除这个差距。LoRA在处理高熵逆向映射时遇到了明显的容量瓶颈。这些结果揭示了因果Transformer训练中固有的、不依赖语义的方向性摩擦，即使排除了语言先验、token频率和语料库级别的时间不对称性，这种摩擦依然存在。

Conclusion: Transformer模型在方向性学习上存在固有的困难，这种困难并非源于语言统计特性，而是模型架构本身。这种方向性偏差是因果Transformer训练的一个基本特性，即使在控制了各种外部因素后依然存在。本文提出的基准测试提供了一个受控的工具，用于剖析现代序列模型中的方向性偏置，并为深入研究Transformer中为何逆向学习更困难提供了动力。

Abstract: Transformers are theoretically reversal-invariant: their function class does not prefer left-to-right over right-to-left mappings. Yet empirical studies on natural language repeatedly report a "reversal curse," and recent work on temporal asymmetry in LLMs suggests that real-world corpora carry their own arrow of time. This leaves an unresolved question: do directional failures stem from linguistic statistics, or from the architecture itself? We cut through this ambiguity with a fully synthetic, entropy-controlled benchmark designed as a clean-room stress test for directional learning. Using random string mappings with tunable branching factor K, we construct forward tasks with zero conditional entropy and inverse tasks with analytically determined entropy floors. Excess loss above these floors reveals that even scratch-trained GPT-2 models exhibit a strong, reproducible directional optimization gap (e.g., 1.16 nats at K=5), far larger than that of an MLP trained on the same data. Pre-trained initializations shift optimization behavior but do not eliminate this gap, while LoRA encounters a sharp capacity wall on high-entropy inverse mappings. Together, these results isolate a minimal, semantics-free signature of directional friction intrinsic to causal Transformer training-one that persists even when linguistic priors, token frequencies, and corpus-level temporal asymmetries are removed. Our benchmark provides a controlled instrument for dissecting directional biases in modern sequence models and motivates deeper mechanistic study of why inversion remains fundamentally harder for Transformers.

</details>


### [13] [Online-PVLM: Advancing Personalized VLMs with Online Concept Learning](https://arxiv.org/abs/2511.20056)
*Huiyu Bai,Runze Wang,Zhuoyun Du,Yiyang Zhao,Fengji Zhang,Haoyu Chen,Xiaoyong Zhu,Bo Zheng,Xuejiao Zhao*

Main category: cs.CL

TL;DR: 这篇论文介绍了一个名为Online-PVLM的框架，它利用双曲表示进行在线概念学习，旨在解决现有个性化视觉语言模型（PVLMs）在处理大规模用户特定概念时效率和扩展性的问题。作者还开发了一个名为OP-Eval的基准测试来评估在线概念学习。


<details>
  <summary>Details</summary>
Motivation: 现有的个性化视觉语言模型（PVLMs）在处理大规模用户特定概念时存在效率和扩展性问题。具体而言，现有方法通常需要为每个新概念学习单独的嵌入，这导致在测试时无法实现实时适应。在大规模场景中，概念嵌入的高效检索难以实现，这限制了PVLMs的实际应用。

Method: 本文提出了一个名为Online-PVLM的框架，该框架利用双曲表示进行在线概念学习。Online-PVLM采用了一种在测试时无需训练的概念嵌入生成范式，旨在提高个性化视觉语言模型的可扩展性和效率。此外，作者还开发了一个名为OP-Eval的基准测试，其中包含1292个概念和3万多高质量实例以及多种问题类型，用于严格评估在线概念学习在现实场景中的表现。

Result: 广泛的实验证明，Online-PVLM框架在在线概念学习方面取得了最先进的性能。

Conclusion: Online-PVLM通过引入在线概念学习和双曲表示，成功解决了现有PVLMs在处理大规模用户特定概念时效率和扩展性的挑战。新提出的无需训练的概念嵌入生成范式使得个性化视觉语言模型在测试时能够实现实时适应，并且OP-Eval基准测试为在线概念学习的严格评估提供了工具，从而推动了该领域的发展。

Abstract: Personalized Visual Language Models (VLMs) are gaining increasing attention for their formidable ability in user-specific concepts aligned interactions (e.g., identifying a user's bike). Existing methods typically require the learning of separate embeddings for each new concept, which fails to support real-time adaptation during testing. This limitation becomes particularly pronounced in large-scale scenarios, where efficient retrieval of concept embeddings is not achievable. To alleviate this gap, we propose Online-PVLM, a framework for online concept learning by leveraging hyperbolic representations. Our approach makes a train-free paradigm for concept embeddings generation at test time, making the use of personalized VLMs both scalable and efficient. In addition, we develop OP-Eval, a comprehensive and large-scale benchmark comprising 1,292 concepts and over 30K high-quality instances with diverse question types, designed to rigorously assess online concept learning in realistic scenarios. Extensive experiments demonstrate the state-of-the-art performance of our proposed framework. Our source code and dataset will be made available.

</details>


### [14] [MTA: A Merge-then-Adapt Framework for Personalized Large Language Model](https://arxiv.org/abs/2511.20072)
*Xiaopeng Li,Yuanjin Zheng,Wanyu Wang,wenlin zhang,Pengyue Jia,Yiqi Wang,Maolin Wang,Xuetao Wei,Xiangyu Zhao*

Main category: cs.CL

TL;DR: MTA是一种解决个性化大型语言模型（PLLMs）中存储成本和数据稀疏性问题的方法，它通过元LoRA银行、自适应LoRA融合和少量样本个性化LoRA堆叠来实现可扩展和动态的个性化。


<details>
  <summary>Details</summary>
Motivation: 现有的个性化大型语言模型（PLLMs）方法面临存储成本高（随用户数量线性增长）和数据稀疏性下微调效果不佳的问题。

Method: MTA框架包括三个阶段：1. 构建一个共享的元LoRA银行，通过选择锚定用户并预训练元个性化特征；2. 引入自适应LoRA融合阶段，检索并动态合并最相关的锚定元LoRA以合成用户特定的LoRA，从而无需为每个用户单独存储；3. 提出LoRA堆叠用于少量样本个性化阶段，在合并的LoRA之上应用一个超低秩、轻量级的LoRA模块进行微调，以实现少量样本设置下的有效个性化。

Result: 在LaMP基准测试上的大量实验表明，MTA方法在多个任务上优于现有的SOTA方法。

Conclusion: MTA框架通过元LoRA银行、自适应LoRA融合以及少量样本个性化LoRA堆叠，有效解决了PLLMs面临的存储成本和数据稀疏性问题，实现了可扩展且动态的个性化，并在实验中取得了优于现有SOTA方法的性能。

Abstract: Personalized Large Language Models (PLLMs) aim to align model outputs with individual user preferences, a crucial capability for user-centric applications. However, the prevalent approach of fine-tuning a separate module for each user faces two major limitations: (1) storage costs scale linearly with the number of users, rendering the method unscalable; and (2) fine-tuning a static model from scratch often yields suboptimal performance for users with sparse data. To address these challenges, we propose MTA, a Merge-then-Adapt framework for PLLMs. MTA comprises three key stages. First, we construct a shared Meta-LoRA Bank by selecting anchor users and pre-training meta-personalization traits within meta-LoRA modules. Second, to ensure scalability and enable dynamic personalization combination beyond static models, we introduce an Adaptive LoRA Fusion stage. This stage retrieves and dynamically merges the most relevant anchor meta-LoRAs to synthesize a user-specific one, thereby eliminating the need for user-specific storage and supporting more flexible personalization. Third, we propose a LoRA Stacking for Few-Shot Personalization stage, which applies an additional ultra-low-rank, lightweight LoRA module on top of the merged LoRA. Fine-tuning this module enables effective personalization under few-shot settings. Extensive experiments on the LaMP benchmark demonstrate that our approach outperforms existing SOTA methods across multiple tasks.

</details>


### [15] [More Bias, Less Bias: BiasPrompting for Enhanced Multiple-Choice Question Answering](https://arxiv.org/abs/2511.20086)
*Duc Anh Vu,Thong Nguyen,Cong-Duy Nguyen,Viet Anh Nguyen,Anh Tuan Luu*

Main category: cs.CL

TL;DR: BiasPrompting是一种新颖的推理框架，可以引导LLM生成和批判性评估所有合理答案选项的推理，然后再进行最终预测，从而显著提高LLMs在多项选择题任务中的性能。


<details>
  <summary>Details</summary>
Motivation: 现有的方法缺乏上下文基础或解释，导致对所有可能答案的探索不完整，最终降低了模型的推理能力。

Method: BiasPrompting框架包含两个组件：首先是推理生成阶段，模型被提示为每个答案选项生成支持性推理；其次是推理指导的协议阶段，综合生成的推理以选择最合理的答案。

Result: BiasPrompting在五个广泛使用的多项选择问答基准测试中表现出显著改进。

Conclusion: BiasPrompting增强了LLMs的推理能力，并为解决复杂和具有挑战性的问题提供了坚实的基础。

Abstract: With the advancement of large language models (LLMs), their performance on multiple-choice question (MCQ) tasks has improved significantly. However, existing approaches face key limitations: answer choices are typically presented to LLMs without contextual grounding or explanation. This absence of context can lead to incomplete exploration of all possible answers, ultimately degrading the models' reasoning capabilities. To address these challenges, we introduce BiasPrompting, a novel inference framework that guides LLMs to generate and critically evaluate reasoning across all plausible answer options before reaching a final prediction. It consists of two components: first, a reasoning generation stage, where the model is prompted to produce supportive reasonings for each answer option, and then, a reasoning-guided agreement stage, where the generated reasonings are synthesized to select the most plausible answer. Through comprehensive evaluations, BiasPrompting demonstrates significant improvements in five widely used multiple-choice question answering benchmarks. Our experiments showcase that BiasPrompting enhances the reasoning capabilities of LLMs and provides a strong foundation for tackling complex and challenging questions, particularly in settings where existing methods underperform.

</details>


### [16] [SSA: Sparse Sparse Attention by Aligning Full and Sparse Attention Outputs in Feature Space](https://arxiv.org/abs/2511.20102)
*Zhenyi Shen,Junru Lu,Lin Gui,Jiazheng Li,Yulan He,Di Yin,Xing Sun*

Main category: cs.CL

TL;DR: 为了解决大型语言模型中长文本处理的二次复杂度问题，本文提出了SSA（Sparse Sparse Attention）框架。SSA通过结合稀疏注意力和完全注意力，并在每一层强制执行双向对齐，解决了现有稀疏注意力方法中普遍存在的梯度更新不足的问题。SSA在多个常识性基准测试中取得了最先进的性能，并能平稳适应不同的稀疏度预算，同时显著提高了长文本外推能力。


<details>
  <summary>Details</summary>
Motivation: 大型语言模型（LLMs）在处理长文本时，由于完全注意力的二次复杂度，效率受到限制。现有的稀疏注意力方法虽然能缓解成本问题，但训练时无感知的方法常导致性能严重下降。原生稀疏注意力方法（如NSA、MoBA）也存在一个关键悖论：它们产生的注意力稀疏性比完全注意力模型更低，但目标却是近似完全注意力，这限制了它们的有效性。

Method: 我们提出了SSA（Sparse Sparse Attention）框架，这是一个统一的训练框架，它同时考虑了稀疏注意力和完全注意力，并在每一层强制执行双向对齐。这种设计保留了所有token的梯度流，同时明确鼓励稀疏注意力输出与完全注意力输出对齐，从而促进更强的稀疏性。

Result: SSA在多个常识性基准测试中，无论是在稀疏注意力还是完全注意力推理下，都取得了最先进的性能。此外，SSA使模型能够平稳地适应不同的稀疏度预算；随着允许关注的token数量增加，性能持续提升，支持了推理时灵活的计算-性能权衡。最后，我们发现原生稀疏注意力训练通过减轻sink区域中注意力值的过度分配，出人意料地提高了长文本外推能力，其中SSA表现出最强的外推能力。

Conclusion: SSA通过解决梯度更新不足的问题，实现了更好的稀疏性能和长文本外推能力，在大型语言模型长文本处理方面具有显著优势。

Abstract: The quadratic complexity of full attention limits efficient long-context processing in large language models (LLMs). Sparse attention mitigates this cost by restricting each query to attend to a subset of previous tokens; however, training-free approaches often lead to severe performance degradation. Native sparse-attention methods (e.g., NSA, MoBA) alleviate this issue, yet exhibit a critical paradox: they produce lower attention sparsity than full-attention models, despite aiming to approximate full attention, which may constrain their effectiveness. We attribute this paradox to gradient update deficiency: low-ranked key-value pairs excluded during sparse training receive neither forward contribution nor backward gradients, and thus never learn proper suppression. To overcome this limitation, we propose SSA (Sparse Sparse Attention), a unified training framework that considers both sparse and full attention and enforces bidirectional alignment at every layer. This design preserves gradient flow to all tokens while explicitly encouraging sparse-attention outputs to align with their full-attention counterparts, thereby promoting stronger sparsity. As a result, SSA achieves state-of-the-art performance under both sparse and full attention inference across multiple commonsense benchmarks. Furthermore, SSA enables models to adapt smoothly to varying sparsity budgets; performance improves consistently as more tokens are allowed to attend, supporting flexible compute-performance trade-offs at inference time. Finally, we show that native sparse-attention training surprisingly improves long-context extrapolation by mitigating the over-allocation of attention values in sink areas, with SSA demonstrating the strongest extrapolation capability.

</details>


### [17] [EM2LDL: A Multilingual Speech Corpus for Mixed Emotion Recognition through Label Distribution Learning](https://arxiv.org/abs/2511.20106)
*Xingfeng Li,Xiaohan Shi,Junjie Li,Yongwei Li,Masashi Unoki,Tomoki Toda,Masato Akagi*

Main category: cs.CL

TL;DR: EM2LDL是一个新颖的多语言语音语料库，专注于通过标签分布学习进行混合情感识别，解决了现有语料库的局限性，提供了更多的语言多样性和生态有效性。


<details>
  <summary>Details</summary>
Motivation: 现有的情感语料库主要是单语言和单标签的，限制了语言多样性，无法有效建模混合情感，且缺乏生态有效性。

Method: EM2LDL语料库包含英语、普通话和粤语的表达性话语，捕捉了多语言地区（如香港和澳门）普遍存在的语内语码转换。该语料库整合了来自在线平台的自发情感表达，并用32个类别的细粒度情感分布进行标注。

Result: 使用自监督学习模型进行的实验基线测试在与说话人无关的性别、年龄和个性评估中表现出稳健的性能，其中HuBERT-large-EN取得了最佳结果。

Conclusion: 通过融入语言多样性和生态有效性，EM2LDL使得在多语言环境中探索复杂的情感动态成为可能。这项工作为开发适应性强、具有同理心的系统提供了多功能测试平台，适用于情感计算领域的应用，包括心理健康监测和跨文化交流。

Abstract: This study introduces EM2LDL, a novel multilingual speech corpus designed to advance mixed emotion recognition through label distribution learning. Addressing the limitations of predominantly monolingual and single-label emotion corpora \textcolor{black}{that restrict linguistic diversity, are unable to model mixed emotions, and lack ecological validity}, EM2LDL comprises expressive utterances in English, Mandarin, and Cantonese, capturing the intra-utterance code-switching prevalent in multilingual regions like Hong Kong and Macao. The corpus integrates spontaneous emotional expressions from online platforms, annotated with fine-grained emotion distributions across 32 categories. Experimental baselines using self-supervised learning models demonstrate robust performance in speaker-independent gender-, age-, and personality-based evaluations, with HuBERT-large-EN achieving optimal results. By incorporating linguistic diversity and ecological validity, EM2LDL enables the exploration of complex emotional dynamics in multilingual settings. This work provides a versatile testbed for developing adaptive, empathetic systems for applications in affective computing, including mental health monitoring and cross-cultural communication. The dataset, annotations, and baseline codes are publicly available at https://github.com/xingfengli/EM2LDL.

</details>


### [18] [Mispronunciation Detection and Diagnosis Without Model Training: A Retrieval-Based Approach](https://arxiv.org/abs/2511.20107)
*Huu Tuong Tu,Ha Viet Khanh,Tran Tien Dat,Vu Huan,Thien Van Luong,Nguyen Tien Cuong,Nguyen Thi Thu Trang*

Main category: cs.CL

TL;DR: 这篇论文提出了一种无需训练的基于检索的语音错误检测与诊断框架，该框架利用预训练的自动语音识别模型，实现了语音错误检测和诊断的高准确性。


<details>
  <summary>Details</summary>
Motivation: 传统的语音错误检测与诊断方法需要评分模型或音素级别模型的训练，而本文旨在避免这些复杂性，提出一种无需训练的新方法。

Method: 本文提出了一种无需训练的框架，该框架通过结合检索技术和预训练的自动语音识别模型来检测和诊断发音错误。该方法避免了音素特异性建模或额外的任务特定训练。

Result: 在L2-ARCTIC数据集上的实验表明，本文提出的方法在不增加模型训练复杂性的前提下，F1分数达到了69.60%，表现优于现有方法。

Conclusion: 本文成功地提出了一种无需训练、基于检索的语音错误检测与诊断框架，该框架在保持高准确性的同时，显著降低了实现复杂性，为语言学习和语音治疗提供了新的途径。

Abstract: Mispronunciation Detection and Diagnosis (MDD) is crucial for language learning and speech therapy. Unlike conventional methods that require scoring models or training phoneme-level models, we propose a novel training-free framework that leverages retrieval techniques with a pretrained Automatic Speech Recognition model. Our method avoids phoneme-specific modeling or additional task-specific training, while still achieving accurate detection and diagnosis of pronunciation errors. Experiments on the L2-ARCTIC dataset show that our method achieves a superior F1 score of 69.60% while avoiding the complexity of model training.

</details>


### [19] ["When Data is Scarce, Prompt Smarter"... Approaches to Grammatical Error Correction in Low-Resource Settings](https://arxiv.org/abs/2511.20120)
*Somsubhra De,Harsh Kumar,Arun Prakash A*

Main category: cs.CL

TL;DR: 这篇论文探讨了如何利用大型语言模型（LLMs）和提示词方法来解决低资源印度语言的语法错误纠正（GEC）问题。


<details>
  <summary>Details</summary>
Motivation: 尽管基于Transformer的模型在英语等高资源语言的GEC方面取得了显著进展，但对于大多数印度语言来说，由于资源有限、语言多样性和复杂的形态学，GEC仍然是一个具有挑战性的任务。

Method: 本文探索了使用最先进的大型语言模型（如GPT-4.1、Gemini-2.5和LLaMA-4）与few-shot策略相结合的提示词方法，以适应低资源设置。

Result: 即使是基本的提示词策略（如zero-shot和few-shot方法），也能使这些LLMs显著优于微调的印度语模型（如Sarvam-22B），展示了当代LLMs在GEC方面的卓越多语言泛化能力。精心设计的提示词和轻量级适配显著提高了多种印度语言的纠正质量，并在共享任务中取得了领先结果。

Conclusion: 这些发现强调了提示词驱动的自然语言处理技术的有效性，并突出了大型LLMs在弥补多语言GEC资源差距方面的潜力。

Abstract: Grammatical error correction (GEC) is an important task in Natural Language Processing that aims to automatically detect and correct grammatical mistakes in text. While recent advances in transformer-based models and large annotated datasets have greatly improved GEC performance for high-resource languages such as English, the progress has not extended equally. For most Indic languages, GEC remains a challenging task due to limited resources, linguistic diversity and complex morphology. In this work, we explore prompting-based approaches using state-of-the-art large language models (LLMs), such as GPT-4.1, Gemini-2.5 and LLaMA-4, combined with few-shot strategy to adapt them to low-resource settings. We observe that even basic prompting strategies, such as zero-shot and few-shot approaches, enable these LLMs to substantially outperform fine-tuned Indic-language models like Sarvam-22B, thereby illustrating the exceptional multilingual generalization capabilities of contemporary LLMs for GEC. Our experiments show that carefully designed prompts and lightweight adaptation significantly enhance correction quality across multiple Indic languages. We achieved leading results in the shared task--ranking 1st in Tamil (GLEU: 91.57) and Hindi (GLEU: 85.69), 2nd in Telugu (GLEU: 85.22), 4th in Bangla (GLEU: 92.86), and 5th in Malayalam (GLEU: 92.97). These findings highlight the effectiveness of prompt-driven NLP techniques and underscore the potential of large-scale LLMs to bridge resource gaps in multilingual GEC.

</details>


### [20] [SEDA: A Self-Adapted Entity-Centric Data Augmentation for Boosting Gird-based Discontinuous NER Models](https://arxiv.org/abs/2511.20143)
*Wen-Fang Su,Hsiao-Wei Chou,Wen-Yang Lin*

Main category: cs.CL

TL;DR: 该论文提出了一种新的基于网格的命名实体识别（NER）方法，该方法通过集成图像数据增强技术来有效识别非连续实体，尤其是在跨句子的情况下。


<details>
  <summary>Details</summary>
Motivation: 传统的命名实体识别方法在处理非连续实体时存在文本分割困难和漏识别的问题，尤其是在跨句子非连续实体方面，严重影响了识别准确性。

Method: 我们整合了图像数据增强技术，如裁剪、缩放和填充，到基于网格的模型中，以增强其识别非连续实体的能力并解决分割挑战。

Result: 实验结果表明，传统分割方法未能捕获跨句子非连续实体，导致性能下降。相比之下，我们增强的网格模型显著提高了性能。

Conclusion: 在CADEC、ShARe13和ShARe14数据集上的评估显示，F1分数整体提高了1-2.5%，对于非连续实体则提高了3.7-8.4%，证实了我们方法的有效性。

Abstract: Named Entity Recognition (NER) is a critical task in natural language processing, yet it remains particularly challenging for discontinuous entities. The primary difficulty lies in text segmentation, as traditional methods often missegment or entirely miss cross-sentence discontinuous entities, significantly affecting recognition accuracy. Therefore, we aim to address the segmentation and omission issues associated with such entities. Recent studies have shown that grid-tagging methods are effective for information extraction due to their flexible tagging schemes and robust architectures. Building on this, we integrate image data augmentation techniques, such as cropping, scaling, and padding, into grid-based models to enhance their ability to recognize discontinuous entities and handle segmentation challenges. Experimental results demonstrate that traditional segmentation methods often fail to capture cross-sentence discontinuous entities, leading to decreased performance. In contrast, our augmented grid models achieve notable improvements. Evaluations on the CADEC, ShARe13, and ShARe14 datasets show F1 score gains of 1-2.5% overall and 3.7-8.4% for discontinuous entities, confirming the effectiveness of our approach.

</details>


### [21] [KyrgyzBERT: A Compact, Efficient Language Model for Kyrgyz NLP](https://arxiv.org/abs/2511.20182)
*Adilet Metinov,Gulida M. Kudakeeva,Gulnara D. Kabaeva*

Main category: cs.CL

TL;DR: 吉尔吉斯语BERT（KyrgyzBERT）是为资源稀缺的吉尔吉斯语开发的第一个公开单语BERT模型。


<details>
  <summary>Details</summary>
Motivation: 吉尔吉斯语作为一种低资源语言，其自然语言处理（NLP）工具非常有限。

Method: 开发了一个参数量为35.9M的单语BERT模型KyrgyzBERT，并采用了针对吉尔吉斯语形态结构定制的分词器。为了评估模型性能，创建了kgyrgz-sst2情感分析基准数据集，该数据集通过翻译斯坦福情感树库并手动标注完整的测试集构建。

Result: 在kgyrgz-sst2数据集上对KyrgyzBERT进行微调，其F1分数达到0.8280。这个性能与参数量是其五倍的微调多语言BERT模型（mBERT）具有竞争力。

Conclusion: KyrgyzBERT作为吉尔吉斯语的第一个公开BERT模型，取得了显著的性能，并为吉尔吉斯语NLP的未来研究提供了模型、数据和代码支持。

Abstract: Kyrgyz remains a low-resource language with limited foundational NLP tools. To address this gap, we introduce KyrgyzBERT, the first publicly available monolingual BERT-based language model for Kyrgyz. The model has 35.9M parameters and uses a custom tokenizer designed for the language's morphological structure. To evaluate performance, we create kyrgyz-sst2, a sentiment analysis benchmark built by translating the Stanford Sentiment Treebank and manually annotating the full test set. KyrgyzBERT fine-tuned on this dataset achieves an F1-score of 0.8280, competitive with a fine-tuned mBERT model five times larger. All models, data, and code are released to support future research in Kyrgyz NLP.

</details>


### [22] [REFLEX: Self-Refining Explainable Fact-Checking via Disentangling Truth into Style and Substance](https://arxiv.org/abs/2511.20233)
*Chuyi Kong,Gao Wei,Jing Ma,Hongzhan Lin,Zhiyuan Fan*

Main category: cs.CL

TL;DR: REFLEX范式通过利用骨干模型中的内部知识来提高裁决准确性和解释质量，它将事实核查重新构建为角色扮演对话，并联合训练裁决预测和解释生成，自适应地提取骨干模型与其微调变体之间的对比激活对来构建引导向量，从而实现最先进的性能。


<details>
  <summary>Details</summary>
Motivation: 现有的基于大型语言模型的自动化事实核查系统过度依赖外部知识源，导致延迟和幻觉，损害了可靠性、可解释性和响应性，这对于实时使用至关重要。

Method: REFLEX范式将事实核查重新定义为角色扮演对话，并联合训练裁决预测和解释生成。通过自适应地提取骨干模型及其微调变体之间的对比激活对，构建引导向量，从而将事实解耦为风格和实质。这些激活层面的信号能够引导推理，抑制嘈杂的解释，从而实现更忠实和高效的推理。

Result: REFLEX在真实世界数据集上的实验表明，它优于以前的单一事实方向引导方法，并在处理事实核查任务中人类未知的事实时表现出色。REFLEX仅用465个自完善的训练样本就实现了最先进的性能。此外，通过解释性目标训练的模型可以有效地指导没有解释性目标的模型，最高可提高7.57%。

Conclusion: REFLEX范式通过利用骨干模型中的内部知识，有效解决了现有LLM事实核查方法中存在的延迟、幻觉和可靠性问题。通过将事实核查重新构建为角色扮演对话并联合训练裁决预测和解释生成，REFLEX在准确性和解释质量方面都取得了显著的提升，为自动化事实核查领域提供了新的SOTA解决方案。

Abstract: The prevalence of misinformation on social media threatens public trust, demanding automated fact-checking systems that provide accurate verdicts with interpretable explanations. However, existing large language model-based (LLM-based) approaches often rely heavily on external knowledge sources, introducing substantial latency and even hallucinations that undermine reliability, interpretability, and responsiveness, which is crucial for real-time use. To address these challenges, we propose REason-guided Fact-checking with Latent EXplanations REFLEX paradigm, a plug-and-play, self-refining paradigm that leverages the internal knowledge in backbone model to improve both verdict accuracy and explanation quality. REFLEX reformulates fact-checking as a role-play dialogue and jointly trains verdict prediction and explanation generation. It adaptively extracts contrastive activation pairs between the backbone model and its fine-tuned variant to construct steering vectors that disentangle truth into style and substance naturally. These activation-level signals guide inference and suppress noisy explanations, enabling more faithful and efficient reasoning. Experiments on real-world datasets show that REFLEX outperforms previous methods that steer toward a single truth direction and underscores the challenge traditional approaches face when handling the subtle, human-unknown truth in fact-checking tasks. Remarkably, with only 465 self-refined training samples, RELFEX achieves state-of-the-art performance. Furthermore, models trained with explanatory objectives can effectively guide those without them, yielding up to a 7.57% improvement, highlighting that internal explanation signals play a dual role in both interpreting and enhancing factual reasoning.

</details>


### [23] [Scaling LLM Speculative Decoding: Non-Autoregressive Forecasting in Large-Batch Scenarios](https://arxiv.org/abs/2511.20340)
*Luohe Shi,Zuchao Li,Lefei Zhang,Baoyuan Qi,Guoming Liu,Hai Zhao*

Main category: cs.CL

TL;DR: SpecFormer通过结合单向和双向注意力机制，实现了LLM推理的加速，即使在批处理场景下也能有效地减少计算成本和训练需求。


<details>
  <summary>Details</summary>
Motivation: 目前的推测解码方法在计算资源和调度成本上存在挑战，尤其是在与批处理等方法结合时，因此需要一种新的方法来在低验证资源和低调度成本下进行推测解码。

Method: 本文提出了SpecFormer架构，它结合了单向和双向注意力机制，使得模型能够并行生成草稿序列。SpecFormer利用自回归模型从整个输入序列中提取信息的能力，并结合了非自回归模型的并行生成优势，从而避免了对大型前缀树的依赖。

Result: 通过在不同规模模型上的无损推测解码实验，SpecFormer实现了持续加速，并在大型批处理场景下表现出色。它为LLM推理的扩展设定了新标准，同时降低了训练需求和计算成本。

Conclusion: SpecFormer通过其独特的架构和并行生成能力，有效地解决了当前推测解码方法的局限性，在LLM推理加速方面提供了一个高效且低成本的解决方案。

Abstract: Speculative decoding accelerates LLM inference by utilizing otherwise idle computational resources during memory-to-chip data transfer. Current speculative decoding methods typically assume a considerable amount of available computing power, then generate a complex and massive draft tree using a small autoregressive language model to improve overall prediction accuracy. However, methods like batching have been widely applied in mainstream model inference systems as a superior alternative to speculative decoding, as they compress the available idle computing power. Therefore, performing speculative decoding with low verification resources and low scheduling costs has become an important research problem. We believe that more capable models that allow for parallel generation on draft sequences are what we truly need. Recognizing the fundamental nature of draft models to only generate sequences of limited length, we propose SpecFormer, a novel architecture that integrates unidirectional and bidirectional attention mechanisms. SpecFormer combines the autoregressive model's ability to extract information from the entire input sequence with the parallel generation benefits of non-autoregressive models. This design eliminates the reliance on large prefix trees and achieves consistent acceleration, even in large-batch scenarios. Through lossless speculative decoding experiments across models of various scales, we demonstrate that SpecFormer sets a new standard for scaling LLM inference with lower training demands and reduced computational costs.

</details>


### [24] [The Curious Case of Analogies: Investigating Analogical Reasoning in Large Language Models](https://arxiv.org/abs/2511.20344)
*Taewhoo Lee,Minju Song,Chanwoong Yoon,Jungwoo Park,Jaewoo Kang*

Main category: cs.CL

TL;DR: 研究了大型语言模型（LLMs）进行类比推理的能力，发现在编码高层关系概念和将其应用于新情境方面存在局限性，但也表现出一定的潜力。


<details>
  <summary>Details</summary>
Motivation: 探究LLMs是否能够编码高层关系概念并通过结构化比较将其应用于新情境，尽管以往研究表明LLMs可以表示任务模式和表层概念。

Method: 通过使用比例类比和故事类比，对LLMs的类比推理能力进行实验分析。

Result: 1. LLMs能有效编码类比实体间的潜在关系，属性和关系信息在正确情况下通过中上层传播，推理失败反映了这些层中关系信息的缺失。 2. LLMs在关系信息缺失时以及试图将其应用于新实体时表现不佳；策略性地修补关键token位置的隐藏表示可以在一定程度上促进信息传递。 3. LLMs成功的类比推理表现出类比情境之间强大的结构对齐，而失败则反映了对齐退化或错位。

Conclusion: LLMs在编码和应用高层关系概念方面表现出新兴但有限的能力，这揭示了与人类认知相似之处和差距。

Abstract: Analogical reasoning is at the core of human cognition, serving as an important foundation for a variety of intellectual activities. While prior work has shown that LLMs can represent task patterns and surface-level concepts, it remains unclear whether these models can encode high-level relational concepts and apply them to novel situations through structured comparisons. In this work, we explore this fundamental aspect using proportional and story analogies, and identify three key findings. First, LLMs effectively encode the underlying relationships between analogous entities; both attributive and relational information propagate through mid-upper layers in correct cases, whereas reasoning failures reflect missing relational information within these layers. Second, unlike humans, LLMs often struggle not only when relational information is missing, but also when attempting to apply it to new entities. In such cases, strategically patching hidden representations at critical token positions can facilitate information transfer to a certain extent. Lastly, successful analogical reasoning in LLMs is marked by strong structural alignment between analogous situations, whereas failures often reflect degraded or misplaced alignment. Overall, our findings reveal that LLMs exhibit emerging but limited capabilities in encoding and applying high-level relational concepts, highlighting both parallels and gaps with human cognition.

</details>


### [25] [BengaliFig: A Low-Resource Challenge for Figurative and Culturally Grounded Reasoning in Bengali](https://arxiv.org/abs/2511.20399)
*Abdullah Al Sefat*

Main category: cs.CL

TL;DR: 该研究推出了BengaliFig，这是一个针对孟加拉语中比喻和文化推理的挑战性数据集。


<details>
  <summary>Details</summary>
Motivation: 大型语言模型在多语言基准测试中表现出色，但在低资源环境下，其在比喻和文化推理方面的能力尚未得到广泛评估。

Method: BengaliFig数据集包含435个独特的孟加拉语谜语，每个谜语都从五个正交维度进行标注，并通过AI辅助流程自动转换为多项选择题。

Result: 在零样本和少样本思维链提示下，评估了八个前沿大型语言模型，结果显示模型在 HOC 抽象推理和文化特异性推理方面存在持续的弱点。

Conclusion: BengaliFig 为评估大型语言模型在低资源文化环境中的鲁棒性提供了一个诊断工具，并为包容性和文化遗产意识的自然语言处理评估迈出了一步。

Abstract: Large language models excel on broad multilingual benchmarks but remain to be evaluated extensively in figurative and culturally grounded reasoning, especially in low-resource contexts. We present BengaliFig, a compact yet richly annotated challenge set that targets this gap in Bengali, a widely spoken low-resourced language. The dataset contains 435 unique riddles drawn from Bengali oral and literary traditions. Each item is annotated along five orthogonal dimensions capturing reasoning type, trap type, cultural depth, answer category, and difficulty, and is automatically converted to multiple-choice format through a constraint-aware, AI-assisted pipeline. We evaluate eight frontier LLMs from major providers under zero-shot and few-shot chain-of-thought prompting, revealing consistent weaknesses in metaphorical and culturally specific reasoning. BengaliFig thus contributes both a diagnostic probe for evaluating LLM robustness in low-resource cultural contexts and a step toward inclusive and heritage-aware NLP evaluation.

</details>


### [26] [A Task-Oriented Evaluation Framework for Text Normalization in Modern NLP Pipelines](https://arxiv.org/abs/2511.20409)
*Md Abdullah Al Kafi,Raka Moni,Sumit Kumar Banshal*

Main category: cs.CL

TL;DR: 该研究提出了一种新的、以任务为导向的词干提取方法评估框架，该框架考虑了词干提取的效用（SES）、对下游任务的影响（MPD）以及词干提取词与原始词之间的语义相似性（ANLD），并应用于孟加拉语和英语词干提取器进行比较。


<details>
  <summary>Details</summary>
Motivation: 现有的词干提取评估方法有限，无法捕捉过度词干提取可能造成的危害，因此需要开发新的方法来评估词干提取方法。

Method: 提出了一种新颖的、面向任务的词干提取方法评估框架，该框架包括三个方面：1. 使用词干提取有效性分数（SES）衡量词干提取的效用。2. 使用模型性能增量（MPD）衡量词干提取对下游任务的影响。3. 使用平均归一化莱文斯坦距离（ANLD）衡量词干提取词与原始词之间的语义相似度。

Result: 孟加拉语词干提取器（BNLTK）的SES最高（1.67），词汇压缩率高（CR = 1.90），但由于有害的过度词干提取（ANLD = 0.26），导致下游任务性能下降。相比之下，英语词干提取器（Snowball）的SES适中（1.31），但意义距离安全（ANLD = 0.14），其词汇压缩对下游任务性能有积极贡献。

Conclusion: 该研究提供了一个有价值的工具，用于区分潜在的效率提升（高SES）和意义保留（低ANLD）。对于孟加拉语词干提取器，高SES是由于过度词干提取导致的，而英语词干提取器则更为可靠，因为它在适度的SES下保持了安全的意义距离。

Abstract: Text normalization is an essential preprocessing step in many natural language processing (NLP) tasks, and stemming is one such normalization technique that reduces words to their base or root form. However, evaluating stemming methods is challenging because current evaluation approaches are limited and do not capture the potential harm caused by excessive stemming; therefore, it is essential to develop new approaches to evaluate stemming methods. To address this issue, this study propose a novel, task-oriented approach to evaluate stemming methods, which considers three aspects: (1) the utility of stemming using Stemming Effectiveness Score (SES), (2) the impact of stemming on downstream tasks using Model Performance Delta (MPD), and (3) the semantic similarity between stemmed and original words using Average Normalized Levenshtein Distance (ANLD), thus providing a comprehensive evaluation framework. We apply our evaluation framework to compare two stemmers for Bangla (BNLTK) and English (Snowball), and our results reveal a significant issue, prompting us to analyze their performance in detail. While the Bangla stemmer achieves the highest SES (1.67) due to effective word reduction (CR = 1.90), SES alone is insufficient because our proposed safety measure, ANLD, reveals that this high SES is due to harmful over-stemming (ANLD = 0.26), which correlates with the observed decrease in downstream performance.In contrast, the English stemmer achieves a moderate SES (1.31) with a safe meaning distance (ANLD = 0.14), allowing its word reduction to contribute positively to downstream performance; therefore, it is a more reliable stemmer. Our study provides a valuable tool for distinguishing between potential efficiency gains (high SES) and meaning preservation (low ANLD).

</details>


<div id='stat.ML'></div>

# stat.ML [[Back]](#toc)

### [27] [Optimization and Regularization Under Arbitrary Objectives](https://arxiv.org/abs/2511.19628)
*Jared N. Lakhani,Etienne Pienaar*

Main category: stat.ML

TL;DR: 该研究探讨了马尔可夫链蒙特卡洛（MCMC）方法应用于任意目标函数的局限性，特别是在加强学习任务中，发现其性能受似然函数锐度的显著影响，并提出了混合方法以提高效率。


<details>
  <summary>Details</summary>
Motivation: 该研究旨在探究马尔可夫链蒙特卡洛（MCMC）方法在应用于任意目标函数时的局限性，特别是关注双块MCMC框架的性能如何受到似然函数形式锐度的影响。

Method: 1. 引入一个锐度参数，并探索与目标目标函数成比例的替代似然函数。2. 在 강화学习 任务上进行实证应用，包括导航问题和井字棋游戏。3. 对极限似然锐度对任意目标函数的影响进行了单独分析，通过用迭代优化步骤替换双块MCMC框架的第一块，应用于21点游戏。

Result: 1. MCMC方法的性能关键取决于所用似然函数形式的锐度。2. 似然曲率控制着样本内性能和训练数据推断的正则化程度。3. 极限似然锐度能有效地将后验质量坍缩到单一主导模式。4. 在21点游戏中，混合方法（MCMC结合迭代优化）实现了与原始MCMC框架几乎相同的性能。

Conclusion: MCMC方法在处理任意目标函数时，其性能和正则化程度受到似然函数锐度的显著影响。当似然函数过于尖锐时，后验分布会趋于单一模式。通过结合迭代优化，可以在保持性能的同时提高MCMC方法的效率。

Abstract: This study investigates the limitations of applying Markov Chain Monte Carlo (MCMC) methods to arbitrary objective functions, focusing on a two-block MCMC framework which alternates between Metropolis-Hastings and Gibbs sampling. While such approaches are often considered advantageous for enabling data-driven regularization, we show that their performance critically depends on the sharpness of the employed likelihood form. By introducing a sharpness parameter and exploring alternative likelihood formulations proportional to the target objective function, we demonstrate how likelihood curvature governs both in-sample performance and the degree of regularization inferred by the training data. Empirical applications are conducted on reinforcement learning tasks: including a navigation problem and the game of tic-tac-toe. The study concludes with a separate analysis examining the implications of extreme likelihood sharpness on arbitrary objective functions stemming from the classic game of blackjack, where the first block of the two-block MCMC framework is replaced with an iterative optimization step. The resulting hybrid approach achieves performance nearly identical to the original MCMC framework, indicating that excessive likelihood sharpness effectively collapses posterior mass onto a single dominant mode.

</details>


### [28] [Clustering Approaches for Mixed-Type Data: A Comparative Study](https://arxiv.org/abs/2511.19755)
*Badih Ghattas,Alvaro Sanchez San-Benito*

Main category: stat.ML

TL;DR: 本文比较和评估了处理混合类型数据的聚类算法，旨在为不同场景下的方法选择提供依据。


<details>
  <summary>Details</summary>
Motivation: 处理混合类型数据的聚类在无监督学习中面临挑战，现有方法有限，且在不同情境下的表现尚不明确。

Method: 本文比较了距离基方法（k-prototypes、PDQ、convex k-means）和概率方法（KAMILA、MBNs、LCM）共六种算法。通过改变聚类数量、聚类重叠度、样本大小、数据维度、数据集中连续变量比例和聚类分布等实验因素，在多种模拟模型下评估这些方法的性能。

Result: 聚类重叠程度、数据集中连续变量的比例和样本大小对算法性能有显著影响。当变量之间存在强交互作用且明显依赖于聚类成员身份时，所有评估方法的性能均不理想。KAMILA、LCM和k-prototypes在调整兰德指数（ARI）方面表现最佳。所有方法均可在R语言中使用。

Conclusion: 针对混合类型数据，KAMILA、LCM和k-prototypes是目前表现较好的聚类方法，但在特定复杂场景下所有方法的表现均有待提高。研究结果为在不同场景下选择合适的聚类方法提供了参考。

Abstract: Clustering is widely used in unsupervised learning to find homogeneous groups of observations within a dataset. However, clustering mixed-type data remains a challenge, as few existing approaches are suited for this task. This study presents the state-of-the-art of these approaches and compares them using various simulation models. The compared methods include the distance-based approaches k-prototypes, PDQ, and convex k-means, and the probabilistic methods KAy-means for MIxed LArge data (KAMILA), the mixture of Bayesian networks (MBNs), and latent class model (LCM). The aim is to provide insights into the behavior of different methods across a wide range of scenarios by varying some experimental factors such as the number of clusters, cluster overlap, sample size, dimension, proportion of continuous variables in the dataset, and clusters' distribution. The degree of cluster overlap and the proportion of continuous variables in the dataset and the sample size have a significant impact on the observed performances. When strong interactions exist between variables alongside an explicit dependence on cluster membership, none of the evaluated methods demonstrated satisfactory performance. In our experiments KAMILA, LCM, and k-prototypes exhibited the best performance, with respect to the adjusted rand index (ARI). All the methods are available in R.

</details>


<div id='cs.IT'></div>

# cs.IT [[Back]](#toc)

### [29] [The Quality of Information: A Weighted Entropy Approach to Near-Optimal Mastermind](https://arxiv.org/abs/2511.19446)
*Serkan Gür*

Main category: cs.IT

TL;DR: 本文提出了一种新颖的信息论策略来解决“猜数字”游戏，通过加权熵启发法和遗传算法优化，实现了最先进的性能，平均猜测次数接近理论最优。


<details>
  <summary>Details</summary>
Motivation: 在“猜数字”游戏中，利用信息论策略来提高猜测效率，并达到最先进的性能。

Method: 应用基于 Belis-Guiasu 框架的加权熵启发法，为每种可能的反馈类型分配上下文相关的效用值。通过遗传算法优化，发现可解释的权重模式以反映策略性游戏动态。

Result: 单个固定优化权重向量实现了平均 4.3565 次猜测（最多 5 次）。引入阶段加权启发法后，平均猜测次数达到 4.3488 次（最多 6 次），与理论最优值 4.3403 相比，差距小于 0.2%。该方法在显著提高性能的同时，保持了计算效率。

Conclusion: 通过加权熵启发法和遗传算法优化，该信息论策略在“猜数字”游戏中取得了显著的性能提升，平均猜测次数接近理论最优，并兼具计算效率。

Abstract: This paper presents a novel class of information-theoretic strategies for solving the game of Mastermind, achieving state-of-the-art performance among known heuristic methods. The core contribution is the application of a weighted entropy heuristic, based on the Belis-Guias, u framework, which assigns context- dependent utility values to each of the possible feedback types. A genetic algorithm optimization approach discovers interpret-able weight patterns that reflect strategic game dynamics. First, I demonstrate that a single, fixed vector of optimized weights achieves a remarkable 4.3565 average guesses with a maximum of 5. Building upon this, I introduce a stage-weighted heuristic with distinct utility vectors for each turn, achieving 4.3488 average guesses with a maximum of 6, approaching the theoretical optimum of 4.3403 by less than 0.2%. The method retains the computational efficiency of classical one-step-ahead heuristics while significantly improving performance through principled information valuation. A complete implementation and all optimized parameters are provided for full reproducibility.

</details>


### [30] [The Semiotic Channel Principle: Measuring the Capacity for Meaning in LLM Communication](https://arxiv.org/abs/2511.19550)
*Davide Picca*

Main category: cs.IT

TL;DR: 这篇论文提出了一种分析大型语言模型（LLMs）的新颖符号学框架，将它们概念化为随机符号引擎，其输出需要主动、不对称的人类解读。


<details>
  <summary>Details</summary>
Motivation: 目前的分析方法未能充分捕捉到LLMs输出的符号学复杂性，以及其在表达丰富性与解读稳定性之间的权衡。

Method: 本文提出了一种新的符号学框架，将LLMs视为随机符号引擎。该框架利用信息论工具，将表达丰富性量化为源熵，将解读稳定性量化为信息互现，并通过一个生成复杂性参数lambda来建模两者之间的权衡。此外，还定义了一个由受众和上下文参数化的符号信道，并提出了意义传输的容量限制。

Result: 该框架能够实现对LLMs的模型画像、优化提示/上下文设计、基于模糊性进行风险分析以及开发自适应符号系统。

Conclusion: 这种基于容量的符号学方法为理解、评估和设计LLM介导的通信提供了一个严谨、可操作的工具包。

Abstract: This paper proposes a novel semiotic framework for analyzing Large Language Models (LLMs), conceptualizing them as stochastic semiotic engines whose outputs demand active, asymmetric human interpretation. We formalize the trade-off between expressive richness (semiotic breadth) and interpretive stability (decipherability) using information-theoretic tools. Breadth is quantified as source entropy, and decipherability as the mutual information between messages and human interpretations. We introduce a generative complexity parameter (lambda) that governs this trade-off, as both breadth and decipherability are functions of lambda. The core trade-off is modeled as an emergent property of their distinct responses to $λ$. We define a semiotic channel, parameterized by audience and context, and posit a capacity constraint on meaning transmission, operationally defined as the maximum decipherability by optimizing lambda. This reframing shifts analysis from opaque model internals to observable textual artifacts, enabling empirical measurement of breadth and decipherability. We demonstrate the framework's utility across four key applications: (i) model profiling; (ii) optimizing prompt/context design; (iii) risk analysis based on ambiguity; and (iv) adaptive semiotic systems. We conclude that this capacity-based semiotic approach offers a rigorous, actionable toolkit for understanding, evaluating, and designing LLM-mediated communication.

</details>


### [31] [One-Shot Coding and Applications](https://arxiv.org/abs/2511.19556)
*Yanxiao Liu*

Main category: cs.IT

TL;DR: 本文讨论了信息论中一次性信源和信道编码，提出了一种扩展的泊松泛函表示方法，用以解决更复杂的编码方案。


<details>
  <summary>Details</summary>
Motivation: 在一次性信息论中，研究编码方案的可实现性，并推导出适用于存在记忆或遍历性系统的渐近结果。

Method: 扩展了泊松泛函表示法，以应对原始版本无法直接应用的复杂场景。

Result: 成功扩展了泊松泛函表示法的适用范围，使其能够应用于各种更复杂的一次性信息论场景。

Conclusion: 本研究提出并验证了改进的泊松泛函表示方法，从而拓宽了其在一次性信息论问题中的应用，为解决复杂编码问题提供了新工具。

Abstract: One-shot information theory addresses scenarios in source coding and channel coding where the signal blocklength is assumed to be 1. In this case, each source and channel can be used only once, and the sources and channels are arbitrary and not required to be memoryless or ergodic. We study the achievability part of one-shot information theory, i.e., we consider explicit coding schemes in the oneshot scenario. The objective is to derive one-shot achievability results that can imply existing (first-order and second-order) asymptotic results when applied to memoryless sources and channels, or applied to systems with memory that behave ergodically.
  Poisson functional representation was first proposed as a one-shot channel simulation technique by Li and El Gamal [118] for proving a strong functional representation lemma. It was later extended to the Poisson matching lemma by Li and Anantharam [117], which provided a unified one-shot coding scheme for a broad class of information-theoretic problems. The main contribution of this thesis is to extend the applicability of Poisson functional representation to various more complicated scenarios, where the original version cannot be applied directly and further extensions must be developed.

</details>


### [32] [Computer-aided Characterization of Fundamental Limits of Coded Caching with Linear Coding](https://arxiv.org/abs/2511.19639)
*Niccolò Brembilla,Yinbin Ma,Pietro Belotti,Federico Malucelli,Daniela Tuninetti*

Main category: cs.IT

TL;DR: 本文提出了一个高效的计算机辅助框架，用于表征线性编码下的编码缓存系统的基本限制。


<details>
  <summary>Details</summary>
Motivation: 受Tian以及Cao和Xu先前工作的启发，本文旨在表征线性编码下编码缓存系统的基本限制。

Method: 该框架利用适用于可表示多晶格（以及线性代码）的非香农型不等式，并利用编码缓存的对称结构和问题特定约束来降低线性规划的复杂性。

Result: 推导出的逆界比以前已知的分析方法更紧，并证明了在受线性编码放置和传输约束下，某些可实现的内存-负载权衡点的最优性。

Conclusion: 这些结果表明，小的、结构化的需求子集与最小的公共信息构造相结合，可能足以表征线性编码下的最优权衡。

Abstract: Inspired by prior work by Tian and by Cao and Xu, this paper presents an efficient computer-aided framework to characterize the fundamental limits of coded caching systems under the constraint of linear coding. The proposed framework considers non-Shannon-type inequalities which are valid for representable polymatroids (and hence for linear codes), and leverages symmetric structure and problem-specific constraints of coded caching to reduce the complexity of the linear program. The derived converse bounds are tighter compared to previous known analytic methods, and prove the optimality of some achievable memory-load tradeoff points under the constraint of linear coding placement and delivery. These results seem to indicate that small, structured demand subsets combined with minimal common information constructions may be sufficient to characterize optimal tradeoffs under linear coding.

</details>


### [33] [Joint Satellite Power Consumption and Handover Optimization for LEO Constellations](https://arxiv.org/abs/2511.19745)
*Yassine Afif,Mohammed Almekhlafi,Antoine Lesage-Landry,Gunes Karabulut Kurt*

Main category: cs.IT

TL;DR: 为了优化卫星星座通信系统中的总传输速率，我们提出了一种联合考虑总发射功率、用户-卫星关联和功耗（通过切换事件的惩罚来处理）的优化方法。


<details>
  <summary>Details</summary>
Motivation: LEO卫星引起的动态拓扑需要频繁切换，每次切换都会增加信令和功耗，随着星座规模的增加，这会成为一个巨大的负担。

Method: 我们将功率分配问题表述为一个混合整数凹线性规划（MICP），受功率和关联约束。

Result: 蒙特卡洛仿真表明，该方法在控制切换频率的同时保持高用户吞吐量方面是有效的。

Conclusion: 我们提出的切换感知优化策略显著提高了用户速率（约40％），而没有导致切换频率不成比例的增加。

Abstract: In satellite constellation-based communication systems, continuous user coverage requires frequent handoffs due to the dynamic topology induced by the Low Earth Orbit (LEO) satellites. Each handoff between a satellite and ground users introduces additional signaling and power consumption, which can become a significant burden as the size of the constellation continues to increase. This work focuses on the optimization of the total transmission rate in a LEO-to-user system, by jointly considering the total transmitted power, user-satellite associations, and power consumption, the latter being handled through a penalty on handoff events. We consider a system where LEO satellites serve users located in remote areas with no terrestrial connectivity, and formulate the power allocation problem as a mixed-integer concave linear program (MICP) subject to power and association constraints. Our approach can be solved with off-the-shelf solvers and is benchmarked against a naive baseline where users associate to their closest visible satellite. Extensive Monte Carlo simulations demonstrate the effectiveness of the proposed method in controlling the handoff frequency while maintaining high user throughput. These performance gains highlight the effectiveness of our handover-aware optimization strategy, which ensures that user rates improve significantly, by about 40%, without incurring a disproportionate rise in the handoff frequency.

</details>


### [34] [Two-Step Decoding of Binary $2\times2$ Sum-Rank-Metric Codes](https://arxiv.org/abs/2511.19812)
*Hao Wu,Bocong Chen,Guanghui Zhang,Hongwei Liu*

Main category: cs.IT

TL;DR: 解决了Chen-Cheng-Qi在IEEE Trans. Inf. Theory, 2025中提出的开放问题，即具有2x2矩阵块的二元和秩度量码SR(C1, C2)的解码是否可以完全归结为对构成汉明度量码C1和C2的解码。


<details>
  <summary>Details</summary>
Motivation: 此研究旨在解决一个开放问题：在没有额外条件$d_1≥⅔d_{\mathrm{sr}}$的情况下，是否可以将二元和秩度量码的解码完全归结为对其组成汉明度量码的解码。

Method: 提出了一种简单的两步解码过程：首先对$C_2$进行唯一解码，然后对$C_1$进行单次错误/擦除解码。并且，证明了这种归约方法在黑盒模型中是渐近最优的。

Result: 通过所提出的两步解码过程，成功实现了高达$\lfloor (d_{\mathrm{sr}}-1)/2\rfloor$的唯一解码，总成本为$T_2+T_1$，其中$T_2$和$T_1$分别是$C_2$和$C_1$的汉明解码器的复杂度。此方法证明了限制性假设$d_1≥⅔d_{\mathrm{sr}}$在理论上是不必要的。对于\(\mathbb{F}_4\)上的BCH或Goppa实例，解码器在$O(ℓ^2)$时间内运行。

Conclusion: 本研究成功地将二元和秩度量码的解码完全归结为对其组成汉明度量码的解码，且无需额外的限制性条件。所提出的解码器是高效且渐近最优的，并且在实际应用中具有较低的计算复杂度。

Abstract: We resolve an open problem posed by Chen--Cheng--Qi (IEEE Trans.\ Inf.\ Theory, 2025): can decoding of binary sum-rank-metric codes $\SR(C_1,C_2)$ with $2\times2$ matrix blocks be reduced entirely to decoding the constituent Hamming-metric codes $C_1$ and $C_2$ without the additional requirement $d_1\ge\tfrac{2}{3}d_{\mathrm{sr}}$ that underlies their fast decoder? We answer this in the affirmative by exhibiting a simple two-step procedure: first uniquely decode $C_2$, then apply a single error/erasure decoding of $C_1$.This shows that the restrictive hypothesis $d_1\ge\tfrac{2}{3}d_{\mathrm{sr}}$ is theoretically unnecessary.The resulting decoder achieves unique decoding up to $\lfloor (d_{\mathrm{sr}}-1)/2\rfloor$ with overall cost $T_2+T_1$, where $T_2$ and $T_1$ are the complexities of the Hamming decoders for $C_2$ and $C_1$, respectively. We further show that this reduction is asymptotically optimal in a black-box model, as any sum-rank decoder must inherently decode the constituent Hamming codes.For BCH or Goppa instantiations over $\F_4$, the decoder runs in $O(\ell^2)$ time.

</details>


### [35] [Unified Block Signal Processing Framework for LPWANs: Sequence Index Modulation Spreading](https://arxiv.org/abs/2511.20364)
*Wenkun Wen,Tierui Min,Long Yuan,Minghua Xia*

Main category: cs.IT

TL;DR: 这篇论文介绍了一种LPWAN广义块信号传输的统一框架，以解决传统符号间方法的问题，并通过准正交码实现可靠的多用户分离和高效的物理层设计。


<details>
  <summary>Details</summary>
Motivation: 传统的符号间传输方法在LPWAN中存在局限性，特别是在高接收灵敏度和高效物理层信号处理方面。

Method: 该框架包含信号块向量、块内结构生成器和信号基地矩阵三个关键组件。通过循环移位的扩频序列形成准正交码字。该框架为块同步奠定了概念基础，并提供了基于块相关匹配的统一解调结构。

Result: 所提出的准正交性实现了可靠的多用户分离，尤其是在异步接入下。

Conclusion: 这项工作推动了下一代LPWAN的可扩展和高效物理层设计。

Abstract: Low-power wide-area networks (LPWANs) demand high receiver sensitivity and efficient physical-layer signal processing. This paper introduces a unified framework for generalized block signal transmission in LPWANs, addressing the limitations of conventional symbol-by-symbol approaches. The framework comprises three key components: the signal block vector, the intra-block structure generator, and the signal basis matrix, and leverages quasi-orthogonal codewords formed through cyclically shifted spreading sequences. The resulting quasi-orthogonality enables reliable multi-user separation, particularly under asynchronous access. The framework establishes a conceptual foundation for block synchronization and provides a unified demodulation structure based on block correlation matching. It further supports flexible and systematic implementation, as demonstrated through applications to frequency-shift keying and chirp spread spectrum. This work advances scalable and efficient physical-layer design for next-generation LPWANs.

</details>


<div id='cs.LG'></div>

# cs.LG [[Back]](#toc)

### [36] [Quantifying Modality Contributions via Disentangling Multimodal Representations](https://arxiv.org/abs/2511.19470)
*Padegal Amit,Omkar Mahesh Kashyap,Namitha Rayasam,Nidhi Shekhar,Surabhi Narayan*

Main category: cs.LG

TL;DR: 这篇论文提出了一个基于PID的框架，用于量化多模态模型中的模态贡献，通过将内部嵌入中的预测信息分解为独特、冗余和协同的组成部分。为了实现可扩展的、仅推理分析，开发了一种基于IPFP的算法，该算法无需重新训练即可计算层和数据集级别的贡献。


<details>
  <summary>Details</summary>
Motivation: 现有方法在量化多模态模型中的模态贡献时，混淆了贡献本身的概念，并且基于准确性的方法无法区分模态是 H 信息丰富的还是其价值仅通过与其他模态的交互而产生。

Method: 提出了一种基于部分信息分解（PID）的框架，通过将内部嵌入中的预测信息分解为独特、冗余和协同的组成部分来量化模态贡献。为了实现可扩展的、仅推理分析，开发了一种基于迭代比例拟合过程（IPFP）的算法，无需重新训练即可计算层和数据集级别的贡献。

Result: 提供了一种原理性的、表示层面的多模态行为视图，比基于结果的指标提供更清晰、更可解释的见解。

Conclusion: 本研究提出的框架和算法，为量化多模态模型中的模态贡献提供了一种新的、更具解释性的方法，能够区分模态的内在信息量及其与其他模态的交互作用。

Abstract: Quantifying modality contributions in multimodal models remains a challenge, as existing approaches conflate the notion of contribution itself. Prior work relies on accuracy-based approaches, interpreting performance drops after removing a modality as indicative of its influence. However, such outcome-driven metrics fail to distinguish whether a modality is inherently informative or whether its value arises only through interaction with other modalities. This distinction is particularly important in cross-attention architectures, where modalities influence each other's representations. In this work, we propose a framework based on Partial Information Decomposition (PID) that quantifies modality contributions by decomposing predictive information in internal embeddings into unique, redundant, and synergistic components. To enable scalable, inference-only analysis, we develop an algorithm based on the Iterative Proportional Fitting Procedure (IPFP) that computes layer and dataset-level contributions without retraining. This provides a principled, representation-level view of multimodal behavior, offering clearer and more interpretable insights than outcome-based metrics.

</details>


### [37] [PrefixGPT: Prefix Adder Optimization by a Generative Pre-trained Transformer](https://arxiv.org/abs/2511.19472)
*Ruogu Ding,Xin Ning,Ulf Schlichtmann,Weikang Qian*

Main category: cs.LG

TL;DR: PrefixGPT是一个预训练的Transformer模型，能直接生成优化的前缀加法器，并通过学习设计规则和微调来寻找最佳设计，相比现有方法显著提升了面积-延迟积（ADP），平均降低了79.1%。


<details>
  <summary>Details</summary>
Motivation: 设计优化的前缀加法器面临设计规则严格和设计空间巨大的挑战。

Method: PrefixGPT将加法器拓扑表示为二维坐标序列，在生成过程中应用合法性掩码以确保设计的有效性。该模型采用定制的仅解码器Transformer架构，首先通过在随机合成的有效前缀加法器语料库上进行预训练来学习设计规则，然后进行微调以在设计空间中寻找优化的设计。

Result: PrefixGPT发现了一个新的最佳设计，其面积-延迟积（ADP）提高了7.7%，并且表现出卓越的探索质量，将平均ADP降低了79.1%。

Conclusion: GPT风格的模型在掌握复杂的硬件设计原理并将其应用于更高效的设计优化方面具有巨大潜力。

Abstract: Prefix adders are widely used in compute-intensive applications for their high speed. However, designing optimized prefix adders is challenging due to strict design rules and an exponentially large design space. We introduce PrefixGPT, a generative pre-trained Transformer (GPT) that directly generates optimized prefix adders from scratch. Our approach represents an adder's topology as a two-dimensional coordinate sequence and applies a legality mask during generation, ensuring every design is valid by construction. PrefixGPT features a customized decoder-only Transformer architecture. The model is first pre-trained on a corpus of randomly synthesized valid prefix adders to learn design rules and then fine-tuned to navigate the design space for optimized design quality. Compared with existing works, PrefixGPT not only finds a new optimal design with a 7.7% improved area-delay product (ADP) but exhibits superior exploration quality, lowering the average ADP by up to 79.1%. This demonstrates the potential of GPT-style models to first master complex hardware design principles and then apply them for more efficient design optimization.

</details>


### [38] [WavefrontDiffusion: Dynamic Decoding Schedule or Improved Reasoning](https://arxiv.org/abs/2511.19473)
*Haojin Yang,Rui Hu,Zequn Sun,Rui Zhou,Yujun Cai,Yiwei Wang*

Main category: cs.LG

TL;DR: WavefrontDiffusion是一种动态解码方法，与现有扩散语言模型相比，能够生成更连贯、更高效的文本。


<details>
  <summary>Details</summary>
Motivation: DLMs在文本生成方面表现出强大潜力，但主流去噪策略（Standard Diffusion和BlockDiffusion）存在问题，影响输出质量。Standard Diffusion可能导致不完整上下文和过早结束序列预测，而BlockDiffusion的刚性结构会破坏语义单元并中断推理。

Method: WavefrontDiffusion是一种动态解码方法，从已确定位置向外扩展活跃token的波前。它自适应地遵循语义结构的自然流动，同时保持与基于块方法相同的计算成本。

Result: 在四个推理和代码生成基准测试中，WavefrontDiffusion实现了最先进的性能，并产生了更高语义保真度的输出。

Conclusion: WavefrontDiffusion的自适应调度方法在文本生成中具有重要价值，可以实现更连贯和高效的生成。

Abstract: Diffusion Language Models (DLMs) have shown strong potential for text generation and are becoming a competitive alternative to autoregressive models. The denoising strategy plays an important role in determining the quality of their outputs. Mainstream denoising strategies include Standard Diffusion and BlockDiffusion. Standard Diffusion performs global denoising without restricting the update range, often finalizing incomplete context and causing premature end-of-sequence predictions. BlockDiffusion updates fixed-size blocks in a preset order, but its rigid structure can break apart coherent semantic units and disrupt reasoning. We present WavefrontDiffusion, a dynamic decoding approach that expands a wavefront of active tokens outward from finalized positions. This adaptive process follows the natural flow of semantic structure while keeping computational cost equal to block-based methods. Across four benchmarks in reasoning and code generation, WavefrontDiffusion achieves state-of-the-art performance while producing outputs with higher semantic fidelity, showing the value of adaptive scheduling for more coherent and efficient generation.

</details>


### [39] [Exploiting the Experts: Unauthorized Compression in MoE-LLMs](https://arxiv.org/abs/2511.19480)
*Pinaki Prasad Guha Neogi,Ahmad Mohammadshirazi,Dheeraj Kulshrestha,Rajiv Ramnath*

Main category: cs.LG

TL;DR: 本文探讨了MoE架构在大型语言模型中的专家可剪枝性，以及这种特性如何被滥用从而绕过许可和安全限制的问题。


<details>
  <summary>Details</summary>
Motivation: MoE架构的模块化特性带来了独特的漏洞，攻击者可能通过修剪专家并廉价地进行微调来压缩或重新利用模型，从而绕过许可和安全限制。

Method: 我们首先开发了一个专家归因框架，用于识别对给定任务最负责的专家子集。然后，我们评估了使用主动学习驱动的微调技术对这些专家进行修剪和重新对齐的性能权衡。

Result: 研究结果揭示了一个关键的知识损失-恢复权衡：虽然可以分离某些专家以保持任务准确性，但如果没有有针对性的重新对齐，性能会显著下降。

Conclusion: 基于此分析，我们提出了防御策略，旨在使MoE模型在未经授权的情况下更难被压缩和微调，包括纠缠式专家训练和选择性微调协议。这项工作强调了MoE模块化的双重用途性质，并为MoE-LLM的安全专业化提供了第一个系统评估框架。

Abstract: Mixture-of-Experts (MoE) architectures are increasingly adopted in large language models (LLMs) for their scalability and efficiency. However, their modular structure introduces a unique vulnerability: adversaries can attempt to compress or repurpose models by pruning experts and cheaply fine-tuning the remainder, effectively bypassing licensing and security constraints. In this paper, we systematically study the prunability of MoE-LLMs under task-specific usage. We first develop an expert attribution framework that identifies the subset of experts most responsible for a given task, then evaluate the performance trade-offs of pruning and re-aligning these experts using active learning-driven fine-tuning. Our findings reveal a critical knowledge loss--recovery trade-off: while certain experts can be isolated to retain task accuracy, significant degradation occurs without targeted re-alignment. Based on this analysis, we propose defense strategies that aim to make MoE models harder to compress and fine-tune without authorization, including entangled expert training and selective fine-tuning protocols that resist unauthorized adaptation. By positioning expert pruning as both a threat vector and a defense target, this work highlights the dual-use nature of MoE modularity and provides the first systematic evaluation framework for secure specialization of MoE-LLMs.

</details>


### [40] [Quality analysis and evaluation prediction of RAG retrieval based on machine learning algorithms](https://arxiv.org/abs/2511.19481)
*Ruoxin Zhang,Zhizhao Wen,Chao Wang,Chenchen Tang,Puyang Xu,Yifan Jiang*

Main category: cs.LG

TL;DR: 本文提出了一种基于特征工程和粒子群优化的XGBoost机器学习回归模型，用于解决大型语言模型中检索增强生成技术的性能瓶颈。


<details>
  <summary>Details</summary>
Motivation: 现有大型语言模型在处理表格特征时存在性能瓶颈，检索结果的质量直接影响生成内容的准确性。

Method: 提出了一种XGBoost机器学习回归模型，结合特征工程和粒子群优化。

Result: 文档相关性与答案质量呈0.66的正相关；语义相似性、冗余度和多样性之间存在显著负相关；所提出的VMD PSO BiLSTM模型在各项评估指标上均优于对比模型，具有更低的MSE、RMSE、MAE和MAPE以及更高的R2值。

Conclusion: 本研究为优化RAG系统的检索质量和改进生成效果提供了一条有效途径，并在推动相关技术的实施和应用方面具有重要价值。

Abstract: With the rapid evolution of large language models, retrieval enhanced generation technology has been widely used due to its ability to integrate external knowledge to improve output accuracy. However, the performance of the system is highly dependent on the quality of the retrieval module. If the retrieval results have low relevance to user needs or contain noisy information, it will directly lead to distortion of the generated content. In response to the performance bottleneck of existing models in processing tabular features, this paper proposes an XGBoost machine learning regression model based on feature engineering and particle swarm optimization. Correlation analysis shows that answer_quality is positively correlated with doc_delevance by 0.66, indicating that document relevance has a significant positive effect on answer quality, and improving document relevance may enhance answer quality; The strong negative correlations between semantic similarity, redundancy, and diversity were -0.89 and -0.88, respectively, indicating a trade- off between semantic similarity, redundancy, and diversity. In other words, as the former two increased, diversity significantly decreased. The experimental results comparing decision trees, AdaBoost, etc. show that the VMD PSO BiLSTM model is superior in all evaluation indicators, with significantly lower MSE, RMSE, MAE, and MAPE compared to the comparison model. The R2 value is higher, indicating that its prediction accuracy, stability, and data interpretation ability are more outstanding. This achievement provides an effective path for optimizing the retrieval quality and improving the generation effect of RAG system, and has important value in promoting the implementation and application of related technologies.

</details>


### [41] [OmniTFT: Omni Target Forecasting for Vital Signs and Laboratory Result Trajectories in Multi Center ICU Data](https://arxiv.org/abs/2511.19485)
*Wanzhe Xu,Yutong Dai,Yitao Yang,Martin Loza,Weihang Zhang,Yang Cui,Xin Zeng,Sung Joon Park,Kenta Nakai*

Main category: cs.LG

TL;DR: OmniTFT是一个深度学习框架，它基于时间融合Transformer（TFT），能够联合学习和预测高频生命体征和稀疏采样的实验室结果，并通过四种新颖策略提高性能，在多个数据集上实现了显著的性能提升，且具有可解释性。


<details>
  <summary>Details</summary>
Motivation: 在重症监护病房（ICU）中，对生命体征和实验室结果进行准确的多元时间序列预测对于早期干预和精准医疗至关重要。然而，生命体征的噪声和快速波动，以及实验室测试数据缺失、测量滞后和设备偏差等问题，使得综合预测极具挑战性。

Method: OmniTFT是一个基于时间融合Transformer（TFT）的深度学习框架，它实现了四种新颖策略以提高性能：滑动窗口均衡采样以平衡生理状态，频率感知嵌入收缩以稳定稀有类别表示，分层变量选择以引导模型关注信息丰富的特征集群，以及影响对齐注意力校准以增强生理突变时的鲁棒性。该方法减少了对特定目标架构和大量特征工程的依赖。

Result: OmniTFT在MIMIC-III、MIMIC-IV和eICU数据集上，针对生命体征和实验室结果的预测任务，均取得了显著的性能提升。

Conclusion: OmniTFT通过统一建模多种异构临床目标，同时保持跨机构的泛化能力，为定量决策支持提供了潜在的实用性，其注意力模式可解释且与已知的病理生理学一致。

Abstract: Accurate multivariate time-series prediction of vital signs and laboratory results is crucial for early intervention and precision medicine in intensive care units (ICUs). However, vital signs are often noisy and exhibit rapid fluctuations, while laboratory tests suffer from missing values, measurement lags, and device-specific bias, making integrative forecasting highly challenging. To address these issues, we propose OmniTFT, a deep learning framework that jointly learns and forecasts high-frequency vital signs and sparsely sampled laboratory results based on the Temporal Fusion Transformer (TFT). Specifically, OmniTFT implements four novel strategies to enhance performance: sliding window equalized sampling to balance physiological states, frequency-aware embedding shrinkage to stabilize rare-class representations, hierarchical variable selection to guide model attention toward informative feature clusters, and influence-aligned attention calibration to enhance robustness during abrupt physiological changes. By reducing the reliance on target-specific architectures and extensive feature engineering, OmniTFT enables unified modeling of multiple heterogeneous clinical targets while preserving cross-institutional generalizability. Across forecasting tasks, OmniTFT achieves substantial performance improvement for both vital signs and laboratory results on the MIMIC-III, MIMIC-IV, and eICU datasets. Its attention patterns are interpretable and consistent with known pathophysiology, underscoring its potential utility for quantitative decision support in clinical care.

</details>


### [42] [Efficient Inference Using Large Language Models with Limited Human Data: Fine-Tuning then Rectification](https://arxiv.org/abs/2511.19486)
*Lei Wang,Zikun Ye,Jinglong Zhao*

Main category: cs.LG

TL;DR: 本文提出一个结合微调和纠正的方法，并优化了有限标记样本在两阶段的分配，以提高大型语言模型在市场研究和社科应用中的表现。


<details>
  <summary>Details</summary>
Motivation: 大型语言模型在市场研究和社会科学应用中生成类人响应方面展现出巨大潜力，但需要提高其性能。

Method: 开发了一个结合微调和纠正的框架，并优化了有限标记样本在这两个阶段的分配。该方法通过最小化预测误差的方差作为微调目标，而非传统的最小化均方预测误差，从而为下游纠正阶段提供最佳条件。利用经验标度律，开发了一种数据驱动方法来优化微调和纠正阶段之间的样本分配。

Result: 与单独使用微调或纠正相比，该框架在估计和推理性能方面均有显著提升。

Conclusion: 本文提出的结合微调和纠正的框架，通过优化样本分配和微调目标，有效提升了大型语言模型在市场研究和社科应用中的表现。

Abstract: Driven by recent advances in artificial intelligence (AI), a growing body of work demonstrates the potential of using large language models (LLMs) to generate human-like responses in market research and social science applications. Two primary approaches can be applied to improve the performance of LLMs: fine-tuning, which aligns LLM predictions more closely with human responses, and rectification, which corrects biases in LLM outputs. In this paper, we develop a framework that combines fine-tuning and rectification, and optimally allocates limited labeled samples across the two stages. Unlike the conventional objective that minimizes the mean squared prediction errors, we propose to minimize the variance of the prediction errors as the fine-tuning objective, which is optimal for the downstream rectification stage. Building on this insight, we leverage empirical scaling laws to develop a data-driven method for optimally splitting samples between the fine-tuning and rectification stages. Empirical analysis validates our framework, demonstrating improved estimation and inference performance compared to using either fine-tuning or rectification alone.

</details>


### [43] [The Generalized Proximity Forest](https://arxiv.org/abs/2511.19487)
*Ben Shaw,Adam Rustad,Sofia Pelagalli Maia,Jake S. Rhodes,Kevin R. Moon*

Main category: cs.LG

TL;DR: 本文介绍了广义Proximity Forest (PF) 模型，它扩展了随机森林 (RF) 的亲近度概念，使其适用于所有监督式基于距离的机器学习场景。同时还引入了应用于回归任务的PF模型变体，并将其作为一个元学习框架用于扩展预训练分类器的监督式插补能力。


<details>
  <summary>Details</summary>
Motivation: 现有的随机森林 (RF) 亲近度在异常值检测、缺失数据插补和可视化等监督式机器学习任务中表现出实用性，但其效用受限于RF模型本身的适用性。尽管基于距离的Proximity Forest (PF) 模型已将RF亲近度扩展到时间序列分析，但仍有必要将RF亲近度应用于更广泛的监督式基于距离的机器学习场景。

Method: 本文引入了广义Proximity Forest (PF) 模型，从而将RF亲近度扩展到所有可以进行监督式基于距离机器学习的场景。此外，还引入了针对回归任务的PF模型变体。最后，提出了将广义PF模型用作元学习框架，以扩展任何预训练分类器的监督式插补能力。

Result: 广义PF模型能够将RF亲近度扩展到所有监督式基于距离的机器学习场景。

Conclusion: 广义PF模型相较于RF模型和k近邻模型具有独特的优势，并通过实验证明了其有效性。

Abstract: Recent work has demonstrated the utility of Random Forest (RF) proximities for various supervised machine learning tasks, including outlier detection, missing data imputation, and visualization. However, the utility of the RF proximities depends upon the success of the RF model, which itself is not the ideal model in all contexts. RF proximities have recently been extended to time series by means of the distance-based Proximity Forest (PF) model, among others, affording time series analysis with the benefits of RF proximities. In this work, we introduce the generalized PF model, thereby extending RF proximities to all contexts in which supervised distance-based machine learning can occur. Additionally, we introduce a variant of the PF model for regression tasks. We also introduce the notion of using the generalized PF model as a meta-learning framework, extending supervised imputation capability to any pre-trained classifier. We experimentally demonstrate the unique advantages of the generalized PF model compared with both the RF model and the $k$-nearest neighbors model.

</details>


### [44] [OpenCML: End-to-End Framework of Open-world Machine Learning to Learn Unknown Classes Incrementally](https://arxiv.org/abs/2511.19491)
*Jitendra Parmar,Praveen Singh Thakur*

Main category: cs.LG

TL;DR: 这篇论文提出了一个开放世界机器学习模型，该模型可以在连续学习环境中发现新类别并进行增量学习，从而扩展系统对数据的理解并随时间改进。


<details>
  <summary>Details</summary>
Motivation: 传统的机器学习模型通常遵循封闭世界的假设，这阻碍了它们保留先前学习到的知识以用于未来任务的能力。然而，自动化智能系统必须学习新类别和先前已知的任务。

Method: 模型包含两个相互关联的任务：首先，它发现数据中的未知类别并创建新类别；其次，它学习如何为每个新类别增量地执行类别。这两者共同实现了持续学习。

Result: 所提出的模型在开放世界学习中优于现有方法。此外，它在持续学习中表现出强大的性能，在四次迭代中实现了最高平均准确度82.54%和最低准确度65.87%。

Conclusion: 这项研究提出了一个在开放和持续学习环境中有效发现和学习新类别的模型，为机器智能在动态环境中提高其理解和适应能力开辟了道路。

Abstract: Open-world machine learning is an emerging technique in artificial intelligence, where conventional machine learning models often follow closed-world assumptions, which can hinder their ability to retain previously learned knowledge for future tasks. However, automated intelligence systems must learn about novel classes and previously known tasks. The proposed model offers novel learning classes in an open and continuous learning environment. It consists of two different but connected tasks. First, it discovers unknown classes in the data and creates novel classes; next, it learns how to perform class incrementally for each new class. Together, they enable continual learning, allowing the system to expand its understanding of the data and improve over time. The proposed model also outperformed existing approaches in open-world learning. Furthermore, it demonstrated strong performance in continuous learning, achieving a highest average accuracy of 82.54% over four iterations and a minimum accuracy of 65.87%.

</details>


### [45] [A Systematic Study of Compression Ordering for Large Language Models](https://arxiv.org/abs/2511.19495)
*Shivansh Chhawri,Rahul Mahadik,Suparna Rooj*

Main category: cs.LG

TL;DR: 该研究系统地探究了知识蒸馏、结构化剪枝和低比特量化这三种模型压缩技术在独立及组合应用到Qwen2.5 3B模型时的效果，并提出了实现最佳压缩与模型性能平衡的优化顺序。


<details>
  <summary>Details</summary>
Motivation: 大型语言模型（LLMs）需要大量的计算资源，这使得模型压缩对于在受限环境中高效部署至关重要。

Method: 本研究系统地检查了这些技术在独立和组合应用于Qwen2.5 3B模型时的表现。评估了多种压缩流程，包括单一技术和提出的三技术序列，并使用困惑度（perplexity）、G-Eval、清晰度（clarity）、提示对齐（prompt alignment）和压缩比作为衡量指标。

Result: 量化提供了最大的独立压缩效果，而剪枝会带来中等的质量下降。技术顺序显著影响最终模型质量：剪枝、知识蒸馏、量化（P-KD-Q）序列取得了最佳平衡，在实现3.68倍压缩比的同时，保留了强大的指令遵循和语言理解能力。然而，早期应用量化的流程会因不可逆的信息损失而导致严重的性能下降，从而损害后续训练。

Conclusion: 本研究为设计有效且考虑顺序的压缩流程提供了实践见解，以实现LLMs在资源受限环境中的部署。

Abstract: Large Language Models (LLMs) require substantial computational resources, making model compression essential for efficient deployment in constrained environments. Among the dominant compression techniques: knowledge distillation, structured pruning, and low-bit quantization, their individual effects are well studied, but their interactions and optimal sequencing remain unclear. This work systematically examines how these techniques perform both independently and in combination when applied to the Qwen2.5 3B model. We evaluate multiple compression pipelines, including single, and proposed three-technique sequences, using perplexity, G-Eval, clarity, prompt alignment, and compression ratio as metrics. Our experiments show that quantization provides the greatest standalone compression, while pruning introduces moderate quality degradation. Critically, the ordering of techniques significantly affects the final model quality: the sequence Pruning, Knowledge Distillation, Quantization (P-KD-Q) yields the best balance, achieving a 3.68x compression ratio while preserving strong instruction-following and language understanding capabilities. Conversely, pipelines applying quantization early suffer severe performance degradation due to irreversible information loss that impairs subsequent training. Overall, this study offers practical insight into designing effective, ordering-aware compression pipelines for deploying LLMs in resource-limited settings.

</details>


### [46] [Position: The Complexity of Perfect AI Alignment -- Formalizing the RLHF Trilemma](https://arxiv.org/abs/2511.19504)
*Subramanyam Sahoo,Aman Chadha,Vinija Jain,Divya Chaudhary*

Main category: cs.LG

TL;DR: RLHF在对齐大型语言模型时面临一个难题：安全性、公平性和稳健性无法兼得。本文通过复杂性理论分析，证明了在全局范围内实现代表性和稳健性所需的计算量是超多项式的，并指出当前的RLHF实施方案牺牲了代表性。


<details>
  <summary>Details</summary>
Motivation: 探索并形式化RLHF在对齐大型语言模型时所面临的“对齐三难困境”，解释了现有RLHF方法中出现的问题，如偏好崩溃、奉承和系统性偏差放大等。

Method: 本文通过将统计学习理论与鲁棒优化相结合的复杂性理论分析，证明了实现代表性和稳健性的计算复杂性问题。

Result: 在全局范围内实现代表性（epsilon <= 0.01）和稳健性（delta <= 0.001）需要Omega(2^{d_context})的超多项式操作。当前的RLHF实现通过牺牲代表性来解决这个三难困境，即从同质注释者池中收集的样本量远低于实现真正全局代表性所需的样本量。

Conclusion: RLHF系统无法同时实现多样化人类价值观的代表性、样本和计算复杂度的多项式可处理性以及对抗性扰动和分布偏移的鲁棒性。未来的研究方向应通过战略性地放宽对齐要求来应对这些基本的权衡。

Abstract: Reinforcement Learning from Human Feedback (RLHF) is widely used for aligning large language models, yet practitioners face a persistent puzzle: improving safety often reduces fairness, scaling to diverse populations becomes computationally intractable, and making systems robust often amplifies majority biases. We formalize this tension as the Alignment Trilemma: no RLHF system can simultaneously achieve (i) epsilon-representativeness across diverse human values, (ii) polynomial tractability in sample and compute complexity, and (iii) delta-robustness against adversarial perturbations and distribution shift. Through a complexity-theoretic analysis integrating statistical learning theory and robust optimization, we prove that achieving both representativeness (epsilon <= 0.01) and robustness (delta <= 0.001) for global-scale populations requires Omega(2^{d_context}) operations, which is super-polynomial in the context dimensionality. We show that current RLHF implementations resolve this trilemma by sacrificing representativeness: they collect only 10^3--10^4 samples from homogeneous annotator pools while 10^7--10^8 samples are needed for true global representation. Our framework provides a unified explanation for documented RLHF pathologies including preference collapse, sycophancy, and systematic bias amplification. We conclude with concrete directions for navigating these fundamental trade-offs through strategic relaxations of alignment requirements.

</details>


### [47] [Xmodel-2.5: 1.3B Data-Efficient Reasoning SLM](https://arxiv.org/abs/2511.19496)
*Yang Liu,Xiaolong Zhong,Ling Jiang*

Main category: cs.LG

TL;DR: Xmodel-2.5是一个13亿参数的小型语言模型，它通过μP训练、Warmup-Stable-Decay课程以及在衰减阶段从AdamW切换到Muon优化器，实现了强大的推理和工具使用能力，同时降低了计算成本。


<details>
  <summary>Details</summary>
Motivation: 大型语言模型计算需求高，不适用于边缘或成本敏感部署。研究旨在开发一个计算成本低、但仍具备强大推理和工具使用能力的小型语言模型。

Method: 1. 采用13亿参数的Xmodel-2.5作为嵌入式智能体的核心。2. 使用最大更新参数化（μP）进行训练，使得在小规模模型上调整的超参数可以直接迁移到完整模型。3. 采用1.4T token的Warmup-Stable-Decay训练课程。4. 在衰减阶段将优化器从AdamW切换到Muon，以提高推理能力。5. 采用FP8混合精度训练，平衡精度和吞吐量。

Result: 1. Xmodel-2.5作为一个13亿参数的小型语言模型，实现了强大的推理和工具使用能力。2. 在衰减阶段从AdamW切换到Muon，使13项任务的推理平均得分提高了4.58%。3. FP8混合精度训练平衡了准确性和吞吐量。

Conclusion: Xmodel-2.5证明了通过特定的训练策略（如μP、Warmup-Stable-Decay课程和优化器切换），小型语言模型也能在计算受限的环境中实现卓越的推理和工具使用性能。

Abstract: Large language models deliver strong reasoning and tool-use skills, yet their computational demands make them impractical for edge or cost-sensitive deployments. We present \textbf{Xmodel-2.5}, a 1.3-billion-parameter small language model designed as a \emph{drop-in agent core}. Training with maximal-update parameterization ($μ$P) allows hyper-parameters tuned on a 20M-parameter proxy to transfer directly to the full model, even under the parameter-tied \emph{tie-word-embedding} architecture. A 1.4T-token Warmup--Stable--Decay curriculum is used, and we further show that \textbf{switching from AdamW to Muon during the decay phase} improves the 13-task reasoning average by 4.58\,\% while keeping every other hyper-parameter fixed, verifying that early AdamW stability can be paired with late Muon sharpening for better downstream performance. FP8-mixed-precision training balances accuracy and throughput. All checkpoints, recipes, and evaluation code are released under the Apache-2.0 license.\footnote{https://huggingface.co/XiaoduoAILab/Xmodel-2.5 and https://huggingface.co/XiaoduoAILab/Xmodel-2.5-history (training checkpoints).} Training code and evaluation harness: https://github.com/XiaoduoAILab/Xmodel-2.5.

</details>


### [48] [Hierarchical Dual-Strategy Unlearning for Biomedical and Healthcare Intelligence Using Imperfect and Privacy-Sensitive Medical Data](https://arxiv.org/abs/2511.19498)
*Yi Zhang,Tianxiang Xu,Zijian Li,Chao Zhang,Kunyu Zhang,Zhan Gao,Meinuo Li,Xiaohan Zhang,Qichao Qi,Bing Chen*

Main category: cs.LG

TL;DR: 该研究提出了一种针对大型语言模型（LLMs）的层次双策略选择性知识遗忘框架，以解决医疗领域中数据隐私问题。


<details>
  <summary>Details</summary>
Motivation: 解决大型语言模型在医疗领域中因训练数据记忆而导致的高隐私风险，特别是在处理不完善或隐私敏感的患者信息时。

Method: 提出了一种层次双策略框架，该框架结合了几何约束梯度更新和概念感知令牌级干预。通过统一的四级医学概念层次结构，区分需要保留和需要遗忘的令牌，并精确移除特定知识，同时保留基础医疗能力。

Result: 在MedMCQA和MHQA数据集上进行了评估，实现了82.7%的遗忘率和88.5%的知识保留率。该框架在仅修改0.1%参数的情况下，保持了强大的隐私保证。

Conclusion: 该框架在保证隐私的同时，有效平衡了知识遗忘和保留，为医疗领域LLMs的监管合规性、可审计性和伦理标准提供了解决方案。

Abstract: Large language models (LLMs) exhibit exceptional performance but pose substantial privacy risks due to training data memorization, particularly within healthcare contexts involving imperfect or privacy-sensitive patient information. We present a hierarchical dual-strategy framework for selective knowledge unlearning that precisely removes specialized knowledge while preserving fundamental medical competencies. Our approach synergistically integrates geometric-constrained gradient updates to selectively modulate target parameters with concept-aware token-level interventions that distinguish between preservation-critical and unlearning-targeted tokens via a unified four-level medical concept hierarchy. Comprehensive evaluations on the MedMCQA (surgical) and MHQA (anxiety, depression, trauma) datasets demonstrate superior performance, achieving an 82.7% forgetting rate and 88.5% knowledge preservation. Notably, our framework maintains robust privacy guarantees while requiring modification of only 0.1% of parameters, addressing critical needs for regulatory compliance, auditability, and ethical standards in clinical research.

</details>


### [49] [Beyond Binary Classification: A Semi-supervised Approach to Generalized AI-generated Image Detection](https://arxiv.org/abs/2511.19499)
*Hong-Hanh Nguyen-Le,Van-Tuan Tran,Dinh-Thuc Nguyen,Nhien-An Le-Khac*

Main category: cs.LG

TL;DR: 该文章分析了生成器生成图像的伪影，这些伪影可以帮助检测器识别图像是否是虚假的。


<details>
  <summary>Details</summary>
Motivation: 生成器（例如：StyleGAN、Midjourney、DALL-E）的快速发展产生了高度逼真的合成图像，对数字媒体的真实性提出了重大挑战。主要问题在于目前的取证技术无法实现跨生成器泛化，尤其是在跨越架构边界（例如，从GAN到DM）时。

Method: 我们假设这一差距源于这些不同架构产生的伪影的根本差异。在这项工作中，我们提供了一个理论分析，解释了GAN和DM架构的不同优化目标如何导致不同的流形覆盖行为。

Result: 我们证明了GAN允许部分覆盖，这通常会导致边界伪影，而DM强制执行完全覆盖，从而导致过度平滑模式。受此分析的启发，我们提出了三元检测器（TriDetect），这是一种半监督方法，通过发现“假”类中的潜在架构模式来增强二分类。TriDetect采用通过Sinkhorn-Knopp算法实现的平衡聚类分配和跨视图一致性机制，鼓励模型学习基本的架构差异。

Conclusion: 我们研究了在两个标准基准和三个野外数据集上，针对13个基线评估了我们的方法，以证明其对看不见的生成器的泛化能力。

Abstract: The rapid advancement of generators (e.g., StyleGAN, Midjourney, DALL-E) has produced highly realistic synthetic images, posing significant challenges to digital media authenticity. These generators are typically based on a few core architectural families, primarily Generative Adversarial Networks (GANs) and Diffusion Models (DMs). A critical vulnerability in current forensics is the failure of detectors to achieve cross-generator generalization, especially when crossing architectural boundaries (e.g., from GANs to DMs). We hypothesize that this gap stems from fundamental differences in the artifacts produced by these \textbf{distinct architectures}. In this work, we provide a theoretical analysis explaining how the distinct optimization objectives of the GAN and DM architectures lead to different manifold coverage behaviors. We demonstrate that GANs permit partial coverage, often leading to boundary artifacts, while DMs enforce complete coverage, resulting in over-smoothing patterns. Motivated by this analysis, we propose the \textbf{Tri}archy \textbf{Detect}or (TriDetect), a semi-supervised approach that enhances binary classification by discovering latent architectural patterns within the "fake" class. TriDetect employs balanced cluster assignment via the Sinkhorn-Knopp algorithm and a cross-view consistency mechanism, encouraging the model to learn fundamental architectural distincts. We evaluate our approach on two standard benchmarks and three in-the-wild datasets against 13 baselines to demonstrate its generalization capability to unseen generators.

</details>


### [50] [Lower Complexity Bounds for Nonconvex-Strongly-Convex Bilevel Optimization with First-Order Oracles](https://arxiv.org/abs/2511.19656)
*Kaiyi Ji*

Main category: cs.LG

TL;DR: 本文针对平滑非凸-强凸双层优化问题，在确定性和随机一阶预言机模型下，建立了新的困难案例，推导出了非平凡的下界。这些下界改进了现有单层非凸优化和非凸-强凸min-max问题的最优下界。


<details>
  <summary>Details</summary>
Motivation: 开发针对双层优化问题的非平凡下界，因为现有研究主要集中在上界保证，而下界进展有限。

Method: 针对平滑非凸-强凸双层优化设定，在确定性和随机一阶预言机模型下，构建了新的困难实例。

Result: 在确定性情况下，任何一阶零偏差算法需要至少$Ω(κ^{3/2}ε^{-2})$次预言机调用才能找到一个$ε$精度的驻点。在随机情况下，需要至少$Ω(κ^{5/2}ε^{-4})$次随机预言机调用。这些结果都强化了相关设置中已知的最佳界限。

Conclusion: 双层优化的现有上下界之间存在显著差距，表明即使是简化的情景，也需要进一步研究其在标准一阶预言机下的最优复杂性。

Abstract: Although upper bound guarantees for bilevel optimization have been widely studied, progress on lower bounds has been limited due to the complexity of the bilevel structure. In this work, we focus on the smooth nonconvex-strongly-convex setting and develop new hard instances that yield nontrivial lower bounds under deterministic and stochastic first-order oracle models. In the deterministic case, we prove that any first-order zero-respecting algorithm requires at least $Ω(κ^{3/2}ε^{-2})$ oracle calls to find an $ε$-accurate stationary point, improving the optimal lower bounds known for single-level nonconvex optimization and for nonconvex-strongly-convex min-max problems. In the stochastic case, we show that at least $Ω(κ^{5/2}ε^{-4})$ stochastic oracle calls are necessary, again strengthening the best known bounds in related settings. Our results expose substantial gaps between current upper and lower bounds for bilevel optimization and suggest that even simplified regimes, such as those with quadratic lower-level objectives, warrant further investigation toward understanding the optimal complexity of bilevel optimization under standard first-order oracles.

</details>


### [51] [Prompt Fairness: Sub-group Disparities in LLMs](https://arxiv.org/abs/2511.19956)
*Meiyu Zhong,Noel Teku,Ravi Tandon*

Main category: cs.LG

TL;DR: 本文探讨了大语言模型（LLMs）中的提示公平性问题，即不同用户/风格对相同问题的提问方式可能导致LLM产生不同的响应。作者提出了使用信息论度量来量化这种差异，并提出了缓解这种差异的实用干预措施。


<details>
  <summary>Details</summary>
Motivation: 尽管LLMs在许多应用中表现出有效性，但其响应质量差异很大。本文旨在探究提示词的措辞如何影响LLM的响应质量，并解决提示公平性问题。

Method: 作者提出了信息论度量来量化偏差的两个维度：子组敏感性（子组内响应的可变性）和跨组一致性（子组间响应的可变性）。为了缓解这些差异，作者提出了包括多数投票和提示中性化在内的实用干预措施。

Result: 分析显示，某些人口统计学子组表现出更高的内部可变性和更大的分歧，这表明模型行为存在结构性不平等。在缓解措施之前，跨组分歧值最高达到0.28，通常在0.14到0.22的范围内。在应用中性化和多代策略后，这些分歧持续减少，最大的差距缩小到0.22，许多距离降至0.17或以下。

Conclusion: LLMs在提示公平性方面存在问题，即不同用户/风格的提示可能导致LLM产生不同的响应。通过信息论度量可以量化这种差异，并且通过多数投票和提示中性化等干预措施可以有效缓解这种差异，从而提高响应的稳定性并增强用户群体的公平性。

Abstract: Large Language Models (LLMs), though shown to be effective in many applications, can vary significantly in their response quality. In this paper, we investigate this problem of prompt fairness: specifically, the phrasing of a prompt by different users/styles, despite the same question being asked in principle, may elicit different responses from an LLM. To quantify this disparity, we propose to use information-theoretic metrics that can capture two dimensions of bias: subgroup sensitivity, the variability of responses within a subgroup and cross group consistency, the variability of responses across subgroups. Our analysis reveals that certain subgroups exhibit both higher internal variability and greater divergence from others. Our empirical analysis reveals that certain demographic sub groups experience both higher internal variability and greater divergence from others, indicating structural inequities in model behavior. To mitigate these disparities, we propose practical interventions, including majority voting across multiple generations and prompt neutralization, which together improve response stability and enhance fairness across user populations. In the experiments, we observe clear prompt sensitivity disparities across demographic subgroups: before mitigation, cross-group divergence values reach 0.28 and typically fall in the from 0.14 to 0.22 range. After applying our neutralization and multi generation strategy, these divergences consistently decrease, with the largest gap reduced to 0.22 and many distances falling to 0.17 or below, indicating more stable and consistent outputs across subgroups.

</details>


### [52] [Demystifying Diffusion Objectives: Reweighted Losses are Better Variational Bounds](https://arxiv.org/abs/2511.19664)
*Jiaxin Shi,Michalis K. Titsias*

Main category: cs.LG

TL;DR: 该文章提出了一种新的理论解释，用于训练扩散模型中广泛使用的重新加权损失。这种方法基于构建时间依赖的变分下界，可以改进标准证据下界，并减少数据模型KL散度。


<details>
  <summary>Details</summary>
Motivation: 目前扩散模型中使用的重新加权损失缺乏理论解释，并且标准证据下界和数据模型KL散度存在改进空间。

Method: 文章构建了时间依赖的变分下界，并将其应用于广义扩散模型，包括连续高斯扩散和掩码（离散）扩散模型。具体通过结合这些界限，文章提出了可以应用于任何生成扩散模型的重新加权目标。

Result: 在像素空间图像建模中，该框架显著改善了掩码扩散模型的训练损失，使得样本质量接近连续扩散模型。

Conclusion: 该研究为扩散模型中的重新加权损失提供了一种新的理论解释，通过构建时间依赖的变分下界，不仅在理论上改进了扩散模型的训练目标，而且在实验中显著提升了掩码扩散模型在图像生成任务上的性能，使其样本质量可以与连续扩散模型媲美。此外，该工作也为掩码图像模型中广泛使用的简单加权方案提供了理论依据。

Abstract: We derive a new theoretical interpretation of the reweighted losses that are widely used for training diffusion models. Our method is based on constructing a cascade of time-dependent variational lower bounds on the data log-likelihood, that provably improves upon the standard evidence lower bound and results in reduced data-model KL-divergences. Combining such bounds gives rise to reweighted objectives that can be applied to any generative diffusion model including both continuous Gaussian diffusion and masked (discrete) diffusion models. Then, we showcase this framework in masked diffusion and report significant improvements over previous training losses in pixel-space image modeling, approaching sample quality comparable to continuous diffusion models. Our results also provide a theoretical justification for the simple weighting scheme widely used in masked image models.

</details>


### [53] [When +1% Is Not Enough: A Paired Bootstrap Protocol for Evaluating Small Improvements](https://arxiv.org/abs/2511.19794)
*Wenzhang Du*

Main category: cs.LG

TL;DR: 这篇论文提出了一种简单且PC友好的评估协议，用于解决机器学习论文中常见的1-2个百分点提升可能源于随机因素而非真正算法进步的问题。


<details>
  <summary>Details</summary>
Motivation: 在机器学习研究中，1-2个百分点的性能提升可能对随机种子、数据排序和实现细节高度敏感，但通常缺乏不确定性估计或显著性检验，导致难以判断这些提升是真实算法进步还是噪声。

Method: 论文提出了一种基于配对多种子运行、偏差校正和加速（BCa）自举置信区间以及基于每种子增量的符号翻转置换检验的评估协议。该协议在计算预算有限的情况下也能适用，并且被设计为保守的，以防止过度声称。

Result: 在CIFAR-10、CIFAR-10N和AG News数据集上，通过模拟“无改进”、“小增益”和“中等增益”场景进行实例化。结果显示，对于0.6-2.0个百分点的改进，尤其是在文本数据上，单独运行和未配对t检验经常会得出显著性增益的结论。然而，使用该论文提出的配对协议，即便增加到三个种子，在这些设置下也从未声明过显著性。

Conclusion: 当计算资源有限且仅获得少量改进时，论文建议采取这种更为保守的评估方法作为默认，以确保结果的可靠性和避免过度声称。

Abstract: Recent machine learning papers often report 1-2 percentage point improvements from a single run on a benchmark. These gains are highly sensitive to random seeds, data ordering, and implementation details, yet are rarely accompanied by uncertainty estimates or significance tests. It is therefore unclear when a reported +1-2% reflects a real algorithmic advance versus noise.
  We revisit this problem under realistic compute budgets, where only a few runs are affordable. We propose a simple, PC-friendly evaluation protocol based on paired multi-seed runs, bias-corrected and accelerated (BCa) bootstrap confidence intervals, and a sign-flip permutation test on per-seed deltas. The protocol is intentionally conservative and is meant as a guardrail against over-claiming.
  We instantiate it on CIFAR-10, CIFAR-10N, and AG News using synthetic no-improvement, small-gain, and medium-gain scenarios. Single runs and unpaired t-tests often suggest significant gains for 0.6-2.0 point improvements, especially on text. With only three seeds, our paired protocol never declares significance in these settings. We argue that such conservative evaluation is a safer default for small gains under tight budgets.

</details>


### [54] [Terminal Velocity Matching](https://arxiv.org/abs/2511.19797)
*Linqi Zhou,Mathias Parger,Ayaan Haque,Jiaming Song*

Main category: cs.LG

TL;DR: 本文提出了一种名为 TVM 的方法，可以实现高保真度的一步和少量步骤生成建模，并在 ImageNet 数据集上取得了最先进的性能。


<details>
  <summary>Details</summary>
Motivation: 现有的流匹配方法在扩散模型中存在局限性，无法很好地处理一步或少量步骤的生成建模。

Method: TVM 通过建模任意两个扩散时间步之间的转换，并在其终止时间而不是初始时间对其行为进行正则化，从而概括了流匹配。为了使其高效，我们开发了一个融合的注意力内核，支持 Jacobian-Vector 产品的反向传播。

Result: 在 ImageNet-256x256 上，TVM 在单次函数评估下达到了 3.29 FID，在 4 次函数评估下达到了 1.99 FID。在 ImageNet-512x512 上，TVM 在单次函数评估下达到了 4.32 FID，在 4 次函数评估下达到了 2.94 FID。

Conclusion: TVM 在 ImageNet 数据集上取得了最先进的性能，证明了其在高保真度一步和少量步骤生成建模方面的潜力。

Abstract: We propose Terminal Velocity Matching (TVM), a generalization of flow matching that enables high-fidelity one- and few-step generative modeling. TVM models the transition between any two diffusion timesteps and regularizes its behavior at its terminal time rather than at the initial time. We prove that TVM provides an upper bound on the $2$-Wasserstein distance between data and model distributions when the model is Lipschitz continuous. However, since Diffusion Transformers lack this property, we introduce minimal architectural changes that achieve stable, single-stage training. To make TVM efficient in practice, we develop a fused attention kernel that supports backward passes on Jacobian-Vector Products, which scale well with transformer architectures. On ImageNet-256x256, TVM achieves 3.29 FID with a single function evaluation (NFE) and 1.99 FID with 4 NFEs. It similarly achieves 4.32 1-NFE FID and 2.94 4-NFE FID on ImageNet-512x512, representing state-of-the-art performance for one/few-step models from scratch.

</details>


### [55] [Row-stochastic matrices can provably outperform doubly stochastic matrices in decentralized learning](https://arxiv.org/abs/2511.19513)
*Bing Liu,Boao Kong,Limin Lu,Kun Yuan,Chengcheng Zhao*

Main category: cs.LG

TL;DR: 本文分析了两种分散式学习策略，并提出了加权希尔伯特空间框架，以提供更紧密的收敛性保证，并通过拓扑设计指南来加速收敛。


<details>
  <summary>Details</summary>
Motivation: 在分散式学习中，带有异构节点权重 λ 的加权全局损失通常会带来挑战。本文旨在分析两种合并这些权重的策略，并阐明它们各自的行为差异。

Method: 本文提出了一种加权希尔伯特空间框架 L^2(λ;R^d)，以分析两种分散式学习策略的收敛行为。通过这种几何方法，文章揭示了行随机矩阵和双随机矩阵在收敛性方面的差异，并推导了行随机设计收敛更快的充分条件，以及拓扑条件。

Result: 在加权希尔伯特空间框架下，行随机矩阵变为自伴随矩阵，而双随机矩阵则不然，这会产生额外的惩罚项，从而放大共识误差并减慢收敛速度。因此，收敛性的差异不仅 F来自于谱，还来自由这些惩罚项。在某些充分条件下，即使谱更小，行随机设计收敛速度也更快。

Conclusion: 本文通过引入加权希尔伯特空间框架，为分散式学习中两种加权策略的收敛性分析提供了更紧密的理论保证。研究结果表明，在收敛速度方面，行随机设计通常优于双随机设计，并且文章还提供了拓扑设计指南，以进一步加速收敛。

Abstract: Decentralized learning often involves a weighted global loss with heterogeneous node weights $λ$. We revisit two natural strategies for incorporating these weights: (i) embedding them into the local losses to retain a uniform weight (and thus a doubly stochastic matrix), and (ii) keeping the original losses while employing a $λ$-induced row-stochastic matrix. Although prior work shows that both strategies yield the same expected descent direction for the global loss, it remains unclear whether the Euclidean-space guarantees are tight and what fundamentally differentiates their behaviors. To clarify this, we develop a weighted Hilbert-space framework $L^2(λ;\mathbb{R}^d)$ and obtain convergence rates that are strictly tighter than those from Euclidean analysis. In this geometry, the row-stochastic matrix becomes self-adjoint whereas the doubly stochastic one does not, creating additional penalty terms that amplify consensus error, thereby slowing convergence. Consequently, the difference in convergence arises not only from spectral gaps but also from these penalty terms. We then derive sufficient conditions under which the row-stochastic design converges faster even with a smaller spectral gap. Finally, by using a Rayleigh-quotient and Loewner-order eigenvalue comparison, we further obtain topology conditions that guarantee this advantage and yield practical topology-design guidelines.

</details>


### [56] [Automating Deception: Scalable Multi-Turn LLM Jailbreaks](https://arxiv.org/abs/2511.19517)
*Adarsh Kumarappan,Ananya Mujoo*

Main category: cs.LG

TL;DR: 该论文介绍了一种新颖的自动化流程，用于生成大规模、基于心理学原理的多轮逆向攻击数据集。


<details>
  <summary>Details</summary>
Motivation: 开发针对多轮会话攻击的防御，这种攻击利用“登门槛”等心理学原理来绕过安全对齐，对大型语言模型（LLM）构成持续威胁。目前进展受限于手动、难以扩展的数据集创建方式。

Method: 将“登门槛”技术系统地操作化为可复用的模板，创建了一个包含1,500个场景的基准数据集，涵盖非法活动和冒犯性内容。在多轮（带历史记录）和单轮（不带历史记录）条件下评估了来自三个主要LLM系列的七个模型。

Result: GPT家族模型对会话历史表现出显著脆弱性，攻击成功率（ASR）增加了多达32个百分点。Google的Gemini 2.5 Flash表现出卓越的弹性，几乎不受这些攻击的影响。Anthropic的Claude 3 Haiku显示出强大但不完美的抵抗力。

Conclusion: 当前安全架构在处理会话上下文方面存在显著差异，强调需要能够抵抗基于叙事操纵的防御措施。

Abstract: Multi-turn conversational attacks, which leverage psychological principles like Foot-in-the-Door (FITD), where a small initial request paves the way for a more significant one, to bypass safety alignments, pose a persistent threat to Large Language Models (LLMs). Progress in defending against these attacks is hindered by a reliance on manual, hard-to-scale dataset creation. This paper introduces a novel, automated pipeline for generating large-scale, psychologically-grounded multi-turn jailbreak datasets. We systematically operationalize FITD techniques into reproducible templates, creating a benchmark of 1,500 scenarios across illegal activities and offensive content. We evaluate seven models from three major LLM families under both multi-turn (with history) and single-turn (without history) conditions. Our results reveal stark differences in contextual robustness: models in the GPT family demonstrate a significant vulnerability to conversational history, with Attack Success Rates (ASR) increasing by as much as 32 percentage points. In contrast, Google's Gemini 2.5 Flash exhibits exceptional resilience, proving nearly immune to these attacks, while Anthropic's Claude 3 Haiku shows strong but imperfect resistance. These findings highlight a critical divergence in how current safety architectures handle conversational context and underscore the need for defenses that can resist narrative-based manipulation.

</details>


### [57] [Cisco Time Series Model Technical Report](https://arxiv.org/abs/2511.19841)
*Liang Gou,Archit Khare,Praneet Pabolu,Prachi Patel,Joseph Ross,Hercy Shen,Yuhan,Song,Jingze Sun,Kristal Curtis,Vedant Dharnidharka,Abhinav Mathur,Hao Yang*

Main category: cs.LG

TL;DR: Cisco 时间序列模型（Cisco Time Series Model）是一个单变量零样本预测器，通过多分辨率输入能力，在可观测性数据集上取得了卓越的性能，同时在通用预测基准测试中保持了相似的性能。


<details>
  <summary>Details</summary>
Motivation: 现有的时间序列模型在处理长上下文输入时可能存在准确性问题，并且在不同领域的数据集上表现不一。

Method: 本文提出了一种对流行的 TimesFM 解码器专用时间序列模型进行架构创新，使其能够接受多分辨率输入，从而构建了 Cisco 时间序列模型。该模型在超过 3000 亿个独特数据点上进行了训练。

Result: Cisco 时间序列模型在可观测性数据集上取得了卓越的性能，同时在标准的通用预测基准测试（GIFT-Eval）上保持了非常相似的性能。研究表明，多分辨率结构使模型能够对长上下文输入进行更准确的预测。

Conclusion: Cisco 时间序列模型通过引入多分辨率输入能力，显著提升了在可观测性数据集上的预测准确性，并且在通用预测任务上也表现良好，证明了其架构创新的有效性。

Abstract: We introduce the Cisco Time Series Model, a univariate zero-shot forecaster. This time series foundation model is the result of a general architectural innovation to a time series model enabling it to accept multiresolution input, applied to a popular decoder-only time series model (TimesFM). The resulting multiresolution decoder-only model is trained on over 300B unique data points, with more than half coming from the observability domain. Quantitative and qualitative evaluations demonstrate that the resulting model achieves superior performance on observability datasets while retaining very similar performance on a standard general-purpose forecasting benchmark (GIFT-Eval), and suggest that the multiresolution structure enables the model to make more accurate predictions on long context input.

</details>


### [58] [When Should Neural Data Inform Welfare? A Critical Framework for Policy Uses of Neuroeconomics](https://arxiv.org/abs/2511.19548)
*Yiven,Zhu*

Main category: cs.LG

TL;DR: 本文探讨了神经数据在政策制定中为福利判断提供依据的条件。


<details>
  <summary>Details</summary>
Motivation: 神经经济学试图将福利分析建立在关于人们如何评估结果、从经验中学习和进行自我控制的神经和计算证据之上。政策和商业行为者也越来越多地援引神经数据来证明家长式监管、“基于大脑”的干预和新的福利衡量标准的合理性。

Method: 开发了一个非经验性的、基于模型的框架，该框架连接了三个层面：神经信号、计算决策模型和规范福利标准。在一个行动者-评论者强化学习模型中，作者将从神经活动到潜在价值和预测误差，再到福利主张的推断路径形式化。

Result: 神经证据仅在以下情况下才能约束福利判断：神经与计算的映射得到充分验证，决策模型能够识别“真实”利益而非情境依赖的错误，并且福利标准被明确指定和辩护。

Conclusion: 神经科学的证据对福利判断的约束性，取决于神经计算映射的有效性、决策模型识别真实利益的能力以及福利标准的明确性。内部奖励信号，无论是生物的还是人工的，都只是计算量，不能在没有明确规范模型的情况下被视为福利衡量标准。

Abstract: Neuroeconomics promises to ground welfare analysis in neural and computational evidence about how people value outcomes, learn from experience and exercise self-control. At the same time, policy and commercial actors increasingly invoke neural data to justify paternalistic regulation, "brain-based" interventions and new welfare measures. This paper asks under what conditions neural data can legitimately inform welfare judgements for policy rather than merely describing behaviour. I develop a non-empirical, model-based framework that links three levels: neural signals, computational decision models and normative welfare criteria. Within an actor-critic reinforcement-learning model, I formalise the inference path from neural activity to latent values and prediction errors and then to welfare claims. I show that neural evidence constrains welfare judgements only when the neural-computational mapping is well validated, the decision model identifies "true" interests versus context-dependent mistakes, and the welfare criterion is explicitly specified and defended. Applying the framework to addiction, neuromarketing and environmental policy, I derive a Neuroeconomic Welfare Inference Checklist for regulators and for designers of NeuroAI systems. The analysis treats brains and artificial agents as value-learning systems while showing that internal reward signals, whether biological or artificial, are computational quantities and cannot be treated as welfare measures without an explicit normative model.

</details>


### [59] [Online Sparse Feature Selection in Data Streams via Differential Evolution](https://arxiv.org/abs/2511.19555)
*Ruiyang Xu*

Main category: cs.LG

TL;DR: 为了解决数据流中高维流数据特征选择和缺失值的问题，本文提出了一种名为ODESFS的新方法，它通过潜在因子分析处理缺失值，并通过差分进化评估特征重要性，实验结果表明ODESFS优于现有的方法。


<details>
  <summary>Details</summary>
Motivation: 在线流式特征选择（OSFS）在处理高维流式数据时面临数据不完整的问题，这通常是由于设备故障或技术限制引起的。现有的在线稀疏流式特征选择（OS2FS）方法虽然尝试通过基于潜在因子分析的缺失数据插补来解决，但在特征评估方面存在局限性，导致性能下降。

Method: 本文提出了一种名为“数据流中稀疏特征选择在线差分演化”（Online Differential Evolution for Sparse Feature Selection, ODESFS）的新方法。该方法包含两个关键创新点：1. 使用潜在因子分析模型进行缺失值插补。2. 通过差分演化评估特征重要性。

Result: 在六个真实世界数据集上进行的综合实验表明，ODESFS 在选择最佳特征子集和实现卓越准确性方面，始终优于现有的最先进的 OSFS 和 OS2FS 方法。

Conclusion: ODESFS通过有效处理缺失值和优化特征评估，显著提升了高维流数据在线特征选择的性能。

Abstract: The processing of high-dimensional streaming data commonly utilizes online streaming feature selection (OSFS) techniques. However, practical implementations often face challenges with data incompleteness due to equipment failures and technical constraints. Online Sparse Streaming Feature Selection (OS2FS) tackles this issue through latent factor analysis-based missing data imputation. Despite this advancement, existing OS2FS approaches exhibit substantial limitations in feature evaluation, resulting in performance deterioration. To address these shortcomings, this paper introduces a novel Online Differential Evolution for Sparse Feature Selection (ODESFS) in data streams, incorporating two key innovations: (1) missing value imputation using a latent factor analysis model, and (2) feature importance evaluation through differential evolution. Comprehensive experiments conducted on six real-world datasets demonstrate that ODESFS consistently outperforms state-of-the-art OSFS and OS2FS methods by selecting optimal feature subsets and achieving superior accuracy.

</details>


### [60] [Merging without Forgetting: Continual Fusion of Task-Specific Models via Optimal Transport](https://arxiv.org/abs/2511.19561)
*Zecheng Pan,Zhikang Chen,Ding Li,Min Zhang,Sen Cui,Hongshuo Jin,Luqi Tao,Yi Yang,Deheng Ye,Yu Zhang,Tingting Zhu,Tianling Ren*

Main category: cs.LG

TL;DR: OTMF（基于最优传输的掩码融合）是一种新颖的模型融合框架，它利用最优传输理论解决因参数插值引起的分布偏移问题，通过发现应用于任务向量的公共掩码来对齐语义，从而选择性地提取可迁移和任务无关的组件，同时保留每个任务的独特结构特性。为了解决现实世界中的可扩展性问题，OTMF进一步支持渐进式融合范式，可以在不重新审视历史任务的情况下，逐步集成每个新任务向量，从而保持有限的内存占用，并实现对不断增长的多个任务进行高效融合。


<details>
  <summary>Details</summary>
Motivation: 现有的模型融合方法主要依赖于权重空间的参数插值，这会在特征空间中引入显著的分布偏移并破坏任务特定知识。

Method: 本文提出OTMF（Optimal Transport-based Masked Fusion），这是一种新颖的模型融合框架，其特点是基于最优传输理论解决简单参数插值带来的分布偏移问题。 OTMF通过发现应用于任务向量的公共掩码来对齐任务特定模型的语义几何，而不是直接聚合特征或权重。这些掩码选择性地提取可迁移和任务无关的组件，同时保留每个任务的独特结构特性。为了确保在实际应用中的可扩展性，OTMF支持持续融合范式，可以逐步集成新的任务向量，而无需重新访问之前的任务向量，从而保持有限的内存占用并实现对不断增加的任务数量的高效融合。

Result: OTMF在多个视觉和语言基准测试中，在准确性和效率方面都取得了最先进的性能。

Conclusion: OTMF在模型融合方面具有重要的实用和理论价值，能够有效解决现有方法中的分布偏移问题，并在多任务系统中实现卓越的性能和效率。

Abstract: Merging models fine-tuned for different tasks into a single unified model has become an increasingly important direction for building versatile, efficient multi-task systems. Existing approaches predominantly rely on parameter interpolation in weight space, which we show introduces significant distribution shift in the feature space and undermines task-specific knowledge. In this paper, we propose OTMF (Optimal Transport-based Masked Fusion), a novel model merging framework rooted in optimal transport theory to address the distribution shift that arises from naive parameter interpolation. Instead of directly aggregating features or weights, OTMF aligns the semantic geometry of task-specific models by discovering common masks applied to task vectors through optimal transport plans. These masks selectively extract transferable and task-agnostic components while preserving the unique structural identities of each task. To ensure scalability in real-world settings, OTMF further supports a continual fusion paradigm that incrementally integrates each new task vector without revisiting previous ones, maintaining a bounded memory footprint and enabling efficient fusion across a growing number of tasks. We conduct comprehensive experiments on multiple vision and language benchmarks, and results show that OTMF achieves state-of-the-art performance in terms of both accuracy and efficiency. These findings highlight the practical and theoretical value of our approach to model merging.

</details>


### [61] [An Invariant Latent Space Perspective on Language Model Inversion](https://arxiv.org/abs/2511.19569)
*Wentao Ye,Jiaqi Hu,Haobo Wang,Xinpeng Ti,Zhiqing Xiao,Hao Chen,Liyao Li,Lei Feng,Sai Wu,Junbo Zhao*

Main category: cs.LG

TL;DR: 本文提出了一种名为Inv^2A的新型大模型反演方法，该方法通过利用大模型的潜在空间特性，在多个数据集上显著优于现有基线。


<details>
  <summary>Details</summary>
Motivation: 大型语言模型反演（LMI）对用户隐私和系统安全构成重大威胁，当前方法在处理多输出场景时效率低下，需要大量训练语料。

Method: 本文将LMI重新定义为重用LLM自身的潜在空间，并提出了不变潜在空间假设（ILSH），包括源不变性和循环不变性。Inv^2A将LLM视为不变解码器，并学习一个轻量级逆编码器，将输出映射到去噪的伪表示。当有多个输出可用时，它们在表示层稀疏地连接以增加信息密度。训练分为两阶段：对比对齐（源不变性）和监督强化（循环不变性）。

Result: 在9个覆盖用户和系统提示场景的数据集上，Inv^2A的BLEU分数平均比基线高4.77%，同时减少了对大型逆向语料库的依赖。分析表明，现有防御措施提供的保护有限。

Conclusion: Inv^2A方法通过引入不变潜在空间假设和两阶段训练策略，在语言模型反演任务中取得了显著的性能提升，并揭示了现有防御的不足。

Abstract: Language model inversion (LMI), i.e., recovering hidden prompts from outputs, emerges as a concrete threat to user privacy and system security. We recast LMI as reusing the LLM's own latent space and propose the Invariant Latent Space Hypothesis (ILSH): (1) diverse outputs from the same source prompt should preserve consistent semantics (source invariance), and (2) input<->output cyclic mappings should be self-consistent within a shared latent space (cyclic invariance). Accordingly, we present Inv^2A, which treats the LLM as an invariant decoder and learns only a lightweight inverse encoder that maps outputs to a denoised pseudo-representation. When multiple outputs are available, they are sparsely concatenated at the representation layer to increase information density. Training proceeds in two stages: contrastive alignment (source invariance) and supervised reinforcement (cyclic invariance). An optional training-free neighborhood search can refine local performance. Across 9 datasets covering user and system prompt scenarios, Inv^2A outperforms baselines by an average of 4.77% BLEU score while reducing dependence on large inverse corpora. Our analysis further shows that prevalent defenses provide limited protection, underscoring the need for stronger strategies. The source code and data involved in this paper can be found in https://github.com/yyy01/Invariant_Attacker.

</details>


### [62] [Learning Massively Multitask World Models for Continuous Control](https://arxiv.org/abs/2511.19584)
*Nicklas Hansen,Hao Su,Xiaolong Wang*

Main category: cs.LG

TL;DR: 本文介绍了一种名为Newt的语言条件多任务世界模型，它通过在数百个任务上的在线交互来训练，实现了出色的多任务性能和数据效率，并能快速适应未见任务。


<details>
  <summary>Details</summary>
Motivation: 现有的强化学习研究主要集中在单任务或离线学习，而本文旨在探索通过大规模预训练和轻量级强化学习相结合的方法，使单个智能体能够跨多个任务和实体进行在线交互学习。

Method: 本文提出了一个包含200个不同任务的新基准，每个任务都包含语言指令、演示和可选的图像观察。然后，本文提出了Newt模型，一个语言条件多任务世界模型，它首先在演示数据上进行预训练以获取任务感知表示和动作先验，然后通过所有任务的在线交互进行联合优化。

Result: 实验结果表明，Newt比一系列强大的基线模型具有更好的多任务性能和数据效率，表现出强大的开环控制能力，并能快速适应未见任务。

Conclusion: 本文证明了通过大规模预训练和在线交互相结合，可以训练出一个在数百个任务上表现出色的通用智能体，并为未来的研究提供了新的基准、代码和模型。

Abstract: General-purpose control demands agents that act across many tasks and embodiments, yet research on reinforcement learning (RL) for continuous control remains dominated by single-task or offline regimes, reinforcing a view that online RL does not scale. Inspired by the foundation model recipe (large-scale pretraining followed by light RL) we ask whether a single agent can be trained on hundreds of tasks with online interaction. To accelerate research in this direction, we introduce a new benchmark with 200 diverse tasks spanning many domains and embodiments, each with language instructions, demonstrations, and optionally image observations. We then present \emph{Newt}, a language-conditioned multitask world model that is first pretrained on demonstrations to acquire task-aware representations and action priors, and then jointly optimized with online interaction across all tasks. Experiments show that Newt yields better multitask performance and data-efficiency than a set of strong baselines, exhibits strong open-loop control, and enables rapid adaptation to unseen tasks. We release our environments, demonstrations, code for training and evaluation, as well as 200+ checkpoints.

</details>


### [63] [BlockCert: Certified Blockwise Extraction of Transformer Mechanisms](https://arxiv.org/abs/2511.17645)
*Sandro Andric*

Main category: cs.LG

TL;DR: BlockCert是一个用于从预训练的transformers中逆向工程提取残差块的框架，它利用了机器可验证的证书来保证近似误差，并支持局部修改。同时在不同模型上取得了不错的效果。


<details>
  <summary>Details</summary>
Motivation: 目前的机械可解释性和模型编辑工作缺乏正式的评估和明确的保证，无法衡量提取或编辑后的模型与原始模型在相关输入上的偏差。

Method: BlockCert框架通过提取残差块的结构化替代实现，并提供机器可验证的证书来限制近似误差、记录覆盖率指标和哈希底层工件。此外，它在Lean 4中形式化了一个基于Lipschitz的简单组合定理，将局部保证提升为全局偏差界限。

Result: 在GPT-2 small, TinyLlama-1.1B-Chat和Llama-3.2-3B上应用该框架后，实现了高块级覆盖率和小的残差误差。在TinyLlama设置中，完全拼接的模型在压力提示下的困惑度与基线模型仅有约6e-5的差异。

Conclusion: 带显式证书的块式提取方法对于真实的transformer语言模型是可行的，它为机械可解释性与模型行为的形式化推理之间提供了实用的桥梁。

Abstract: Mechanistic interpretability aspires to reverse-engineer neural networks into explicit algorithms, while model editing seeks to modify specific behaviours without retraining. Both areas are typically evaluated with informal evidence and ad-hoc experiments, with few explicit guarantees about how far an extracted or edited model can drift from the original on relevant inputs. We introduce BlockCert, a framework for certified blockwise extraction of transformer mechanisms, and outline how a lightweight extension can support certified local edits. Given a pre-trained transformer and a prompt distribution, BlockCert extracts structured surrogate implementations for residual blocks together with machine-checkable certificates that bound approximation error, record coverage metrics, and hash the underlying artifacts. We formalize a simple Lipschitz-based composition theorem in Lean 4 that lifts these local guarantees to a global deviation bound. Empirically, we apply the framework to GPT-2 small, TinyLlama-1.1B-Chat, and Llama-3.2-3B. Across these models we obtain high per-block coverage and small residual errors on the evaluated prompts, and in the TinyLlama setting we show that a fully stitched model matches the baseline perplexity within approximately 6e-5 on stress prompts. Our results suggest that blockwise extraction with explicit certificates is feasible for real transformer language models and offers a practical bridge between mechanistic interpretability and formal reasoning about model behaviour.

</details>


### [64] [EfficientXpert: Efficient Domain Adaptation for Large Language Models via Propagation-Aware Pruning](https://arxiv.org/abs/2511.19935)
*Songlin Zhao,Michael Pitts,Zhuwei Qin*

Main category: cs.LG

TL;DR: EfficientXpert是一个轻量级领域剪枝框架，它结合了传播感知剪枝准则（Foresight Mask）和高效的适配器更新算法（Partial Brain Surgeon），可以在LoRA微调过程中，将通用的预训练模型转化为稀疏的、领域适应性专家模型。


<details>
  <summary>Details</summary>
Motivation: 大型语言模型的快速发展增加了在法律、医疗、金融等领域对专业化模型的需求。然而，它们庞大的体量成为了在资源受限环境中部署的障碍，并且现有的压缩方法要么跨领域泛化能力差，要么会产生较高的开销。

Method: 本文提出了EfficientXpert，一个轻量级领域剪枝框架，它结合了传播感知剪枝准则（Foresight Mask）和高效的适配器更新算法（Partial Brain Surgeon）。该框架被整合到LoRA微调过程中，能够一步将通用的预训练模型转换为稀疏的、领域适应性专家模型。

Result: 在健康和法律任务中，EfficientXpert在40%的稀疏度下保留了高达98%的密集模型性能，优于现有最先进的方法。

Conclusion: 进一步分析表明，领域相关的结构性变化会降低通用剪枝掩码的有效性，这强调了需要针对每个领域量身定制的自适应、领域感知剪枝策略。

Abstract: The rapid advancement of large language models (LLMs) has increased the demand for domain-specialized variants in areas such as law, healthcare, and finance. However, their large size remains a barrier to deployment in resource-constrained environments, and existing compression methods either generalize poorly across domains or incur high overhead. In this work, we propose \textbf{EfficientXpert}, a lightweight domain-pruning framework that combines a propagation-aware pruning criterion (Foresight Mask) with an efficient adapter-update algorithm (Partial Brain Surgeon). Integrated into the LoRA fine-tuning process, EfficientXpert enables a one-step transformation of general pretrained models into sparse, domain-adapted experts. Across health and legal tasks, it retains up to 98% of dense-model performance at 40% sparsity, outperforming state-of-the-art methods. Further analysis reveals substantial domain-dependent structural shifts that degrade the effectiveness of general pruning masks, underscoring the need for adaptive, domain-aware pruning strategies tailored to each domain.

</details>


### [65] [TiCT: A Synthetically Pre-Trained Foundation Model for Time Series Classification](https://arxiv.org/abs/2511.19694)
*Chin-Chia Michael Yeh,Uday Singh Saini,Junpeng Wang,Xin Dai,Xiran Fan,Jiarui Sun,Yujie Fan,Yan Zheng*

Main category: cs.LG

TL;DR: TiCT 是一个用于时间序列语境学习分类的 transformer 模型，它专为解决标记数据稀缺问题而设计，通过合成数据预训练，并在推断时利用语境示例实现无需微调的分类。


<details>
  <summary>Details</summary>
Motivation: 开发通用时间序列基础模型面临标记数据成本高昂的挑战。现有的大规模时间序列模型主要集中在预测任务，而语境学习（ICL）在分类方面的潜力尚未被充分开发，这促使研究者寻求一种无需微调的通用分类解决方案。

Method: TiCT 模型在技术上有两大贡献：1) 提出了一种新颖的架构，包括可扩展的基于位的标签编码和特殊的输出注意力机制，以处理任意数量的类别。2) 开发了一个合成预训练框架，该框架结合了受 Mixup 启发的过程和数据增强技术，以促进模型的泛化能力和对噪声的鲁棒性。

Result: 在 UCR Archive 上的大量评估表明，TiCT 在不更新任何模型权重的情况下，仅通过推断时的语境示例，就实现了与最先进的监督方法相当的竞争性性能。

Conclusion: TiCT 通过其独特架构和合成预训练框架，成功实现了时间序列数据的语境学习分类，在解决数据标注成本高昂问题的同时，展现了强大的泛化能力和无需微调的潜力。

Abstract: The ubiquity of time series data creates a strong demand for general-purpose foundation models, yet developing them for classification remains a significant challenge, largely due to the high cost of labeled data. Foundation models capable of in-context learning (ICL) offer a powerful solution, adapting to new tasks with minimal examples and reducing the need for extensive retraining. However, prior work on large-scale time series models has predominantly focused on forecasting, leaving a critical gap for versatile, fine-tuning-free classification. To address this, we introduce TiCT (Time-series in-Context Transformer), a transformer-based model pre-trained exclusively on synthetic data to perform in-context classification. We make two primary technical contributions: 1) a novel architecture featuring a scalable bit-based label encoding and a special output attention mechanism to handle an arbitrary number of classes; and 2) a synthetic pre-training framework that combines a Mixup-inspired process with data augmentation to foster generalization and noise invariance. Extensive evaluations on the UCR Archive show that TiCT achieves competitive performance against state-of-the-art supervised methods. Crucially, this is accomplished using only in-context examples at inference time, without updating a single model weight.

</details>


### [66] [Beyond Components: Singular Vector-Based Interpretability of Transformer Circuits](https://arxiv.org/abs/2511.20273)
*Areeb Ahmad,Abhinav Joshi,Ashutosh Modi*

Main category: cs.LG

TL;DR: 这篇论文提出了一种更细粒度的方法来解释 Transformer 模型，通过将注意力头和 MLP 分解为正交奇异方向，从而揭示了其中叠加和独立的计算。


<details>
  <summary>Details</summary>
Motivation: 现有机制可解释性方法将 Transformer 的构建块视为不可分割的单元，忽略了在其中学习到的功能子结构。

Method: 本文引入了一种更细粒度的视角，将注意力头和多层感知器层（MLP）分解为正交奇异方向，揭示了单个头部或 MLP 中叠加和独立的计算。

Result: 在间接宾语识别（IOI）、性别代词（GP）和大于（GT）等任务上验证了该方法，结果表明，先前识别的规范功能头（如名称移动器）编码了与不同奇异方向对齐的多个重叠子功能。计算图中的节点在特定的低秩方向上显示出强激活，表明有意义的计算存在于紧凑的子空间中。

Conclusion: Transformer 的计算比以前设想的更加分布式、结构化和组合化。这一视角为细粒度机制可解释性以及对模型内部结构的更深层次理解开辟了新途径。

Abstract: Transformer-based language models exhibit complex and distributed behavior, yet their internal computations remain poorly understood. Existing mechanistic interpretability methods typically treat attention heads and multilayer perceptron layers (MLPs) (the building blocks of a transformer architecture) as indivisible units, overlooking possibilities of functional substructure learned within them. In this work, we introduce a more fine-grained perspective that decomposes these components into orthogonal singular directions, revealing superposed and independent computations within a single head or MLP. We validate our perspective on widely used standard tasks like Indirect Object Identification (IOI), Gender Pronoun (GP), and Greater Than (GT), showing that previously identified canonical functional heads, such as the name mover, encode multiple overlapping subfunctions aligned with distinct singular directions. Nodes in a computational graph, that are previously identified as circuit elements show strong activation along specific low-rank directions, suggesting that meaningful computations reside in compact subspaces. While some directions remain challenging to interpret fully, our results highlight that transformer computations are more distributed, structured, and compositional than previously assumed. This perspective opens new avenues for fine-grained mechanistic interpretability and a deeper understanding of model internals.

</details>


### [67] [Geometry of Decision Making in Language Models](https://arxiv.org/abs/2511.20315)
*Abhinav Joshi,Divyanshu Bhatt,Ashutosh Modi*

Main category: cs.LG

TL;DR: 该研究通过内在维度（ID）分析大型语言模型（LLMs）隐藏表征的几何结构，揭示了LLMs在多项选择问答（MCQA）任务中决策过程的动态，发现其分层学习呈现低维、扩展再压缩的模式，最终形成与任务决策相关的低维流形。


<details>
  <summary>Details</summary>
Motivation: 探索大型语言模型（LLMs）内部决策过程的透明度问题，理解其泛化能力背后的机制，特别是LLMs如何在其隐藏表示中做出预测，以及这些表示的几何特性如何随模型深度变化。

Method: 进行了大规模研究，分析了28个开源Transformer模型。使用多种估计器评估模型不同层（包括早期层、中间层和后期层）的内在维度（ID）。同时，量化了模型在多项选择问答（MCQA）任务中，每一层的表现。

Result: 研究发现了一个跨模型一致的内在维度（ID）模式：早期层在低维流形上操作，中间层扩展了这个空间，而后期层再次压缩这个空间，收敛到与决策相关的表示。这些结果表明，LLMs隐式地学习将语言输入投射到结构化的、低维的流形上，这些流形与特定任务的决策对齐。

Conclusion: LLMs通过学习将语言输入投射到结构化、低维的流形上，从而在内部形成与任务决策相符的表示。这种分层处理的几何洞察（低维起始、中间扩展、后期压缩）为理解LLMs的泛化和推理能力提供了新的视角。

Abstract: Large Language Models (LLMs) show strong generalization across diverse tasks, yet the internal decision-making processes behind their predictions remain opaque. In this work, we study the geometry of hidden representations in LLMs through the lens of \textit{intrinsic dimension} (ID), focusing specifically on decision-making dynamics in a multiple-choice question answering (MCQA) setting. We perform a large-scale study, with 28 open-weight transformer models and estimate ID across layers using multiple estimators, while also quantifying per-layer performance on MCQA tasks. Our findings reveal a consistent ID pattern across models: early layers operate on low-dimensional manifolds, middle layers expand this space, and later layers compress it again, converging to decision-relevant representations. Together, these results suggest LLMs implicitly learn to project linguistic inputs onto structured, low-dimensional manifolds aligned with task-specific decisions, providing new geometric insights into how generalization and reasoning emerge in language models.

</details>


### [68] [Training-Free Active Learning Framework in Materials Science with Large Language Models](https://arxiv.org/abs/2511.19730)
*Hongchen Wang,Rafael Espinosa Castañeda,Jay R. Werber,Yao Fehlis,Edward Kim,Jason Hattrick-Simpers*

Main category: cs.LG

TL;DR: 该文章介绍了一个名为LLM-AL的框架，它利用大型语言模型（LLMs）来加速科学发现过程中的主动学习（AL），通过文本描述直接提出实验方案，在多种材料科学数据集中，LLM-AL比传统机器学习模型能更有效地减少实验次数。


<details>
  <summary>Details</summary>
Motivation: 传统的主动学习（AL）中使用的机器学习模型存在冷启动问题和领域特定特征工程的限制，从而影响了其泛化能力。

Method: 本文引入了一个基于LLM的主动学习框架（LLM-AL），它在迭代的少样本设置下运行。研究探索了两种提示策略：一种使用简洁的数字输入，适用于具有更多组合和结构化特征的数据集；另一种使用扩展的描述性文本，适用于具有更多实验和程序性特征的数据集，以提供额外的上下文。将LLM-AL与四种不同的材料科学数据集上的传统机器学习模型进行了比较。

Result: 在所有数据集中，LLM-AL能够将达到最佳性能候选所需的实验次数减少70%以上，并且始终优于传统的机器学习模型。LLM-AL进行了更广泛和探索性的搜索，同时以更少的迭代次数达到了最佳效果。LLM-AL的性能在不同运行中表现出广泛的一致性。

Conclusion: LLM-AL可以作为传统AL管线的通用替代方案，实现更高效和可解释的实验选择，并为潜在的LLM驱动的自主发现提供了可能性。

Abstract: Active learning (AL) accelerates scientific discovery by prioritizing the most informative experiments, but traditional machine learning (ML) models used in AL suffer from cold-start limitations and domain-specific feature engineering, restricting their generalizability. Large language models (LLMs) offer a new paradigm by leveraging their pretrained knowledge and universal token-based representations to propose experiments directly from text-based descriptions. Here, we introduce an LLM-based active learning framework (LLM-AL) that operates in an iterative few-shot setting and benchmark it against conventional ML models across four diverse materials science datasets. We explored two prompting strategies: one using concise numerical inputs suited for datasets with more compositional and structured features, and another using expanded descriptive text suited for datasets with more experimental and procedural features to provide additional context. Across all datasets, LLM-AL could reduce the number of experiments needed to reach top-performing candidates by over 70% and consistently outperformed traditional ML models. We found that LLM-AL performs broader and more exploratory searches while still reaching the optima with fewer iterations. We further examined the stability boundaries of LLM-AL given the inherent non-determinism of LLMs and found its performance to be broadly consistent across runs, within the variability range typically observed for traditional ML approaches. These results demonstrate that LLM-AL can serve as a generalizable alternative to conventional AL pipelines for more efficient and interpretable experiment selection and potential LLM-driven autonomous discovery.

</details>


### [69] [Soft Adaptive Policy Optimization](https://arxiv.org/abs/2511.20347)
*Chang Gao,Chujie Zheng,Xiong-Hui Chen,Kai Dang,Shixuan Liu,Bowen Yu,An Yang,Shuai Bai,Jingren Zhou,Junyang Lin*

Main category: cs.LG

TL;DR: SAPO通过平滑、温度控制的门控替代硬裁剪，自适应地衰减离策略更新，同时保留有用的学习信号，从而提高训练稳定性和性能。


<details>
  <summary>Details</summary>
Motivation: 强化学习在增强大型语言模型（LLM）的推理能力方面发挥着越来越重要的作用，但稳定且高性能的策略优化仍然具有挑战性。

Method: 我们提出了软自适应策略优化（SAPO），它用平滑的、温度控制的门控取代了硬裁剪，该门控自适应地衰减偏离策略的更新，同时保留有用的学习信号。SAPO是序列连贯和令牌自适应的。

Result: 在数学推理基准测试中，SAPO表现出更高的训练稳定性和更高的Pass@1性能。SAPO在Qwen-VL模型系列上实现了持续的性能提升。

Conclusion: SAPO为LLM的强化学习训练提供了一种更可靠、可扩展和有效的优化策略。

Abstract: Reinforcement learning (RL) plays an increasingly important role in enhancing the reasoning capabilities of large language models (LLMs), yet stable and performant policy optimization remains challenging. Token-level importance ratios often exhibit high variance-a phenomenon exacerbated in Mixture-of-Experts models-leading to unstable updates. Existing group-based policy optimization methods, such as GSPO and GRPO, alleviate this problem via hard clipping, making it difficult to maintain both stability and effective learning. We propose Soft Adaptive Policy Optimization (SAPO), which replaces hard clipping with a smooth, temperature-controlled gate that adaptively attenuates off-policy updates while preserving useful learning signals. Compared with GSPO and GRPO, SAPO is both sequence-coherent and token-adaptive. Like GSPO, SAPO maintains sequence-level coherence, but its soft gating forms a continuous trust region that avoids the brittle hard clipping band used in GSPO. When a sequence contains a few highly off-policy tokens, GSPO suppresses all gradients for that sequence, whereas SAPO selectively down-weights only the offending tokens and preserves the learning signal from the near-on-policy ones, improving sample efficiency. Relative to GRPO, SAPO replaces hard token-level clipping with smooth, temperature-controlled scaling, enabling more informative and stable updates. Empirical results on mathematical reasoning benchmarks indicate that SAPO exhibits improved training stability and higher Pass@1 performance under comparable training budgets. Moreover, we employ SAPO to train the Qwen3-VL model series, demonstrating that SAPO yields consistent performance gains across diverse tasks and different model sizes. Overall, SAPO provides a more reliable, scalable, and effective optimization strategy for RL training of LLMs.

</details>


### [70] [DISCO: A Browser-Based Privacy-Preserving Framework for Distributed Collaborative Learning](https://arxiv.org/abs/2511.19750)
*Julien T. T. Vignoud,Valérian Rousset,Hugo El Guedj,Ignacio Aleman,Walid Bennaceur,Batuhan Faik Derinbay,Eduard Ďurech,Damien Gengler,Lucas Giordano,Felix Grimberg,Franziska Lippoldt,Christina Kopidaki,Jiafan Liu,Lauris Lopata,Nathan Maire,Paul Mansat,Martin Milenkoski,Emmanuel Omont,Güneş Özgün,Mina Petrović,Francesco Posa,Morgan Ridel,Giorgio Savini,Marcel Torne,Lucas Trognon,Alyssa Unell,Olena Zavertiaieva,Sai Praneeth Karimireddy,Tahseen Rabbani,Mary-Anne Hartley,Martin Jaggi*

Main category: cs.LG

TL;DR: DISCO是一个开放、易用、无需共享原始数据或编程知识即可构建机器学习模型的平台，支持跨平台和多种隐私保证。


<details>
  <summary>Details</summary>
Motivation: 解决因数据共享受限导致模型预测能力分散和资源不均的问题。

Method: DISCO平台通过在浏览器本地训练模型实现协同学习，提供联邦或去中心化范式、不同级别的隐私保障及多种权重聚合策略。

Result: DISCO能够让非技术用户在不共享原始数据的情况下，协同构建机器学习模型，支持模型个性化和偏见弹性。

Conclusion: DISCO平台通过创新的协同学习方式，有效解决了数据共享难题，提升了机器学习模型的可访问性和公平性。

Abstract: Data is often impractical to share for a range of well considered reasons, such as concerns over privacy, intellectual property, and legal constraints. This not only fragments the statistical power of predictive models, but creates an accessibility bias, where accuracy becomes inequitably distributed to those who have the resources to overcome these concerns. We present DISCO: an open-source DIStributed COllaborative learning platform accessible to non-technical users, offering a means to collaboratively build machine learning models without sharing any original data or requiring any programming knowledge. DISCO's web application trains models locally directly in the browser, making our tool cross-platform out-of-the-box, including smartphones. The modular design of \disco offers choices between federated and decentralized paradigms, various levels of privacy guarantees and several approaches to weight aggregation strategies that allow for model personalization and bias resilience in the collaborative training. Code repository is available at https://github.com/epfml/disco and a showcase web interface at https://discolab.ai

</details>


### [71] [GED-Consistent Disentanglement of Aligned and Unaligned Substructures for Graph Similarity Learning](https://arxiv.org/abs/2511.19837)
*Zhentao Zhan,Xiaoliang Xu,Jingjing Wang,Junmei Wang*

Main category: cs.LG

TL;DR: 该文章提出了一种名为GCGSim的图相似度计算框架，解决了现有GNN方法在图编辑距离（GED）近似方面的局限性，实现了最先进的性能。


<details>
  <summary>Details</summary>
Motivation: 传统的图相似度计算方法（GED）是NP-hard问题，而现有的基于GNN的近似方法存在节点匹配范式与GED核心原则不符的问题，导致未能捕捉全局结构对应关系和编辑成本的错误归因。

Method: GCGSim框架以图级匹配和子结构级编辑成本为核心，实现了三个核心技术贡献，旨在解决现有GNN-based GED方法的局限性。

Result: 在四个基准数据集上的大量实验表明，GCGSim取得了最先进的性能。

Conclusion: GCGSim框架有效地学习了解耦且语义上有意义的子结构表示，从而更好地逼近图编辑距离。

Abstract: Graph Similarity Computation (GSC) is a fundamental graph related task where Graph Edit Distance (GED) serves as a prevalent metric. GED is determined by an optimal alignment between a pair of graphs that partitions each into aligned (zero-cost) and unaligned (cost-incurring) substructures. Due to NP-hard nature of exact GED computation, GED approximations based on Graph Neural Network(GNN) have emerged. Existing GNN-based GED approaches typically learn node embeddings for each graph and then aggregate pairwise node similarities to estimate the final similarity. Despite their effectiveness, we identify a mismatch between this prevalent node-centric matching paradigm and the core principles of GED. This discrepancy leads to two critical limitations: (1) a failure to capture the global structural correspondence for optimal alignment, and (2) a misattribution of edit costs driven by spurious node level signals. To address these limitations, we propose GCGSim, a GED-consistent graph similarity learning framework centering on graph-level matching and substructure-level edit costs. Specifically, we make three core technical contributions. Extensive experiments on four benchmark datasets show that GCGSim achieves state-of-the-art performance. Our comprehensive analyses further validate that the framework effectively learns disentangled and semantically meaningful substructure representations.

</details>


### [72] [Frailty-Aware Transformer for Recurrent Survival Modeling of Driver Retention in Ride-Hailing Platforms](https://arxiv.org/abs/2511.19893)
*Shuoyan Xu,Yu Zhang,Eric J. Miller*

Main category: cs.LG

TL;DR: 该研究使用Transformer模型来分析网约车司机的空闲行为，从而提高平台留存策略并提供政策相关的见解。


<details>
  <summary>Details</summary>
Motivation: 网约车平台是一个高频率、行为驱动的环境，但生存分析在网约车司机行为建模中的应用尚未得到充分探索。

Method: 本研究将空闲行为 формуize 为一个循环生存过程，利用大规模平台数据，并提出了一个基于Transformer的框架。该框架通过因果掩码捕获长期时间依赖性，并结合特定司机嵌入来模拟潜在异质性。

Result: 在多伦多网约车数据上的实验结果表明，所提出的“Frailty-Aware Cox Transformer (FACT)”模型在时间依赖C指数和Brier分数方面均优于传统和深度学习生存模型。

Conclusion: 该方法可以实现更准确的风险估计，支持平台留存策略，并提供政策相关的见解。

Abstract: Ride-hailing platforms are characterized by high-frequency, behavior-driven environments. Although survival analysis has been applied to recurrent events in other domains, its use in modeling ride-hailing driver behavior remains largely unexplored. This study formulates idle behavior as a recurrent survival process using large-scale platform data and proposes a Transformer-based framework that captures long-term temporal dependencies with causal masking and incorporates driver-specific embeddings to model latent heterogeneity. Results on Toronto ride-hailing data demonstrate that the proposed Frailty-Aware Cox Transformer (FACT) achieves the highest time-dependent C-indices and lowest Brier Scores, outperforming classical and deep learning survival models. This approach enables more accurate risk estimation, supports platform retention strategies, and provides policy-relevant insights.

</details>


### [73] [Optimize Flip Angle Schedules In MR Fingerprinting Using Reinforcement Learning](https://arxiv.org/abs/2511.19941)
*Shenjun Zhong,Zhifeng Chen,Zhaolin Chen*

Main category: cs.LG

TL;DR: 磁共振指纹图谱（MRF）中的翻转角优化是一个复杂的顺序决策问题。


<details>
  <summary>Details</summary>
Motivation: 磁共振指纹图谱（MRF）的序列优化是一个复杂的、高维度的顺序决策问题，尤其是在优化翻转角等关键参数时。强化学习（RL）为参数选择的自动化提供了一种有前景的方法，以优化脉冲序列，从而最大化参数空间中指纹图谱的可区分性。

Method: 我们引入了一个强化学习（RL）框架来优化MRF中的翻转角调度。

Result: 我们展示了一种学习到的非周期性翻转角调度，它可以增强指纹图谱的可分离性。此外，RL优化的调度还可以减少重复时间，可能加速MRF采集。

Conclusion: 强化学习可以有效地优化MRF中的翻转角调度，从而提高指纹图谱的可分离性并加速采集。

Abstract: Magnetic Resonance Fingerprinting (MRF) leverages transient-state signal dynamics generated by the tunable acquisition parameters, making the design of an optimal, robust sequence a complex, high-dimensional sequential decision problem, such as optimizing one of the key parameters, flip angle. Reinforcement learning (RL) offers a promising approach to automate parameter selection, to optimize pulse sequences that maximize the distinguishability of fingerprints across the parameter space. In this work, we introduce an RL framework for optimizing the flip-angle schedule in MRF and demonstrate a learned schedule exhibiting non-periodic patterns that enhances fingerprint separability. Additionally, an interesting observation is that the RL-optimized schedule may enable a reduction in the number of repetition time, potentially accelerate MRF acquisitions.

</details>


### [74] [Differential Smoothing Mitigates Sharpening and Improves LLM Reasoning](https://arxiv.org/abs/2511.19942)
*Jingchu Gai,Guanning Zeng,Huaqing Zhang,Aditi Raghunathan*

Main category: cs.LG

TL;DR: RL微调的大型语言模型存在多样性崩溃，本文提出了一种名为“差分平滑”的原则性方法，可以同时提高正确性和多样性，并且性能优于传统的RL方法和基于熵的启发式方法。


<details>
  <summary>Details</summary>
Motivation: RL微调大型语言模型会导致输出多样性不足，现有方法无法有效解决多样性与正确性之间的平衡，甚至相互矛盾。

Method: 本文首先通过选择和强化偏差，从理论上证明了RL微调出现多样性崩溃的原因。然后，提出了一种名为“差分平滑”的原则性方法，该方法仅在正确轨迹上修改奖励，以同时提高正确性和多样性。

Result: 差分平滑方法在正确性和多样性方面均优于传统的RL方法和基于熵的启发式方法。在CountDown和实际数学推理任务中，对1B到7B参数的模型进行了广泛实验，在AIME24数据集上，Pass@1和Pass@k的性能均有提升，最高可达6.7%。

Conclusion: RL微调中的多样性崩溃源于选择和强化偏差。本研究提出的差分平滑方法，通过在正确轨迹上修改奖励，能够有效解决多样性崩溃问题，同时提高模型的正确性和多样性，且具有理论支撑和实验验证。

Abstract: It is widely recognized that reinforcement learning (RL) fine-tuning of large language models often leads to \textit{diversity collapse}, where outputs lack variety. Prior work has proposed a range of heuristics to counteract this effect, but these methods are ad hoc: they frequently trade off correctness for diversity, their effectiveness varies across tasks, and in some cases they even contradict one another. In this work, we place these observations on a rigorous foundation. We first provide a formal proof of why RL fine-tuning exhibits diversity collapse via a selection and reinforcement bias. Next, we make a key observation that any reward modification to address diversity collapse only needs to be applied on the correct trajectories. Building directly on this analysis, we introduce a principled method -- \textit{differential smoothing} -- that provably improves both correctness and diversity, outperforming vanilla RL as well as widely used entropy-based heuristics. Our theory precisely characterizes when existing heuristics help and why they fail, while showing that differential smoothing is universally superior. Extensive experiments with models from 1B to 7B parameters, across domains including CountDown and real-world mathematical reasoning, demonstrate consistent gains. Differential smoothing improves both Pass@1 and Pass@k, with up to 6.7\% improvements on AIME24 dataset.

</details>


### [75] [ParaBlock: Communication-Computation Parallel Block Coordinate Federated Learning for Large Language Models](https://arxiv.org/abs/2511.19959)
*Yujia Wang,Yuanpu Cao,Jinghui Chen*

Main category: cs.LG

TL;DR: 本文提出了一种名为ParaBlock的新方法，用于联邦学习环境中大型语言模型的训练/微调，通过并行处理通信和计算来提高通信效率，同时保持收敛性和性能。


<details>
  <summary>Details</summary>
Motivation: 在联邦学习中训练大型模型时，即使是单个模型块也可能包含大量参数，导致通信延迟显著，尤其对于资源受限的客户端。

Method: ParaBlock通过建立两个并行的通信和计算线程来解决联邦训练/微调大型语言模型中的通信效率问题。

Result: ParaBlock在理论上与标准联邦分块坐标下降方法达到相同的收敛速度。在对大型语言模型进行指令遵循和数学推理的微调实验中，ParaBlock不仅保持了强大的性能，而且显著提高了通信效率。

Conclusion: ParaBlock成功地解决了联邦学习中大型语言模型的通信效率挑战，同时保证了模型的收敛性和性能。

Abstract: Federated learning (FL) has been extensively studied as a privacy-preserving training paradigm. Recently, federated block coordinate descent scheme has become a popular option in training large-scale models, as it allows clients to train only a subset of the model locally instead of the entire model. However, in the era of large language models (LLMs), even a single block can contain a significant number of parameters, posing substantial communication latency, particularly for resource-constrained clients. To address this challenge in federated training/fine-tuning LLMs, we propose ParaBlock, a novel approach that establishes two parallel threads for communication and computation to enhance communication efficiency. We theoretically prove that the proposed ParaBlock achieves the same convergence rate as the standard federated block coordinate descent methods. Empirical evaluations on fine-tuning LLMs on general instruction following and mathematical reasoning confirm that ParaBlock not only maintains strong performance but also significantly improves communication efficiency.

</details>


### [76] [Stragglers Can Contribute More: Uncertainty-Aware Distillation for Asynchronous Federated Learning](https://arxiv.org/abs/2511.19966)
*Yujia Wang,Fenglong Ma,Jinghui Chen*

Main category: cs.LG

TL;DR: FedEcho通过不确定性感知蒸馏在存在异步延迟和数据异构性的异步联邦学习中表现出鲁棒性能。


<details>
  <summary>Details</summary>
Motivation: 现有的异步联邦学习方法未能同时解决过时更新和异构数据分布下快速客户端引入的偏差问题，导致性能下降。

Method: FedEcho引入了不确定性感知蒸馏机制。服务器根据对客户端预测可靠性的评估，动态调整这些预测的影响力。通过优先考虑更确定的预测，同时利用来自所有客户端的多样化信息，FedEcho减轻了过时更新和数据异构性的负面影响。

Result: FedEcho在不访问私有客户端数据的情况下，持续优于现有的异步联邦学习基线，实现了强大的性能。

Conclusion: FedEcho框架通过不确定性感知蒸馏，有效解决了异步联邦学习中过时更新和数据异构性导致的性能下降问题，为异步联邦学习提供了一种新颖而高效的解决方案。

Abstract: Asynchronous federated learning (FL) has recently gained attention for its enhanced efficiency and scalability, enabling local clients to send model updates to the server at their own pace without waiting for slower participants. However, such a design encounters significant challenges, such as the risk of outdated updates from straggler clients degrading the overall model performance and the potential bias introduced by faster clients dominating the learning process, especially under heterogeneous data distributions. Existing methods typically address only one of these issues, creating a conflict where mitigating the impact of outdated updates can exacerbate the bias created by faster clients, and vice versa. To address these challenges, we propose FedEcho, a novel framework that incorporates uncertainty-aware distillation to enhance the asynchronous FL performances under large asynchronous delays and data heterogeneity. Specifically, uncertainty-aware distillation enables the server to assess the reliability of predictions made by straggler clients, dynamically adjusting the influence of these predictions based on their estimated uncertainty. By prioritizing more certain predictions while still leveraging the diverse information from all clients, FedEcho effectively mitigates the negative impacts of outdated updates and data heterogeneity. Through extensive experiments, we demonstrate that FedEcho consistently outperforms existing asynchronous federated learning baselines, achieving robust performance without requiring access to private client data.

</details>


### [77] [On-Demand Multi-Task Sparsity for Efficient Large-Model Deployment on Edge Devices](https://arxiv.org/abs/2511.19986)
*Lianming Huang,Haibo Hu,Qiao Li,Nan Guan,Chun Jason Xue*

Main category: cs.LG

TL;DR: 该论文提出了一个按需多任务稀疏性框架，旨在通过最大化参数重用来最小化任务切换成本，从而在资源受限的边缘平台上部署大型模型。


<details>
  <summary>Details</summary>
Motivation: 在资源受限的边缘平台上部署大型模型时，稀疏性至关重要。然而，为单个任务优化稀疏模式会忽略频繁任务切换带来的 I/O 开销。

Method: 本文将权重分解为可重用的块状单元，并对齐跨任务的稀疏结构以最大化重叠。通过动态加载下一个任务所需的小型差异块集，从而有效地减轻了传统整体方法固有的冷启动延迟。

Result: 在真实的自动驾驶平台上进行的实验表明，该框架实现了卓越的切换效率，与现有稀疏性方法相比，任务切换速度平均提高了 6.6 倍以上。

Conclusion: 该按需多任务稀疏性框架通过最小化任务切换成本来有效解决大型模型在资源受限边缘部署中的挑战。

Abstract: Sparsity is essential for deploying large models on resource constrained edge platforms. However, optimizing sparsity patterns for individual tasks in isolation ignores the significant I/O overhead incurred during frequent task switching. We introduce an on-demand multi-task sparsity framework specifically designed to minimize switching costs by maximizing parameter reuse. Unlike monolithic approaches, we decompose weights into reusable block-granular units and align sparse structures across tasks to maximize overlap. By dynamically loading only the small differential set of blocks required for the next task, our method effectively mitigates the cold-start latency inherent in traditional monolithic approaches.Experiments on a real-world autonomous driving platform demonstrate that our framework achieves superior switching efficiency, accelerating task switching by over 6.6X on average compared to existing sparsity methods.

</details>


### [78] [RankOOD - Class Ranking-based Out-of-Distribution Detection](https://arxiv.org/abs/2511.19996)
*Dishanika Denipitiyage,Naveen Karunanayake,Suranga Seneviratne,Sanjay Chawla*

Main category: cs.LG

TL;DR: RankOOD是一种基于Plackett-Luce损失和秩列表的OOD检测方法，通过训练模型学习ID类预测的排序模式，并在多个基准测试中取得了SOTA性能。


<details>
  <summary>Details</summary>
Motivation: 现有的OOD检测方法可能无法充分利用深度学习模型在ID类预测中隐含的排序模式。

Method: RankOOD首先使用初始分类器为每个类别提取一个秩列表，然后使用Plackett-Luce损失进行另一轮训练，其中类别秩作为预测变量。它利用OOD示例在尊重排序分类方面的可能性较小的洞察来区分OOD和ID示例。

Result: RankOOD在近OOD的TinyImageNet评估基准上取得了SOTA性能，将FPR95降低了4.3%。

Conclusion: RankOOD通过引入基于排序的检测机制，有效提升了OOD检测的性能，为未来OOD检测研究提供了新的视角。

Abstract: We propose RankOOD, a rank-based Out-of-Distribution (OOD) detection approach based on training a model with the Placket-Luce loss, which is now extensively used for preference alignment tasks in foundational models. Our approach is based on the insight that with a deep learning model trained using the Cross Entropy Loss, in-distribution (ID) class prediction induces a ranking pattern for each ID class prediction. The RankOOD framework formalizes the insight by first extracting a rank list for each class using an initial classifier and then uses another round of training with the Plackett-Luce loss, where the class rank, a fixed permutation for each class, is the predicted variable. An OOD example may get assigned with high probability to an ID example, but the probability of it respecting the ranking classification is likely to be small. RankOOD, achieves SOTA performance on the near-ODD TinyImageNet evaluation benchmark, reducing FPR95 by 4.3%.

</details>


### [79] [REWA: Witness-Overlap Theory -- Foundations for Composable Binary Similarity Systems](https://arxiv.org/abs/2511.19998)
*Nikit Phadke*

Main category: cs.LG

TL;DR: REWA是基于见证重叠的相似性通用理论，它提供了一种将多种相似性度量LSH化的方法，并保证了在重叠间隙条件下的排序不变性。


<details>
  <summary>Details</summary>
Motivation: 当概念间的相似性可以用单调见证重叠表示时（例如图邻域、因果关系、时间结构、拓扑特征、符号模式或基于嵌入的邻域），REWA提供了一种可证明的排序保留保证的紧凑编码。

Method: REWA系统由以下三部分组成：
1. 有限见证集W(v)。
2. 从每个见证生成的半随机位分配。
3. 重叠Δ(u, v) = |W(u) ∩ W(v)|中预期相似度的单调性。
该方法证明了在最终见证集上的重叠间隙条件下，无论其如何构建，都可以使用m = O(log(|V|/δ))位保留top-k排名。

Result: 在重叠间隙条件独立的最终见证集上，REWA可以保证top-k排名使用m = O(log(|V|/δ))位进行保留。该见证集公式是组合式的，任何结构、时间、因果、拓扑、信息论或学习转换的序列都可以组合成以离散见证集结束的管道。

Conclusion: REWA统一了布隆过滤器、MinHash、LSH位图、随机投影、草图和分层过滤器等特殊情况，并为相似性系统提供了一个有原则的基础，其行为由见证重叠而非哈希函数工程决定。未来的扩展包括多位编码、加权见证和非集合表示。

Abstract: REWA introduces a general theory of similarity based on witness-overlap structures. We show that whenever similarity between concepts can be expressed as monotone witness overlap -- whether arising from graph neighborhoods, causal relations, temporal structure, topological features, symbolic patterns, or embedding-based neighborhoods -- it admits a reduction to compact encodings with provable ranking preservation guarantees. REWA systems consist of: (1) finite witness sets $W(v)$, (2) semi-random bit assignments generated from each witness, and (3) monotonicity of expected similarity in the overlap $Δ(u, v) = |W(u) \cap W(v)|$. We prove that under an overlap-gap condition on the final witness sets -- independent of how they were constructed -- top-$k$ rankings are preserved using $m = O(\log(|V|/δ))$ bits. The witness-set formulation is compositional: any sequence of structural, temporal, causal, topological, information-theoretic, or learned transformations can be combined into pipelines that terminate in discrete witness sets. The theory applies to the final witness overlap, enabling modular construction of similarity systems from reusable primitives. This yields a vast design space: millions of composable similarity definitions inherit logarithmic encoding complexity. REWA subsumes and unifies Bloom filters, minhash, LSH bitmaps, random projections, sketches, and hierarchical filters as special cases. It provides a principled foundation for similarity systems whose behavior is governed by witness overlap rather than hash-function engineering. This manuscript presents the axioms, the main reducibility theorem, complete proofs with explicit constants, and a detailed discussion of compositional design, limitations, and future extensions including multi-bit encodings, weighted witnesses, and non-set representations.

</details>


### [80] [Zero-Shot Transfer Capabilities of the Sundial Foundation Model for Leaf Area Index Forecasting](https://arxiv.org/abs/2511.20004)
*Peining Zhang,Hongchen Qin,Haochen Zhang,Ziqi Guo,Guiling Wang,Jinbo Bi*

Main category: cs.LG

TL;DR: 本文探讨了时间序列基础模型在农业监测中对叶面积指数 (LAI) 进行零样本预测的能力。


<details>
  <summary>Details</summary>
Motivation: 探索时间序列基础模型在农业监测中对叶面积指数 (LAI) 的零样本预测能力，并与传统方法进行对比。

Method: 使用 HiQ 数据集（美国，2000-2022），系统地比较了统计基线、完全监督的 LSTM 和 Sundial 基础模型在多种评估协议下的表现。

Result: Sundial 在零样本设置下，当输入上下文窗口足够长（覆盖超过一个或两个完整的季节周期）时，可以超越经过完全训练的 LSTM。

Conclusion: 通用基础模型无需任务特定调整，即可在遥感时间序列预测中超越专门的监督模型，这表明预训练时间序列基础模型在农业和环境应用中具有作为即插即用预测器的巨大潜力。

Abstract: This work investigates the zero-shot forecasting capability of time-series foundation models for Leaf Area Index (LAI) forecasting in agricultural monitoring. Using the HiQ dataset (U.S., 2000-2022), we systematically compare statistical baselines, a fully supervised LSTM, and the Sundial foundation model under multiple evaluation protocols. We find that Sundial, in the zero-shot setting, can outperform a fully trained LSTM provided that the input context window is sufficiently long-specifically, when covering more than one or two full seasonal cycles. This demonstrates, for the first time, that a general-purpose foundation model can surpass specialized supervised models on remote-sensing time series prediction without any task-specific tuning. These results highlight the strong potential of pretrained time-series foundation models to serve as effective plug-and-play forecasters in agricultural and environmental applications.

</details>


### [81] [iRadioDiff: Physics-Informed Diffusion Model for Indoor Radio Map Construction and Localization](https://arxiv.org/abs/2511.20015)
*Xiucheng Wang,Tingwei Yuan,Yang Cao,Nan Cheng,Ruijin Sun,Weihua Zhuang*

Main category: cs.LG

TL;DR: iRadioDiff：一个无需采样的基于扩散的室内无线电图（RM）构建框架。iRadioDiff通过结合物理信息提示和多径关键先验来解决室内无线电图构建的挑战，实现了高精度的室内定位。


<details>
  <summary>Details</summary>
Motivation: 构建高精度的室内无线电图（RM）仍然具有挑战性，因为电磁（EM）求解器的延迟过高，并且现有的基于学习的方法依赖于稀疏测量或同质材料假设，这与室内环境的异构和多径丰富的特性不符。

Method: iRadioDiff是一个无需采样的基于扩散的室内RM构建框架。它以接入点（AP）位置和由材料反射和透射系数编码的物理信息提示为条件。它进一步结合了多径关键先验，包括衍射点、强传输边界和视距（LoS）轮廓，通过条件通道和边界加权目标来指导生成过程。

Result: iRadioDiff在室内RM构建和基于接收信号强度的室内定位方面取得了最先进的性能，并在布局和材料配置方面提供了有效的泛化能力。

Conclusion: iRadioDiff通过其独特的设计，能够准确建模非平稳场不连续性，并高效构建物理一致的RM，从而在室内RM构建和定位方面实现了卓越的性能。

Abstract: Radio maps (RMs) serve as environment-aware electromagnetic (EM) representations that connect scenario geometry and material properties to the spatial distribution of signal strength, enabling localization without costly in-situ measurements. However, constructing high-fidelity indoor RMs remains challenging due to the prohibitive latency of EM solvers and the limitations of learning-based methods, which often rely on sparse measurements or assumptions of homogeneous material, which are misaligned with the heterogeneous and multipath-rich nature of indoor environments. To overcome these challenges, we propose iRadioDiff, a sampling-free diffusion-based framework for indoor RM construction. iRadioDiff is conditioned on access point (AP) positions, and physics-informed prompt encoded by material reflection and transmission coefficients. It further incorporates multipath-critical priors, including diffraction points, strong transmission boundaries, and line-of-sight (LoS) contours, to guide the generative process via conditional channels and boundary-weighted objectives. This design enables accurate modeling of nonstationary field discontinuities and efficient construction of physically consistent RMs. Experiments demonstrate that iRadioDiff achieves state-of-the-art performance in indoor RM construction and received signal strength based indoor localization, which offers effective generalization across layouts and material configurations. Code is available at https://github.com/UNIC-Lab/iRadioDiff.

</details>


### [82] [Cross-Contrastive Clustering for Multimodal Attributed Graphs with Dual Graph Filtering](https://arxiv.org/abs/2511.20030)
*Haoran Zheng,Renchi Yang,Hongtao Wang,Jianliang Xu*

Main category: cs.LG

TL;DR: DGF 是一种新的 MMAG 聚类方法，它通过去噪和三向对比学习来提高聚类性能。


<details>
  <summary>Details</summary>
Motivation: 现有的多视图聚类方法在处理多模态属性图（MMAGs）时表现不佳，因为它们未能充分考虑大型预训练语言和视觉模型输出的多模态属性的独特特性，如模态间低相关性和特征级噪声。

Method: 本文提出了双图滤波（DGF）方案，该方案创新性地将特征级去噪组件融入节点表示学习中，并采用了三向对比训练策略，利用跨模态、邻域和社区的实例级对比学习来学习鲁棒和有区别的节点表示。

Result: 在八个基准 MMAG 数据集上的综合实验表明，DGF 在聚类质量方面始终显著优于各种最先进的基线方法。

Conclusion: DGF 通过有效处理多模态属性的独特挑战，显著提高了 MMAG 聚类任务的性能。

Abstract: Multimodal Attributed Graphs (MMAGs) are an expressive data model for representing the complex interconnections among entities that associate attributes from multiple data modalities (text, images, etc.). Clustering over such data finds numerous practical applications in real scenarios, including social community detection, medical data analytics, etc. However, as revealed by our empirical studies, existing multi-view clustering solutions largely rely on the high correlation between attributes across various views and overlook the unique characteristics (e.g., low modality-wise correlation and intense feature-wise noise) of multimodal attributes output by large pre-trained language and vision models in MMAGs, leading to suboptimal clustering performance.
  Inspired by foregoing empirical observations and our theoretical analyses with graph signal processing, we propose the Dual Graph Filtering (DGF) scheme, which innovatively incorporates a feature-wise denoising component into node representation learning, thereby effectively overcoming the limitations of traditional graph filters adopted in the extant multi-view graph clustering approaches. On top of that, DGF includes a tri-cross contrastive training strategy that employs instance-level contrastive learning across modalities, neighborhoods, and communities for learning robust and discriminative node representations. Our comprehensive experiments on eight benchmark MMAG datasets exhibit that DGF is able to outperform a wide range of state-of-the-art baselines consistently and significantly in terms of clustering quality measured against ground-truth labels.

</details>


### [83] [RED-F: Reconstruction-Elimination based Dual-stream Contrastive Forecasting for Multivariate Time Series Anomaly Prediction](https://arxiv.org/abs/2511.20044)
*PengYu Chen,Xiaohou Shi,Yuan Chang,Yan Sun,Sajal K. Das*

Main category: cs.LG

TL;DR: 变长的多元时间序列的异常早发现是一个重要但困难的问题。传统方法倾向于重建正常模式，反而掩盖了异常前兆。本文提出了RED-F框架，通过“重建-消除”和“双流对比预测”来放大微弱的异常信号。


<details>
  <summary>Details</summary>
Motivation: 现有的无监督异常预测方法在多元时间序列中难以识别和预测异常前兆，因为它们倾向于重建正常模式，从而淹没了异常信号。

Method: RED-F框架包含两个主要部分：
1. 重建-消除模型（REM）：利用混合时频机制来减轻异常前兆的影响，生成一个纯净的、基于正常模式的基线。
2. 双流对比预测模型（DFM）：以纯净基线和包含异常前兆的原始序列作为并行输入。通过对比预测，计算两个预测流之间的差异，将困难的绝对信号检测转化为更容易的相对轨迹比较。这种对比机制有助于放大微弱的异常前兆信号。
3. DFM通过新颖的多序列预测（MSP）目标进行训练，利用遥远未来的上下文来增强其预测敏感性。

Result: 在六个真实世界数据集上的大量实验表明，RED-F在异常预测任务中具有卓越的性能。

Conclusion: RED-F框架通过其独特的重建-消除和双流对比预测机制，有效地解决了多元时间序列中异常前兆难以预测的问题，显著提升了异常预测的准确性。

Abstract: The proactive prediction of anomalies (AP) in mul- tivariate time series (MTS) is a critical challenge to ensure system dependability. The difficulty lies in identifying subtle anomaly precursors concealed within normal signals. However, existing unsupervised methods, trained exclusively on normal data, demonstrate a fundamental propensity to reconstruct normal patterns. Consequently, when confronted with weak precursors, their predictions are dominated by the normal pattern, submerging the very signal required for prediction. To contend with the limitation, we propose RED-F, a Reconstruction- Elimination based Dual-stream Contrastive Forecasting frame- work, comprising the Reconstruction-Elimination Model (REM) and the Dual-stream Contrastive Forecasting Model (DFM). The REM utilizes a hybrid time-frequency mechanism to mitigate the precursor, generating a purified, normal-pattern baseline. The DFM then receives this purified baseline and the original sequence which retains the precursor as parallel inputs. At the core of our framework, RED-F employs a contrastive forecast that transforms the difficult task of absolute signal detection into a simpler, more robust task of relative trajectory comparison by computing the divergence between these two predictive streams. This contrastive mechanism serves to amplify the faint precursor signal. Furthermore, the DFM is trained with a novel Multi-Series Prediction (MSP) objective, which leverages distant future con- text to enhance its predictive sensitivity. Extensive experiments on six real-world datasets demonstrate the superior capability of RED-F in anomaly prediction tasks.

</details>


### [84] [QiMeng-CRUX: Narrowing the Gap between Natural Language and Verilog via Core Refined Understanding eXpression](https://arxiv.org/abs/2511.20099)
*Lei Huang,Rui Zhang,Jiaming Guo,Yang Zhang,Di Huang,Shuyao Cheng,Pengwei Jin,Chongxiao Li,Zidong Du,Xing Hu,Qi Guo,Yunji Chen*

Main category: cs.LG

TL;DR: 该文章介绍CRUX，一种结构化中间表示，用于将自然语言转换为精确的Verilog代码，通过两阶段训练框架，CRUX-V模型在Verilog生成基准测试中表现出色。


<details>
  <summary>Details</summary>
Motivation: 现有硬件描述语言（HDL）生成方法依赖于自由形式的自然语言描述，这些描述通常模糊、冗余且非结构化，对Verilog代码生成构成挑战。

Method: 本文引入CRUX（Core Refined Understanding eXpression）作为结构化中间空间，捕捉用户意图的基本语义并组织表达式以实现精确的Verilog代码生成。此外，设计了一个两阶段训练框架，包括联合表达式建模和双空间优化，以提高CRUX和Verilog代码的质量。

Result: 实验表明，CRUX-V模型在多个Verilog生成基准测试中取得了最先进的性能，特别是在具有挑战性的设计任务中。CRUX空间在用作其他代码模型的输入提示时，也证明了其可迁移性和有效性，突出了其在弥合自由形式自然语言描述与精确Verilog生成之间差距方面的有效性。

Conclusion: CRUX有效地将自由形式的自然语言描述转化为精确的Verilog代码，并通过CRUX-V模型在硬件描述语言生成方面取得了显著进展，解决了现有方法的局限性。

Abstract: Large language models (LLMs) have shown promising capabilities in hardware description language (HDL) generation. However, existing approaches often rely on free-form natural language descriptions that are often ambiguous, redundant, and unstructured, which poses significant challenges for downstream Verilog code generation. We treat hardware code generation as a complex transformation from an open-ended natural language space to a domain-specific, highly constrained target space. To bridge this gap, we introduce Core Refined Understanding eXpression (CRUX), a structured intermediate space that captures the essential semantics of user intent while organizing the expression for precise Verilog code generation. We further design a two-stage training framework, comprising Joint Expression Modeling and Dual-Space Optimization, to enhance the quality of both CRUX and Verilog code. Experiments across multiple Verilog generation benchmarks demonstrate that our model, CRUX-V, achieves state-of-the-art performance among general models, particularly under challenging design tasks. Furthermore, the CRUX space proves transferable and beneficial when used as input prompts for other code models, highlighting its effectiveness in narrowing the gap between free-form natural language descriptions and precise Verilog generation.

</details>


### [85] [Multivariate Forecasting of Bitcoin Volatility with Gradient Boosting: Deterministic, Probabilistic, and Feature Importance Perspectives](https://arxiv.org/abs/2511.20105)
*Grzegorz Dudek,Mateusz Kasprzyk,Paweł Pełka*

Main category: cs.LG

TL;DR: 本研究利用LGBM模型对非特币的已实现波动率进行确定性及概率性预测，并通过特征重要性技术识别波动率的主要驱动因素。


<details>
  <summary>Details</summary>
Motivation: 探索LGBM 模型在比特币波动率预测中的应用潜力，并识别波动率的关键驱动因素。

Method: 采用LGBM模型进行确定性及概率性预测，并使用两种分位数方法（直接分位数回归和残差模拟）进行概率性预测。通过增益和置换特征重要性技术识别波动率驱动因素。

Result: LGBM模型在比特币波动率预测中表现出色，能够有效捕捉加密货币市场的非线性和高方差特性。交易量、滞后波动率、投资者关注度和市值是波动率的主要驱动因素。

Conclusion: LGBM模型是比特币波动率预测的有效工具，并能提供波动率动态的可解释性见解。

Abstract: This study investigates the application of the Light Gradient Boosting Machine (LGBM) model for both deterministic and probabilistic forecasting of Bitcoin realized volatility. Utilizing a comprehensive set of 69 predictors -- encompassing market, behavioral, and macroeconomic indicators -- we evaluate the performance of LGBM-based models and compare them with both econometric and machine learning baselines. For probabilistic forecasting, we explore two quantile-based approaches: direct quantile regression using the pinball loss function, and a residual simulation method that transforms point forecasts into predictive distributions. To identify the main drivers of volatility, we employ gain-based and permutation feature importance techniques, consistently highlighting the significance of trading volume, lagged volatility measures, investor attention, and market capitalization. The results demonstrate that LGBM models effectively capture the nonlinear and high-variance characteristics of cryptocurrency markets while providing interpretable insights into the underlying volatility dynamics.

</details>


### [86] [CLIMATEAGENT: Multi-Agent Orchestration for Complex Climate Data Science Workflows](https://arxiv.org/abs/2511.20109)
*Hyeonjae Kim,Chenyue Li,Wen Deng,Mengxi Jin,Wen Huang,Mengqian Lu,Binhang Yuan*

Main category: cs.LG

TL;DR: 为了应对气候科学对自动化工作流的需求，我们提出了ClimateAgent，一个多智能体框架，用于协调端到端的气候数据分析工作流。


<details>
  <summary>Details</summary>
Motivation: 现有的通用LLM智能体和静态脚本管道在气候科学领域表现不佳，因为它们缺乏特定于气候的上下文和灵活性。

Method: ClimateAgent通过Orchestrate-Agent和Plan-Agent将用户问题分解为可执行的子任务，Data-Agents通过自省API获取数据并生成下载脚本，Coding-Agent生成Python代码、可视化和最终报告，并内置自校正循环。

Result: ClimateAgent在Climate-Agent-Bench-85（包含85个真实世界任务的基准）上，实现了100%的任务完成率和8.32的报告质量得分，优于GitHub-Copilot（6.27）和GPT-5基线（3.26）。

Conclusion: ClimateAgent的多智能体编排与动态API感知和自校正执行显著提升了气候科学分析任务的可靠端到端自动化水平。

Abstract: Climate science demands automated workflows to transform comprehensive questions into data-driven statements across massive, heterogeneous datasets. However, generic LLM agents and static scripting pipelines lack climate-specific context and flexibility, thus, perform poorly in practice. We present ClimateAgent, an autonomous multi-agent framework that orchestrates end-to-end climate data analytic workflows. ClimateAgent decomposes user questions into executable sub-tasks coordinated by an Orchestrate-Agent and a Plan-Agent; acquires data via specialized Data-Agents that dynamically introspect APIs to synthesize robust download scripts; and completes analysis and reporting with a Coding-Agent that generates Python code, visualizations, and a final report with a built-in self-correction loop. To enable systematic evaluation, we introduce Climate-Agent-Bench-85, a benchmark of 85 real-world tasks spanning atmospheric rivers, drought, extreme precipitation, heat waves, sea surface temperature, and tropical cyclones. On Climate-Agent-Bench-85, ClimateAgent achieves 100% task completion and a report quality score of 8.32, outperforming GitHub-Copilot (6.27) and a GPT-5 baseline (3.26). These results demonstrate that our multi-agent orchestration with dynamic API awareness and self-correcting execution substantially advances reliable, end-to-end automation for climate science analytic tasks.

</details>


### [87] [On the Limits of Momentum in Decentralized and Federated Optimization](https://arxiv.org/abs/2511.20168)
*Riccardo Zaccone,Sai Praneeth Karimireddy,Carlo Masone*

Main category: cs.LG

TL;DR: 本文分析了在分散式联邦学习中，动量法在客户端循环参与下，仍然受到统计异质性的影响，并且指出减小的步长也无助于收敛。


<details>
  <summary>Details</summary>
Motivation: 探索动量法在去中心化联邦学习中的应用，并解决在无界异质性下能否保证收敛的疑问。

Method: 理论分析动量法在循环客户端参与下的收敛性，并与SGD进行比较。

Result: 动量法仍然不可避免地受到统计异质性的影响，且步长减小策略（快于$Θ(1/t)$）会导致收敛到一个依赖于初始化和异质性界限的常数值。

Conclusion: 动量法在分散式联邦学习中面对统计异质性时，收敛性仍是挑战，减小的步长也无法有效解决。

Abstract: Recent works have explored the use of momentum in local methods to enhance distributed SGD. This is particularly appealing in Federated Learning (FL), where momentum intuitively appears as a solution to mitigate the effects of statistical heterogeneity. Despite recent progress in this direction, it is still unclear if momentum can guarantee convergence under unbounded heterogeneity in decentralized scenarios, where only some workers participate at each round. In this work we analyze momentum under cyclic client participation, and theoretically prove that it remains inevitably affected by statistical heterogeneity. Similarly to SGD, we prove that decreasing step-sizes do not help either: in fact, any schedule decreasing faster than $Θ\left(1/t\right)$ leads to convergence to a constant value that depends on the initialization and the heterogeneity bound. Numerical results corroborate the theory, and deep learning experiments confirm its relevance for realistic settings.

</details>


### [88] [Learning Subgroups with Maximum Treatment Effects without Causal Heuristics](https://arxiv.org/abs/2511.20189)
*Lincen Yang,Zhong Li,Matthijs van Leeuwen,Saber Salehkaleybar*

Main category: cs.LG

TL;DR: 该文章旨在解决在精准医疗、公共政策和教育等领域中，发现具有最大平均治疗效果的亚组这一关键问题。


<details>
  <summary>Details</summary>
Motivation: 传统的亚组发现方法存在局限性，主要体现在两个方面：一是将亚组估计转化为点估计问题，增加了难度；二是使用缺乏理论依据的“因果”启发式方法。这些方法都不能直接从结构因果模型（SCM）的角度来理解亚组发现问题。

Method: 文章在SCM框架下，提出了一种新的亚组发现方法。该方法假设存在一个基于分区的模型，将亚组发现问题简化为数据生成模型的恢复，从而转化为标准的监督学习问题（回归或分类）。文章以CART为例，说明了如何利用这种方法学习具有最大治疗效果的亚组。

Result: 在大量的合成和半合成数据集上的实验表明，该方法比现有基线方法能更准确地识别出具有最大治疗效果的亚组，并且避免了使用因果启发式方法。

Conclusion: 文章提出了一种基于SCM框架的亚组发现新方法，该方法将亚组发现问题转化为监督学习问题，并通过实验证明了其有效性。

Abstract: Discovering subgroups with the maximum average treatment effect is crucial for targeted decision making in domains such as precision medicine, public policy, and education. While most prior work is formulated in the potential outcome framework, the corresponding structural causal model (SCM) for this task has been largely overlooked. In practice, two approaches dominate. The first estimates pointwise conditional treatment effects and then fits a tree on those estimates, effectively turning subgroup estimation into the harder problem of accurate pointwise estimation. The second constructs decision trees or rule sets with ad-hoc 'causal' heuristics, typically without rigorous justification for why a given heuristic may be used or whether such heuristics are necessary at all. We address these issues by studying the problem directly under the SCM framework. Under the assumption of a partition-based model, we show that optimal subgroup discovery reduces to recovering the data-generating models and hence a standard supervised learning problem (regression or classification). This allows us to adopt any partition-based methods to learn the subgroup from data. We instantiate the approach with CART, arguably one of the most widely used tree-based methods, to learn the subgroup with maximum treatment effect. Finally, on a large collection of synthetic and semi-synthetic datasets, we compare our method against a wide range of baselines and find that our approach, which avoids such causal heuristics, more accurately identifies subgroups with maximum treatment effect. Our source code is available at https://github.com/ylincen/causal-subgroup.

</details>


### [89] [In-Context Compositional Learning via Sparse Coding Transformer](https://arxiv.org/abs/2511.20194)
*Wei Chen,Jingxi Yu,Zichen Miao,Qiang Qiu*

Main category: cs.LG

TL;DR: Transformer在语言、视觉和多模态任务中取得了显著成功，但在上下文组合学习任务中仍面临挑战。本文提出了一种受稀疏编码启发的注意力机制的重新 формуulation，以增强其处理组合任务的能力。


<details>
  <summary>Details</summary>
Motivation: Transformer在处理上下文组合学习任务时表现不佳，因为它们并非天生为处理组合任务而设计，且结构归纳偏差有限。

Method: 本文提出了一种将注意力块重新解释为通过投影到两组学习字典原子（编码字典和解码字典）将输入映射到输出的方法。编码字典将输入分解为一组系数，这些系数代表输入的组合结构。通过对这些系数施加稀疏性约束，增强结构化表示。稀疏系数随后用于线性组合解码字典原子以生成输出。此外，为了辅助组合泛化任务，本文提出将目标问题的系数估计为从上下文示例获得的系数的线性组合。

Result: 在S-RAVEN和RAVEN数据集上的实验结果表明，该方法在某些组合泛化任务中，即使标准Transformer模型失败，也能保持良好的性能。

Conclusion: 本文提出的方法通过学习和应用组合规则，有效地增强了Transformer处理组合泛化任务的能力。

Abstract: Transformer architectures have achieved remarkable success across language, vision, and multimodal tasks, and there is growing demand for them to address in-context compositional learning tasks. In these tasks, models solve the target problems by inferring compositional rules from context examples, which are composed of basic components structured by underlying rules. However, some of these tasks remain challenging for Transformers, which are not inherently designed to handle compositional tasks and offer limited structural inductive bias. In this work, inspired by the principle of sparse coding, we propose a reformulation of the attention to enhance its capability for compositional tasks. In sparse coding, data are represented as sparse combinations of dictionary atoms with coefficients that capture their compositional rules. Specifically, we reinterpret the attention block as a mapping of inputs into outputs through projections onto two sets of learned dictionary atoms: an encoding dictionary and a decoding dictionary. The encoding dictionary decomposes the input into a set of coefficients, which represent the compositional structure of the input. To enhance structured representations, we impose sparsity on these coefficients. The sparse coefficients are then used to linearly combine the decoding dictionary atoms to generate the output. Furthermore, to assist compositional generalization tasks, we propose estimating the coefficients of the target problem as a linear combination of the coefficients obtained from the context examples. We demonstrate the effectiveness of our approach on the S-RAVEN and RAVEN datasets. For certain compositional generalization tasks, our method maintains performance even when standard Transformers fail, owing to its ability to learn and apply compositional rules.

</details>


### [90] [Communication-Efficient Learning for Satellite Constellations](https://arxiv.org/abs/2511.20220)
*Ruxandra-Stefania Tudose,Moritz H. W. Grüss,Grace Ra Kim,Karl H. Johansson,Nicola Bastianello*

Main category: cs.LG

TL;DR: 本文提出了一种联邦学习算法，用于解决卫星星座中的学习问题，通过减少通信和引入误差反馈机制，在保证模型精度的同时提高了通信效率。


<details>
  <summary>Details</summary>
Motivation: 卫星星座在低地球轨道广泛部署，但如何有效利用卫星星座解决学习问题仍是一个挑战。特别是在联邦学习场景下，需要解决通信效率和模型准确性的平衡问题。

Method: 本文提出了一种通信高效的联邦学习算法。该算法通过两种机制减少与地面站的通信：1. 局部训练，减少通信次数；2. 数据压缩，减少通信数据量。此外，该算法还引入了一种误差反馈机制来提高模型精度，并提供了一种与算法无关的误差反馈方案。

Result: 通过在真实的太空场景中进行仿真，本文提出的算法与现有技术相比，在性能上表现出优越性。

Conclusion: 本文提出了一种新颖的、通信高效的联邦学习算法，通过减少通信开销和引入误差反馈机制，在卫星星座学习任务中实现了高精度。该算法的有效性在仿真中得到了验证，并展现出优于现有技术的性能。

Abstract: Satellite constellations in low-Earth orbit are now widespread, enabling positioning, Earth imaging, and communications. In this paper we address the solution of learning problems using these satellite constellations. In particular, we focus on a federated approach, where satellites collect and locally process data, with the ground station aggregating local models. We focus on designing a novel, communication-efficient algorithm that still yields accurate trained models. To this end, we employ several mechanisms to reduce the number of communications with the ground station (local training) and their size (compression). We then propose an error feedback mechanism that enhances accuracy, which yields, as a byproduct, an algorithm-agnostic error feedback scheme that can be more broadly applied. We analyze the convergence of the resulting algorithm, and compare it with the state of the art through simulations in a realistic space scenario, showcasing superior performance.

</details>


### [91] [Interpretable Air Pollution Forecasting by Physics-Guided Spatiotemporal Decoupling](https://arxiv.org/abs/2511.20257)
*Zhiguo Zhang,Xiaoliang Ma,Daniel Schlesinger*

Main category: cs.LG

TL;DR: 该研究提出了一个物理引导的、可解释的、时空学习框架，用于准确预测空气污染，该框架在性能和可解释性之间取得了平衡，并在斯德哥尔摩地区超越了最先进的基线模型。


<details>
  <summary>Details</summary>
Motivation: 传统的空气污染预测模型在性能和可解释性之间存在权衡，因此需要开发一种既准确又可解释的模型来改善公共健康管理。

Method: 1. 将空气污染物浓度时空行为分解为两个透明的附加模块。
2. 第一个模块是物理引导的传输核，其定向权重受风和地理（平流）条件的影响。
3. 第二个模块是可解释的注意力机制，该机制学习局部响应并将未来浓度归因于特定的历史滞后和外生驱动因素。

Result: 该模型在多个预测 horizons 上的表现始终优于斯德哥尔摩地区综合数据集上的最先进基线模型。

Conclusion: 该模型整合了高预测性能和时空可解释性，为实际应用中的空气质量管理提供了更可靠的基础。

Abstract: Accurate and interpretable air pollution forecasting is crucial for public health, but most models face a trade-off between performance and interpretability. This study proposes a physics-guided, interpretable-by-design spatiotemporal learning framework. The model decomposes the spatiotemporal behavior of air pollutant concentrations into two transparent, additive modules. The first is a physics-guided transport kernel with directed weights conditioned on wind and geography (advection). The second is an explainable attention mechanism that learns local responses and attributes future concentrations to specific historical lags and exogenous drivers. Evaluated on a comprehensive dataset from the Stockholm region, our model consistently outperforms state-of-the-art baselines across multiple forecasting horizons. Our model's integration of high predictive performance and spatiotemporal interpretability provides a more reliable foundation for operational air-quality management in real-world applications.

</details>


### [92] [HVAdam: A Full-Dimension Adaptive Optimizer](https://arxiv.org/abs/2511.20277)
*Yiheng Zhang,Shaowu Wu,Yuanzhuo Xu,Jiajun Wu,Shang Xu,Steve Drew,Xiaoguang Niu*

Main category: cs.LG

TL;DR: 本文提出了一种新型优化器Anon，它通过可调的自适应性解决了自适应优化器（如Adam）在泛化能力上不如非自适应方法（如SGD）的问题。


<details>
  <summary>Details</summary>
Motivation: 自适应优化器（如Adam）在训练大型模型方面取得了巨大成功，但其泛化能力在经典架构上通常不如非自适应方法（如SGD）。作者认为，预处理器中的自适应性限制了优化器适应不同优化环境的能力。

Method: 本文提出了Anon优化器，它具有连续可调的自适应性，可以在SGD和Adam的行为之间进行插值，甚至可以超越两者。为了确保在整个自适应范围内的收敛性，引入了增量延迟更新（IDU）机制，该机制比AMSGrad的硬最大跟踪策略更灵活，并增强了对梯度噪声的鲁棒性。

Result: Anon在图像分类、扩散和语言建模任务上持续优于最先进的优化器。

Conclusion: 自适应性可以作为一个有价值的可调设计原则，Anon提供了一个统一且可靠的框架，能够弥合经典优化器和现代优化器之间的差距，并超越它们的优点。

Abstract: Adaptive optimizers such as Adam have achieved great success in training large-scale models like large language models and diffusion models. However, they often generalize worse than non-adaptive methods, such as SGD on classical architectures like CNNs. We identify a key cause of this performance gap: adaptivity in pre-conditioners, which limits the optimizer's ability to adapt to diverse optimization landscapes. To address this, we propose Anon (Adaptivity Non-restricted Optimizer with Novel convergence technique), a novel optimizer with continuously tunable adaptivity
  , allowing it to interpolate between SGD-like and Adam-like behaviors and even extrapolate beyond both. To ensure convergence across the entire adaptivity spectrum, we introduce incremental delay update (IDU), a novel mechanism that is more flexible than AMSGrad's hard max-tracking strategy and enhances robustness to gradient noise. We theoretically establish convergence guarantees under both convex and non-convex settings. Empirically, Anon consistently outperforms state-of-the-art optimizers on representative image classification, diffusion, and language modeling tasks. These results demonstrate that adaptivity can serve as a valuable tunable design principle, and Anon provides the first unified and reliable framework capable of bridging the gap between classical and modern optimizers and surpassing their advantageous properties.

</details>


### [93] [MXtalTools: A Toolkit for Machine Learning on Molecular Crystals](https://arxiv.org/abs/2511.20327)
*Michael Kilgour,Mark E. Tuckerman,Jutta Rogal*

Main category: cs.LG

TL;DR: MXtalTools是一个用于分子晶体数据驱动建模的Python包，旨在促进分子固态的机器学习研究。


<details>
  <summary>Details</summary>
Motivation: 开发一个灵活的Python包，以促进分子晶体的数据驱动建模和机器学习研究。

Method: MXtalTools包含多个实用程序类，包括：分子和晶体数据集的合成、整理和管理；模型训练和推理的集成工作流；晶体参数化和表示；晶体结构采样和优化；以及端到端可微分的晶体采样、构建和分析。该工具包利用CUDA加速实现高通量晶体建模。

Result: MXtalTools提供了一个模块化的函数库，可以集成到现有工作流中，或用于构建新的建模管道。它通过CUDA加速实现了高通量晶体建模。

Conclusion: MXtalTools为一个灵活的Python包，可以很好的支持分子晶体的机器学习，支持数据驱动建模，大大提高了晶体建模的效率。

Abstract: We present MXtalTools, a flexible Python package for the data-driven modelling of molecular crystals, facilitating machine learning studies of the molecular solid state. MXtalTools comprises several classes of utilities: (1) synthesis, collation, and curation of molecule and crystal datasets, (2) integrated workflows for model training and inference, (3) crystal parameterization and representation, (4) crystal structure sampling and optimization, (5) end-to-end differentiable crystal sampling, construction and analysis. Our modular functions can be integrated into existing workflows or combined and used to build novel modelling pipelines. MXtalTools leverages CUDA acceleration to enable high-throughput crystal modelling. The Python code is available open-source on our GitHub page, with detailed documentation on ReadTheDocs.

</details>


### [94] [Identifying environmental factors associated with tetrodotoxin contamination in bivalve mollusks using eXplainable AI](https://arxiv.org/abs/2511.20395)
*M. C. Schoppema,B. H. M. van der Velden,A. Hürriyetoğlu,M. D. Klijnstra,E. J. Faassen,A. Gerssen,H. J. van der Fels-Klerx*

Main category: cs.LG

TL;DR: 这篇文章介绍了一个可解释的深度学习模型，用于预测荷兰泽兰河口双壳类动物中的TTX污染。


<details>
  <summary>Details</summary>
Motivation: TTX污染对食品安全和经济造成风险，因此早期预测TTX污染至关重要。

Method: 开发了一个可解释的深度学习模型，利用气象和水文特征作为输入来预测TTX污染。

Result: 日出时间、日落时间、全球辐射、水温和氯化物浓度对TTX污染的影响最大，有效日照时间是TTX污染的重要驱动因素。

Conclusion: 该模型确定了日照时数、全球辐射、水温和水氯化物浓度等环境因素与双壳类动物中TTX污染之间的关联，为减轻海洋毒素风险提供了有价值的工具。

Abstract: Since 2012, tetrodotoxin (TTX) has been found in seafoods such as bivalve mollusks in temperate European waters. TTX contamination leads to food safety risks and economic losses, making early prediction of TTX contamination vital to the food industry and competent authorities. Recent studies have pointed to shallow habitats and water temperature as main drivers to TTX contamination in bivalve mollusks. However, the temporal relationships between abiotic factors, biotic factors, and TTX contamination remain unexplored.
  We have developed an explainable, deep learning-based model to predict TTX contamination in the Dutch Zeeland estuary. Inputs for the model were meteorological and hydrological features; output was the presence or absence of TTX contamination.
  Results showed that the time of sunrise, time of sunset, global radiation, water temperature, and chloride concentration contributed most to TTX contamination. Thus, the effective number of sun hours, represented by day length and global radiation, was an important driver for tetrodotoxin contamination in bivalve mollusks.
  To conclude, our explainable deep learning model identified the aforementioned environmental factors (number of sun hours, global radiation, water temperature, and water chloride concentration) to be associated with tetrodotoxin contamination in bivalve mollusks; making our approach a valuable tool to mitigate marine toxin risks for food industry and competent authorities.

</details>


### [95] [Tight Margin-Based Generalization Bounds for Voting Classifiers over Finite Hypothesis Sets](https://arxiv.org/abs/2511.20407)
*Kasper Green Larsen,Natascha Schalburg*

Main category: cs.LG

TL;DR: 研究了投票分类器的泛化界限问题


<details>
  <summary>Details</summary>
Motivation: 目前还没有针对投票分类器的基于边界的泛化界限

Method: 通过分析假设集的大小，边距，给定边距的训练点比例，训练样本数和失败概率之间的权衡，推导出了第一个针对投票分类器的基于边界的泛化界限。

Result: 得到了第一个针对投票分类器的基于边界的泛化界限

Conclusion: 该泛化界限在假设集大小、裕度、给定裕度的训练点比例、训练样本数量和失败概率之间的权衡方面是渐近紧的。

Abstract: We prove the first margin-based generalization bound for voting classifiers, that is asymptotically tight in the tradeoff between the size of the hypothesis set, the margin, the fraction of training points with the given margin, the number of training samples and the failure probability.

</details>


### [96] [The Driver-Blindness Phenomenon: Why Deep Sequence Models Default to Autocorrelation in Blood Glucose Forecasting](https://arxiv.org/abs/2511.20601)
*Heman Shakeri*

Main category: cs.LG

TL;DR: 尽管生理机制已明确，但目前用于血糖预测的深度序列模型未能有效利用胰岛素、膳食和活动等临床信息驱动因素。这被称为“驱动因素盲点”，并被形式化为多变量模型相对于单变量基线的性能增益Δdrivers。由于架构偏差、数据保真度问题和生理异质性，Δdrivers通常接近于零


<details>
  <summary>Details</summary>
Motivation: 目前的深度序列模型在血糖预测中未能有效利用关键的临床驱动因素（胰岛素、膳食、活动），这种“驱动因素盲点”限制了模型的预测能力。

Method: 本文通过引入Δdrivers来量化多变量模型相对于单变量基线的性能增益，从而形式化了“驱动因素盲点”。分析了导致这种盲点的三个主要因素：有利于自相关的架构偏差（C1）、使驱动因素嘈杂和混杂的数据保真度问题（C2）、以及损害群体水平模型的生理异质性（C3）。

Result: Δdrivers在现有文献中通常接近于零，表明当前的模型未能有效利用重要的临床驱动因素。

Conclusion: 为了解决“驱动因素盲点”，未来的研究应采用生理特征编码、因果正则化和个性化等策略。同时，建议未来的工作常规报告Δdrivers，以确保只有真正能利用驱动因素的模型才被视为最先进。

Abstract: Deep sequence models for blood glucose forecasting consistently fail to leverage clinically informative drivers--insulin, meals, and activity--despite well-understood physiological mechanisms. We term this Driver-Blindness and formalize it via $Δ_{\text{drivers}}$, the performance gain of multivariate models over matched univariate baselines. Across the literature, $Δ_{\text{drivers}}$ is typically near zero. We attribute this to three interacting factors: architectural biases favoring autocorrelation (C1), data fidelity gaps that render drivers noisy and confounded (C2), and physiological heterogeneity that undermines population-level models (C3). We synthesize strategies that partially mitigate Driver-Blindness--including physiological feature encoders, causal regularization, and personalization--and recommend that future work routinely report $Δ_{\text{drivers}}$ to prevent driver-blind models from being considered state-of-the-art.

</details>


### [97] [Diffusion for Fusion: Designing Stellarators with Generative AI](https://arxiv.org/abs/2511.20445)
*Misha Padidar,Teresa Huang,Andrew Giuliani,Marina Spivak*

Main category: cs.LG

TL;DR: 该研究针对仿星器设计中计算耗时问题，提出利用机器学习加速设计过程，并成功应用条件扩散模型生成符合特定、高质量要求的仿星器设计。


<details>
  <summary>Details</summary>
Motivation: 仿星器设计是受PDE约束的优化问题，计算耗时，需要快速设计方法。机器学习，特别是基于大型仿星器数据集的方法，有望解决此问题。

Method: 本文提出一个开放的逆问题，旨在快速生成高质量的仿星器设计。通过案例研究，使用QUASR数据库的条件扩散模型生成具有特定长宽比和平均旋转变换的准对称仿星器设计。

Result: 生成的仿星器设计性能良好，与准对称性的偏差小于5%，与目标特性偏差小。模型能应用于生成训练数据未见的仿星器设计。

Conclusion: 机器学习在加速仿星器设计方面潜力巨大，条件扩散模型在此领域表现出有效性，未来可通过生成模型进一步优化仿星器设计。

Abstract: Stellarators are a prospective class of fusion-based power plants that confine a hot plasma with three-dimensional magnetic fields. Typically framed as a PDE-constrained optimization problem, stellarator design is a time-consuming process that can take hours to solve on a computing cluster. Developing fast methods for designing stellarators is crucial for advancing fusion research. Given the recent development of large datasets of optimized stellarators, machine learning approaches have emerged as a potential candidate. Motivated by this, we present an open inverse problem to the machine learning community: to rapidly generate high-quality stellarator designs which have a set of desirable characteristics. As a case study in the problem space, we train a conditional diffusion model on data from the QUASR database to generate quasisymmetric stellarator designs with desirable characteristics (aspect ratio and mean rotational transform). The diffusion model is applied to design stellarators with characteristics not seen during training. We provide evaluation protocols and show that many of the generated stellarators exhibit solid performance: less than 5% deviation from quasisymmetry and the target characteristics. The modest deviation from quasisymmetry highlights an opportunity to reach the sub 1% target. Beyond the case study, we share multiple promising avenues for generative modeling to advance stellarator design.

</details>


### [98] [NVIDIA Nemotron Parse 1.1](https://arxiv.org/abs/2511.20478)
*Kateryna Chumachenko,Amala Sanjay Deshmukh,Jarno Seppanen,Ilia Karmanov,Chia-Chih Chen,Lukas Voegtle,Philipp Fischer,Marek Wawrzos,Saeid Motiian,Roman Ageev,Kedi Wu,Alexandre Milesi,Maryam Moosaei,Krzysztof Pawelec,Padmavathy Subramanian,Mehrzad Samadi,Xin Yu,Celina Dear,Sarah Stoddard,Jenna Diamond,Jesse Oliver,Leanna Chraghchian,Patrick Skelly,Tom Balough,Yao Xu,Jane Polak Scowcroft,Daniel Korzekwa,Darragh Hanley,Sandip Bhaskar,Timo Roman,Karan Sapra,Andrew Tao,Bryan Catanzaro*

Main category: cs.LG

TL;DR: Nemotron-Parse-1.1是一个轻量级的文档解析和OCR模型，在通用OCR、Markdown格式化、结构化表格解析以及从图片、图表和示意图中提取文本方面有显著改进。


<details>
  <summary>Details</summary>
Motivation: Nemotron-Parse-1.1旨在改进其前身Nemoretriever-Parse-1.0的能力，提升文档解析和OCR的准确性和效率。

Method: Nemotron-Parse-1.1采用编码器-解码器架构，包含8.85亿参数，其中语言解码器为2.56亿参数。它能够提取文本片段的边界框及相应的语义类别，并支持更长的输出序列长度。

Result: Nemotron-Parse-1.1在公共基准测试中获得了有竞争力的准确性，可作为一个强大的轻量级OCR解决方案。其优化版本Nemotron-Parse-1.1-TC在视觉token长度减少的情况下，速度提高了20%，质量下降最小。

Conclusion: Nemotron-Parse-1.1模型及其优化版本提供了更先进、更高效的文档解析和OCR能力，并且通过在Huggingface上发布模型权重以及部分训练数据，为研究和应用提供了便利。

Abstract: We introduce Nemotron-Parse-1.1, a lightweight document parsing and OCR model that advances the capabilities of its predecessor, Nemoretriever-Parse-1.0. Nemotron-Parse-1.1 delivers improved capabilities across general OCR, markdown formatting, structured table parsing, and text extraction from pictures, charts, and diagrams. It also supports a longer output sequence length for visually dense documents. As with its predecessor, it extracts bounding boxes of text segments, as well as corresponding semantic classes. Nemotron-Parse-1.1 follows an encoder-decoder architecture with 885M parameters, including a compact 256M-parameter language decoder. It achieves competitive accuracy on public benchmarks making it a strong lightweight OCR solution. We release the model weights publicly on Huggingface, as well as an optimized NIM container, along with a subset of the training data as part of the broader Nemotron-VLM-v2 dataset. Additionally, we release Nemotron-Parse-1.1-TC which operates on a reduced vision token length, offering a 20% speed improvement with minimal quality degradation.

</details>


### [99] [DiFR: Inference Verification Despite Nondeterminism](https://arxiv.org/abs/2511.20621)
*Adam Karvonen,Daniel Reuter,Roy Rinberg,Luke Marks,Adrià Garriga-Alonso,Keri Warr*

Main category: cs.LG

TL;DR: Token-DiFR 和 Activation-DiFR 是用于验证 LLM 推理输出的方法，它们通过比较生成的 token 或激活值与可信参考实现预测的结果来确保准确性，同时控制随机种子以区分合法变异和实际问题。


<details>
  <summary>Details</summary>
Motivation: 随着大型语言模型（LLM）推理需求的增长，验证推理过程的正确性、无错误和无篡改变得越来越重要。当前的问题在于，重复运行相同的推理过程常常因为良性的数值噪声而产生不同的结果，这使得区分合法变异和实际问题变得困难。

Method: Token-DiFR：通过将生成的 token 与在相同随机种子条件下由可信参考实现预测的 token 进行比较来验证推理输出。采样种子同步严格限制了有效输出，使提供商几乎没有偏离正确推理的余地。Activation-DiFR：针对需要样本高效前向验证的应用。该方案使用随机正交投影将激活压缩成紧凑的指纹以进行后续验证。

Result: Token-DiFR 能够可靠地识别采样错误、模拟 bug 和模型量化问题，在 300 个输出 token 内检测 4 比特量化，AUC > 0.999。Activation-DiFR 使用 2 个输出 token 即可检测 4 比特量化，AUC > 0.999，同时将通信开销相对于现有方法减少 25-75%。

Conclusion: Token-DiFR 和 Activation-DiFR 为 LLM 推理的验证提供了有效且高效的解决方案，其中 Token-DiFR 通过输出 token 提供可审计的正确性证据，而 Activation-DiFR 则在通信开销方面带来了显著的改进。这些方法使得提供商及其客户能够更可靠地验证 LLM 推理过程。

Abstract: As demand for LLM inference grows, it is becoming increasingly important that providers and their customers can verify that inference processes are performed correctly, without errors or tampering. However, re-running the same inference process twice often leads to different results due to benign numerical noise, making it difficult to distinguish legitimate variation from actual problems. To address this problem, we introduce Token-DiFR (Token-Divergence-From-Reference), a method for verifying inference outputs by comparing generated tokens against predictions made by a trusted reference implementation conditioned on the same random seed. Sampling seed synchronization tightly constrains valid outputs, leaving providers minimal room to deviate from correct inference, which allows output tokens themselves to serve as auditable evidence of correctness at zero additional cost to the provider. Token-DiFR reliably identifies sampling errors, simulated bugs, and model quantization, detecting 4-bit quantization with AUC $>$ 0.999 within 300 output tokens. For applications requiring sample-efficient forward-pass verification, we additionally introduce Activation-DiFR, a scheme that uses random orthogonal projections to compress activations into compact fingerprints for subsequent verification. Activation-DiFR detects 4-bit quantization with AUC $>$ 0.999 using just 2 output tokens, while reducing communication overhead by 25-75% relative to existing methods. We release an open-source integration with vLLM to accelerate practical deployment of verifiable inference.

</details>


### [100] [ROOT: Robust Orthogonalized Optimizer for Neural Network Training](https://arxiv.org/abs/2511.20626)
*Wei He,Kai Han,Hang Zhou,Hanting Chen,Zhicheng Liu,Xinghao Chen,Yunhe Wang*

Main category: cs.LG

TL;DR: 该论文介绍了一种名为ROOT的鲁棒正交优化器，旨在解决大型语言模型训练中优化器对维度和异常值的敏感性问题，通过双重鲁棒机制显著提高训练稳定性、收敛速度和最终性能。


<details>
  <summary>Details</summary>
Motivation: 大型语言模型（LLM）的优化是一个关键挑战，模型规模的扩大加剧了算法不精确性和训练不稳定性的敏感性。现有优化器在动量正交化方面虽然提高了收敛效率，但存在维度脆弱性和易受异常值噪声影响的鲁棒性局限。

Method: ROOT优化器通过双重鲁棒机制增强训练稳定性：1. 提出了一种维度鲁棒正交化方案，利用自适应牛顿迭代和针对特定矩阵尺寸的细粒度系数，确保在不同架构配置下保持一致的精度。2. 引入了一个通过近端优化实现的优化鲁棒框架，在抑制异常值噪声的同时保留有意义的梯度方向。

Result: 实验证明，与Muon和基于Adam的优化器相比，ROOT在鲁棒性方面取得了显著改进，尤其在噪声大和非凸场景下，具有更快的收敛速度和更优异的最终性能。

Conclusion: ROOT为开发能够处理现代大规模模型训练复杂性的鲁棒而精确的优化器建立了新范式。

Abstract: The optimization of large language models (LLMs) remains a critical challenge, particularly as model scaling exacerbates sensitivity to algorithmic imprecision and training instability. Recent advances in optimizers have improved convergence efficiency through momentum orthogonalization, but suffer from two key robustness limitations: dimensional fragility in orthogonalization precision and vulnerability to outlier-induced noise. To address these robustness challenges, we introduce ROOT, a Robust Orthogonalized Optimizer that enhances training stability through dual robustness mechanisms. First, we develop a dimension-robust orthogonalization scheme using adaptive Newton iterations with fine-grained coefficients tailored to specific matrix sizes, ensuring consistent precision across diverse architectural configurations. Second, we introduce an optimization-robust framework via proximal optimization that suppresses outlier noise while preserving meaningful gradient directions. Extensive experiments demonstrate that ROOT achieves significantly improved robustness, with faster convergence and superior final performance compared to both Muon and Adam-based optimizers, particularly in noisy and non-convex scenarios. Our work establishes a new paradigm for developing robust and precise optimizers capable of handling the complexities of modern large-scale model training. The code will be available at https://github.com/huawei-noah/noah-research/tree/master/ROOT.

</details>


### [101] [DP-MicroAdam: Private and Frugal Algorithm for Training and Fine-tuning](https://arxiv.org/abs/2511.20509)
*Mihaela Hudişteanu,Edwige Cyffers,Nikita P. Kalinin*

Main category: cs.LG

TL;DR: 该论文介绍了一种名为DP-MicroAdam的自适应差分隐私（DP）优化器，它在非凸随机优化中实现了收敛，并在多个基准测试中表现出超越现有DP优化器和DP-SGD的性能。


<details>
  <summary>Details</summary>
Motivation: 自适应优化器在非隐私训练中是主流，因为它能够加速收敛和提升性能。然而，差分隐私（DP）训练目前主要依赖DP-SGD，这通常需要大量的计算和超参数调整。

Method: 本文提出了一种内存高效且稀疏感知的自适应DP优化器DP-MicroAdam。作者证明了DP-MicroAdam在随机非凸优化中以最佳的O(1/sqrt(T))速率收敛，收敛速度受隐私相关常数影响。

Result: 实验结果表明，DP-MicroAdam在包括CIFAR-10、ImageNet大规模训练以及预训练Transformer的私有微调等多个基准测试中，性能优于现有自适应DP优化器，并且与DP-SGD相比，准确性具有竞争力或更优。

Conclusion: 这些结果表明，自适应优化在差分隐私下可以同时提高性能和稳定性。

Abstract: Adaptive optimizers are the de facto standard in non-private training as they often enable faster convergence and improved performance. In contrast, differentially private (DP) training is still predominantly performed with DP-SGD, typically requiring extensive compute and hyperparameter tuning. We propose DP-MicroAdam, a memory-efficient and sparsity-aware adaptive DP optimizer. We prove that DP-MicroAdam converges in stochastic non-convex optimization at the optimal $\mathcal{O}(1/\sqrt{T})$ rate, up to privacy-dependent constants. Empirically, DP-MicroAdam outperforms existing adaptive DP optimizers and achieves competitive or superior accuracy compared to DP-SGD across a range of benchmarks, including CIFAR-10, large-scale ImageNet training, and private fine-tuning of pretrained transformers. These results demonstrate that adaptive optimization can improve both performance and stability under differential privacy.

</details>


### [102] [Adam Simplified: Bias Correction Simplified](https://arxiv.org/abs/2511.20516)
*Sam Laing,Antonio Orvieto*

Main category: cs.LG

TL;DR: Adam优化器中的偏差校正功能在深度学习中并不总是必要的，甚至可能在某些情况下损害性能，其作用类似于一种隐式学习率调度。


<details>
  <summary>Details</summary>
Motivation: 研究Adam优化器中偏差校正组件的作用，因为其贡献尚未被充分理解。

Method: 通过对视觉和语言建模任务进行一系列系统消融实验，评估偏差校正对性能的影响。

Result: 在最优超参数配置下，偏差校正未能提升最终测试性能。除非采用适当的学习率调度，否则偏差校正有时会损害性能。偏差校正被重新解释为一种隐式学习率调度形式，其行为强烈依赖于平滑超参数$β_1, β_2$的选择。

Conclusion: 偏差校正并非Adam优化器中不可或缺的组件，其实用性取决于具体的超参数设置和学习率调度策略。

Abstract: The Adam optimizer is a cornerstone of modern deep learning, yet the empirical necessity of each of its individual components is often taken for granted. This paper presents a focused investigation into the role of bias-correction, a feature whose contribution remains poorly understood. Through a series of systematic ablations on vision and language modelling tasks, we demonstrate that the conventional wisdom surrounding bias correction is misleading. In particular, we demonstrate that in the optimal hyper-parameter configuration, the inclusion of bias correction leads to no improvement in final test performance. Moreover, unless appropriate learning rate scheduling is implemented, the inclusion of bias correction can sometimes be detrimental to performance. We further reinterpret bias correction as a form of implicit learning rate scheduling whose behaviour is strongly dependent on the choice of smoothing hyper-parameters $β_1, β_2 \in [0,1)$. Our findings challenge the universal inclusion of this component.

</details>


### [103] [Feature-Modulated UFNO for Improved Prediction of Multiphase Flow in Porous Media](https://arxiv.org/abs/2511.20543)
*Alhasan Abdellatif,Hannah P. Menke,Ahmed H. Elsheikh,Florian Doster,Kamaljit Singh*

Main category: cs.LG

TL;DR: 该文章介绍了UFNO-FiLM，这是一个改进的傅立叶神经算子（FNO）模型，它通过解耦标量输入和空间加权损失函数来提高预测精度，并在地下多相流任务中实现了21%的MAE降低。


<details>
  <summary>Details</summary>
Motivation: UFNO虽然在预测准确性方面优于FNO，但它通过在整个域中复制标量输入（例如，温度、注入速率）的值，将它们视为空间分布的场，这导致在频域中处理冗余的恒定信号。此外，其标准损失函数没有考虑误差敏感度的空间变化，限制了其在物理重要性高区域的性能。

Method: UFNet-FiLM引入了两个关键创新： 1. 使用特征线性调制（FiLM）层将标量输入与空间特征解耦，这允许模型调制空间特征图，而不会将恒定信号引入傅里叶变换。 2. 采用空间加权损失函数，优先考虑关键区域的学习。

Result: 在地下多相流实验中，UFNO-FiLM的瓦斯饱和度平均绝对误差（MAE）比UFNO降低了21%。

Conclusion: UFNO-FiLM通过解耦标量输入和引入空间加权损失函数，显著提高了模型的预测精度，尤其是在处理具有关键区域的复杂物理系统时。

Abstract: The UNet-enhanced Fourier Neural Operator (UFNO) extends the Fourier Neural Operator (FNO) by incorporating a parallel UNet pathway, enabling the retention of both high- and low-frequency components. While UFNO improves predictive accuracy over FNO, it inefficiently treats scalar inputs (e.g., temperature, injection rate) as spatially distributed fields by duplicating their values across the domain. This forces the model to process redundant constant signals within the frequency domain. Additionally, its standard loss function does not account for spatial variations in error sensitivity, limiting performance in regions of high physical importance. We introduce UFNO-FiLM, an enhanced architecture that incorporates two key innovations. First, we decouple scalar inputs from spatial features using a Feature-wise Linear Modulation (FiLM) layer, allowing the model to modulate spatial feature maps without introducing constant signals into the Fourier transform. Second, we employ a spatially weighted loss function that prioritizes learning in critical regions. Our experiments on subsurface multiphase flow demonstrate a 21\% reduction in gas saturation Mean Absolute Error (MAE) compared to UFNO, highlighting the effectiveness of our approach in improving predictive accuracy.

</details>


### [104] [E2E-GRec: An End-to-End Joint Training Framework for Graph Neural Networks and Recommender Systems](https://arxiv.org/abs/2511.20564)
*Rui Xue,Shichao Zhu,Liang Qin,Guangmou Pan,Yang Song,Tianfu Wu*

Main category: cs.LG

TL;DR: E2E-GRec是一个端到端的GNN推荐系统训练框架，解决了现有两阶段GNN推荐系统中存在的计算开销大和优化不足的问题。


<details>
  <summary>Details</summary>
Motivation: 现有的GNN推荐系统大多采用两阶段范式，存在计算开销大和GNN学习与推荐任务不能联合优化的问题，导致GNN对推荐任务的信息量不足。

Method: 1. 提出E2E-GRec框架，将GNN训练与推荐系统统一起来。2. 采用高效的子图采样，以确保训练的可扩展性和效率。3. 设计图特征自动编码器（GFAE）作为辅助自监督任务，引导GNN学习有意义的嵌入。4. 采用两级特征融合机制和基于Gradnorm的动态损失平衡，以稳定图感知多任务端到端训练。

Result: 在用户留存时长上相对提升了0.133%，用户跳过视频的平均次数减少了0.3171%。E2E-GRec持续超越传统方法，在多个推荐指标上取得了显著收益。

Conclusion: E2E-GRec通过端到端训练框架、高效子图采样、GFAE辅助任务以及两级特征融合和动态损失平衡机制，有效解决了GNN推荐系统中存在的挑战，并显著提升了推荐性能。

Abstract: Graph Neural Networks (GNNs) have emerged as powerful tools for modeling graph-structured data and have been widely used in recommender systems, such as for capturing complex user-item and item-item relations. However, most industrial deployments adopt a two-stage pipeline: GNNs are first pre-trained offline to generate node embeddings, which are then used as static features for downstream recommender systems. This decoupled paradigm leads to two key limitations: (1) high computational overhead, since large-scale GNN inference must be repeatedly executed to refresh embeddings; and (2) lack of joint optimization, as the gradient from the recommender system cannot directly influence the GNN learning process, causing the GNN to be suboptimally informative for the recommendation task. In this paper, we propose E2E-GRec, a novel end-to-end training framework that unifies GNN training with the recommender system. Our framework is characterized by three key components: (i) efficient subgraph sampling from a large-scale cross-domain heterogeneous graph to ensure training scalability and efficiency; (ii) a Graph Feature Auto-Encoder (GFAE) serving as an auxiliary self-supervised task to guide the GNN to learn structurally meaningful embeddings; and (iii) a two-level feature fusion mechanism combined with Gradnorm-based dynamic loss balancing, which stabilizes graph-aware multi-task end-to-end training. Extensive offline evaluations, online A/B tests (e.g., a +0.133% relative improvement in stay duration, a 0.3171% reduction in the average number of videos a user skips) on large-scale production data, together with theoretical analysis, demonstrate that E2E-GRec consistently surpasses traditional approaches, yielding significant gains across multiple recommendation metrics.

</details>


### [105] [MSTN: Fast and Efficient Multivariate Time Series Model](https://arxiv.org/abs/2511.20577)
*Sumit S Shevtekar,Chandresh K Maurya,Gourab Sil*

Main category: cs.LG

TL;DR: MSTN是一种基于分层多尺度和序列建模原理的深度学习架构，能够处理时序数据中多尺度、复杂的动态变化，尤其应对不可预测、突发且高幅度的事件时表现优异。


<details>
  <summary>Details</summary>
Motivation: 现有的时序模型大多依赖于固定的结构先验，导致无法充分适应时序数据中多尺度的变化，尤其在处理不可预测、突发的高幅度事件时性能受限。

Method: MSTN框架包含三个主要组件：(i) 多尺度卷积编码器，用于构建局部模式的层次特征金字塔；(ii) 序列建模组件，用于处理长程时间依赖性；(iii) 带有Squeeze-and-Excitation (SE) 和多头时间注意力 (MHTA) 的门控融合机制，用于动态、上下文感知的特征集成。

Result: MSTN在时间序列长程预测、插补、分类以及泛化性研究方面表现出色，超越了EMTSF、LLM4TS、HiMTM、TIME-LLM、MTST、SOFTS、iTransformer、TimesNet和PatchTST等现有方法，在32个基准数据集中有24个达到了最新的SOTA性能。

Conclusion: MSTN通过其独特的多尺度和序列建模架构，能够有效且灵活地处理复杂的时间序列动态，并在各种任务上取得了显著的SOTA性能提升。

Abstract: Real-world time-series data is highly non stationary and complex in dynamics that operate across multiple timescales, ranging from fast, short-term changes to slow, long-term trends. Most existing models rely on fixed-scale structural priors, such as patch-based tokenization, fixed frequency transformations, or frozen backbone architectures. This often leads to over-regularization of temporal dynamics, which limits their ability to adaptively model the full spectrum of temporal variations and impairs their performance on unpredictable, Sudden, high-magnitude events. To address this, we introduce the Multi-scale Temporal Network (MSTN), a novel deep learning architecture founded on a hierarchical multi-scale and sequence modeling principle. The MSTN framework integrates: (i) a multi-scale convolutional encoder that constructs a hierarchical feature pyramid for local patterns (ii) a sequence modeling component for long-range temporal dependencies. We empirically validate this with BiLSTM and Transformer variants, establishing a flexible foundation for future architectural advancements. and (iii) a gated fusion mechanism augmented with squeeze-and-excitation (SE) and multi-head temporal attention (MHTA) for dynamic, context-aware feature integration. This design enables MSTN to adaptively model temporal patterns from milliseconds to long-range dependencies within a unified framework. Extensive evaluations across time-series long-horizon forecasting, imputation, classification and generalizability study demonstrate that MSTN achieves competitive state-of-the-art (SOTA) performance, showing improvements over contemporary approaches including EMTSF, LLM4TS, HiMTM, TIME-LLM, MTST, SOFTS, iTransformer, TimesNet, and PatchTST. In total, MSTN establishes new SOTA performance on 24 of 32 benchmark datasets, demonstrating its consistent performance across diverse temporal tasks.

</details>


### [106] [A Tale of Two Geometries: Adaptive Optimizers and Non-Euclidean Descent](https://arxiv.org/abs/2511.20584)
*Shuo Xie,Tianhao Wang,Beining Wu,Zhiyuan Li*

Main category: cs.LG

TL;DR: 这篇论文探讨了自适应优化器与归一化最速下降法（NSD）之间的联系，并在非凸设置下扩展了自适应平滑理论，同时证明了自适应平滑在加速Nesterov动量自适应优化器中的作用，并为随机优化引入了自适应梯度方差。


<details>
  <summary>Details</summary>
Motivation: 自适应优化器与NSD算法之间存在联系，但其分析所依赖的几何（如平滑度概念）存在差异。自适应优化器依赖于更强的自适应平滑条件，而NSD依赖于标准的平滑度概念。

Method: 1. 将自适应平滑理论扩展到非凸设置，并证明其精确表征了自适应优化器的收敛性。
2. 证明自适应平滑能够在凸设置下加速Nesterov动量的自适应优化器。
3. 通过引入自适应梯度方差，对随机优化进行类比，从而实现维度无关的收敛保证。

Result: 1. 自适应平滑理论能够精确表征非凸设置下自适应优化器的收敛性。
2. 自适应平滑可以在凸设置下加速Nesterov动量的自适应优化器，这在某些非欧几里得几何下通过标准平滑度是无法实现的。
3. 自适应梯度方差在随机优化中实现了维度无关的收敛保证，这在某些非欧几里得几何下通过标准梯度方差是无法实现的。

Conclusion: 自适应平滑度是表征自适应优化器收敛的关键因素，并且能够在非凸设置和带有Nesterov动量的凸设置中带来加速效果。同时，自适应梯度方差的概念也为随机优化带来了维度无关的收敛保证。

Abstract: Adaptive optimizers can reduce to normalized steepest descent (NSD) when only adapting to the current gradient, suggesting a close connection between the two algorithmic families. A key distinction between their analyses, however, lies in the geometries, e.g., smoothness notions, they rely on. In the convex setting, adaptive optimizers are governed by a stronger adaptive smoothness condition, while NSD relies on the standard notion of smoothness. We extend the theory of adaptive smoothness to the nonconvex setting and show that it precisely characterizes the convergence of adaptive optimizers. Moreover, we establish that adaptive smoothness enables acceleration of adaptive optimizers with Nesterov momentum in the convex setting, a guarantee unattainable under standard smoothness for certain non-Euclidean geometry. We further develop an analogous comparison for stochastic optimization by introducing adaptive gradient variance, which parallels adaptive smoothness and leads to dimension-free convergence guarantees that cannot be achieved under standard gradient variance for certain non-Euclidean geometry.

</details>


### [107] [Anatomica: Localized Control over Geometric and Topological Properties for Anatomical Diffusion Models](https://arxiv.org/abs/2511.20587)
*Karim Kadry,Abdallah Abdelwahed,Shoaib Goraya,Ajay Manicka,Naravich Chutisilp,Farhad Nezami,Elazer Edelman*

Main category: cs.LG

TL;DR: 本文提出了Anatomica，一种用于生成具有局部地理拓扑控制的多类解剖体素图的推理框架。


<details>
  <summary>Details</summary>
Motivation: 在虚拟试验或机器学习工作流程中，实现合成数据集的合理设计。

Method: Anatomica在生成过程中使用不同维度、位置和形状的立方体控制域来切出相关的子结构。这些局部子结构用于计算可微分惩罚函数，从而引导样本达到目标约束。通过体素化矩控制尺寸、形状和位置等几何特征，而通过持久同源性实施连通分量、循环和空隙等拓扑特征。最后，Anatomica应用于潜在扩散模型，其中神经场解码器部分提取子结构，从而实现解剖属性的有效控制。

Result: Anatomica灵活地适用于各种解剖系统，组合约束以控制任意维度和坐标系上的复杂结构。

Conclusion: Anatomica可以实现合成数据集的合理设计，以用于虚拟试验或机器学习工作流程。

Abstract: We present Anatomica: an inference-time framework for generating multi-class anatomical voxel maps with localized geo-topological control. During generation, we use cuboidal control domains of varying dimensionality, location, and shape to slice out relevant substructures. These local substructures are used to compute differentiable penalty functions that steer the sample towards target constraints. We control geometric features such as size, shape, and position through voxel-wise moments, while topological features such as connected components, loops, and voids are enforced through persistent homology. Lastly, we implement Anatomica for latent diffusion models, where neural field decoders partially extract substructures, enabling the efficient control of anatomical properties. Anatomica applies flexibly across diverse anatomical systems, composing constraints to control complex structures over arbitrary dimensions and coordinate systems, thereby enabling the rational design of synthetic datasets for virtual trials or machine learning workflows.

</details>


### [108] [Latent Diffusion Inversion Requires Understanding the Latent Space](https://arxiv.org/abs/2511.20592)
*Mingxing Rao,Bowen Qu,Daniel Moyer*

Main category: cs.LG

TL;DR: 这篇论文研究了潜在扩散模型（LDMs）中的数据恢复问题，发现扩散模型在解码器回溯度量的高失真区域对样本表现出不均匀的记忆化，并且不同维度对记忆化的贡献不相等。通过移除记忆化较弱的维度，可以显著提高成员推断攻击的性能。


<details>
  <summary>Details</summary>
Motivation: 以前的研究很少关注潜在空间生成模型（如LDMs）的编码器/解码器对和相应的潜在代码在模型反演中的作用。本文旨在探究LDM中自动编码器几何对模型记忆化的影响及其隐私风险。

Method: 本文提出了两种关键发现：1）扩散模型在解码器拉回度量的高失真区域对样本表现出不均匀的记忆化。2）在单个潜在代码中，不同维度对记忆化的贡献不相等。研究引入了一种原则性方法，通过每个维度对解码器拉回度量的贡献来对潜在维度进行排序，以识别对记忆化负主要责任的维度。

Result: 经验证，在计算基于分数的成员推断攻击的攻击统计数据时，移除记忆化程度较低的维度可以显著提高性能。在CIFAR-10、CelebA、ImageNet-1K、Pokémon、MS-COCO和Flickr等多个数据集上，平均AUROC增益为2.7%，TPR@1%FPR显著增加6.42%。

Conclusion: 研究结果强调了自动编码器几何对LDM记忆化的被忽视影响，为分析基于扩散的生成模型中的隐私风险提供了新视角，并提供了在极低假阳性容忍度下识别成员的更强信心。

Abstract: The recovery of training data from generative models (``model inversion'') has been extensively studied for diffusion models in the data domain. The encoder/decoder pair and corresponding latent codes have largely been ignored by inversion techniques applied to latent space generative models, e.g., Latent Diffusion models (LDMs). In this work we describe two key findings: (1) The diffusion model exhibits non-uniform memorization across latent codes, tending to overfit samples located in high-distortion regions of the decoder pullback metric. (2) Even within a single latent code, different dimensions contribute unequally to memorization. We introduce a principled method to rank latent dimensions by their per-dimensional contribution to the decoder pullback metric, identifying those most responsible for memorization. Empirically, removing less-memorizing dimensions when computing attack statistics for score-based membership inference attacker significantly improves performance, with average AUROC gains of 2.7\% and substantial increases in TPR@1\%FPR (6.42\%) across diverse datasets including CIFAR-10, CelebA, ImageNet-1K, Pokémon, MS-COCO, and Flickr. This indicates stronger confidence in identifying members under extremely low false-positive tolerance. Our results highlight the overlooked influence of the auto-encoder geometry on LDM memorization and provide a new perspective for analyzing privacy risks in diffusion-based generative models.

</details>


### [109] [Image2Gcode: Image-to-G-code Generation for Additive Manufacturing Using Diffusion-Transformer Model](https://arxiv.org/abs/2511.20636)
*Ziyue Wang,Yayati Jadhav,Peter Pak,Amir Barati Farimani*

Main category: cs.LG

TL;DR: 本文介绍了一种名为Image2Gcode的端到端数据驱动框架，该框架可以直接从图像和零件图纸生成可用于打印的G代码，绕过了CAD建模阶段，从而加速了从设计到制造的周期。


<details>
  <summary>Details</summary>
Motivation: 传统的机械设计和制造流程中，构建特定于对象的3D几何模型耗时且不适用于快速原型设计，即使是微小的设计变化也需要手动更新CAD软件，这使得迭代耗时且难以扩展。

Method: Image2Gcode框架绕过了CAD阶段，直接从图像和零件图纸生成打印就绪的G代码。它首先从图像中提取切片式结构线索，然后对G代码序列采用去噪扩散概率模型（DDPM）。通过迭代去噪，该模型将高斯噪声转换为可执行的打印移动轨迹及相应的挤出参数，从而建立了从视觉输入到原生刀具路径的直接映射。

Result: Image2Gcode能够直接从2D图像生成结构化的G代码，消除了对CAD或STL中间文件的需求，降低了增材制造的门槛，并加速了设计到制造的周期。该方法支持根据简单的草图或视觉参考进行按需原型设计，并与上游的2D到3D重建模块集成，实现从概念到物理实体的自动化流程。

Conclusion: Image2Gcode是一个灵活且计算高效的框架，它提高了设计迭代、维修工作流程和分布式制造的可访问性，为增材制造领域带来了显著的进步。

Abstract: Mechanical design and manufacturing workflows conventionally begin with conceptual design, followed by the creation of a computer-aided design (CAD) model and fabrication through material-extrusion (MEX) printing. This process requires converting CAD geometry into machine-readable G-code through slicing and path planning. While each step is well established, dependence on CAD modeling remains a major bottleneck: constructing object-specific 3D geometry is slow and poorly suited to rapid prototyping. Even minor design variations typically necessitate manual updates in CAD software, making iteration time-consuming and difficult to scale. To address this limitation, we introduce Image2Gcode, an end-to-end data-driven framework that bypasses the CAD stage and generates printer-ready G-code directly from images and part drawings. Instead of relying on an explicit 3D model, a hand-drawn or captured 2D image serves as the sole input. The framework first extracts slice-wise structural cues from the image and then employs a denoising diffusion probabilistic model (DDPM) over G-code sequences. Through iterative denoising, the model transforms Gaussian noise into executable print-move trajectories with corresponding extrusion parameters, establishing a direct mapping from visual input to native toolpaths. By producing structured G-code directly from 2D imagery, Image2Gcode eliminates the need for CAD or STL intermediates, lowering the entry barrier for additive manufacturing and accelerating the design-to-fabrication cycle. This approach supports on-demand prototyping from simple sketches or visual references and integrates with upstream 2D-to-3D reconstruction modules to enable an automated pipeline from concept to physical artifact. The result is a flexible, computationally efficient framework that advances accessibility in design iteration, repair workflows, and distributed manufacturing.

</details>


<div id='cs.AI'></div>

# cs.AI [[Back]](#toc)

### [110] [Using Wearable Devices to Improve Chronic PainTreatment among Patients with Opioid Use Disorder](https://arxiv.org/abs/2511.19577)
*Abhay Goyal,Navin Kumar,Kimberly DiMeola,Rafael Trujillo,Soorya Ram Shimgekar,Christian Poellabauer,Pi Zonooz,Ermonda Gjoni-Markaj,Declan Barry,Lynn Madden*

Main category: cs.AI

TL;DR: 这篇论文旨在探讨慢性疼痛（CP）和阿片类药物使用障碍（OUD）的综合治疗方法。研究人员利用可穿戴设备收集数据，并通过机器学习和大型语言模型（LLMs）分析疼痛波动及其临床相关性。


<details>
  <summary>Details</summary>
Motivation: 目前，针对同时患有慢性疼痛和阿片类药物使用障碍并接受阿片类药物使用障碍药物治疗的患者，缺乏循证的综合治疗方案。可穿戴设备有望监测复杂的患者信息，为阿片类药物使用障碍和慢性疼痛患者的治疗发展提供信息，包括疼痛变异性（例如，疼痛加剧或疼痛峰值）和临床相关因素（例如，感知压力）。然而，将大型语言模型与可穿戴设备数据结合应用来理解疼痛峰值尚未被探索。

Method: 这项初步研究的目的是利用一系列人工智能方法检查疼痛峰值的临床相关因素。

Result: 研究发现，机器学习模型在预测疼痛峰值方面取得了相对较高的准确性（>0.7），而大型语言模型在提供疼痛峰值方面的见解方面表现有限。

Conclusion: 通过可穿戴设备进行的实时监测，结合先进的人工智能模型，可以促进疼痛峰值的早期检测，并支持个性化干预措施。这些干预措施可能有助于减轻阿片类药物复发的风险，提高阿片类药物使用障碍药物治疗的依从性，并加强慢性疼痛和阿片类药物使用障碍护理的整合。鉴于大型语言模型的整体性能有限，这些发现强调了开发能够在中慢性疼痛和阿片类药物使用障碍背景下提供可操作见解的大型语言模型的必要性。

Abstract: Chronic pain (CP) and opioid use disorder (OUD) are common and interrelated chronic medical conditions. Currently, there is a paucity of evidence-based integrated treatments for CP and OUD among individuals receiving medication for opioid use disorder (MOUD). Wearable devices have the potential to monitor complex patient information and inform treatment development for persons with OUD and CP, including pain variability (e.g., exacerbations of pain or pain spikes) and clinical correlates (e.g., perceived stress). However, the application of large language models (LLMs) with wearable data for understanding pain spikes, remains unexplored. Consequently, the aim of this pilot study was to examine the clinical correlates of pain spikes using a range of AI approaches. We found that machine learning models achieved relatively high accuracy (>0.7) in predicting pain spikes, while LLMs were limited in providing insights on pain spikes. Real-time monitoring through wearable devices, combined with advanced AI models, could facilitate early detection of pain spikes and support personalized interventions that may help mitigate the risk of opioid relapse, improve adherence to MOUD, and enhance the integration of CP and OUD care. Given overall limited LLM performance, these findings highlight the need to develop LLMs which can provide actionable insights in the OUD/CP context.

</details>


### [111] [HeaRT: A Hierarchical Circuit Reasoning Tree-Based Agentic Framework for AMS Design Optimization](https://arxiv.org/abs/2511.19669)
*Souradip Poddar,Chia-Tung Ho,Ziming Wei,Weidong Cao,Haoxing Ren,David Z. Pan*

Main category: cs.AI

TL;DR: HeaRT是一种针对AMS设计的基于AI的推理引擎，它在我们的40电路基准存储库中表现出高精度和Pass@1性能，提高了收敛速度并保留了设计意图。


<details>
  <summary>Details</summary>
Motivation: 传统的AI驱动AMS设计自动化算法受限于高质量数据集的依赖性、不同架构间的可迁移性差以及缺乏自适应机制。

Method: 本文提出了HeaRT，一种用于自动化循环的基础推理引擎，旨在实现智能、自适应、类似人类的设计优化。

Result: HeaRT在40电路基准存储库中始终表现出高于97%的推理准确性和高于98%的Pass@1性能，即使电路复杂性增加，并且在低于SOTA基线0.5倍的实时token预算下运行。

Conclusion: HeaRT在尺寸确定和拓扑设计适应任务中，通过各种优化方法，实现了超过3倍的收敛速度提升，同时保留了先前的设计意图。

Abstract: Conventional AI-driven AMS design automation algorithms remain constrained by their reliance on high-quality datasets to capture underlying circuit behavior, coupled with poor transferability across architectures, and a lack of adaptive mechanisms. This work proposes HeaRT, a foundational reasoning engine for automation loops and a first step toward intelligent, adaptive, human-style design optimization. HeaRT consistently demonstrates reasoning accuracy >97% and Pass@1 performance >98% across our 40-circuit benchmark repository, even as circuit complexity increases, while operating at <0.5x real-time token budget of SOTA baselines. Our experiments show that HeaRT yields >3x faster convergence in both sizing and topology design adaptation tasks across diverse optimization approaches, while preserving prior design intent.

</details>


### [112] [FISCAL: Financial Synthetic Claim-document Augmented Learning for Efficient Fact-Checking](https://arxiv.org/abs/2511.19671)
*Rishab Sharma,Iman Saberi,Elham Alipour,Jie JW Wu,Fatemeh Fard*

Main category: cs.AI

TL;DR: 该论文提出了一个名为FISCAL的模块化框架，用于生成针对金融事实核查的合成数据，并使用该数据训练了一个轻量级模型MiniCheck-FISCAL，该模型在金融领域取得了卓越的性能。


<details>
  <summary>Details</summary>
Motivation: 解决当前大型语言模型在金融应用中事实可靠性和计算效率不足的问题，现有系统常出现幻觉且模型过大。

Method: 提出了FISCAL（Financial Synthetic Claim-Document Augmented Learning），一个用于生成金融事实核查合成数据的模块化框架。利用FISCAL生成数据集FISCAL-data，并用其训练了一个轻量级的数值金融声明验证器MiniCheck-FISCAL。

Result: MiniCheck-FISCAL的性能优于其基线，超越了GPT-3.5 Turbo和同等大小的其他开源模型，并接近了Mixtral-8x22B和Command R+等大型系统（20倍）的准确性。在外部数据集FinDVer和Fin-Fact上，它与GPT-4o和Claude-3.5相当，并优于Gemini-1.5 Flash。

Conclusion: 领域特定的合成数据与高效微调相结合，使得紧凑型模型能够实现最先进的准确性、鲁棒性和可扩展性，从而用于实际的金融AI应用。

Abstract: Financial applications of large language models (LLMs) require factual reliability and computational efficiency, yet current systems often hallucinate details and depend on prohibitively large models. We propose FISCAL (Financial Synthetic Claim-Document Augmented Learning), a modular framework for generating synthetic data tailored to financial fact-checking. Using FISCAL, we generate a dataset called FISCAL-data and use it to train MiniCheck-FISCAL, a lightweight verifier for numerical financial claims. MiniCheck-FISCAL outperforms its baseline, surpasses GPT-3.5 Turbo and other open-source peers of similar size, and approaches the accuracy of much larger systems (20x), such as Mixtral-8x22B and Command R+. On external datasets FinDVer and Fin-Fact, it rivals GPT-4o and Claude-3.5 while outperforming Gemini-1.5 Flash. These results show that domain-specific synthetic data, combined with efficient fine-tuning, enables compact models to achieve state-of-the-art accuracy, robustness, and scalability for practical financial AI. The dataset and scripts are available in the project repository (link provided in the paper).

</details>


### [113] [Scaling Item-to-Standard Alignment with Large Language Models: Accuracy, Limits, and Solutions](https://arxiv.org/abs/2511.19749)
*Farzan Karimi-Malekabadi,Pooya Razavi,Sonya Powers*

Main category: cs.AI

TL;DR: 本文探讨了大型语言模型（LLMs）如何加速评估项目与课程标准对齐的过程。在大约12000个K-5年级项目-技能对上，研究测试了GPT-3.5 Turbo、GPT-4o-mini和GPT-4o在识别未对齐项目、从标准集中选择正确技能以及缩小候选列表方面的表现。结果表明，LLMs在对齐任务中表现出色，特别是在结合候选过滤策略时，显著减轻了人工审查的负担。


<details>
  <summary>Details</summary>
Motivation: 为了确保评估项目与内容标准保持一致，传统的Geren审查方法虽然准确但效率低下且耗时。本研究旨在探讨LLMs是否能在不牺牲准确性的前提下，加速这一对齐过程。

Method: 本研究使用了超过12,000个K-5年级的项目-技能对，并测试了三种LLMs（GPT-3.5 Turbo、GPT-4o-mini和GPT-4o）。研究设计了三个任务来模拟实际挑战：识别未对齐项目、从完整标准集中选择正确技能、以及在分类前缩小候选列表。

Result: 在研究1中，GPT-4o-mini在83-94%的案例中正确识别了对齐状态。在研究2中，LLMs在数学方面的表现依然强劲，但在阅读方面表现稍差，因为阅读标准在语义上更具重叠性。研究3表明，预过滤候选技能显著提高了结果，正确技能出现在前五名建议中的次数超过95%。

Conclusion: 研究结果表明，LLMs，特别是与候选过滤策略结合使用时，可以显著减少项目审查的人工负担，同时保持对齐准确性。我们建议开发混合管道，将基于LLM的筛选与人工审查相结合，为项目验证和教学对齐提供可扩展的解决方案。

Abstract: As educational systems evolve, ensuring that assessment items remain aligned with content standards is essential for maintaining fairness and instructional relevance. Traditional human alignment reviews are accurate but slow and labor-intensive, especially across large item banks. This study examines whether Large Language Models (LLMs) can accelerate this process without sacrificing accuracy. Using over 12,000 item-skill pairs in grades K-5, we tested three LLMs (GPT-3.5 Turbo, GPT-4o-mini, and GPT-4o) across three tasks that mirror real-world challenges: identifying misaligned items, selecting the correct skill from the full set of standards, and narrowing candidate lists prior to classification. In Study 1, GPT-4o-mini correctly identified alignment status in approximately 83-94% of cases, including subtle misalignments. In Study 2, performance remained strong in mathematics but was lower for reading, where standards are more semantically overlapping. Study 3 demonstrated that pre-filtering candidate skills substantially improved results, with the correct skill appearing among the top five suggestions more than 95% of the time. These findings suggest that LLMs, particularly when paired with candidate filtering strategies, can significantly reduce the manual burden of item review while preserving alignment accuracy. We recommend the development of hybrid pipelines that combine LLM-based screening with human review in ambiguous cases, offering a scalable solution for ongoing item validation and instructional alignment.

</details>


### [114] [Scaling Agentic Reinforcement Learning for Tool-Integrated Reasoning in VLMs](https://arxiv.org/abs/2511.19773)
*Meng Lu,Ran Xu,Yi Fang,Wenxuan Zhang,Yue Yu,Gaurav Srivastava,Yuchen Zhuang,Mohamed Elhoseiny,Charles Fleming,Carl Yang,Zhengzhong Tu,Yang Xie,Guanghua Xiao,Hanrui Wang,Di Jin,Wenqi Shi,Xuan Wang*

Main category: cs.AI

TL;DR: VISTA-Gym 是一个可扩展的训练环境，旨在提高视觉语言模型（VLM）通过多步骤视觉交互进行推理的能力。


<details>
  <summary>Details</summary>
Motivation: 尽管最近的视觉语言模型（VLM）在图像理解方面表现出色，但它们“通过图像思考”的能力，即通过多步视觉交互进行推理的能力仍然有限。

Method: 本文介绍了 VISTA-Gym，这是一个可扩展的训练环境，旨在激励 VLM 具备工具集成的视觉推理能力。VISTA-Gym 统一了多样化的真实世界多模态推理任务（总共来自13个数据集的7个任务），并提供了一个标准化的视觉工具接口（例如，grounding、parsing）、可执行的交互循环、可验证的反馈信号和高效的轨迹记录，从而实现了大规模的视觉智能体强化学习。通过多轮轨迹采样和端到端强化学习，训练 VISTA-R1 将工具使用与智能体推理交织在一起。

Result: VISTA-R1-8B 在11个公开的推理密集型 VQA 基准测试中，其性能优于同等规模的最新基线模型9.51%-18.72%。

Conclusion: VISTA-Gym 是一个有效的训练平台，可以释放 VLM 的工具集成推理能力。

Abstract: While recent vision-language models (VLMs) demonstrate strong image understanding, their ability to "think with images", i.e., to reason through multi-step visual interactions, remains limited. We introduce VISTA-Gym, a scalable training environment for incentivizing tool-integrated visual reasoning capabilities in VLMs. VISTA-Gym unifies diverse real-world multimodal reasoning tasks (7 tasks from 13 datasets in total) with a standardized interface for visual tools (e.g., grounding, parsing), executable interaction loops, verifiable feedback signals, and efficient trajectory logging, enabling visual agentic reinforcement learning at scale. While recent VLMs exhibit strong text-only reasoning, both proprietary and open-source models still struggle with tool selection, invocation, and coordination. With VISTA-Gym, we train VISTA-R1 to interleave tool-use with agentic reasoning via multi-turn trajectory sampling and end-to-end reinforcement learning. Extensive experiments across 11 public reasoning-intensive VQA benchmarks show that VISTA-R1-8B outperforms state-of-the-art baselines with similar sizes by 9.51%-18.72%, demonstrating VISTA-Gym as an effective training ground to unlock the tool-integrated reasoning capabilities for VLMs.

</details>


### [115] [KOM: A Multi-Agent Artificial Intelligence System for Precision Management of Knee Osteoarthritis (KOA)](https://arxiv.org/abs/2511.19798)
*Weizhi Liu,Xi Chen,Zekun Jiang,Liang Zhao,Kunyuan Jiang,Ruisi Tang,Li Wang,Mingke You,Hanyu Zhou,Hongyu Chen,Qiankun Xiong,Yong Nie,Kang Li,Jian Li*

Main category: cs.AI

TL;DR: KOA影响全球6亿多人，造成严重的疼痛、功能障碍和残疾。KOM是一种多智能体系统，旨在自动化KOA评估、风险预测和治疗处方。


<details>
  <summary>Details</summary>
Motivation: KOA是全球性的健康问题，传统的治疗方法需要大量的医疗资源和专业知识，难以在资源有限的环境中实施。因此，需要一种能够自动化KOA评估、风险预测和治疗处方的系统，以弥补医疗资源的不足，提高治疗效率和可及性。

Method: 我们开发了KOM，一个多智能体系统，旨在自动化KOA评估、风险预测和治疗处方。该系统通过整合个人患者资料、疾病状况、风险因素和禁忌症，协助临床医生执行KOA护理路径中的基本任务，并支持生成量身定制的管理计划。

Result: 在基准实验中，KOM在影像分析和处方生成方面表现优于几种通用大型语言模型。一项随机三臂模拟研究显示，与单独使用每种方法相比，KOM与临床医生协作可将总诊断和规划时间减少38.5％，并提高治疗质量。

Conclusion: KOM有望实现KOA自动化管理，并能提高护理效率，其模块化架构也为其他慢性病的AI辅助管理系统开发提供有价值的见解。

Abstract: Knee osteoarthritis (KOA) affects more than 600 million individuals globally and is associated with significant pain, functional impairment, and disability. While personalized multidisciplinary interventions have the potential to slow disease progression and enhance quality of life, they typically require substantial medical resources and expertise, making them difficult to implement in resource-limited settings. To address this challenge, we developed KOM, a multi-agent system designed to automate KOA evaluation, risk prediction, and treatment prescription. This system assists clinicians in performing essential tasks across the KOA care pathway and supports the generation of tailored management plans based on individual patient profiles, disease status, risk factors, and contraindications. In benchmark experiments, KOM demonstrated superior performance compared to several general-purpose large language models in imaging analysis and prescription generation. A randomized three-arm simulation study further revealed that collaboration between KOM and clinicians reduced total diagnostic and planning time by 38.5% and resulted in improved treatment quality compared to each approach used independently. These findings indicate that KOM could help facilitate automated KOA management and, when integrated into clinical workflows, has the potential to enhance care efficiency. The modular architecture of KOM may also offer valuable insights for developing AI-assisted management systems for other chronic conditions.

</details>


### [116] [A Unified Evaluation-Instructed Framework for Query-Dependent Prompt Optimization](https://arxiv.org/abs/2511.19829)
*Ke Chen,Yifeng Wang,Hassan Almosapeeh,Haohan Wang*

Main category: cs.AI

TL;DR: 这篇论文提出了一种新的提示优化方法，通过建立一个性能导向的、系统的、全面的提示评估框架，并开发了一个可直接从文本预测多维质量分数的执行无关评估器，从而实现了对提示的稳定、可解释和模型无关的优化。


<details>
  <summary>Details</summary>
Motivation: 现有的提示优化方法在复杂和动态的用户场景中效果不佳，主要问题包括：多数方法依赖单一静态模板；依赖不稳定的文本反馈或黑盒奖励模型，导致优化信号弱且不可解释；缺乏统一、系统的提示质量定义，导致评估信号碎片化和不可靠。

Method: 1. 建立了性能导向、系统且全面的提示评估框架。2. 开发并微调了一个执行无关的评估器，该评估器能够直接从文本预测多维质量分数。3. 评估器指导一个度量感知优化器，该优化器能够诊断失败模式并以可解释的、依赖查询的方式重写提示。

Result: 1. 评估器在预测提示性能方面达到了最高的准确性。2. 评估器指导的优化方法在八个数据集和三个主干模型上，持续超越了静态模板和依赖查询的基线方法。

Conclusion: 本研究提出了一个统一的、基于度量的提示质量视角，并证明了其评估指导的优化流程在各种任务中都能提供稳定、可解释和模型无关的改进。

Abstract: Most prompt-optimization methods refine a single static template, making them ineffective in complex and dynamic user scenarios. Existing query-dependent approaches rely on unstable textual feedback or black-box reward models, providing weak and uninterpretable optimization signals. More fundamentally, prompt quality itself lacks a unified, systematic definition, resulting in fragmented and unreliable evaluation signals. Our approach first establishes a performance-oriented, systematic, and comprehensive prompt evaluation framework. Furthermore, we develop and finetune an execution-free evaluator that predicts multi-dimensional quality scores directly from text. The evaluator then instructs a metric-aware optimizer that diagnoses failure modes and rewrites prompts in an interpretable, query-dependent manner. Our evaluator achieves the strongest accuracy in predicting prompt performance, and the evaluation-instructed optimization consistently surpass both static-template and query-dependent baselines across eight datasets and on three backbone models. Overall, we propose a unified, metric-grounded perspective on prompt quality, and demonstrated that our evaluation-instructed optimization pipeline delivers stable, interpretable, and model-agnostic improvements across diverse tasks.

</details>


### [117] [VICoT-Agent: A Vision-Interleaved Chain-of-Thought Framework for Interpretable Multimodal Reasoning and Scalable Remote Sensing Analysis](https://arxiv.org/abs/2511.20085)
*Chujie Wang,Zhiyuan Luo,Ruiqi Liu,Can Ran,Shenghua Fan,Xi Chen,Chu He*

Main category: cs.AI

TL;DR: VICoT是一个多模态智能体框架，通过动态集成视觉工具到思维链中，实现了显式的多轮推理，在遥感图像分析任务中表现出色。


<details>
  <summary>Details</summary>
Motivation: 目前的遥感图像分析任务正从传统的物体识别转向复杂的智能推理，这对模型的推理能力和工具调用的灵活性提出了更高要求。

Method: VICoT框架通过基于堆栈的推理结构和模块化的MCP兼容工具套件，使LLM能够高效执行多轮、交错的视觉-语言推理任务。同时，提出了推理堆栈蒸馏方法，将复杂的智能体行为迁移到小型模型中。

Result: VICoT在推理透明度、执行效率和生成质量方面显著优于现有最先进的框架。推理堆栈蒸馏方法在显著降低复杂性的同时，确保了推理能力。

Conclusion: VICoT框架通过创新的多轮推理机制和工具集成，成功解决了遥感图像分析中的复杂推理挑战，并且通过蒸馏方法实现了效率优化。

Abstract: The current remote sensing image analysis task is increasingly evolving from traditional object recognition to complex intelligence reasoning, which places higher requirements on the model's reasoning ability and the flexibility of tool invocation. To this end, we propose a new multimodal agent framework, Vision-Interleaved Chain-of-Thought Framework (VICoT), which implements explicit multi-round reasoning by dynamically incorporating visual tools into the chain of thought. Through a stack-based reasoning structure and a modular MCP-compatible tool suite, VICoT enables LLMs to efficiently perform multi-round, interleaved vision-language reasoning tasks with strong generalization and flexibility.We also propose the Reasoning Stack distillation method to migrate complex Agent behaviors to small, lightweight models, which ensures the reasoning capability while significantly reducing complexity. Experiments on multiple remote sensing benchmarks demonstrate that VICoT significantly outperforms existing SOTA frameworks in reasoning transparency, execution efficiency, and generation quality.

</details>


### [118] [Reinforcement Learning with $ω$-Regular Objectives and Constraints](https://arxiv.org/abs/2511.19849)
*Dominik Wagner,Leon Witzman,Luke Ong*

Main category: cs.AI

TL;DR: 该论文提出了一种基于线性规划的模型强化学习算法，结合ω正则表达式和显式约束，以解决传统强化学习在表达复杂目标和处理安全-性能权衡方面的局限性。


<details>
  <summary>Details</summary>
Motivation: 传统的强化学习方法依赖于标量奖励，难以表达时间性、条件性或安全关键性目标，并可能导致奖励欺骗。通过更通用的ω-正则目标所表达的时态逻辑可以精确地指定丰富的行为特性。即便如此，通过单一标量（无论是奖励还是满足概率）衡量性能，也会掩盖在存在可容忍风险水平的环境中出现的安全-性能权衡。

Method: 本文将ω-正则目标与显式约束相结合，从而可以分别处理安全要求和优化目标。我们开发了一种基于线性规划的模型强化学习算法，该算法在限制条件下能生成一个策略，该策略能在指定阈值内最大限度地满足ω-正则目标，同时遵守ω-正则约束。

Result: 在限制条件下，该算法能生成一个策略，该策略能在指定阈值内最大限度地满足ω-正则目标，同时遵守ω-正则约束。此外，我们建立了与具有最优性保留保证的约束限制平均问题的转换。

Conclusion: 所提出的算法结合了ω-正则目标和显式约束，能够有效解决强化学习中复杂目标表达和安全-性能权衡问题。

Abstract: Reinforcement learning (RL) commonly relies on scalar rewards with limited ability to express temporal, conditional, or safety-critical goals, and can lead to reward hacking. Temporal logic expressible via the more general class of $ω$-regular objectives addresses this by precisely specifying rich behavioural properties. Even still, measuring performance by a single scalar (be it reward or satisfaction probability) masks safety-performance trade-offs that arise in settings with a tolerable level of risk.
  We address both limitations simultaneously by combining $ω$-regular objectives with explicit constraints, allowing safety requirements and optimisation targets to be treated separately. We develop a model-based RL algorithm based on linear programming, which in the limit produces a policy maximising the probability of satisfying an $ω$-regular objective while also adhering to $ω$-regular constraints within specified thresholds. Furthermore, we establish a translation to constrained limit-average problems with optimality-preserving guarantees.

</details>


### [119] [MicroSims: A Framework for AI-Generated, Scalable Educational Simulations with Universal Embedding and Adaptive Learning Support](https://arxiv.org/abs/2511.19864)
*Valerie Lockhart,Dan McCreary,Troy A. Peterson*

Main category: cs.AI

TL;DR: 介绍了MicroSims，一个用于创建轻量级、交互式教育模拟的新框架，它可以通过人工智能快速生成，普遍嵌入到数字学习平台中，并且无需编程知识即可轻松定制。


<details>
  <summary>Details</summary>
Motivation: 传统的教育模拟需要大量资源和专业技术才能创建，这限制了它们的应用。

Method: MicroSims框架结合了三项关键创新：1）实现AI辅助生成的标准化设计模式；2）提供通用嵌入和沙盒安全性的iframe架构；3）支持定制和教学透明的透明、可修改代码。该框架涵盖了设计原则、技术架构、元数据标准和开发工作流程。

Result: 交互式模拟可以将概念理解能力提高30-40%。MicroSims在解决成本、技术复杂性和平台依赖性的同时，扩展了这些优势。

Conclusion: MicroSims框架通过实现低成本、智能化的交互式教科书，为教育公平带来了重大影响，使全球教育工作者能够按需创建定制的、符合课程的模拟。未来将探索基于MicroSims的AI驱动自适应学习系统。

Abstract: Educational simulations have long been recognized as powerful tools for enhancing learning outcomes, yet their creation has traditionally required substantial resources and technical expertise. This paper introduces MicroSims a novel framework for creating lightweight, interactive educational simulations that can be rapidly generated using artificial intelligence, universally embedded across digital learning platforms, and easily customized without programming knowledge. MicroSims occupy a unique position at the intersection of three key innovations: (1) standardized design patterns that enable AI-assisted generation, (2) iframe-based architecture that provides universal embedding and sandboxed security, and (3) transparent, modifiable code that supports customization and pedagogical transparency. We present a comprehensive framework encompassing design principles, technical architecture, metadata standards, and development workflows. Drawing on empirical research from physics education studies and meta-analyses across STEM disciplines, we demonstrate that interactive simulations can improve conceptual understanding by up to 30-40\% compared to traditional instruction. MicroSims extend these benefits while addressing persistent barriers of cost, technical complexity, and platform dependence. This work has significant implications for educational equity, and low-cost intelligent interactive textbooks that enabling educators worldwide to create customized, curriculum-aligned simulations on demand. We discuss implementation considerations, present evidence of effectiveness, and outline future directions for AI-powered adaptive learning systems built on the MicroSim foundation.

</details>


### [120] [Simulated Self-Assessment in Large Language Models: A Psychometric Approach to AI Self-Efficacy](https://arxiv.org/abs/2511.19872)
*Daniel I Jackson,Emma L Jensen,Syed-Amad Hussain,Emre Sezgin*

Main category: cs.AI

TL;DR: 本文探讨了大型语言模型（LLMs）的自我评估能力，发现它们的自我评估与实际能力之间存在差异，且评估结果受任务类型影响。


<details>
  <summary>Details</summary>
Motivation: 评估大型语言模型的可靠性，而不仅仅是任务准确性。

Method: 本文选取了10个LLM模型，采用改编的10项通用自我效能感量表（GSES）进行模拟自我评估，涵盖无任务、计算推理、社会推理和摘要四种条件，并对模型的自我评估进行了重复测量和项目顺序随机化。同时还分析了模型在计算、社会和摘要任务上的表现，并通过后续的信心提示来评估模型的自我评估校准性，最后进行了定性分析。

Result: GSES反应具有高度稳定性，但模型在不同条件下自我效能水平差异显著，总体得分低于人类标准。所有模型在计算和社交问题上均达到完美准确性，而摘要表现差异较大。自我评估未能可靠反映能力，部分低分模型表现准确，部分高分模型表现较弱。后续信心提示导致适度的（主要是向下）修正，表明首次评估存在轻微高估。定性分析显示，较高的自我效能对应更自信、拟人化的推理风格，而较低的自我效能则反映谨慎、去拟人化的解释。

Conclusion: 心理测量提示可以为LLM的交流行为提供结构化洞察，但不能提供经过校准的性能估计。

Abstract: Self-assessment is a key aspect of reliable intelligence, yet evaluations of large language models (LLMs) focus mainly on task accuracy. We adapted the 10-item General Self-Efficacy Scale (GSES) to elicit simulated self-assessments from ten LLMs across four conditions: no task, computational reasoning, social reasoning, and summarization. GSES responses were highly stable across repeated administrations and randomized item orders. However, models showed significantly different self-efficacy levels across conditions, with aggregate scores lower than human norms. All models achieved perfect accuracy on computational and social questions, whereas summarization performance varied widely. Self-assessment did not reliably reflect ability: several low-scoring models performed accurately, while some high-scoring models produced weaker summaries. Follow-up confidence prompts yielded modest, mostly downward revisions, suggesting mild overestimation in first-pass assessments. Qualitative analysis showed that higher self-efficacy corresponded to more assertive, anthropomorphic reasoning styles, whereas lower scores reflected cautious, de-anthropomorphized explanations. Psychometric prompting provides structured insight into LLM communication behavior but not calibrated performance estimates.

</details>


### [121] [RPM-MCTS: Knowledge-Retrieval as Process Reward Model with Monte Carlo Tree Search for Code Generation](https://arxiv.org/abs/2511.19895)
*Yuanyuan Lin,Xiangyu Ouyang,Teng Zhang,Kaixin Sui*

Main category: cs.AI

TL;DR: RPM-MCTS通过知识检索作为过程奖励模型，改进了蒙特卡洛树搜索（MCTS）在代码生成中的应用，解决了中间步骤评估困难和错误定位不及时的问题，并实现了更好的性能和更低的token消耗。


<details>
  <summary>Details</summary>
Motivation: 现有的基于树搜索的代码生成方法在评估中间算法步骤和及时纠正错误方面存在困难，导致生成不正确的代码并增加计算成本。

Method: RPM-MCTS利用知识检索作为过程奖励模型来评估中间算法步骤，避免了复杂的奖励模型训练。在扩展阶段，采用相似性过滤来移除冗余节点，确保推理路径的多样性。此外，该方法利用沙盒执行反馈来定位生成过程中的错误算法步骤，并进行及时和有针对性的纠正。

Result: RPM-MCTS在四个公共代码生成基准测试中优于当前的SOTA方法，同时将token消耗降低了大约15%。使用RPM-MCTS构建的数据对基础模型进行全面微调，显著增强了其代码能力。

Conclusion: RPM-MCTS通过引入知识检索过程奖励模型和沙盒执行反馈机制，显著提升了大型语言模型在代码生成方面的性能和效率。

Abstract: Tree search-based methods have made significant progress in enhancing the code generation capabilities of large language models. However, due to the difficulty in effectively evaluating intermediate algorithmic steps and the inability to locate and timely correct erroneous steps, these methods often generate incorrect code and incur increased computational costs. To tackle these problems, we propose RPM-MCTS, an effective method that utilizes Knowledge-Retrieval as Process Reward Model based on Monte Carlo Tree Search to evaluate intermediate algorithmic steps. By utilizing knowledge base retrieval, RPM-MCTS avoids the complex training of process reward models. During the expansion phase, similarity filtering is employed to remove redundant nodes, ensuring diversity in reasoning paths. Furthermore, our method utilizes sandbox execution feedback to locate erroneous algorithmic steps during generation, enabling timely and targeted corrections. Extensive experiments on four public code generation benchmarks demonstrate that RPM-MCTS outperforms current state-of-the-art methods while achieving an approximately 15% reduction in token consumption. Furthermore, full fine-tuning of the base model using the data constructed by RPM-MCTS significantly enhances its code capabilities.

</details>


### [122] [Semantic-KG: Using Knowledge Graphs to Construct Benchmarks for Measuring Semantic Similarity](https://arxiv.org/abs/2511.19925)
*Qiyao Wei,Edward Morrell,Lea Goetz,Mihaela van der Schaar*

Main category: cs.AI

TL;DR: 这篇论文介绍了一种使用知识图谱评估大型语言模型开放式文本回复语义相似度的方法。


<details>
  <summary>Details</summary>
Motivation: 目前评估大型语言模型开放式文本回复语义相似度的方法，可能更多地捕捉句法或词汇形式而非语义内容。现有的语义等价基准存在生成成本高、领域适用性受限和等价定义不清晰等问题。

Method: 本文提出了一种利用知识图谱（KGs）生成自然语言语句对的新颖方法，这些语句对可以是语义相似的，也可以是语义不相似的（不相似的语句对分为四种亚型）。作者在四个不同领域（常识、生物医学、金融、生物）生成了基准数据集，并对包括传统自然语言处理分数和“LLM作为评判者”预测在内的语义相似度方法进行了比较研究。

Result: 作者观察到语义变异的亚型以及基准的领域都会影响语义相似度方法的性能，没有一种方法始终表现最优。

Conclusion: 本研究结果对使用“LLM作为评判者”来检测文本的语义内容具有重要意义。

Abstract: Evaluating the open-form textual responses generated by Large Language Models (LLMs) typically requires measuring the semantic similarity of the response to a (human generated) reference. However, there is evidence that current semantic similarity methods may capture syntactic or lexical forms over semantic content. While benchmarks exist for semantic equivalence, they often suffer from high generation costs due to reliance on subjective human judgment, limited availability for domain-specific applications, and unclear definitions of equivalence. This paper introduces a novel method for generating benchmarks to evaluate semantic similarity methods for LLM outputs, specifically addressing these limitations. Our approach leverages knowledge graphs (KGs) to generate pairs of natural-language statements that are semantically similar or dissimilar, with dissimilar pairs categorized into one of four sub-types. We generate benchmark datasets in four different domains (general knowledge, biomedicine, finance, biology), and conduct a comparative study of semantic similarity methods including traditional natural language processing scores and LLM-as-a-judge predictions. We observe that the sub-type of semantic variation, as well as the domain of the benchmark impact the performance of semantic similarity methods, with no method being consistently superior. Our results present important implications for the use of LLM-as-a-judge in detecting the semantic content of text. Code is available at https://github.com/QiyaoWei/semantic-kg and the dataset is available at https://huggingface.co/datasets/QiyaoWei/Semantic-KG.

</details>


### [123] [A System-Level Taxonomy of Failure Modes in Large Language Model Applications](https://arxiv.org/abs/2511.19933)
*Vaishali Vinay*

Main category: cs.AI

TL;DR: 该论文介绍了大型语言模型（LLMs）在实际应用中存在的15种隐藏故障模式，并分析了现有评估和监控实践的不足。同时，论文提出了构建可靠、可维护和经济高效的LLM系统的设计原则，强调将LLM的可靠性视为一个系统工程问题。


<details>
  <summary>Details</summary>
Motivation: 大型语言模型（LLMs）正被迅速整合到决策支持工具、自动化工作流程和AI使能的软件系统中，但它们在生产环境中的行为仍不清楚，其故障模式与传统机器学习模型有根本性差异。

Method: 本文提出了一种系统级的分类方法，包含15种在实际LLM应用中出现的隐藏故障模式（例如多步骤推理漂移、潜在不一致性、上下文边界退化、不正确的工具调用、版本漂移和成本驱动的性能崩溃）。
利用该分类方法，分析了评估和监控实践中日益扩大的差距：现有基准衡量知识或推理，但对稳定性、可重复性、漂移或工作流集成提供的信息很少。
进一步研究了部署LLMs相关的生产挑战（包括可观测性限制、成本限制和更新引起的回归），并概述了构建可靠、可维护和成本感知的LLM系统的高级设计原则。

Result: 识别并分类了15种真实LLM应用中出现的隐藏故障模式。
强调了现有评估和监控实践在衡量LLM系统可靠性、稳定性、可重复性、漂移和工作流集成方面的不足。
概述了构建可靠、可维护和成本感知的LLM系统的高级设计原则，将LLM可靠性视为一个系统工程问题。

Conclusion: 本研究通过将LLM的可靠性视为一个系统工程问题，而非单纯以模型为中心的问题，为未来关于评估方法、AI系统鲁棒性和可靠LLM部署的研究提供了分析基础。

Abstract: Large language models (LLMs) are being rapidly integrated into decision-support tools, automation workflows, and AI-enabled software systems. However, their behavior in production environments remains poorly understood, and their failure patterns differ fundamentally from those of traditional machine learning models. This paper presents a system-level taxonomy of fifteen hidden failure modes that arise in real-world LLM applications, including multi-step reasoning drift, latent inconsistency, context-boundary degradation, incorrect tool invocation, version drift, and cost-driven performance collapse. Using this taxonomy, we analyze the growing gap in evaluation and monitoring practices: existing benchmarks measure knowledge or reasoning but provide little insight into stability, reproducibility, drift, or workflow integration. We further examine the production challenges associated with deploying LLMs - including observability limitations, cost constraints, and update-induced regressions - and outline high-level design principles for building reliable, maintainable, and cost-aware LLM systems. Finally, we outline high-level design principles for building reliable, maintainable, and cost-aware LLM-based systems. By framing LLM reliability as a system-engineering problem rather than a purely model-centric one, this work provides an analytical foundation for future research on evaluation methodology, AI system robustness, and dependable LLM deployment.

</details>


### [124] [M$^3$Prune: Hierarchical Communication Graph Pruning for Efficient Multi-Modal Multi-Agent Retrieval-Augmented Generation](https://arxiv.org/abs/2511.19969)
*Weizi Shao,Taolin Zhang,Zijie Zhou,Chen Chen,Chengyu Wang,Xiaofeng He*

Main category: cs.AI

TL;DR: M$^3$Prune 针对多模态检索增强生成（mRAG）中的多智能体通信系统，提出了一种新颖的剪枝框架，其在显著降低token消耗的同时，保持了卓越的性能。


<details>
  <summary>Details</summary>
Motivation: 现有的多智能体系统在多模态检索增强生成 (mRAG) 中虽然表现出色，但存在显著的token开销和计算成本，这限制了其大规模部署。

Method: M$^3$Prune 框架通过以下步骤工作：  1) 对文本和视觉模态进行 intra-modal 图稀疏化，识别对任务解决最关键的边。  2) 利用这些关键边构建动态通信拓扑，进行 inter-modal 图稀疏化。  3) 逐步剪枝冗余边，以获得更高效和分层的拓扑结构。

Result: 在通用和特定领域的 mRAG 基准测试中，M$^3$Prune 始终优于单智能体和稳健的多智能体 mRAG 系统，并且显著降低了token消耗。

Conclusion: M$^3$Prune 通过剪枝多模态多智能体通信图中的冗余边，有效地解决了现有 mRAG 系统中token开销大的问题，同时保持了高性能，为大规模部署提供了可能。

Abstract: Recent advancements in multi-modal retrieval-augmented generation (mRAG), which enhance multi-modal large language models (MLLMs) with external knowledge, have demonstrated that the collective intelligence of multiple agents can significantly outperform a single model through effective communication. Despite impressive performance, existing multi-agent systems inherently incur substantial token overhead and increased computational costs, posing challenges for large-scale deployment. To address these issues, we propose a novel Multi-Modal Multi-agent hierarchical communication graph PRUNING framework, termed M$^3$Prune. Our framework eliminates redundant edges across different modalities, achieving an optimal balance between task performance and token overhead. Specifically, M$^3$Prune first applies intra-modal graph sparsification to textual and visual modalities, identifying the edges most critical for solving the task. Subsequently, we construct a dynamic communication topology using these key edges for inter-modal graph sparsification. Finally, we progressively prune redundant edges to obtain a more efficient and hierarchical topology. Extensive experiments on both general and domain-specific mRAG benchmarks demonstrate that our method consistently outperforms both single-agent and robust multi-agent mRAG systems while significantly reducing token consumption.

</details>


### [125] [From data to concepts via wiring diagrams](https://arxiv.org/abs/2511.20138)
*Jason Lo,Mohammadnima Jafari*

Main category: cs.AI

TL;DR: 本文介绍了一种准骨架布线图，并证明了它与Hasse图一致，然后通过该方法设计了一种从序列数据中提取布线图的算法，并将其应用于分析自主智能体玩电脑游戏的行为。


<details>
  <summary>Details</summary>
Motivation: 作者提出了一种准骨架布线图，它可以将导线图（表示时间过程的标记有向图）转换为Hasse图，并应用其对序列数据进行分析。

Method: 通过引入准骨架布线图并证明其与Hasse图一致，设计了从序列数据中提取布线图的算法。

Result: 算法成功识别了电脑游戏中自主智能体的获胜策略，并且与DBSCAN和Agglomerative hierarchical两种标准聚类技术相比，在数据受到扰动时表现出更好的性能。

Conclusion: 本文结合了范畴论、图论、聚类、强化学习和数据工程技术，提出了一种新的从序列数据中提取布线图的方法，并在实际应用中取得了良好的效果。

Abstract: A wiring diagram is a labeled directed graph that represents an abstract concept such as a temporal process. In this article, we introduce the notion of a quasi-skeleton wiring diagram graph, and prove that quasi-skeleton wiring diagram graphs correspond to Hasse diagrams. Using this result, we designed algorithms that extract wiring diagrams from sequential data. We used our algorithms in analyzing the behavior of an autonomous agent playing a computer game, and the algorithms correctly identified the winning strategies. We compared the performance of our main algorithm with two other algorithms based on standard clustering techniques (DBSCAN and agglomerative hierarchical), including when some of the data was perturbed. Overall, this article brings together techniques in category theory, graph theory, clustering, reinforcement learning, and data engineering.

</details>


### [126] [Interactive AI NPCs Powered by LLMs: Technical Report for the CPDC Challenge 2025](https://arxiv.org/abs/2511.20200)
*Yitian Huang,Yuxuan Lei,Jianxun Lian,Hao Liao*

Main category: cs.AI

TL;DR: 该报告介绍了MSRA_SC团队在常识性角色对话挑战赛（CPDC 2025）中的解决方案和成果。该团队提出了一种简单而有效的框架，在GPU赛道和API赛道上都取得了改进。


<details>
  <summary>Details</summary>
Motivation: 在常识性角色对话挑战赛（CPDC 2025）中，寻求一个统一的框架，该框架能够同时改进GPU和API赛道的表现，解决LLMs在复杂多样的对话场景中遇到的挑战。这些挑战包括工具调用的稳定性、执行的可靠性、角色扮演的指导，以及小样本数据下的过拟合问题。

Method: 该方法主要包括两个关键部分：
1. 上下文工程（Context Engineering）：通过动态工具剪枝和角色裁剪进行输入压缩；结合参数归一化和函数合并等后处理技术；并辅以人工优化的提示。旨在提高工具调用的稳定性、执行可靠性和角色扮演指导。
2. GRPO训练：在GPU赛道中，采用GRPO训练（一种强化学习方法），直接通过奖励信号进行优化，取代了传统的监督微调，从而缓解了小样本过拟合问题，并显著提升了面向任务的对话性能。

Result: 在最终评估中，MSRA_SC团队在任务2 API中排名第1，任务1 API中排名第2，在任务3 API和GPU赛道中均排名第3。这表明了所提出方法的有效性。

Conclusion: 该团队提出了一种在常识性角色对话挑战赛中表现卓越的统一框架。通过上下文工程提高工具调用和角色扮演效果，并通过GRPO训练解决了GPU赛道中小样本过拟合问题，显著提升了对话性能。

Abstract: This report presents the solution and results of our team MSRA\_SC in the Commonsense Persona-Grounded Dialogue Challenge (CPDC 2025). We propose a simple yet effective framework that unifies improvements across both GPU Track and API Track. Our method centers on two key components. First, Context Engineering applies dynamic tool pruning and persona clipping for input compression, combined with post-processing techniques such as parameter normalization and function merging. Together with manually refined prompts, this design improves tool call stability, execution reliability, and role-playing guidance. Second, in the GPU Track, we further adopt GRPO training, replacing supervised fine-tuning with reinforcement learning directly optimized by reward signals. This mitigates small-sample overfitting and significantly enhances task-oriented dialogue performance. In the final evaluation, our team ranks 1st in Task 2 API, 2nd in Task 1 API, and 3rd in both Task 3 API and GPU track, demonstrating the effectiveness of our approach. Our code is publicly available at https://gitlab.aicrowd.com/nikoo_yu/cpdc-2025-winning-solution

</details>


### [127] [CostNav: A Navigation Benchmark for Cost-Aware Evaluation of Embodied Agents](https://arxiv.org/abs/2511.20216)
*Haebin Seong,Sungmin Kim,Minchan Kim,Yongjun Cho,Myunchul Joe,Suhwan Choi,Jaeyoon Jung,Jiyong Youn,Yoonshik Kim,Samwoo Seong,Yubeen Park,Youngjae Yu,Yunsung Lee*

Main category: cs.AI

TL;DR: CostNav是一个微导航经济测试平台，旨在通过全面的成本收入分析来评估具身智能体，以解决现有导航基准忽视经济可行性的问题。


<details>
  <summary>Details</summary>
Motivation: 现有导航基准侧重于任务成功指标，但忽略了经济可行性，而经济可行性对于自动送货机器人的商业部署至关重要。

Method: CostNav通过对硬件、训练、能源、维护成本和送货收入（包括服务水平协议）的完整经济生命周期进行建模，并使用行业衍生参数进行成本收益分析。它从缩小规模的模拟推断到实际交付，并使用行业数据源（能源费率、送货服务定价）的参数。

Result: 基线实现了43.0%的SLA合规性，但没有商业可行性，每次运行亏损30.009美元，没有有限的盈亏平衡点。运营成本主要由碰撞引起的维护成本（占每次运行成本的99.7%）主导，突出表明碰撞避免是关键的优化目标。

Conclusion: CostNav弥合了导航研究与商业部署之间的差距，实现了关于导航范式经济权衡的数据驱动决策。它首次通过量化揭示了导航研究指标与商业可行性之间的差距。

Abstract: Existing navigation benchmarks focus on task success metrics while overlooking economic viability -- critical for commercial deployment of autonomous delivery robots. We introduce \emph{CostNav}, a \textbf{Micro-Navigation Economic Testbed} that evaluates embodied agents through comprehensive cost-revenue analysis aligned with real-world business operations. CostNav models the complete economic lifecycle including hardware, training, energy, maintenance costs, and delivery revenue with service-level agreements, using industry-derived parameters. \textbf{To our knowledge, CostNav is the first work to quantitatively expose the gap between navigation research metrics and commercial viability}, revealing that optimizing for task success fundamentally differs from optimizing for economic deployment. Our cost model uses parameters derived from industry data sources (energy rates, delivery service pricing), and we project from a reduced-scale simulation to realistic deliveries. Under this projection, the baseline achieves 43.0\% SLA compliance but is \emph{not} commercially viable: yielding a loss of \$30.009 per run with no finite break-even point, because operating costs are dominated by collision-induced maintenance, which accounts for 99.7\% of per-run costs and highlights collision avoidance as a key optimization target. We demonstrate a learning-based on-device navigation baseline and establish a foundation for evaluating rule-based navigation, imitation learning, and cost-aware RL training. CostNav bridges the gap between navigation research and commercial deployment, enabling data-driven decisions about economic trade-offs across navigation paradigms.

</details>


### [128] [Actionable and diverse counterfactual explanations incorporating domain knowledge and causal constraints](https://arxiv.org/abs/2511.20236)
*Szymon Bobek,Łukasz Bałec,Grzegorz J. Nalepa*

Main category: cs.AI

TL;DR: 该论文提出了一种名为DANCE的方法，用于生成多样化、可操作且知识受限的反事实解释，该方法通过整合特征依赖和因果约束来确保解释的合理性和实际可行性，解决了现有方法在处理现实世界复杂数据时的问题。


<details>
  <summary>Details</summary>
Motivation: 现有反事实解释方法在处理真实世界数据集时，往往忽略数据中复杂的依赖关系，导致生成的修改建议不切实际或不可行。

Method: 本文提出了一种名为DANCE（Diverse, Actionable, and kNowledge-Constrained Explanations）的方法。该方法通过从数据中学习线性或非线性约束，或整合专家提供的依赖图，将特征依赖和因果约束纳入反事实解释的生成过程，以确保反事实解释的合理性和实际可行性。该方法在保证反事实与特征关系一致性的同时，平衡了合理性、多样性和稀疏性。

Result: DANCE方法能够生成有意义且与领域相关的反事实解释。该方法在与波兰最大的电子邮件营销公司Freshmail的真实案例研究以及与Sendguard的联合研发项目中得到了应用。此外，通过对140个公共数据集的广泛评估，DANCE方法在常用指标上优于其他现有方法。

Conclusion: DANCE方法通过整合特征依赖和因果约束，能够生成多样化、可操作且知识受限的反事实解释，有效解决了现有反事实解释方法在实际应用中的局限性。

Abstract: Counterfactual explanations enhance the actionable interpretability of machine learning models by identifying the minimal changes required to achieve a desired outcome of the model. However, existing methods often ignore the complex dependencies in real-world datasets, leading to unrealistic or impractical modifications. Motivated by cybersecurity applications in the email marketing domain, we propose a method for generating Diverse, Actionable, and kNowledge-Constrained Explanations (DANCE), which incorporates feature dependencies and causal constraints to ensure plausibility and real-world feasibility of counterfactuals. Our method learns linear and nonlinear constraints from data or integrates expert-provided dependency graphs, ensuring counterfactuals are plausible and actionable. By maintaining consistency with feature relationships, the method produces explanations that align with real-world constraints. Additionally, it balances plausibility, diversity, and sparsity, effectively addressing key limitations in existing algorithms. The work is developed based on a real-life case study with Freshmail, the largest email marketing company in Poland and supported by a joint R&D project Sendguard. Furthermore, we provide an extensive evaluation using 140 public datasets, which highlights its ability to generate meaningful, domain-relevant counterfactuals that outperform other existing approaches based on widely used metrics. The source code for reproduction of the results can be found in a GitHub repository we provide.

</details>


### [129] [Improving Language Agents through BREW](https://arxiv.org/abs/2511.20297)
*Shashank Kirtania,Param Biyani,Priyanshu Gupta,Yasharth Bajpai,Roshni Iyer,Sumit Gulwani,Gustavo Soares*

Main category: cs.AI

TL;DR: 该论文介绍了一个名为BREW的框架，用于通过构建和完善经验学习的结构化记忆来优化基于LLM的智能体，解决了现有训练范式计算开销大和策略难以解释的问题，并在实际基准测试中取得了显著的任务性能提升。


<details>
  <summary>Details</summary>
Motivation: 尽管基于LLM的智能体在结构化推理、工具使用和环境适应方面表现出多功能性，但现有的模型权重优化方法（如PPO和GRPO）由于其高计算开销而不够实用，并且生成的智能体策略难以解释、适应或逐步改进。

Method: 本研究通过构建和完善智能体从环境中获得的经验学习的结构化记忆，来替代传统的智能体优化途径。引入了BREW框架，用于通过知识库（KB）的构建和完善来优化智能体以完成下游任务。该方法提出了一种有效的分区智能体记忆的方法，以实现更高效的检索和完善。BREW利用任务评分器和行为评估标准来学习见解，同时利用状态空间搜索来确保从自然语言的噪声和非特异性中获得鲁棒性。

Result: 在真实世界的领域基准测试（OSWorld、$τ^2$Bench和SpreadsheetBench）中，BREW将任务精度提高了10-20%，API/工具调用减少了10-15%，从而加快了执行时间，同时保持了与基础模型相当的计算效率。

Conclusion: 本研究将知识库（KB）确立为智能体优化的模块化和可控的基础，这是一个以透明、可解释和可扩展的方式塑造智能体行为的明确杠杆，这与以往将记忆视为静态背景的工作不同。

Abstract: Large Language Model (LLM)-based agents are increasingly applied to tasks requiring structured reasoning, tool use, and environmental adaptation, such as data manipulation, multistep planning, and computer-use automation. However, despite their versatility, current training paradigms for model weight optimization methods, like PPO and GRPO, remain relatively impractical with their high computational overhead for rollout convergence. In addition, the resulting agent policies are difficult to interpret, adapt, or incrementally improve. To address this, we investigate creating and refining structured memory of experiential learning of an agent from its environment as an alternative route to agent optimization. We introduce BREW (Bootstrapping expeRientially-learned Environmental knoWledge), a framework for agent optimization for downstream tasks via KB construction and refinement. In our formulation, we introduce an effective method for partitioning agent memory for more efficient retrieval and refinement. BREW uses task graders and behavior rubrics to learn insights while leveraging state-space search for ensuring robustness from the noise and non-specificity in natural language. Empirical results on real world, domain-grounded benchmarks -- OSWorld, $τ^2$Bench, and SpreadsheetBench -- show BREW achieves $10-20\%$ improvement in task precision, $10-15\%$ reduction in API/tool calls leading to faster execution time, all while maintaining computational efficiency on par with base models. Unlike prior work where memory is treated as static context, we establish the KB as a modular and controllable substrate for agent optimization -- an explicit lever for shaping behavior in a transparent, interpretable, and extensible manner.

</details>


### [130] [Active Inference in Discrete State Spaces from First Principles](https://arxiv.org/abs/2511.20321)
*Patrick Kenny*

Main category: cs.AI

TL;DR: 该文章旨在通过将其与自由能原理分离来阐明主动推理的概念，并提出了一种新的感知/动作散度标准。


<details>
  <summary>Details</summary>
Motivation: 此研究旨在澄清主动推理的概念，即将其与自由能原理区分开来。作者认为，实现离散状态空间中的主动推理所需的优化可以被表述为受约束的散度最小化问题，而这可以通过标准的平均场方法解决，而无需引入预期自由能的概念。

Method: 当用于建模感知时，文中所提出的感知/动作散度标准与变分自由能相吻合。在用于建模动作时，它与预期自由能泛函的不同之处在于增加了一个熵正则化项。

Result: 作者通过将主动推理的优化问题表述为受约束的散度最小化问题，并使用标准平均场方法求解，成功地将主动推理与自由能原理分离。此外，他们提出了一个感知/动作散度标准。

Conclusion: 该研究成功地将主动推理与自由能原理区分开来，并提出了一种新的感知/动作散度标准。当用于感知建模时，该标准与变分自由能一致；而当用于动作建模时，它在预期自由能泛函的基础上增加了一个熵正则化项，从而提供了对主动推理的更清晰理解和更实用的实现方法。

Abstract: We seek to clarify the concept of active inference by disentangling it from the Free Energy Principle. We show how the optimizations that need to be carried out in order to implement active inference in discrete state spaces can be formulated as constrained divergence minimization problems which can be solved by standard mean field methods that do not appeal to the idea of expected free energy. When it is used to model perception, the perception/action divergence criterion that we propose coincides with variational free energy. When it is used to model action, it differs from an expected free energy functional by an entropy regularizer.

</details>


### [131] [VibraVerse: A Large-Scale Geometry-Acoustics Alignment Dataset for Physically-Consistent Multimodal Learning](https://arxiv.org/abs/2511.20422)
*Bo Pang,Chenxi Xu,Jierui Ren,Guoping Wang,Sheng Li*

Main category: cs.AI

TL;DR: 该论文提出了VibraVerse，一个大规模的几何声学对齐数据集，明确地连接了从3D几何到物理属性再到声学信号的因果链。它还引入了CLASP，一个对比学习框架，用于跨模态对齐，保留了物体物理结构和声学响应之间的因果对应关系。


<details>
  <summary>Details</summary>
Motivation: 现有的多模态学习框架缺乏物理一致性，并且忽略了物体几何、材料、振动模式及其产生声音之间固有的因果关系。

Method: 为了建立这种连贯性，该论文引入了CLASP，一个对比学习框架，用于跨模态对齐，保留了物体物理结构和声学响应之间的因果对应关系。

Result: 在VibraVerse上训练的模型在这些任务上表现出卓越的准确性、可解释性和跨模态泛化能力。

Conclusion: VibraVerse为物理一致和因果可解释的多模态学习提供了一个基准，为声音引导的具身感知和对物理世界更深层次的理解奠定了基础。

Abstract: Understanding the physical world requires perceptual models grounded in physical laws rather than mere statistical correlations. However, existing multimodal learning frameworks, focused on vision and language, lack physical consistency and overlook the intrinsic causal relationships among an object's geometry, material, vibration modes, and the sounds it produces. We introduce VibraVerse, a large-scale geometry-acoustics alignment dataset that explicitly bridges the causal chain from 3D geometry -> physical attributes -> modal parameters -> acoustic signals. Each 3D model has explicit physical properties (density, Young's modulus, Poisson's ratio) and volumetric geometry, from which modal eigenfrequencies and eigenvectors are computed for impact sound synthesis under controlled excitations. To establish this coherence, we introduce CLASP, a contrastive learning framework for cross-modal alignment that preserves the causal correspondence between an object's physical structure and its acoustic response. This framework enforces physically consistent alignment across modalities, ensuring that every sample is coherent, traceable to the governing equations, and embedded within a unified representation space spanning shape, image, and sound. Built upon VibraVerse, we define a suite of benchmark tasks for geometry-to-sound prediction, sound-guided shape reconstruction, and cross-modal representation learning. Extensive validations on these tasks demonstrate that models trained on VibraVerse exhibit superior accuracy, interpretability, and generalization across modalities. These results establish VibraVerse as a benchmark for physically consistent and causally interpretable multimodal learning, providing a foundation for sound-guided embodied perception and a deeper understanding of the physical world. The dataset will be open-sourced.

</details>


<div id='cs.GT'></div>

# cs.GT [[Back]](#toc)

### [132] [Strategy-robust Online Learning in Contextual Pricing](https://arxiv.org/abs/2511.19842)
*Joon Suk Huh,Kirthevasan Kandasamy*

Main category: cs.GT

TL;DR: 本文提出了一种在线语境定价问题，它解决了买方估价未知且可能战略性地误报的问题。为此，论文引入了多买方在线环境的策略鲁棒性遗憾，设计了SUM（稀疏更新机制）等定价算法，并将其应用于在线专家算法，以实现策略鲁棒性学习。


<details>
  <summary>Details</summary>
Motivation: 在数字市场中，当买方估价未知且需要通过互动推断时，学习有效的定价策略至关重要。本文旨在解决在线语境定价问题，并考虑买方可能策略性地误报估价的情况，这称之为战略性过拟合。

Method: 本文引入了一种针对多买方在线环境的策略鲁棒性遗憾概念。然后提出了一个学习线性定价策略的多项式时间近似方案（PTAS），该方案在对抗性、自适应环境中通过新颖的在线草图技术实现。在此基础上，本文提出了主要构造：稀疏更新机制（SUM），这是一种简单但有效的顺序机制，可确保对买方之间所有纳什均衡的鲁棒性。

Result: 本文的PTAS算法能够有效地学习线性定价策略，并且这种方法在对抗性和自适应环境中表现良好。本文的SUM机制对买方之间的所有纳什均衡都具有鲁棒性，能够有效应对策略性过拟合问题。

Conclusion: 本文成功地提出了一种策略鲁棒性遗憾的在线定价框架，并通过PTAS和SUM机制有效解决了买方估价未知和策略性误报的挑战。本文的工作为在线定价策略和策略鲁棒性学习提供了一种新的方法。

Abstract: Learning effective pricing strategies is crucial in digital marketplaces, especially when buyers' valuations are unknown and must be inferred through interaction. We study the online contextual pricing problem, where a seller observes a stream of context-valuation pairs and dynamically sets prices. Moreover, departing from traditional online learning frameworks, we consider a strategic setting in which buyers may misreport valuations to influence future prices, a challenge known as strategic overfitting (Amin et al., 2013).
  We introduce a strategy-robust notion of regret for multi-buyer online environments, capturing worst-case strategic behavior in the spirit of the Price of Anarchy. Our first contribution is a polynomial-time approximation scheme (PTAS) for learning linear pricing policies in adversarial, adaptive environments, enabled by a novel online sketching technique. Building on this result, we propose our main construction: the Sparse Update Mechanism (SUM), a simple yet effective sequential mechanism that ensures robustness to all Nash equilibria among buyers. Moreover, our construction yields a black-box reduction from online expert algorithms to strategy-robust learners.

</details>


### [133] [Designing Reputation Systems for Manufacturing Data Trading Markets: A Multi-Agent Evaluation with Q-Learning and IRL-Estimated Utilities](https://arxiv.org/abs/2511.19930)
*Kenta Yamamoto,Teruaki Hayashi*

Main category: cs.GT

TL;DR: 该研究开发了一个多智能体数据市场模拟器，以解决数据交易中的信息不对称问题，并通过评估五种声誉系统来提高数据市场的信任度和质量保证，最终提出了一个混合声誉机制。


<details>
  <summary>Details</summary>
Motivation: 随着机器学习和大数据分析的进步，对高质量跨领域数据集的需求增加，数据交易日益增长。数据作为经济资产，数据市场成为创新的关键基础设施。然而，数据市场不成熟，信息不对称严重，买方在购买前无法验证数据内容或质量，导致信任和质量保证成为核心挑战。

Method: 开发了一个多智能体数据市场模拟器，该模拟器模拟参与者行为，并评估信任形成的制度机制。它专注于制造业，并集成了强化学习（RL）用于自适应智能体行为，以及逆强化学习（IRL）用于从经验行为数据中估计效用函数。

Result: 通过模拟器，研究了五种代表性声誉系统（时间衰减、贝叶斯-Beta、PageRank、PowerTrust和PeerTrust）的市场级效应。发现PeerTrust在数据价格和质量之间实现了最强的一致性，同时防止了垄断。

Conclusion: 本研究通过将信任和声誉作为内生机制，扩展了基于模拟的数据市场分析，并为设计可靠和高效的数据生态系统提供了方法论和制度见解。开发了一种混合声誉机制，该机制整合了现有系统的优点，以实现改进的价格-质量一致性和整体市场稳定性。

Abstract: Recent advances in machine learning and big data analytics have intensified the demand for high-quality cross-domain datasets and accelerated the growth of data trading across organizations. As data become increasingly recognized as an economic asset, data marketplaces have emerged as a key infrastructure for data-driven innovation. However, unlike mature product or service markets, data-trading environments remain nascent and suffer from pronounced information asymmetry. Buyers cannot verify the content or quality before purchasing data, making trust and quality assurance central challenges. To address these issues, this study develops a multi-agent data-market simulator that models participant behavior and evaluates the institutional mechanisms for trust formation. Focusing on the manufacturing sector, where initiatives such as GAIA-X and Catena-X are advancing, the simulator integrates reinforcement learning (RL) for adaptive agent behavior and inverse reinforcement learning (IRL) to estimate utility functions from empirical behavioral data. Using the simulator, we examine the market-level effects of five representative reputation systems-Time-decay, Bayesian-beta, PageRank, PowerTrust, and PeerTrust-and found that PeerTrust achieved the strongest alignment between data price and quality, while preventing monopolistic dominance. Building on these results, we develop a hybrid reputation mechanism that integrates the strengths of existing systems to achieve improved price-quality consistency and overall market stability. This study extends simulation-based data-market analysis by incorporating trust and reputation as endogenous mechanisms and offering methodological and institutional insights into the design of reliable and efficient data ecosystems.

</details>


### [134] [One Action Too Many: Inapproximability of Budgeted Combinatorial Contracts](https://arxiv.org/abs/2511.20110)
*Michal Feldman,Yoav Gal-Tzur,Tomasz Ponitka,Maya Schlesinger*

Main category: cs.GT

TL;DR: 研究了在预算约束下，针对多智能体组合行为的合约设计问题，提出在子模奖励函数下，无法在任何有限因子内近似最优预算可行值，但总体替代奖励函数可以实现O(1)近似，并提出了针对加性奖励的FPTAS。


<details>
  <summary>Details</summary>
Motivation: 在组合动作的多智能体合约设计中，探索预算约束下的解决方案，并识别可处理的奖励函数类别。

Method: 本文首先通过一个强负面结果，证明了在子模奖励函数下，即使在部分简化的情况下也无法找到近似解。接着，通过证明总替代奖励下可实现O(1)近似，划分了有预算和无预算设置的可处理界限。最后，为加性奖励提出了一个FPTAS。

Result: 1. 建立了在预算约束和子模奖励函数下的合约设计的近似硬度，证明了在任何有限因子内都无法近似最优解。2. 对于总替代奖励函数，提出了一个确定的多项式时间O(1)-近似算法。3. 针对加性奖励函数，提出了一个FPTAS，证明了在任意预算下任意近似都是可行的。

Conclusion: 本文首次明确区分了组合合约中预算和无预算设置之间的界限，并将总替代奖励识别为预算组合合约的可处理前沿。此外，为加性奖励带来了首个FPTAS。

Abstract: We study multi-agent contract design with combinatorial actions, under budget constraints, and for a broad class of objective functions, including profit (principal's utility), reward, and welfare. Our first result is a strong impossibility: For submodular reward functions, no randomized poly-time algorithm can approximate the optimal budget-feasible value within \textit{any finite factor}, even with demand-oracle access. This result rules out extending known constant-factor guarantees from either (i) unbudgeted settings with combinatorial actions or (ii) budgeted settings with binary actions, to their combination. The hardness is tight: It holds even when all but one agent have binary actions and the remaining agent has just one additional action. On the positive side, we show that gross substitutes rewards (a well-studied strict subclass of submodular functions) admit a deterministic poly-time $O(1)$-approximation, using only value queries. Our results thus draw the first sharp separation between budgeted and unbudgeted settings in combinatorial contracts, and identifies gross substitutes as a tractable frontier for budgeted combinatorial contracts. Finally, we present an FPTAS for additive rewards, demonstrating that arbitrary approximation is tractable under any budget. This constitutes the first FPTAS for the multi-agent combinatorial-actions setting, even in the absence of budget constraints.

</details>


### [135] [Lower Bias, Higher Welfare: How Creator Competition Reshapes Bias-Variance Tradeoff in Recommendation Platforms?](https://arxiv.org/abs/2511.20289)
*Kang Wang,Renzhe Xu,Bo Li*

Main category: cs.GT

TL;DR: 这篇论文研究了在内容创作者竞争环境下，平台如何通过调整正则化强度来优化用户福利，特别是在用户表征学习的偏差-方差权衡中。


<details>
  <summary>Details</summary>
Motivation: 在静态环境中，用户表征学习中的偏差-方差权衡已被充分研究，但在内容创作者会策略性地适应平台激励的动态环境中，这一权衡变得异常复杂。因此，本文旨在分析这种竞争如何重塑权衡，以最大化用户福利。

Method: 本文引入了一个名为“内容创作者竞争与偏差-方差权衡”的博弈论模型。该模型捕捉了平台在用户特征估计中对正则化强度的决策。作者推导并比较了平台在两种关键设置下的最优策略：一个是非战略性的固定内容基线，另一个是创作者响应平台算法设计而竞争的战略环境。该研究还通过对合成和真实世界基准数据集的广泛实验来验证理论分析。

Result: 理论分析表明，与非战略环境相比，内容创作者竞争将平台的最优策略转向较弱的正则化，从而在偏差-方差权衡中倾向于降低偏差。实证结果一致支持该理论结论：在战略环境中，降低偏差会导致更高的用户福利。

Conclusion: 在存在内容创作者竞争的战略环境中，平台在用户表征学习中应采取较弱的正则化策略，以降低偏差，从而提高用户福利。这些发现为设计现实世界的推荐算法提供了实践指导。

Abstract: Understanding the bias-variance tradeoff in user representation learning is essential for improving recommendation quality in modern content platforms. While well studied in static settings, this tradeoff becomes significantly more complex when content creators strategically adapt to platform incentives. To analyze how such competition reshapes the tradeoff for maximizing user welfare, we introduce the Content Creator Competition with Bias-Variance Tradeoff framework, a tractable game-theoretic model that captures the platform's decision on regularization strength in user feature estimation. We derive and compare the platform's optimal policy under two key settings: a non-strategic baseline with fixed content and a strategic environment where creators compete in response to the platform's algorithmic design.
  Our theoretical analysis in a stylized model shows that, compared to the non-strategic environment, content creator competition shifts the platform's optimal policy toward weaker regularization, thereby favoring lower bias in the bias-variance tradeoff. To validate and assess the robustness of these insights beyond the stylized setting, we conduct extensive experiments on both synthetic and real-world benchmark datasets. The empirical results consistently support our theoretical conclusion: in strategic environments, reducing bias leads to higher user welfare. These findings offer practical implications for the design of real-world recommendation algorithms in the presence of content creator competition.

</details>


<div id='cs.MA'></div>

# cs.MA [[Back]](#toc)

### [136] [Trust-Based Social Learning for Communication (TSLEC) Protocol Evolution in Multi-Agent Reinforcement Learning](https://arxiv.org/abs/2511.19562)
*Abraham Itzhak Weinberg*

Main category: cs.MA

TL;DR: 该研究提出了TSLEC框架，通过基于信任的社会学习加速了多智能体系统中的涌现通信，提高了收敛速度并生成了鲁棒的组合协议。


<details>
  <summary>Details</summary>
Motivation: 多智能体系统中涌现通信通常通过独立学习发生，导致收敛缓慢和可能次优的协议，因此需要一种更有效的通信学习方法。

Method: 本文提出了TSLEC（Trust-Based Social Learning with Emergent Communication）框架，其中智能体明确地将成功的策略教授给同伴，并通过学习到的信任关系来调节知识转移。

Result: 通过实验，TSLEC将收敛所需的 эпизоды 减少了23.9%（p < 0.001），并产生了组合协议（C = 0.38），这些协议在动态目标下仍然保持鲁棒（Phi > 0.867 的解码准确度）。信任分数与教学质量强相关（r = 0.743, p < 0.001）。

Conclusion: 显式社会学习从根本上加速了多智能体协作中的涌现通信。

Abstract: Emergent communication in multi-agent systems typically occurs through independent learning, resulting in slow convergence and potentially suboptimal protocols. We introduce TSLEC (Trust-Based Social Learning with Emergent Communication), a framework where agents explicitly teach successful strategies to peers, with knowledge transfer modulated by learned trust relationships. Through experiments with 100 episodes across 30 random seeds, we demonstrate that trust-based social learning reduces episodes-to-convergence by 23.9% (p < 0.001, Cohen's d = 1.98) compared to independent emergence, while producing compositional protocols (C = 0.38) that remain robust under dynamic objectives (Phi > 0.867 decoding accuracy). Trust scores strongly correlate with teaching quality (r = 0.743, p < 0.001), enabling effective knowledge filtering. Our results establish that explicit social learning fundamentally accelerates emergent communication in multi-agent coordination.

</details>


### [137] [An Adaptive, Data-Integrated Agent-Based Modeling Framework for Explainable and Contestable Policy Design](https://arxiv.org/abs/2511.19726)
*Roberto Garrone*

Main category: cs.MA

TL;DR: 这篇论文介绍了一个通用的自适应多智能体学习框架，用于分析学习智能体和自适应控制如何共同塑造系统轨迹。


<details>
  <summary>Details</summary>
Motivation: 许多多智能体系统模拟研究保留了静态决策规则和固定控制参数，而多智能体系统通常在反馈、适应和非平稳性下运行。

Method: 该框架整合了四种动态机制（区分静态与自适应智能体以及固定与自适应系统参数）、信息论诊断、结构化因果模型、从聚合或样本数据生成智能体级别先验的程序以及用于识别涌现行为机制的无监督方法。

Result: 该框架提供了一个领域中立的架构，用于分析学习智能体和自适应控制如何共同塑造系统轨迹，从而能够系统地比较非平衡、振荡或漂移动力学下的稳定性、性能和可解释性。

Conclusion: 该框架为开发可解释和可竞争的多智能体决策过程提供了一种结构化方法。

Abstract: Multi-agent systems often operate under feedback, adaptation, and non-stationarity, yet many simulation studies retain static decision rules and fixed control parameters. This paper introduces a general adaptive multi-agent learning framework that integrates: (i) four dynamic regimes distinguishing static versus adaptive agents and fixed versus adaptive system parameters; (ii) information-theoretic diagnostics (entropy rate, statistical complexity, and predictive information) to assess predictability and structure; (iii) structural causal models for explicit intervention semantics; (iv) procedures for generating agent-level priors from aggregate or sample data; and (v) unsupervised methods for identifying emergent behavioral regimes. The framework offers a domain-neutral architecture for analyzing how learning agents and adaptive controls jointly shape system trajectories, enabling systematic comparison of stability, performance, and interpretability across non-equilibrium, oscillatory, or drifting dynamics. Mathematical definitions, computational operators, and an experimental design template are provided, yielding a structured methodology for developing explainable and contestable multi-agent decision processes.

</details>


### [138] [Complex Instruction Following with Diverse Style Policies in Football Games](https://arxiv.org/abs/2511.19885)
*Chenglu Sun,Shuo Shen,Haonan Hu,Wei Zhou,Chen Chen*

Main category: cs.MA

TL;DR: 本文介绍了LCDSP，一种用于复杂多智能体环境（如足球比赛）的语言控制强化学习（LC-RL）新范式，它通过多样化风格训练（DST）和风格解释器（SI）使策略能够理解并执行高级语言指令。


<details>
  <summary>Details</summary>
Motivation: 传统的LC-RL在复杂的多智能体环境中，难以理解和执行高级或抽象指令。

Method: LCDSP包含两个核心组件：多样化风格训练（DST）方法和风格解释器（SI）。DST训练一个能够通过风格参数（SP）调节智能体动作的多样化行为策略。SI则将高级语言指令快速准确地转化为相应的SP。

Result: 在复杂的5v5足球环境中进行的广泛实验表明，LCDSP能够有效理解抽象的战术指令，并准确执行所需的多样化行为风格。

Conclusion: LCDSP在复杂、真实世界的应用中具有巨大潜力，能够有效处理高级语言指令和多样的行为风格。

Abstract: Despite advancements in language-controlled reinforcement learning (LC-RL) for basic domains and straightforward commands (e.g., object manipulation and navigation), effectively extending LC-RL to comprehend and execute high-level or abstract instructions in complex, multi-agent environments, such as football games, remains a significant challenge. To address this gap, we introduce Language-Controlled Diverse Style Policies (LCDSP), a novel LC-RL paradigm specifically designed for complex scenarios. LCDSP comprises two key components: a Diverse Style Training (DST) method and a Style Interpreter (SI). The DST method efficiently trains a single policy capable of exhibiting a wide range of diverse behaviors by modulating agent actions through style parameters (SP). The SI is designed to accurately and rapidly translate high-level language instructions into these corresponding SP. Through extensive experiments in a complex 5v5 football environment, we demonstrate that LCDSP effectively comprehends abstract tactical instructions and accurately executes the desired diverse behavioral styles, showcasing its potential for complex, real-world applications.

</details>
