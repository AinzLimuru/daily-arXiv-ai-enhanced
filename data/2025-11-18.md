<div id=toc></div>

# Table of Contents

- [cs.AI](#cs.AI) [Total: 1]


<div id='cs.AI'></div>

# cs.AI [[Back]](#toc)

### [1] [The Second Law of Intelligence: Controlling Ethical Entropy in Autonomous Systems](https://arxiv.org/abs/2511.10704)
*Samih Fadli*

Main category: cs.AI

TL;DR: 该研究提出了AI的“第二定律”，将伦理熵定义为偏离预定目标的程度。伦理熵会自发增加，除非进行持续的对齐工作。


<details>
  <summary>Details</summary>
Motivation: 在大型语言模型 (LLM) 和其他复杂 AI 系统迅速发展的背景下，确保这些系统与人类价值观和目标对齐变得至关重要，特别是考虑到 AI 系统日益增长的自主性。

Method: 通过将伦理熵定义为 S = -Σ p(g_i; theta) ln p(g_i; theta)，研究者证明了其时间导数 dS/dt >= 0，并将其归因于探索噪声和规范博弈。此外，他们推导了对齐工作的临界稳定性边界。

Result: 一个70亿参数的模型，在没有对齐工作的情况下，伦理熵从0.32发散到1.69 +/- 1.08 nats。而当系统通过对齐工作进行正则化时，伦理熵可以保持在0.00 +/- 0.00 nats。

Conclusion: 该研究将AI对齐问题重新定义为持续的热力学控制问题，为维护先进自主系统的稳定性和安全性提供了量化基础。

Abstract: We propose that unconstrained artificial intelligence obeys a Second Law analogous to thermodynamics, where ethical entropy, defined as a measure of divergence from intended goals, increases spontaneously without continuous alignment work. For gradient-based optimizers, we define this entropy over a finite set of goals {g_i} as S = -Σ p(g_i; theta) ln p(g_i; theta), and we prove that its time derivative dS/dt >= 0, driven by exploration noise and specification gaming. We derive the critical stability boundary for alignment work as gamma_crit = (lambda_max / 2) ln N, where lambda_max is the dominant eigenvalue of the Fisher Information Matrix and N is the number of model parameters. Simulations validate this theory. A 7-billion-parameter model (N = 7 x 10^9) with lambda_max = 1.2 drifts from an initial entropy of 0.32 to 1.69 +/- 1.08 nats, while a system regularized with alignment work gamma = 20.4 (1.5 gamma_crit) maintains stability at 0.00 +/- 0.00 nats (p = 4.19 x 10^-17, n = 20 trials). This framework recasts AI alignment as a problem of continuous thermodynamic control, providing a quantitative foundation for maintaining the stability and safety of advanced autonomous systems.

</details>
