<div id=toc></div>

# Table of Contents

- [cs.CV](#cs.CV) [Total: 51]
- [cs.AI](#cs.AI) [Total: 9]
- [cs.MA](#cs.MA) [Total: 3]
- [cs.IT](#cs.IT) [Total: 3]
- [cs.GT](#cs.GT) [Total: 4]


<div id='cs.CV'></div>

# cs.CV [[Back]](#toc)

### [1] [SimULi: Real-Time LiDAR and Camera Simulation with Unscented Transforms](https://arxiv.org/abs/2510.12901)
*Haithem Turki,Qi Wu,Xin Kang,Janick Martinez Esturo,Shengyu Huang,Ruilong Li,Zan Gojcic,Riccardo de Lutio*

Main category: cs.CV

TL;DR: SimULi 是一种新的多传感器模拟方法，能够实时渲染任意相机模型和 LiDAR 数据，解决了现有方法渲染速度慢、无法处理高畸变镜头和跨传感器不一致的问题，并显著提高了渲染速度和精度。


<details>
  <summary>Details</summary>
Motivation: 现有神经渲染方法（如 NeRF 和 3DGS）在自动驾驶汽车等自主机器人的严格测试中存在局限性，例如渲染速度慢、仅支持针孔相机模型，以及在处理多传感器数据时存在跨传感器不一致。

Method: SimULi 通过扩展 3DGUT 以支持 LiDAR 数据，并采用自动切片策略和基于射线的剔除技术，实现了任意相机模型和 LiDAR 数据的实时渲染。为了解决跨传感器不一致问题，SimULi 设计了一种分解的 3D 高斯表示和锚定策略。

Result: SimULi 的渲染速度比光线追踪方法快 10-20 倍，比之前的基于光栅化的方法快 1.5-10 倍。它将平均相机和深度误差降低了高达 40%。在两个自动驾驶数据集上，SimULi 在多项相机和 LiDAR 指标上达到或超过了现有最先进方法的精度。

Conclusion: SimULi 首次实现了对任意相机模型和 LiDAR 数据的实时渲染，解决了现有方法的关键限制，为自动驾驶等领域的自主机器人测试提供了高效、高保真的模拟解决方案。

Abstract: Rigorous testing of autonomous robots, such as self-driving vehicles, is
essential to ensure their safety in real-world deployments. This requires
building high-fidelity simulators to test scenarios beyond those that can be
safely or exhaustively collected in the real-world. Existing neural rendering
methods based on NeRF and 3DGS hold promise but suffer from low rendering
speeds or can only render pinhole camera models, hindering their suitability to
applications that commonly require high-distortion lenses and LiDAR data.
Multi-sensor simulation poses additional challenges as existing methods handle
cross-sensor inconsistencies by favoring the quality of one modality at the
expense of others. To overcome these limitations, we propose SimULi, the first
method capable of rendering arbitrary camera models and LiDAR data in
real-time. Our method extends 3DGUT, which natively supports complex camera
models, with LiDAR support, via an automated tiling strategy for arbitrary
spinning LiDAR models and ray-based culling. To address cross-sensor
inconsistencies, we design a factorized 3D Gaussian representation and
anchoring strategy that reduces mean camera and depth error by up to 40%
compared to existing methods. SimULi renders 10-20x faster than ray tracing
approaches and 1.5-10x faster than prior rasterization-based work (and handles
a wider range of camera models). When evaluated on two widely benchmarked
autonomous driving datasets, SimULi matches or exceeds the fidelity of existing
state-of-the-art methods across numerous camera and LiDAR metrics.

</details>


### [2] [Robust Plant Disease Diagnosis with Few Target-Domain Samples](https://arxiv.org/abs/2510.12909)
*Takafumi Nogami,Satoshi Kagiwada,Hitoshi Iyatomi*

Main category: cs.CV

TL;DR: 植物病害诊断中的深度学习模型在训练环境发生变化时其准确性会大幅下降。本文提出了一种名为TMPS的度量学习框架，通过少量目标域样本显著提高了诊断的鲁棒性，并在实际数据上取得了显著的性能提升。


<details>
  <summary>Details</summary>
Motivation: 目前基于深度学习的植物病害诊断系统在训练环境发生变化时诊断准确性不佳，这主要是由于病害症状的细微变化以及不同图像上下文和环境造成的域间隙。根本原因是训练数据的多样性相对于任务复杂性有限。

Method: 本文提出了一种简单但高度适应性的学习框架，名为“目标感知度量学习与优先级采样”（TMPS），该方法以度量学习为基础。TMPS假设可以访问目标（部署）域中数量有限的带标签样本，并有效利用这些样本来提高诊断的鲁棒性。

Result: TMPS在包含223,073张叶片图像的大规模自动化植物病害诊断任务上进行了评估。通过在训练中加入每个病害仅10个目标域样本，TMPS超越了使用相同组合的源样本和目标样本训练的模型，以及在源数据上预训练后用这些目标样本进行微调的模型。其宏观F1分数平均提高了7.3和3.6个百分点，并且比基线和传统度量学习分别提高了18.7和17.1个百分点。

Conclusion: TMPS框架通过有效利用少量目标域的带标签样本，显著提高了深度学习模型在植物病害诊断任务上的鲁棒性和泛化能力，解决了模型在不同部署环境下性能下降的挑战。

Abstract: Various deep learning-based systems have been proposed for accurate and
convenient plant disease diagnosis, achieving impressive performance. However,
recent studies show that these systems often fail to maintain diagnostic
accuracy on images captured under different conditions from the training
environment -- an essential criterion for model robustness. Many deep learning
methods have shown high accuracy in plant disease diagnosis. However, they
often struggle to generalize to images taken in conditions that differ from the
training setting. This drop in performance stems from the subtle variability of
disease symptoms and domain gaps -- differences in image context and
environment. The root cause is the limited diversity of training data relative
to task complexity, making even advanced models vulnerable in unseen domains.
To tackle this challenge, we propose a simple yet highly adaptable learning
framework called Target-Aware Metric Learning with Prioritized Sampling (TMPS),
grounded in metric learning. TMPS operates under the assumption of access to a
limited number of labeled samples from the target (deployment) domain and
leverages these samples effectively to improve diagnostic robustness. We assess
TMPS on a large-scale automated plant disease diagnostic task using a dataset
comprising 223,073 leaf images sourced from 23 agricultural fields, spanning 21
diseases and healthy instances across three crop species. By incorporating just
10 target domain samples per disease into training, TMPS surpasses models
trained using the same combined source and target samples, and those fine-tuned
with these target samples after pre-training on source data. It achieves
average macro F1 score improvements of 7.3 and 3.6 points, respectively, and a
remarkable 18.7 and 17.1 point improvement over the baseline and conventional
metric learning.

</details>


### [3] [CADE 2.5 - ZeResFDG: Frequency-Decoupled, Rescaled and Zero-Projected Guidance for SD/SDXL Latent Diffusion Models](https://arxiv.org/abs/2510.12954)
*Denis Rychkovskiy,GPT-5*

Main category: cs.CV

TL;DR: CADE 2.5 是一种用于 SD/SDXL 潜在扩散模型的采样器级别引导堆栈，它通过整合频率解耦引导、能量重缩放和零投影来统一 ZeResFDG，并通过轻量级光谱 EMA 来切换模式。同时，它还包含一个免训练的推理时间稳定器 QSilk Micrograin Stabilizer，以提高鲁棒性并生成自然的高频微纹理。


<details>
  <summary>Details</summary>
Motivation: 在不进行再训练的情况下，提高 SD/SDXL 采样器的图像清晰度、提示遵循度以及对伪影的控制，并在高分辨率下生成自然的高频微纹理。

Method: CADE 2.5 包含两个核心模块：1. ZeResFDG：统一了频率解耦引导（重新加权引导信号的低频和高频分量），能量重缩放（将引导预测的每样本幅度与正分支匹配），以及零投影（移除与无条件方向平行的分量）。它还包含一个带有滞后性的轻量级光谱 EMA，可以在采样过程中结构逐渐清晰时在保守模式和细节寻找模式之间切换。2. QSilk Micrograin Stabilizer：一个免训练的推理时间稳定器，它结合了分位数钳位和深度/边缘门控的微细节注入，以提高鲁棒性并生成自然的高频微纹理。

Result: ZeResFDG 在中等引导尺度下，提高了 SD/SDXL 采样器的清晰度、提示遵循度以及对伪影的控制，无需任何再训练。QSilk Micrograin Stabilizer 提高了鲁棒性，并在高分辨率下以可忽略的开销产生了自然的高频微纹理。

Conclusion: CADE 2.5 通过其创新的 ZeResFDG 和 QSilk Micrograin Stabilizer 模块，显著提升了 SD/SDXL 潜在扩散模型的性能，在不进行再训练的情况下改善了图像质量和细节，并增强了生成高分辨率自然微纹理的能力。

Abstract: We introduce CADE 2.5 (Comfy Adaptive Detail Enhancer), a sampler-level
guidance stack for SD/SDXL latent diffusion models. The central module,
ZeResFDG, unifies (i) frequency-decoupled guidance that reweights low- and
high-frequency components of the guidance signal, (ii) energy rescaling that
matches the per-sample magnitude of the guided prediction to the positive
branch, and (iii) zero-projection that removes the component parallel to the
unconditional direction. A lightweight spectral EMA with hysteresis switches
between a conservative and a detail-seeking mode as structure crystallizes
during sampling. Across SD/SDXL samplers, ZeResFDG improves sharpness, prompt
adherence, and artifact control at moderate guidance scales without any
retraining. In addition, we employ a training-free inference-time stabilizer,
QSilk Micrograin Stabilizer (quantile clamp + depth/edge-gated micro-detail
injection), which improves robustness and yields natural high-frequency
micro-texture at high resolutions with negligible overhead. For completeness we
note that the same rule is compatible with alternative parameterizations (e.g.,
velocity), which we briefly discuss in the Appendix; however, this paper
focuses on SD/SDXL latent diffusion models.

</details>


### [4] [Scope: Selective Cross-modal Orchestration of Visual Perception Experts](https://arxiv.org/abs/2510.12974)
*Tianyu Zhang,Suyuchen Wang,Chao Wang,Juan Rodriguez,Ahmed Masry,Xiangru Jian,Yoshua Bengio,Perouz Taslakian*

Main category: cs.CV

TL;DR: 本文提出SCOPE，一个混合编码器（MoEnc）框架，它通过实例级路由动态选择每个图像-文本对的专业编码器，解决了现有视觉-语言模型中多个视觉编码器堆叠效率低下的问题。


<details>
  <summary>Details</summary>
Motivation: 现有的视觉-语言模型（VLMs）通过堆叠多个视觉编码器来提升性能，但这会导致收益递减并大幅增加推理成本。

Method: SCOPE框架包含一个共享编码器和一组路由编码器。一个轻量级路由器利用文本提示和共享视觉特征之间的交叉注意力，为每个图像-文本对动态选择最佳的路由编码器。为了训练路由器，引入了双重熵正则化和辅助损失，以平衡数据集级别的负载分配和实例级别的路由置信度。

Result: SCOPE在只用一个共享编码器和一个路由编码器的情况下，性能超越了同时使用所有四个额外编码器的模型，同时将计算量减少了24-49%。

Conclusion: SCOPE通过智能的编码器选择而非暴力聚合，颠覆了多编码器视觉-语言模型的现有范式，证明了在保持甚至超越性能的同时，可以显著降低计算成本。

Abstract: Vision-language models (VLMs) benefit from multiple vision encoders, but
naively stacking them yields diminishing returns while multiplying inference
costs. We propose SCOPE, a Mixture-of-Encoders (MoEnc) framework that
dynamically selects one specialized encoder per image-text pair via
instance-level routing, unlike token-level routing in traditional MoE. SCOPE
maintains a shared encoder and a pool of routed encoders. A lightweight router
uses cross-attention between text prompts and shared visual features to select
the optimal encoder from the routed encoders. To train this router, we
introduce dual entropy regularization with auxiliary losses to balance
dataset-level load distribution with instance-level routing confidence.
Remarkably, SCOPE with one shared plus one routed encoder outperforms models
using all four extra encoders simultaneously, while reducing compute by
24-49\%. This demonstrates that intelligent encoder selection beats brute-force
aggregation, challenging the prevailing paradigm in multi-encoder VLMs.

</details>


### [5] [SeqBench: Benchmarking Sequential Narrative Generation in Text-to-Video Models](https://arxiv.org/abs/2510.13042)
*Zhengxu Tang,Zizheng Wang,Luning Wang,Zitao Shuai,Chenhao Zhang,Siyu Qian,Yirui Wu,Bohao Wang,Haosong Rao,Zhenyu Yang,Chenwei Wu*

Main category: cs.CV

TL;DR: SeqBench是一个用于评估T2V模型在生成连贯的顺序叙事方面的基准，它揭示了当前T2V模型在处理多事件序列时的局限性。


<details>
  <summary>Details</summary>
Motivation: 现有的T2V模型在生成视觉上吸引人的视频方面取得了显著进展，但难以生成需要通过多个事件进行逻辑推进的连贯顺序叙事。现有T2V基准主要关注视觉质量指标，但未能评估扩展序列上的叙事连贯性。

Method: 我们提出了SeqBench，这是一个用于评估T2V生成中顺序叙事连贯性的综合基准。SeqBench包含一个精心设计的320个提示数据集，涵盖各种叙事复杂性，以及从8个最先进的T2V模型生成的2,560个经过人工标注的视频。此外，我们设计了一个基于动态时间图（DTG）的自动评估指标，它可以有效地捕获长程依赖和时间顺序，同时保持计算效率。

Result: 我们的DTG评估指标与人工标注显示出很强的相关性。通过使用SeqBench进行的系统评估，我们揭示了当前T2V模型的关键局限性：在多动作序列中未能保持一致的对象状态，在多对象场景中产生物理上不合理的结果，以及难以保持顺序动作之间的现实时间安排和排序关系。

Conclusion: SeqBench为评估T2V生成中的叙事连贯性提供了第一个系统框架，并为改进未来模型中的顺序推理能力提供了具体的见解。

Abstract: Text-to-video (T2V) generation models have made significant progress in
creating visually appealing videos. However, they struggle with generating
coherent sequential narratives that require logical progression through
multiple events. Existing T2V benchmarks primarily focus on visual quality
metrics but fail to evaluate narrative coherence over extended sequences. To
bridge this gap, we present SeqBench, a comprehensive benchmark for evaluating
sequential narrative coherence in T2V generation. SeqBench includes a carefully
designed dataset of 320 prompts spanning various narrative complexities, with
2,560 human-annotated videos generated from 8 state-of-the-art T2V models.
Additionally, we design a Dynamic Temporal Graphs (DTG)-based automatic
evaluation metric, which can efficiently capture long-range dependencies and
temporal ordering while maintaining computational efficiency. Our DTG-based
metric demonstrates a strong correlation with human annotations. Through
systematic evaluation using SeqBench, we reveal critical limitations in current
T2V models: failure to maintain consistent object states across multi-action
sequences, physically implausible results in multi-object scenarios, and
difficulties in preserving realistic timing and ordering relationships between
sequential actions. SeqBench provides the first systematic framework for
evaluating narrative coherence in T2V generation and offers concrete insights
for improving sequential reasoning capabilities in future models. Please refer
to https://videobench.github.io/SeqBench.github.io/ for more details.

</details>


### [6] [One Dimensional CNN ECG Mamba for Multilabel Abnormality Classification in 12 Lead ECG](https://arxiv.org/abs/2510.13046)
*Huawei Jiang,Husna Mutahira,Gan Huang,Mannan Saeed Muhammad*

Main category: cs.CV

TL;DR: 该研究介绍了一种名为One Dimensional Convolutional Neural Network Electrocardiogram Mamba的混合框架，它结合了卷积特征提取和Mamba（一种选择性状态空间模型），用于心电图异常检测。该模型在PhysioNet挑战赛数据集上取得了优于现有方法的性能，证明了Mamba基架构在心电图分类中的潜力，并支持早期诊断和个性化治疗。


<details>
  <summary>Details</summary>
Motivation: 传统的深度学习模型在处理长序列心电图信号时性能有限，而心脏异常的准确检测对临床诊断和决策支持至关重要。

Method: 本研究引入了一种名为One Dimensional Convolutional Neural Network Electrocardiogram Mamba（1D CNN-ECG Mamba）的混合框架。该框架将卷积特征提取与Mamba（一种专为有效序列建模而设计的选择性状态空间模型）相结合。模型构建于Vision Mamba的基础之上，Vision Mamba是一种双向变体，它增强了心电图数据中时间依赖性的表示。

Result: 在PhysioNet Computing in Cardiology Challenges 2020和2021的综合实验中，所提出的模型取得了优于现有方法的性能。具体而言，该模型在十二导联心电图上的AUPRC和AUROC分数均显著高于此前表现最佳的算法。

Conclusion: 这些结果证明了基于Mamba的架构在推进可靠心电图分类方面的潜力。这种能力支持早期诊断和个性化治疗，同时增强了远程医疗和资源受限医疗系统的可及性。

Abstract: Accurate detection of cardiac abnormalities from electrocardiogram recordings
is regarded as essential for clinical diagnostics and decision support.
Traditional deep learning models such as residual networks and transformer
architectures have been applied successfully to this task, but their
performance has been limited when long sequential signals are processed.
Recently, state space models have been introduced as an efficient alternative.
In this study, a hybrid framework named One Dimensional Convolutional Neural
Network Electrocardiogram Mamba is introduced, in which convolutional feature
extraction is combined with Mamba, a selective state space model designed for
effective sequence modeling. The model is built upon Vision Mamba, a
bidirectional variant through which the representation of temporal dependencies
in electrocardiogram data is enhanced. Comprehensive experiments on the
PhysioNet Computing in Cardiology Challenges of 2020 and 2021 were conducted,
and superior performance compared with existing methods was achieved.
Specifically, the proposed model achieved substantially higher AUPRC and AUROC
scores than those reported by the best previously published algorithms on
twelve lead electrocardiograms. These results demonstrate the potential of
Mamba-based architectures to advance reliable ECG classification. This
capability supports early diagnosis and personalized treatment, while enhancing
accessibility in telemedicine and resource-constrained healthcare systems.

</details>


### [7] [True Self-Supervised Novel View Synthesis is Transferable](https://arxiv.org/abs/2510.13063)
*Thomas W. Mitchel,Hyunwoo Ryu,Vincent Sitzmann*

Main category: cs.CV

TL;DR: 这篇论文介绍了一种名为XFactor的新型无几何自监督模型，用于实现真正的全新视角合成。


<details>
  <summary>Details</summary>
Motivation: 以往的自监督全新视角合成模型都无法将姿态从一个视频序列转移到另一个视频序列，本文旨在解决这一问题。

Method: XFactor结合了成对姿态估计和简单的输入输出增强方案，从而解耦相机姿态和场景内容，并促进几何推理。

Result: XFactor实现了具有无约束潜在姿态变量的可转移性，并且无需任何3D归纳偏差或多视角几何概念。它显著优于先前的无姿态NVS转换器。

Conclusion: XFactor是第一个能够实现真正NVS的无几何自监督模型，并通过实验证明了其姿态的可转移性和与真实世界姿态的高度相关性。

Abstract: In this paper, we identify that the key criterion for determining whether a
model is truly capable of novel view synthesis (NVS) is transferability:
Whether any pose representation extracted from one video sequence can be used
to re-render the same camera trajectory in another. We analyze prior work on
self-supervised NVS and find that their predicted poses do not transfer: The
same set of poses lead to different camera trajectories in different 3D scenes.
Here, we present XFactor, the first geometry-free self-supervised model capable
of true NVS. XFactor combines pair-wise pose estimation with a simple
augmentation scheme of the inputs and outputs that jointly enables
disentangling camera pose from scene content and facilitates geometric
reasoning. Remarkably, we show that XFactor achieves transferability with
unconstrained latent pose variables, without any 3D inductive biases or
concepts from multi-view geometry -- such as an explicit parameterization of
poses as elements of SE(3). We introduce a new metric to quantify
transferability, and through large-scale experiments, we demonstrate that
XFactor significantly outperforms prior pose-free NVS transformers, and show
that latent poses are highly correlated with real-world poses through probing
experiments.

</details>


### [8] [Counting Hallucinations in Diffusion Models](https://arxiv.org/abs/2510.13080)
*Shuai Fu,Jian Zhou,Qi Chen,Huang Jing,Huy Anh Nguyen,Xiaohan Liu,Zhixiong Zeng,Lin Ma,Quanshi Zhang,Qi Wu*

Main category: cs.CV

TL;DR: 这篇论文介绍了一种量化扩散模型中“计数幻觉”的方法，并创建了一个名为CountHalluSet的数据集来系统评估不同采样条件下模型的幻觉水平，发现常用图像质量评估指标FID未能有效捕捉计数幻觉。


<details>
  <summary>Details</summary>
Motivation: 尽管扩散概率模型在生成任务中表现出色，但仍会产生与现实世界知识相悖的“幻觉”样本，例如生成多余或不合理的物体。目前缺乏系统量化这些幻觉的可行方法，阻碍了解决此问题的进展并掩盖了设计受事实约束的下一代生成模型的潜力。

Method: 作者专注于一种特定形式的幻觉，称之为“计数幻觉”，即生成不正确数量的实例或结构化对象（例如六指手）。为此，他们构建了一个名为CountHalluSet的数据集套件，包含ToyShape、SimObject和RealHand，并定义了明确的计数标准。利用这些数据集，他们开发了一个标准化的评估协议来量化计数幻觉，并系统地研究了DPM中不同的采样条件（包括求解器类型、ODE求解器阶数、采样步长和初始噪声）如何影响计数幻觉水平。

Result: 研究结果表明，不同的采样条件会影响计数幻觉的水平。此外，他们发现广泛使用的图像质量评估指标FID未能始终如一地捕捉到计数幻觉。

Conclusion: 这项工作旨在迈出系统量化扩散模型中幻觉的第一步，并为研究图像生成中的幻觉现象提供了新的见解。

Abstract: Diffusion probabilistic models (DPMs) have demonstrated remarkable progress
in generative tasks, such as image and video synthesis. However, they still
often produce hallucinated samples (hallucinations) that conflict with
real-world knowledge, such as generating an implausible duplicate cup floating
beside another cup. Despite their prevalence, the lack of feasible
methodologies for systematically quantifying such hallucinations hinders
progress in addressing this challenge and obscures potential pathways for
designing next-generation generative models under factual constraints. In this
work, we bridge this gap by focusing on a specific form of hallucination, which
we term counting hallucination, referring to the generation of an incorrect
number of instances or structured objects, such as a hand image with six
fingers, despite such patterns being absent from the training data. To this
end, we construct a dataset suite CountHalluSet, with well-defined counting
criteria, comprising ToyShape, SimObject, and RealHand. Using these datasets,
we develop a standardized evaluation protocol for quantifying counting
hallucinations, and systematically examine how different sampling conditions in
DPMs, including solver type, ODE solver order, sampling steps, and initial
noise, affect counting hallucination levels. Furthermore, we analyze their
correlation with common evaluation metrics such as FID, revealing that this
widely used image quality metric fails to capture counting hallucinations
consistently. This work aims to take the first step toward systematically
quantifying hallucinations in diffusion models and offer new insights into the
investigation of hallucination phenomena in image generation.

</details>


### [9] [DriveCritic: Towards Context-Aware, Human-Aligned Evaluation for Autonomous Driving with Vision-Language Models](https://arxiv.org/abs/2510.13108)
*Jingyu Song,Zhenxin Li,Shiyi Lan,Xinglong Sun,Nadine Chang,Maying Shen,Joshua Chen,Katherine A. Skinner,Jose M. Alvarez*

Main category: cs.CV

TL;DR: 为了解决自动驾驶规划器在复杂场景中与人类判断对齐的挑战，我们提出了DriveCritic框架，它包含一个标注了人类偏好的数据集和一个基于视觉语言模型的评估器，该评估器通过整合视觉和符号上下文来判断轨迹对。实验证明DriveCritic在匹配人类偏好和上下文感知方面优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 现有的自动驾驶规划器评估指标，如EPDMS，在处理复杂情境时缺乏上下文感知能力，导致评估结果与人类判断不符。

Method: 我们提出了DriveCritic框架，包含两个主要贡献：
1. DriveCritic数据集：一个精心策划的挑战性场景集合，这些场景的正确判断需要关键的上下文信息，并标注了成对的人类偏好。
2. DriveCritic模型：一个基于视觉语言模型（VLM）的评估器。该模型通过两阶段的监督学习和强化学习流程进行微调，学习通过整合视觉和符号上下文来裁决轨迹对。

Result: DriveCritic在匹配人类偏好和展示强大的上下文感知能力方面显著优于现有指标和基线方法。

Conclusion: DriveCritic框架为评估自动驾驶系统提供了一个更可靠、与人类判断更一致的基础，特别是在需要上下文感知的复杂场景中。

Abstract: Benchmarking autonomous driving planners to align with human judgment remains
a critical challenge, as state-of-the-art metrics like the Extended Predictive
Driver Model Score (EPDMS) lack context awareness in nuanced scenarios. To
address this, we introduce DriveCritic, a novel framework featuring two key
contributions: the DriveCritic dataset, a curated collection of challenging
scenarios where context is critical for correct judgment and annotated with
pairwise human preferences, and the DriveCritic model, a Vision-Language Model
(VLM) based evaluator. Fine-tuned using a two-stage supervised and
reinforcement learning pipeline, the DriveCritic model learns to adjudicate
between trajectory pairs by integrating visual and symbolic context.
Experiments show DriveCritic significantly outperforms existing metrics and
baselines in matching human preferences and demonstrates strong context
awareness. Overall, our work provides a more reliable, human-aligned foundation
to evaluating autonomous driving systems.

</details>


### [10] [OS-HGAdapter: Open Semantic Hypergraph Adapter for Large Language Models Assisted Entropy-Enhanced Image-Text Alignment](https://arxiv.org/abs/2510.13131)
*Rongjun Chen,Chengsi Yao,Jinchang Ren,Xianxian Zeng,Peixian Wang,Jun Yuan,Jiawen Li,Huimin Zhao,Xu Lu*

Main category: cs.CV

TL;DR: 该文章提出了一种名为OS-HGAdapter的新方法，通过利用大型语言模型的开放语义知识和超图适配器来解决文本-图像跨模态检索中信息熵不平衡的问题，并在Flickr30K和MS-COCO数据集上取得了最先进的性能。


<details>
  <summary>Details</summary>
Motivation: 文本和图像之间的信息熵差异导致了多模态检索中常见的相互检索不平衡问题。

Method: 1. 设计了一种新的提示模板，利用大型语言模型（LLM）来增强文本模态的多义性描述，增加文本模态相对于视觉模态的信息熵。2. 使用超图适配器构建文本和图像模态之间的多边连接，以纠正同义语义的正负匹配错误，并通过将降维映射回原始维度来减少开放语义熵引起的噪声。

Result: 在Flickr30K和MS-COCO基准测试中，OS-HGAdapter在文本到图像检索方面取得了16.8%的提升，在图像到文本检索方面取得了40.1%的提升，超越了现有方法。

Conclusion: OS-HGAdapter通过弥补文本和图像之间的信息熵差距，显著提高了跨模态检索的性能，并在语义对齐任务中达到了最先进的水平。

Abstract: Text-image alignment constitutes a foundational challenge in multimedia
content understanding, where effective modeling of cross-modal semantic
correspondences critically enhances retrieval system performance through joint
embedding space optimization. Given the inherent difference in information
entropy between texts and images, conventional approaches often show an
imbalance in the mutual retrieval of these two modalities. To address this
particular challenge, we propose to use the open semantic knowledge of Large
Language Model (LLM) to fill for the entropy gap and reproduce the alignment
ability of humans in these tasks. Our entropy-enhancing alignment is achieved
through a two-step process: 1) a new prompt template that does not rely on
explicit knowledge in the task domain is designed to use LLM to enhance the
polysemy description of the text modality. By analogy, the information entropy
of the text modality relative to the visual modality is increased; 2) A
hypergraph adapter is used to construct multilateral connections between the
text and image modalities, which can correct the positive and negative matching
errors for synonymous semantics in the same fixed embedding space, whilst
reducing the noise caused by open semantic entropy by mapping the reduced
dimensions back to the original dimensions. Comprehensive evaluations on the
Flickr30K and MS-COCO benchmarks validate the superiority of our Open Semantic
Hypergraph Adapter (OS-HGAdapter), showcasing 16.8\% (text-to-image) and 40.1\%
(image-to-text) cross-modal retrieval gains over existing methods while
establishing new state-of-the-art performance in semantic alignment tasks.

</details>


### [11] [Foveation Improves Payload Capacity in Steganography](https://arxiv.org/abs/2510.13151)
*Lifeng Qiu Lin,Henry Kam,Qi Sun,Kaan Akşit*

Main category: cs.CV

TL;DR: 这篇论文介绍了一种新的隐写技术，该技术通过使用高效的潜在表示和凹点渲染，将现有容量限制从100比特提高到500比特。


<details>
  <summary>Details</summary>
Motivation: 在隐写术中，将元数据和水印嵌入到视觉媒体中是一个重要的应用。然而，现有的方法在容量和准确性方面存在局限性。

Method: 本文提出了一种新的模型，该模型利用高效的潜在表示和凹点渲染来提高隐写容量。

Result: 该模型将现有容量限制从100比特提高到500比特，并且在20万测试比特中，每2000比特只出现1个故障比特，准确性更高。此外，该模型在视觉质量方面也达到了31.47 dB PSNR和0.13 LPIPS，具有可比性。

Conclusion: 所提出的方法通过利用新颖的感知设计来创建多模态潜在表示，有效地提高了隐写容量、准确性和视觉质量。

Abstract: Steganography finds its use in visual medium such as providing metadata and
watermarking. With support of efficient latent representations and foveated
rendering, we trained models that improve existing capacity limits from 100 to
500 bits, while achieving better accuracy of up to 1 failure bit out of 2000,
at 200K test bits. Finally, we achieve a comparable visual quality of 31.47 dB
PSNR and 0.13 LPIPS, showing the effectiveness of novel perceptual design in
creating multi-modal latent representations in steganography.

</details>


### [12] [STT-GS: Sample-Then-Transmit Edge Gaussian Splatting with Joint Client Selection and Power Control](https://arxiv.org/abs/2510.13186)
*Zhen Li,Xibin Jin,Guoliang Li,Shuai Wang,Miaowen Wen,Huseyin Arslan,Derrick Wing Kwan Ng,Chengzhong Xu*

Main category: cs.CV

TL;DR: 该文章提出了一种名为 STT-GS 的新型边缘高斯泼溅 (EGS) 策略，用于在边缘服务器上重建场景，旨在最大限度地提高高斯泼溅 (GS) 质量。


<details>
  <summary>Details</summary>
Motivation: 传统的边缘资源管理方法不适用于 EGS，因为它们侧重于通信吞吐量或通用学习性能，而 EGS 明确旨在最大化 GS 质量。因此，需要一种新的 GS 导向的目标函数，但评估此函数又需要客户端的图像，导致因果关系困境。

Method: 本文提出了一种“先采样后传输”的 EGS (STT-GS) 策略，该策略首先从每个客户端采样一个图像子集作为试点数据进行损失预测。基于第一阶段评估，通信资源优先分配给更有价值的客户端。为了实现高效采样，本文提出了特征域聚类 (FDC) 方案来选择最具代表性的数据，并采用试点传输时间最小化 (PTTM) 来减少试点开销。随后，开发了一个联合客户端选择和功率控制 (JCSPC) 框架，以在通信资源限制下最大化 GS 导向函数。针对该非凸问题，提出了一种基于惩罚交替主化最小化 (PAMM) 算法的低复杂度高效解决方案。

Result: 实验表明，所提出的方案在真实世界数据集上显著优于现有基准。研究发现，GS 导向目标可以通过低采样率（例如 10%）准确预测，并且该方法在视图贡献和通信成本之间取得了出色的权衡。

Conclusion: 所提出的 STT-GS 策略有效解决了边缘高斯泼溅场景重建中的挑战，通过创新的采样和资源管理机制，在保证 GS 质量的同时，优化了通信效率。

Abstract: Edge Gaussian splatting (EGS), which aggregates data from distributed clients
and trains a global GS model at the edge server, is an emerging paradigm for
scene reconstruction. Unlike traditional edge resource management methods that
emphasize communication throughput or general-purpose learning performance, EGS
explicitly aims to maximize the GS qualities, rendering existing approaches
inapplicable. To address this problem, this paper formulates a novel
GS-oriented objective function that distinguishes the heterogeneous view
contributions of different clients. However, evaluating this function in turn
requires clients' images, leading to a causality dilemma. To this end, this
paper further proposes a sample-then-transmit EGS (or STT-GS for short)
strategy, which first samples a subset of images as pilot data from each client
for loss prediction. Based on the first-stage evaluation, communication
resources are then prioritized towards more valuable clients. To achieve
efficient sampling, a feature-domain clustering (FDC) scheme is proposed to
select the most representative data and pilot transmission time minimization
(PTTM) is adopted to reduce the pilot overhead.Subsequently, we develop a joint
client selection and power control (JCSPC) framework to maximize the
GS-oriented function under communication resource constraints. Despite the
nonconvexity of the problem, we propose a low-complexity efficient solution
based on the penalty alternating majorization minimization (PAMM) algorithm.
Experiments unveil that the proposed scheme significantly outperforms existing
benchmarks on real-world datasets. It is found that the GS-oriented objective
can be accurately predicted with low sampling ratios (e.g.,10%), and our method
achieves an excellent tradeoff between view contributions and communication
costs.

</details>


### [13] [Complementary Information Guided Occupancy Prediction via Multi-Level Representation Fusion](https://arxiv.org/abs/2510.13198)
*Rongtao Xu,Jinzhou Lin,Jialei Zhou,Jiahua Dong,Changwei Wang,Ruisheng Wang,Li Guo,Shibiao Xu,Xiaodan Liang*

Main category: cs.CV

TL;DR: CIGOcc是一种基于多级表示融合的两阶段占用预测框架，它提取分割、图形和深度特征并融合它们，同时结合从SAM中提取的知识，实现了最先进的性能。


<details>
  <summary>Details</summary>
Motivation: 现有的相机占用预测方法主要通过结构修改来提高性能，但性能提升有限。很少有研究从表示融合的角度进行探索，导致2D图像中丰富的特征多样性未被充分利用。

Method: CIGOcc是一个两阶段占用预测框架。它从输入图像中提取分割、图形和深度特征，并引入一个可变形的多级融合机制来融合这三种多级特征。此外，CIGOcc还结合了从SAM中提取的知识，以进一步提高预测精度。

Result: CIGOcc在不增加训练成本的情况下，在SemanticKITTI基准测试中实现了最先进的性能。

Conclusion: CIGOcc通过多级表示融合和知识蒸馏，有效提升了相机占用预测的准确性，为自动驾驶中的3D感知提供了新的思路。

Abstract: Camera-based occupancy prediction is a mainstream approach for 3D perception
in autonomous driving, aiming to infer complete 3D scene geometry and semantics
from 2D images. Almost existing methods focus on improving performance through
structural modifications, such as lightweight backbones and complex cascaded
frameworks, with good yet limited performance. Few studies explore from the
perspective of representation fusion, leaving the rich diversity of features in
2D images underutilized. Motivated by this, we propose \textbf{CIGOcc, a
two-stage occupancy prediction framework based on multi-level representation
fusion. \textbf{CIGOcc extracts segmentation, graphics, and depth features from
an input image and introduces a deformable multi-level fusion mechanism to fuse
these three multi-level features. Additionally, CIGOcc incorporates knowledge
distilled from SAM to further enhance prediction accuracy. Without increasing
training costs, CIGOcc achieves state-of-the-art performance on the
SemanticKITTI benchmark. The code is provided in the supplementary material and
will be released https://github.com/VitaLemonTea1/CIGOcc

</details>


### [14] [Paper Copilot: Tracking the Evolution of Peer Review in AI Conferences](https://arxiv.org/abs/2510.13201)
*Jing Yang,Qiyao Wei,Jiaxin Pei*

Main category: cs.CV

TL;DR: 这篇论文介绍了Paper Copilot，一个旨在通过创建持久的数字同行评审档案和开放数据集来改进AI会议同行评审系统的项目。


<details>
  <summary>Details</summary>
Motivation: AI会议数量的快速增长给同行评审系统带来了巨大压力，导致审稿人工作量大、专业不对口、评估标准不一、审稿肤浅以及问责制有限。现有的临时性政策和干预措施反而带来了更多问题。

Method: Paper Copilot通过以下方式解决问题：1. 建立了跨多种计算机科学会议的同行评审持久数字档案；2. 创建了一个开放数据集，使研究人员能够大规模研究同行评审；3. 对ICLR会议跨年度的审稿进行了大规模实证分析。

Result: Paper Copilot发布了基础设施和数据集，支持同行评审演变的可复现研究。

Conclusion: Paper Coper希望这些资源能帮助社区跟踪变化、诊断故障模式，并为改进同行评审系统提供循证依据，使其更加健壮、透明和可靠。

Abstract: The rapid growth of AI conferences is straining an already fragile
peer-review system, leading to heavy reviewer workloads, expertise mismatches,
inconsistent evaluation standards, superficial or templated reviews, and
limited accountability under compressed timelines. In response, conference
organizers have introduced new policies and interventions to preserve review
standards. Yet these ad-hoc changes often create further concerns and confusion
about the review process, leaving how papers are ultimately accepted - and how
practices evolve across years - largely opaque. We present Paper Copilot, a
system that creates durable digital archives of peer reviews across a wide
range of computer-science venues, an open dataset that enables researchers to
study peer review at scale, and a large-scale empirical analysis of ICLR
reviews spanning multiple years. By releasing both the infrastructure and the
dataset, Paper Copilot supports reproducible research on the evolution of peer
review. We hope these resources help the community track changes, diagnose
failure modes, and inform evidence-based improvements toward a more robust,
transparent, and reliable peer-review system.

</details>


### [15] [Sample-Centric Multi-Task Learning for Detection and Segmentation of Industrial Surface Defects](https://arxiv.org/abs/2510.13226)
*Hang-Cheng Dong,Yibo Jiao,Fupeng Wei,Guodong Liu,Dong Ye,Bingguo Liu*

Main category: cs.CV

TL;DR: 本文提出了一种以样本为中心的多任务学习框架，用于工业表面缺陷检测，通过样本级监督和像素级定位，解决了缺陷检测中前景-背景不平衡、缺陷稀疏性和低对比度等挑战，显著提高了样本级决策的可靠性和缺陷定位的完整性。


<details>
  <summary>Details</summary>
Motivation: 工业表面缺陷检测在实际生产线中面临严峻挑战，包括前景-背景极端不平衡、缺陷稀疏且尺度分布长尾、对比度低等问题。传统的像素级训练和评估容易被大的同质区域主导，导致模型难以关注小型或低对比度缺陷，从而影响部署。现有模型在像素重叠度指标上表现良好，但在样本级别，特别对于稀疏或细长缺陷，稳定性不足，根本原因是优化目标与质量控制决策粒度不匹配。

Method: 本文提出了一种以样本为中心的多任务学习框架和评估套件。该方法基于共享编码器架构，共同学习样本级缺陷分类和像素级掩模定位。样本级监督调节特征分布，并在梯度层面持续提升对小型和低对比度缺陷的召回率，而分割分支则保留边界和形状细节，以增强每个样本的决策稳定性并减少漏检。本文还提出了与决策相关联的评估指标Seg_mIoU和Seg_Recall，消除了经典mIoU因空样本或真阴性样本导致的偏差，并将定位质量与样本级决策紧密结合。

Result: 在两个基准数据集上的实验表明，本文提出的方法显著提高了样本级决策的可靠性和缺陷定位的完整性。

Conclusion: 本文通过提出一种样本中心的缺陷检测多任务学习框架，有效解决了工业缺陷检测中存在的挑战，提高了模型在实际应用中的性能和稳定性。

Abstract: Industrial surface defect inspection for sample-wise quality control (QC)
must simultaneously decide whether a given sample contains defects and localize
those defects spatially. In real production lines, extreme
foreground-background imbalance, defect sparsity with a long-tailed scale
distribution, and low contrast are common. As a result, pixel-centric training
and evaluation are easily dominated by large homogeneous regions, making it
difficult to drive models to attend to small or low-contrast defects-one of the
main bottlenecks for deployment. Empirically, existing models achieve strong
pixel-overlap metrics (e.g., mIoU) but exhibit insufficient stability at the
sample level, especially for sparse or slender defects. The root cause is a
mismatch between the optimization objective and the granularity of QC
decisions. To address this, we propose a sample-centric multi-task learning
framework and evaluation suite. Built on a shared-encoder architecture, the
method jointly learns sample-level defect classification and pixel-level mask
localization. Sample-level supervision modulates the feature distribution and,
at the gradient level, continually boosts recall for small and low-contrast
defects, while the segmentation branch preserves boundary and shape details to
enhance per-sample decision stability and reduce misses. For evaluation, we
propose decision-linked metrics, Seg_mIoU and Seg_Recall, which remove the bias
of classical mIoU caused by empty or true-negative samples and tightly couple
localization quality with sample-level decisions. Experiments on two benchmark
datasets demonstrate that our approach substantially improves the reliability
of sample-level decisions and the completeness of defect localization.

</details>


### [16] [What "Not" to Detect: Negation-Aware VLMs via Structured Reasoning and Token Merging](https://arxiv.org/abs/2510.13232)
*Inha Kang,Youngsun Lim,Seonho Lee,Jiho Choi,Junsuk Choe,Hyunjung Shim*

Main category: cs.CV

TL;DR: 本文提出了一种解决视觉语言模型中否定理解缺陷（即肯定偏差）的方法，特别是在描述对象检测任务中。通过引入CoVAND数据集和NegToMe模块，显著提高了模型在否定基准上的性能，降低了误报率。


<details>
  <summary>Details</summary>
Motivation: 目前最先进的视觉语言模型在理解否定方面存在严重缺陷，即肯定偏差，尤其是在描述对象检测（DOD）任务中。

Method: 1. 提出了一个名为CoVAND的新数据集，该数据集通过系统的思维链（CoT）和基于VQA的流程生成高质量、实例接地的否定数据。
2. 提出了一个名为NegToMe的轻量级适应方法，这是一个新颖的文本标记合并模块，通过将否定提示与属性组合成连贯的语义短语，从根本上解决标记化过程中否定提示的结构性丢失，从而在输入级别保持正确的极性。
3. NegToMe模块与参数高效的LoRA微调方法相结合。

Result: 该方法显著提高了模型在具有挑战性的否定基准上的性能，降低了误报率。在OVDEval上，NMS-AP提高了10.8个点，并展示了对SoTA VLM的泛化能力。

Conclusion: 这项工作在解决现实世界检测应用中的否定理解方面迈出了关键一步。

Abstract: State-of-the-art vision-language models (VLMs) suffer from a critical failure
in understanding negation, often referred to as affirmative bias. This
limitation is particularly severe in described object detection (DOD) tasks. To
address this, we propose two primary contributions: (1) a new dataset pipeline
and (2) a novel, lightweight adaptation recipe. First, we introduce CoVAND, a
dataset constructed with a systematic chain-of-thought (CoT) and VQA-based
pipeline to generate high-quality, instance-grounded negation data. Second, we
propose NegToMe, a novel text token merging module that directly tackles the
architectural cause of affirmative bias. NegToMe fundamentally addresses the
structural loss of negation cues in tokenization, grouping them with attributes
into coherent semantic phrases. It maintains correct polarity at the input
level, enabling robust negation understanding even with limited data. For
instance, to prevent a model from treating the fragmented tokens "not" and
"girl" as simply "girl", NegToMe binds them into a single token whose meaning
is correctly distinguished from that of "girl" alone. This module is integrated
with a parameter-efficient and strategic LoRA fine-tuning approach. Our method
significantly improves performance on challenging negation benchmarks with a
lowered false positive rate, boosting NMS-AP by up to +10.8 points on OVDEval
and demonstrating generalization to SoTA VLMs. This work marks a crucial step
forward in addressing negation understanding for real-world detection
applications.

</details>


### [17] [EPIPTrack: Rethinking Prompt Modeling with Explicit and Implicit Prompts for Multi-Object Tracking](https://arxiv.org/abs/2510.13235)
*Yukuan Zhang,Jiarui Zhao,Shangqing Nie,Jin Kuang,Shengsheng Wang*

Main category: cs.CV

TL;DR: 该论文提出了EPIPTrack，一个利用显式和隐式提示进行动态目标建模和语义对齐的统一多模态视觉-语言跟踪框架，解决了现有方法中静态文本描述缺乏适应性和易产生幻觉的问题。


<details>
  <summary>Details</summary>
Motivation: 现有跟踪方法依赖于大型语言模型的静态文本描述，这导致它们无法适应目标状态的实时变化，并且容易出现幻觉。

Method: EPIPTrack通过显式提示将空间运动信息转化为自然语言描述以提供时空指导，以及通过隐式提示结合伪词和可学习描述符构建个性化知识表示来捕捉外观属性。两种提示都通过CLIP文本编码器进行动态调整，以响应目标状态的变化。此外，还设计了一个判别性特征增强器来增强视觉和跨模态表示。

Result: 在MOT17、MOT20和DanceTrack上的大量实验表明，EPIPTrack在不同场景中均优于现有跟踪器，表现出强大的适应性和卓越的性能。

Conclusion: EPIPTrack通过动态调整的显式和隐式提示，有效解决了多模态目标跟踪中静态文本描述的局限性，显著提升了跟踪的鲁棒性和性能。

Abstract: Multimodal semantic cues, such as textual descriptions, have shown strong
potential in enhancing target perception for tracking. However, existing
methods rely on static textual descriptions from large language models, which
lack adaptability to real-time target state changes and prone to
hallucinations. To address these challenges, we propose a unified multimodal
vision-language tracking framework, named EPIPTrack, which leverages explicit
and implicit prompts for dynamic target modeling and semantic alignment.
Specifically, explicit prompts transform spatial motion information into
natural language descriptions to provide spatiotemporal guidance. Implicit
prompts combine pseudo-words with learnable descriptors to construct
individualized knowledge representations capturing appearance attributes. Both
prompts undergo dynamic adjustment via the CLIP text encoder to respond to
changes in target state. Furthermore, we design a Discriminative Feature
Augmentor to enhance visual and cross-modal representations. Extensive
experiments on MOT17, MOT20, and DanceTrack demonstrate that EPIPTrack
outperforms existing trackers in diverse scenarios, exhibiting robust
adaptability and superior performance.

</details>


### [18] [FlyAwareV2: A Multimodal Cross-Domain UAV Dataset for Urban Scene Understanding](https://arxiv.org/abs/2510.13243)
*Francesco Barbato,Matteo Caligiuri,Pietro Zanuttigh*

Main category: cs.CV

TL;DR: FlyAwareV2是一个包含真实和合成无人机图像的多模态数据集，旨在解决无人机在城市环境中计算机视觉算法开发中数据收集和标注的挑战。


<details>
  <summary>Details</summary>
Motivation: 收集和标注真实的无人机数据既困难又昂贵，限制了城市环境中无人机计算机视觉算法的开发。

Method: FlyAwareV2数据集在SynDrone和FlyAware数据集的基础上，增加了多模态数据（RGB、深度、语义标签），涵盖了不同的天气和白天条件。它还通过最先进的单目深度估计计算了真实样本的深度图，并提供了RGB和多模态语义分割的基准测试，以及合成到真实域适应的研究。

Result: FlyAwareV2数据集提供了丰富的标注和环境多样性，为基于无人机的3D城市场景理解研究提供了宝贵的资源。

Conclusion: FlyAwareV2通过提供一个多模态、多样化且包含合成和真实数据的无人机数据集，有效解决了无人机计算机视觉领域数据稀缺和标注困难的问题，有望推动3D城市场景理解算法的发展。

Abstract: The development of computer vision algorithms for Unmanned Aerial Vehicle
(UAV) applications in urban environments heavily relies on the availability of
large-scale datasets with accurate annotations. However, collecting and
annotating real-world UAV data is extremely challenging and costly. To address
this limitation, we present FlyAwareV2, a novel multimodal dataset encompassing
both real and synthetic UAV imagery tailored for urban scene understanding
tasks. Building upon the recently introduced SynDrone and FlyAware datasets,
FlyAwareV2 introduces several new key contributions: 1) Multimodal data (RGB,
depth, semantic labels) across diverse environmental conditions including
varying weather and daytime; 2) Depth maps for real samples computed via
state-of-the-art monocular depth estimation; 3) Benchmarks for RGB and
multimodal semantic segmentation on standard architectures; 4) Studies on
synthetic-to-real domain adaptation to assess the generalization capabilities
of models trained on the synthetic data. With its rich set of annotations and
environmental diversity, FlyAwareV2 provides a valuable resource for research
on UAV-based 3D urban scene understanding.

</details>


### [19] [CymbaDiff: Structured Spatial Diffusion for Sketch-based 3D Semantic Urban Scene Generation](https://arxiv.org/abs/2510.13245)
*Li Liang,Bo Miao,Xinyu Wang,Naveed Akhtar,Jordan Vice,Ajmal Mian*

Main category: cs.CV

TL;DR: SketchSem3D是首个大规模户外3D语义场景生成基准数据集，包含Sketch-based SemanticKITTI和Sketch-based KITTI-360两个子集。我们还提出了Cylinder Mamba Diffusion (CymbaDiff)模型，该模型显著增强了户外3D场景生成的空间连贯性，并在SketchSem3D上取得了优异的语义一致性、空间真实感和跨数据集泛化能力。


<details>
  <summary>Details</summary>
Motivation: 目前缺乏公开的、有良好标注的户外3D语义场景生成数据集，这限制了该领域的发展。

Method: 本文引入了SketchSem3D数据集，它是第一个用于从抽象手绘草图和卫星图像伪标注生成3D户外语义场景的大规模基准数据集。该数据集包含Sketch-based SemanticKITTI和Sketch-based KITTA-360两个子集。同时，本文提出了Cylinder Mamba Diffusion (CymbaDiff)模型，该模型通过施加结构化的空间排序、明确捕获圆柱形连续性和垂直层次结构，并保留生成的场景中的物理邻域关系和全局上下文，显著增强了户外3D场景生成的空间连贯性。

Result: CymbaDiff在SketchSem3D数据集上通过广泛实验证明，其在语义一致性、空间真实感和跨数据集泛化能力方面均表现出色。

Conclusion: SketchSem3D数据集的发布和CymbaDiff模型的提出，为户外3D语义场景生成领域的研究提供了新的基准和SOTA模型，有望推动该领域的发展。

Abstract: Outdoor 3D semantic scene generation produces realistic and semantically rich
environments for applications such as urban simulation and autonomous driving.
However, advances in this direction are constrained by the absence of publicly
available, well-annotated datasets. We introduce SketchSem3D, the first
large-scale benchmark for generating 3D outdoor semantic scenes from abstract
freehand sketches and pseudo-labeled annotations of satellite images.
SketchSem3D includes two subsets, Sketch-based SemanticKITTI and Sketch-based
KITTI-360 (containing LiDAR voxels along with their corresponding sketches and
annotated satellite images), to enable standardized, rigorous, and diverse
evaluations. We also propose Cylinder Mamba Diffusion (CymbaDiff) that
significantly enhances spatial coherence in outdoor 3D scene generation.
CymbaDiff imposes structured spatial ordering, explicitly captures cylindrical
continuity and vertical hierarchy, and preserves both physical neighborhood
relationships and global context within the generated scenes. Extensive
experiments on SketchSem3D demonstrate that CymbaDiff achieves superior
semantic consistency, spatial realism, and cross-dataset generalization. The
code and dataset will be available at
https://github.com/Lillian-research-hub/CymbaDiff

</details>


### [20] [Map the Flow: Revealing Hidden Pathways of Information in VideoLLMs](https://arxiv.org/abs/2510.13251)
*Minji Kim,Taekyung Kim,Bohyung Han*

Main category: cs.CV

TL;DR: 这篇论文通过机械可解释性技术分析了VideoLLMs的内部信息流，揭示了模型中时序推理和视频-语言 T统一的规律，并提出了一种通过选择有效信息路径来提高模型性能和可解释性的方法。


<details>
  <summary>Details</summary>
Motivation: 尽管VideoLLMs在视频问答（VideoQA）等方面取得了进展，但其内部提取和传播视频与文本信息的机制尚不清楚。

Method: 本研究采用机械可解释性技术，分析了VideoLLMs的内部信息流。

Result: 分析揭示了VideoLLMs中一致的模式：1）时序推理始于早期到中期层中的跨帧交互；2）随后中期层中进行渐进式视频-语言整合，这通过视频表示和包含时序概念的语言嵌入之间的对齐来实现；3）整合完成后，模型在中后期层准备生成正确答案；4）通过选择有效的信息路径，VideoLLMs可以保持其VideoQA性能，同时抑制大量的注意力，例如在LLaVA-NeXT-7B-Video-FT中抑制了58%的注意力。

Conclusion: 这些发现揭示了VideoLLMs执行时序推理的蓝图，并为提高模型可解释性和下游泛化能力提供了实用的见解。

Abstract: Video Large Language Models (VideoLLMs) extend the capabilities of
vision-language models to spatiotemporal inputs, enabling tasks such as video
question answering (VideoQA). Despite recent advances in VideoLLMs, their
internal mechanisms on where and how they extract and propagate video and
textual information remain less explored. In this study, we investigate the
internal information flow of VideoLLMs using mechanistic interpretability
techniques. Our analysis reveals consistent patterns across diverse VideoQA
tasks: (1) temporal reasoning in VideoLLMs initiates with active cross-frame
interactions in early-to-middle layers, (2) followed by progressive
video-language integration in middle layers. This is facilitated by alignment
between video representations and linguistic embeddings containing temporal
concepts. (3) Upon completion of this integration, the model is ready to
generate correct answers in middle-to-late layers. (4) Based on our analysis,
we show that VideoLLMs can retain their VideoQA performance by selecting these
effective information pathways while suppressing a substantial amount of
attention edges, e.g., 58% in LLaVA-NeXT-7B-Video-FT. These findings provide a
blueprint on how VideoLLMs perform temporal reasoning and offer practical
insights for improving model interpretability and downstream generalization.
Our project page with the source code is available at
https://map-the-flow.github.io

</details>


### [21] [End-to-End Multi-Modal Diffusion Mamba](https://arxiv.org/abs/2510.13253)
*Chunhao Lu,Qiang Lu,Meichen Dong,Jake Luo*

Main category: cs.CV

TL;DR: MDM模型通过统一的VAE和Mamba驱动的多步选择扩散模型，实现了多模态处理的统一，并在高维数据处理方面展现出卓越性能，超越了现有端到端模型，并与SOTA模型竞争。


<details>
  <summary>Details</summary>
Motivation: 当前端到端多模态模型中编码器和解码器分离，阻碍了多模态联合表示学习。

Method: 提出了一种名为MDM（多模态扩散Mamba）的新型架构。MDM利用基于Mamba的多步选择扩散模型，通过统一的变分自动编码器（VAE）逐步生成和细化特定模态的信息，该VAE同时用于编码和解码。

Result: MDM在图像生成、图像字幕、视觉问答、文本理解和推理任务等领域显著优于现有端到端模型（MonoFormer、LlamaGen和Chameleon等），并能有效与GPT-4V、Gemini Pro和Mistral等SOTA模型竞争。

Conclusion: MDM在统一多模态处理的同时保持了计算效率，为端到端多模态架构开辟了新方向。

Abstract: Current end-to-end multi-modal models utilize different encoders and decoders
to process input and output information. This separation hinders the joint
representation learning of various modalities. To unify multi-modal processing,
we propose a novel architecture called MDM (Multi-modal Diffusion Mamba). MDM
utilizes a Mamba-based multi-step selection diffusion model to progressively
generate and refine modality-specific information through a unified variational
autoencoder for both encoding and decoding. This innovative approach allows MDM
to achieve superior performance when processing high-dimensional data,
particularly in generating high-resolution images and extended text sequences
simultaneously. Our evaluations in areas such as image generation, image
captioning, visual question answering, text comprehension, and reasoning tasks
demonstrate that MDM significantly outperforms existing end-to-end models
(MonoFormer, LlamaGen, and Chameleon etc.) and competes effectively with SOTA
models like GPT-4V, Gemini Pro, and Mistral. Our results validate MDM's
effectiveness in unifying multi-modal processes while maintaining computational
efficiency, establishing a new direction for end-to-end multi-modal
architectures.

</details>


### [22] [Self-Augmented Visual Contrastive Decoding](https://arxiv.org/abs/2510.13315)
*Eun Woo Im,Muhammad Kashif Ali,Vivek Gupta*

Main category: cs.CV

TL;DR: 本文提出了一种无需训练的解码策略，通过自增强提示和自适应阈值算法，显著提高了大型视觉语言模型（LVLMs）的事实一致性，解决了幻觉问题。


<details>
  <summary>Details</summary>
Motivation: 大型视觉语言模型（LVLMs）存在幻觉问题，现有方法使用通用视觉增强，未能充分利用文本查询的特定上下文，导致效果受限。

Method: 本文提出了一种无需训练的解码策略，包含两项关键贡献：1. 自增强提示策略：利用模型内在知识，动态对齐查询与视觉增强之间的语义。2. 自适应阈值算法：根据输出稀疏性自适应调整下一个候选token的大小，利用logit分布的全部信息。

Result: 在四个LVLMs和七个基准测试中进行了广泛实验，结果表明所提出的解码策略比最先进的解码方法显著增强了事实一致性。

Conclusion: 本研究强调了整合依赖于查询的增强和感知熵的解码对于提高LVLMs有效生成的重要性。

Abstract: Large Vision-Language Models (LVLMs) have demonstrated remarkable multimodal
capabilities, but they inherit the tendency to hallucinate from their
underlying language models. While visual contrastive decoding has been proposed
to mitigate this issue, existing methods often apply generic visual
augmentations that disregard the specific context provided by the text query,
limiting their effectiveness. This study introduces a novel training-free
decoding strategy that addresses these limitations, featuring two key
contributions. First, a self-augmentation prompting strategy that leverages the
intrinsic knowledge of the model to dynamically align semantics between the
query and the visual augmentation. Second, an adaptive thresholding algorithm
that adaptively adjusts next token candidate size based on the output sparsity,
utilizing full information from the logit distribution. Extensive experiments
across four LVLMs and seven benchmarks demonstrate that the proposed decoding
significantly enhances factual consistency compared to state-of-the-art
decoding methods. This work highlights the importance of integrating
query-dependent augmentation and entropy-aware decoding for improving effective
generation of LVLMs.

</details>


### [23] [Universal Image Restoration Pre-training via Masked Degradation Classification](https://arxiv.org/abs/2510.13282)
*JiaKui Hu,Zhengjian Yao,Lujia Jin,Yinghao Chen,Yanye Lu*

Main category: cs.CV

TL;DR: 该研究提出了一种名为 MaskDCPT 的掩码退化分类预训练方法，用于图像恢复任务。


<details>
  <summary>Details</summary>
Motivation: 传统的预训练方法在图像恢复方面存在局限性，MaskDCPT 旨在通过结合退化类型分类和图像重建来提高图像恢复的性能和鲁棒性。

Method: MaskDCPT 包含一个编码器和两个解码器。编码器从掩码的低质量输入图像中提取特征，分类解码器识别退化类型，而重建解码器重建相应的高质量图像。该方法结合了掩码图像建模和对比学习，以获得通用的图像恢复表示。

Result: MaskDCPT 显著改善了 CNN 和 Transformer 的性能。在 5D all-in-one 恢复任务中，PSNR 至少增加了 3.77 dB；在真实世界退化场景中，PIQE 比基线降低了 34.8%。它对未见过的退化类型和水平也表现出强大的泛化能力。此外，还发布了包含 250 万个配对恢复样本的 UIR-2.5M 数据集。

Conclusion: MaskDCPT 方法通过结合分类和重建任务，有效地提升了图像恢复的性能和泛化能力。所提出的数据集也为未来的研究提供了宝贵的资源。

Abstract: This study introduces a Masked Degradation Classification Pre-Training method
(MaskDCPT), designed to facilitate the classification of degradation types in
input images, leading to comprehensive image restoration pre-training. Unlike
conventional pre-training methods, MaskDCPT uses the degradation type of the
image as an extremely weak supervision, while simultaneously leveraging the
image reconstruction to enhance performance and robustness. MaskDCPT includes
an encoder and two decoders: the encoder extracts features from the masked
low-quality input image. The classification decoder uses these features to
identify the degradation type, whereas the reconstruction decoder aims to
reconstruct a corresponding high-quality image. This design allows the
pre-training to benefit from both masked image modeling and contrastive
learning, resulting in a generalized representation suited for restoration
tasks. Benefit from the straightforward yet potent MaskDCPT, the pre-trained
encoder can be used to address universal image restoration and achieve
outstanding performance. Implementing MaskDCPT significantly improves
performance for both convolution neural networks (CNNs) and Transformers, with
a minimum increase in PSNR of 3.77 dB in the 5D all-in-one restoration task and
a 34.8% reduction in PIQE compared to baseline in real-world degradation
scenarios. It also emergences strong generalization to previously unseen
degradation types and levels. In addition, we curate and release the UIR-2.5M
dataset, which includes 2.5 million paired restoration samples across 19
degradation types and over 200 degradation levels, incorporating both synthetic
and real-world data. The dataset, source code, and models are available at
https://github.com/MILab-PKU/MaskDCPT.

</details>


### [24] [Language as a Label: Zero-Shot Multimodal Classification of Everyday Postures under Data Scarcity](https://arxiv.org/abs/2510.13364)
*MingZe Tang,Jubal Chandy Jacob*

Main category: cs.CV

TL;DR: 该研究探讨了在零样本图像分类中，提示（prompt）设计对识别视觉相似类别（如人类姿态）的影响。


<details>
  <summary>Details</summary>
Motivation: 探究提示设计如何影响视觉相似类别的零样本分类，例如人类姿态识别。

Method: 使用三种不同详细程度的提示，在包含285张图像的COCO数据子集上，评估了OpenCLIP、MetaCLIP 2和SigLip等现代视觉-语言模型（VLMs）。

Result: 对于高性能模型（MetaCLIP 2和OpenCLIP），最简单、最基础的提示表现最佳，增加描述性细节会导致性能显著下降（例如，MetaCLIP 2的多类准确率从68.8%降至55.1%），这种现象被称为“提示过拟合”。而性能较低的SigLip模型在面对模棱两可的类别时，通过更具描述性、基于身体线索的提示可以提高分类效果。

Conclusion: 对于高性能的VLMs，在零样本分类中，简洁的提示优于详细的提示，避免了“提示过拟合”现象。而对于性能较低的模型，详细的提示有助于提高分类精度。

Abstract: Recent Vision-Language Models (VLMs) enable zero-shot classification by
aligning images and text in a shared space, a promising approach for
data-scarce conditions. However, the influence of prompt design on recognizing
visually similar categories, such as human postures, is not well understood.
This study investigates how prompt specificity affects the zero-shot
classification of sitting, standing, and walking/running on a small, 285-image
COCO-derived dataset. A suite of modern VLMs, including OpenCLIP, MetaCLIP 2,
and SigLip, were evaluated using a three-tiered prompt design that
systematically increases linguistic detail. Our findings reveal a compelling,
counter-intuitive trend: for the highest-performing models (MetaCLIP 2 and
OpenCLIP), the simplest, most basic prompts consistently achieve the best
results. Adding descriptive detail significantly degrades performance for
instance, MetaCLIP 2's multi-class accuracy drops from 68.8\% to 55.1\% a
phenomenon we term "prompt overfitting". Conversely, the lower-performing
SigLip model shows improved classification on ambiguous classes when given more
descriptive, body-cue-based prompts.

</details>


### [25] [Novel Class Discovery for Point Cloud Segmentation via Joint Learning of Causal Representation and Reasoning](https://arxiv.org/abs/2510.13307)
*Yang Li,Aming Wu,Zihao Zhang,Yahong Han*

Main category: cs.CV

TL;DR: 这篇论文提出了一个针对点云分割（3D-NCD）的新颖类发现方法，通过引入结构因果模型（SCM）来学习因果表示和推理，从而实现从未标记的3D类中分割出新的3D类。


<details>
  <summary>Details</summary>
Motivation: 以往的新颖类发现方法在点云表示与基类标签之间以及基类与新类点之间的表示相关性学习上可能存在不足，导致新类推断混淆。

Method: 本文引入了一个结构因果模型（SCM）来重新形式化3D-NCD问题，并提出了“因果表示与推理联合学习”的新方法。首先，通过SCM分析基类表示中的潜在混杂因素以及基类和新类之间的因果关系。然后，设计了一种因果表示原型来消除混杂因素，以捕获基类的因果表示。最后，使用图结构来建模基类因果表示原型与新类原型之间的因果关系，从而实现从基类到新类的因果推理。

Result: 在3D和2D NCD语义分割上的大量实验和可视化结果表明，该方法具有优越性。

Conclusion: 通过引入结构因果模型和因果表示与推理联合学习方法，可以有效地进行点云分割的新颖类发现，消除混杂因素并实现从基类到新类的因果推理。

Abstract: In this paper, we focus on Novel Class Discovery for Point Cloud Segmentation
(3D-NCD), aiming to learn a model that can segment unlabeled (novel) 3D classes
using only the supervision from labeled (base) 3D classes. The key to this task
is to setup the exact correlations between the point representations and their
base class labels, as well as the representation correlations between the
points from base and novel classes. A coarse or statistical correlation
learning may lead to the confusion in novel class inference. lf we impose a
causal relationship as a strong correlated constraint upon the learning
process, the essential point cloud representations that accurately correspond
to the classes should be uncovered. To this end, we introduce a structural
causal model (SCM) to re-formalize the 3D-NCD problem and propose a new method,
i.e., Joint Learning of Causal Representation and Reasoning. Specifically, we
first analyze hidden confounders in the base class representations and the
causal relationships between the base and novel classes through SCM. We devise
a causal representation prototype that eliminates confounders to capture the
causal representations of base classes. A graph structure is then used to model
the causal relationships between the base classes' causal representation
prototypes and the novel class prototypes, enabling causal reasoning from base
to novel classes. Extensive experiments and visualization results on 3D and 2D
NCD semantic segmentation demonstrate the superiorities of our method.

</details>


### [26] [InstantSfM: Fully Sparse and Parallel Structure-from-Motion](https://arxiv.org/abs/2510.13310)
*Jiankun Zhong,Zitong Zhan,Quankai Gao,Ziyu Chen,Haozhe Lou,Jiageng Mao,Ulrich Neumann,Yue Wang*

Main category: cs.CV

TL;DR: 这篇论文介绍了一种名为InstantSfM的新方法，它利用GPU并行计算加速了Structure-from-Motion（SfM）流程，克服了传统方法和基于深度学习的方法在处理大规模数据集时的局限性。


<details>
  <summary>Details</summary>
Motivation: 传统的SfM方法（如COLMAP和GLOMAP）在处理大规模场景时存在计算开销大、灵活性有限的问题。另一方面，基于深度学习的SfM方法（如VGGSfM和VGGT）在输入视图数量增加时，GPU内存消耗急剧增加，无法扩展到数千个视图。

Method: 本论文通过利用GPU并行计算来加速标准SfM流程的每个关键阶段，特别是基于稀疏感知的BA优化，将这些技术扩展到统一的全局SfM框架中的BA和GP加速。

Result: 在不同规模的数据集（例如5000张图像，VGGSfM和VGGT会内存不足）上进行的广泛实验表明，该方法比COLMAP提速高达约40倍，同时实现了始终相当甚至更高的重建精度。

Conclusion: InstantSfM通过充分利用GPU并行计算，在保持高精度的同时显著提升了SfM处理大规模数据的速度和效率，弥补了现有SfM方法的不足。

Abstract: Structure-from-Motion (SfM), a method that recovers camera poses and scene
geometry from uncalibrated images, is a central component in robotic
reconstruction and simulation. Despite the state-of-the-art performance of
traditional SfM methods such as COLMAP and its follow-up work, GLOMAP, naive
CPU-specialized implementations of bundle adjustment (BA) or global positioning
(GP) introduce significant computational overhead when handling large-scale
scenarios, leading to a trade-off between accuracy and speed in SfM. Moreover,
the blessing of efficient C++-based implementations in COLMAP and GLOMAP comes
with the curse of limited flexibility, as they lack support for various
external optimization options. On the other hand, while deep learning based SfM
pipelines like VGGSfM and VGGT enable feed-forward 3D reconstruction, they are
unable to scale to thousands of input views at once as GPU memory consumption
increases sharply as the number of input views grows. In this paper, we unleash
the full potential of GPU parallel computation to accelerate each critical
stage of the standard SfM pipeline. Building upon recent advances in
sparse-aware bundle adjustment optimization, our design extends these
techniques to accelerate both BA and GP within a unified global SfM framework.
Through extensive experiments on datasets of varying scales (e.g. 5000 images
where VGGSfM and VGGT run out of memory), our method demonstrates up to about
40 times speedup over COLMAP while achieving consistently comparable or even
improved reconstruction accuracy. Our project page can be found at
https://cre185.github.io/InstantSfM/.

</details>


### [27] [Modeling Cultural Bias in Facial Expression Recognition with Adaptive Agents](https://arxiv.org/abs/2510.13557)
*David Freire-Obregón,José Salas-Cáceres,Javier Lorenzo-Navarro,Oliverio J. Santana,Daniel Hernández-Sosa,Modesto Castrillón-Santana*

Main category: cs.CV

TL;DR: 该文章介绍了一种基于agent的流式基准，用于评估跨文化组成和渐进模糊对人脸识别鲁棒性的影响。


<details>
  <summary>Details</summary>
Motivation: 现有的面部表情识别（FER）评估通常假设数据同质且图像质量高，但在文化差异和感知退化的视觉条件下，FER必须保持鲁棒性。

Method: 文章引入了一个基于agent的流式基准。每个agent在一个冻结的CLIP特征空间中运行，并带有一个轻量级的残差适配器，该适配器在sigma=0时在线训练并在测试期间固定。Agent在5x5的网格上移动和交互，环境则提供带有sigma调度高斯模糊的输入。研究了单文化群体（仅西方、仅亚洲）和混合环境，包括平衡（5/5）和不平衡（8/2、2/8）的组成，以及不同的空间接触结构。

Result: 结果显示，不同文化群体之间存在明显不对称的性能下降曲线：JAFFE（亚洲）人群在低模糊度下保持较高性能，但在中等模糊阶段表现出更急剧的下降；而KDEF（西方）人群的下降则更为均匀。混合人群表现出中间模式，其中平衡的混合缓解了早期性能下降，但不平衡的设置在高模糊度下则放大了多数群体的弱点。

Conclusion: 这些发现量化了文化组成和交互结构如何影响FER在感知条件恶化时的鲁棒性。

Abstract: Facial expression recognition (FER) must remain robust under both cultural
variation and perceptually degraded visual conditions, yet most existing
evaluations assume homogeneous data and high-quality imagery. We introduce an
agent-based, streaming benchmark that reveals how cross-cultural composition
and progressive blurring interact to shape face recognition robustness. Each
agent operates in a frozen CLIP feature space with a lightweight residual
adapter trained online at sigma=0 and fixed during testing. Agents move and
interact on a 5x5 lattice, while the environment provides inputs with
sigma-scheduled Gaussian blur. We examine monocultural populations
(Western-only, Asian-only) and mixed environments with balanced (5/5) and
imbalanced (8/2, 2/8) compositions, as well as different spatial contact
structures. Results show clear asymmetric degradation curves between cultural
groups: JAFFE (Asian) populations maintain higher performance at low blur but
exhibit sharper drops at intermediate stages, whereas KDEF (Western)
populations degrade more uniformly. Mixed populations exhibit intermediate
patterns, with balanced mixtures mitigating early degradation, but imbalanced
settings amplify majority-group weaknesses under high blur. These findings
quantify how cultural composition and interaction structure influence the
robustness of FER as perceptual conditions deteriorate.

</details>


### [28] [Visual Interestingness Decoded: How GPT-4o Mirrors Human Interests](https://arxiv.org/abs/2510.13316)
*Fitim Abdullahu,Helmut Grabner*

Main category: cs.CV

TL;DR: 本文探讨了大型多模态模型（LMMs）在理解视觉趣味性方面的潜力，发现GPT-4o与人类评估存在部分一致性，并优于现有最先进方法。


<details>
  <summary>Details</summary>
Motivation: 探索大型多模态模型（LMMs）是否能够理解视觉趣味性，并评估其与人类评估的一致性。

Method: 通过比较分析，将GPT-4o（一种领先的LMM）的预测与人类评估进行对比，以了解视觉趣味性概念在模型中被捕获的程度。利用GPT-4o生成的（共同）趣味性标签作为训练数据，蒸馏知识到学习排序模型中。

Result: 研究显示GPT-4o与人类评估之间存在部分一致性。与最先进的方法相比，GPT-4o已经能够很好地捕捉视觉趣味性概念。

Conclusion: GPT-4o在理解视觉趣味性方面展现出强大潜力，并为深入理解人类兴趣铺平了道路。其能够有效地对图像对进行趣味性标注，可用于训练学习排序模型。

Abstract: Our daily life is highly influenced by what we consume and see. Attracting
and holding one's attention -- the definition of (visual) interestingness -- is
essential. The rise of Large Multimodal Models (LMMs) trained on large-scale
visual and textual data has demonstrated impressive capabilities. We explore
these models' potential to understand to what extent the concepts of visual
interestingness are captured and examine the alignment between human
assessments and GPT-4o's, a leading LMM, predictions through comparative
analysis. Our studies reveal partial alignment between humans and GPT-4o. It
already captures the concept as best compared to state-of-the-art methods.
Hence, this allows for the effective labeling of image pairs according to their
(commonly) interestingness, which are used as training data to distill the
knowledge into a learning-to-rank model. The insights pave the way for a deeper
understanding of human interest.

</details>


### [29] [MVCustom: Multi-View Customized Diffusion via Geometric Latent Rendering and Completion](https://arxiv.org/abs/2510.13702)
*Minjung Shin,Hyunin Cho,Sooyeon Go,Jin-Hwa Kim,Youngjung Uh*

Main category: cs.CV

TL;DR: 本文提出了一种名为MVCustom的新型扩散模型框架，旨在解决多视角生成中相机位姿控制和基于提示词定制的统一问题。MVCustom通过特征场表示学习主体身份和几何，并在推理阶段引入深度感知特征渲染和一致性感知潜在补全技术，以实现多视角一致性和定制化保真。


<details>
  <summary>Details</summary>
Motivation: 现有的多视角生成模型不支持几何一致性的定制，而定制模型又缺乏明确的视角控制，难以统一。

Method: MVCustom是一个基于扩散的框架，在训练阶段使用特征场表示和密集时空注意力增强的文本到视频扩散骨干来学习主体的身份和几何。在推理阶段，引入了深度感知特征渲染和一致性感知潜在补全两种新颖技术。

Result: MVCustom是唯一一个同时实现忠实多视角生成和定制的框架。

Conclusion: MVCustom框架有效地结合了多视角相机位姿控制和定制化功能，填补了现有技术的空白，为可控生成模型提供了新的解决方案。

Abstract: Multi-view generation with camera pose control and prompt-based customization
are both essential elements for achieving controllable generative models.
However, existing multi-view generation models do not support customization
with geometric consistency, whereas customization models lack explicit
viewpoint control, making them challenging to unify. Motivated by these gaps,
we introduce a novel task, multi-view customization, which aims to jointly
achieve multi-view camera pose control and customization. Due to the scarcity
of training data in customization, existing multi-view generation models, which
inherently rely on large-scale datasets, struggle to generalize to diverse
prompts. To address this, we propose MVCustom, a novel diffusion-based
framework explicitly designed to achieve both multi-view consistency and
customization fidelity. In the training stage, MVCustom learns the subject's
identity and geometry using a feature-field representation, incorporating the
text-to-video diffusion backbone enhanced with dense spatio-temporal attention,
which leverages temporal coherence for multi-view consistency. In the inference
stage, we introduce two novel techniques: depth-aware feature rendering
explicitly enforces geometric consistency, and consistent-aware latent
completion ensures accurate perspective alignment of the customized subject and
surrounding backgrounds. Extensive experiments demonstrate that MVCustom is the
only framework that simultaneously achieves faithful multi-view generation and
customization.

</details>


### [30] [Group-Wise Optimization for Self-Extensible Codebooks in Vector Quantized Models](https://arxiv.org/abs/2510.13331)
*Hong-Kai Zheng,Piji Li*

Main category: cs.CV

TL;DR: 本文提出Group-VQ，一种对VQ-VAE码本进行组优化和训练无关的码本重采样方法，以解决码本崩溃问题并提高重建性能和码本大小调整的灵活性。


<details>
  <summary>Details</summary>
Motivation: VQ-VAE模型中存在的码本崩溃问题，以及现有方法（隐式静态码本或联合优化整个码本）对码本学习能力的限制，导致重建质量下降。

Method: 本文提出Group-VQ方法，采用组内独立优化、组间联合优化的方式对码本进行组优化。同时引入了一种训练无关的码本重采样方法，可以在训练后调整码本大小。

Result: 在各种设置下的图像重建实验中，Group-VQ在重建指标上表现出更好的性能。训练后的码本采样方法在调整码本大小时也实现了预期的灵活性。

Conclusion: Group-VQ通过组优化和训练无关的码本重采样方法，有效解决了VQ-VAE中的码本崩溃问题，提高了重建性能，并增加了码本大小调整的灵活性。

Abstract: Vector Quantized Variational Autoencoders (VQ-VAEs) leverage self-supervised
learning through reconstruction tasks to represent continuous vectors using the
closest vectors in a codebook. However, issues such as codebook collapse
persist in the VQ model. To address these issues, existing approaches employ
implicit static codebooks or jointly optimize the entire codebook, but these
methods constrain the codebook's learning capability, leading to reduced
reconstruction quality. In this paper, we propose Group-VQ, which performs
group-wise optimization on the codebook. Each group is optimized independently,
with joint optimization performed within groups. This approach improves the
trade-off between codebook utilization and reconstruction performance.
Additionally, we introduce a training-free codebook resampling method, allowing
post-training adjustment of the codebook size. In image reconstruction
experiments under various settings, Group-VQ demonstrates improved performance
on reconstruction metrics. And the post-training codebook sampling method
achieves the desired flexibility in adjusting the codebook size.

</details>


### [31] [Scaling Vision Transformers for Functional MRI with Flat Maps](https://arxiv.org/abs/2510.13768)
*Connor Lane,Daniel Z. Kaplan,Tanishq Mathew Abraham,Paul S. Scotti*

Main category: cs.CV

TL;DR: 将4D fMRI数据转换为2D fMRI活动平铺图的视频，并使用时空掩码自动编码器（MAE）框架在Vision Transformers上进行训练。


<details>
  <summary>Details</summary>
Motivation: 解决现代深度学习架构与功能性MRI（fMRI）数据之间的模态差异，并探索如何表示fMRI数据以用于模型输入。

Method: 将4D体积fMRI数据转换为2D fMRI活动平铺图的视频，并使用时空掩码自动编码器（MAE）框架在来自人类连接组项目的2.3K小时fMRI平铺图视频上训练Vision Transformers。

Result: 掩码fMRI建模性能随数据集大小的增加而提高，并遵循严格的幂律缩放定律。下游分类基准测试表明，模型学习了丰富的表示，支持跨受试者的细粒度状态解码以及跨大脑状态变化的受试者特异性性状解码。

Conclusion: 本研究成功地将4D fMRI数据转换为2D fMRI平铺图视频，并利用Vision Transformers和MAE框架学习了fMRI数据的丰富表示。这一成果为fMRI数据构建基础模型奠定了基础，并为未来fMRI数据的分析和理解提供了新的方向。

Abstract: A key question for adapting modern deep learning architectures to functional
MRI (fMRI) is how to represent the data for model input. To bridge the modality
gap between fMRI and natural images, we transform the 4D volumetric fMRI data
into videos of 2D fMRI activity flat maps. We train Vision Transformers on 2.3K
hours of fMRI flat map videos from the Human Connectome Project using the
spatiotemporal masked autoencoder (MAE) framework. We observe that masked fMRI
modeling performance improves with dataset size according to a strict power
scaling law. Downstream classification benchmarks show that our model learns
rich representations supporting both fine-grained state decoding across
subjects, as well as subject-specific trait decoding across changes in brain
state. This work is part of an ongoing open science project to build foundation
models for fMRI data. Our code and datasets are available at
https://github.com/MedARC-AI/fmri-fm.

</details>


### [32] [Bee: A High-Quality Corpus and Full-Stack Suite to Unlock Advanced Fully Open MLLMs](https://arxiv.org/abs/2510.13795)
*Yi Zhang,Bolin Ni,Xin-Sheng Chen,Heng-Rui Zhang,Yongming Rao,Houwen Peng,Qinglin Lu,Han Hu,Meng-Hao Guo,Shi-Min Hu*

Main category: cs.CV

TL;DR: 本文介绍了Honey-Data-15M数据集、HoneyPipe数据处理管道和DataStudio框架，并提出了名为Bee-8B的模型。


<details>
  <summary>Details</summary>
Motivation: 目前，完全开放的多模态大型语言模型（MLLMs）在性能上落后于专有模型，主要原因是监督微调（SFT）的数据质量存在显著差距。现有的开源数据集通常存在广泛的噪声，并且缺乏复杂的推理数据（如思维链CoT），这阻碍了模型高级能力的发展。

Method: 1. 引入了Honey-Data-15M数据集，包含约1500万个问答对，通过多种清洗技术处理，并采用新颖的双层（短CoT和长CoT）CoT丰富策略进行增强。2. 引入了HoneyPipe数据整理管道及其底层框架DataStudio，为社区提供了一个透明且适应性强的数据整理方法。3. 在Honey-Data-15M上训练了一个8B模型Bee-8B。

Result: Bee-8B模型在完全开放的MLLMs中建立了新的最先进水平（SOTA），其性能与InternVL3.5-8B等半开放模型具有竞争力，在某些情况下甚至超越它们。

Conclusion: 数据质量的原则性关注是开发具有高度竞争力的完全开放MLLMs的关键途径。

Abstract: Fully open multimodal large language models (MLLMs) currently lag behind
proprietary counterparts, primarily due to a significant gap in data quality
for supervised fine-tuning (SFT). Existing open-source datasets are often
plagued by widespread noise and a critical deficit in complex reasoning data,
such as Chain-of-Thought (CoT), which hinders the development of advanced model
capabilities. Addressing these challenges, our work makes three primary
contributions. First, we introduce Honey-Data-15M, a new SFT dataset comprising
approximately 15 million QA pairs, processed through multiple cleaning
techniques and enhanced with a novel dual-level (short and long) CoT enrichment
strategy. Second, we introduce HoneyPipe, the data curation pipeline, and its
underlying framework DataStudio, providing the community with a transparent and
adaptable methodology for data curation that moves beyond static dataset
releases. Finally, to validate our dataset and pipeline, we train Bee-8B, an 8B
model on Honey-Data-15M. Experiments show that Bee-8B establishes a new
state-of-the-art (SOTA) for fully open MLLMs, achieving performance that is
competitive with, and in some cases surpasses, recent semi-open models such as
InternVL3.5-8B. Our work delivers to the community a suite of foundational
resources, including: the Honey-Data-15M corpus; the full-stack suite
comprising HoneyPipe and DataStudio; training recipes; an evaluation harness;
and the model weights. This effort demonstrates that a principled focus on data
quality is a key pathway to developing fully open MLLMs that are highly
competitive with their semi-open counterparts.

</details>


### [33] [Generative Universal Verifier as Multimodal Meta-Reasoner](https://arxiv.org/abs/2510.13804)
*Xinchen Zhang,Xiaoying Zhang,Youbin Wu,Yanbin Cao,Renrui Zhang,Ruihang Chu,Ling Yang,Yujiu Yang*

Main category: cs.CV

TL;DR: 该论文介绍了一种名为“生成式通用验证器”（GUV）的新概念和插件，旨在提高视觉语言模型和统一多模态模型的多模态推理能力，使其能够对视觉结果进行反思和改进。


<details>
  <summary>Details</summary>
Motivation: 现有的视觉语言模型在视觉验证方面表现不佳，与人类水平存在显著差距。因此，存在开发能够可靠地对视觉结果进行验证和改进的系统。

Method: 1. 建立了ViVerBench基准，用于评估多模态推理中的视觉结果。2. 设计了自动化流程来构建大规模视觉验证数据，并训练了OmniVerifier-7B模型。3. 提出了OmniVerifier-TTS，一种测试时序缩放范式，利用通用验证器增强生成能力。4. 将通用验证器扩展到更广泛的世界建模交错推理场景。

Result: 1. 现有视觉语言模型在ViVerBench上的表现不佳。2. OmniVerifier-7B在ViVerBench上实现了显著改进（+8.3）。3. OmniVerifier-TTS在T2I-ReasonBench（+3.7）和GenEval++（+4.3）上实现了改进，优于现有的并行测试时缩放方法。

Conclusion: 所提出的生成式通用验证器（GUV）及其实现OmniVerifier，通过可靠的视觉验证，显著提高了多模态推理的可靠性和可控性，从而在生成过程中实现了可靠的反思和可扩展的测试时细化。

Abstract: We introduce Generative Universal Verifier, a novel concept and plugin
designed for next-generation multimodal reasoning in vision-language models and
unified multimodal models, providing the fundamental capability of reflection
and refinement on visual outcomes during the reasoning and generation process.
This work makes three main contributions: (1) We build ViVerBench, a
comprehensive benchmark spanning 16 categories of critical tasks for evaluating
visual outcomes in multimodal reasoning. Results show that existing VLMs
consistently underperform across these tasks, underscoring a substantial gap
from human-level capability in reliable visual verification. (2) We design two
automated pipelines to construct large-scale visual verification data and train
OmniVerifier-7B, the first omni-capable generative verifier trained for
universal visual verification and achieves notable gains on ViVerBench(+8.3).
Through training, we identify three atomic capabilities in visual verification
and demonstrate how they generalize and interact synergistically. (3) We
propose OmniVerifier-TTS, a sequential test-time scaling paradigm that
leverages the universal verifier to bridge image generation and editing within
unified models, enhancing the upper bound of generative ability through
iterative fine-grained optimization. Beyond generation, we extend universal
verifier to broader world-modeling interleaved reasoning scenarios.
Empirically, OmniVerifier-TTS achieves improvements on T2I-ReasonBench(+3.7),
and GenEval++(+4.3), outperforming existing parallel test-time scaling methods,
such as Best-of-N. By endowing multimodal reasoning with reliable visual
verification, OmniVerifier advances both reliable reflection during generation
and scalable test-time refinement, marking a step toward more trustworthy and
controllable next-generation reasoning systems.

</details>


### [34] [DepthVLA: Enhancing Vision-Language-Action Models with Depth-Aware Spatial Reasoning](https://arxiv.org/abs/2510.13375)
*Tianyuan Yuan,Yicheng Liu,Chenhao Lu,Zhuoguang Chen,Tao Jiang,Hang Zhao*

Main category: cs.CV

TL;DR: DepthVLA是一种新型VLA模型，通过集成预训练深度预测模块和混合Transformer设计，显著提升了空间推理能力，并在真实世界和模拟环境中均超越了现有SOTA方法。


<details>
  <summary>Details</summary>
Motivation: 现有的视觉语言动作（VLA）模型在需要精确空间推理的任务上表现不佳，因为它们从视觉语言模型（VLM）继承的空间推理能力有限。现有VLA模型依赖大量的动作数据预训练来将VLM与3D空间关联起来，这不仅降低了训练效率，而且对于精确的空间理解仍然不足。

Method: 本文提出了DepthVLA，一种简单但有效的VLA架构，通过预训练的深度预测模块明确地融入空间感知。DepthVLA采用混合Transformer设计，统一了VLM、深度Transformer和动作专家，具有完全共享的注意力机制，形成了一个具有增强空间推理能力的端到端模型。

Result: 在现实世界和模拟环境中的大量评估表明，DepthVLA优于现有的SOTA方法，在现实世界任务中取得了78.5%的进展（对比此前的65.0%），在LIBERO模拟器中取得了94.9%的进展（对比此前的93.6%），在Simpler模拟器中取得了74.8%的进展（对比此前的58.8%）。

Conclusion: DepthVLA通过显式地整合空间感知和混合Transformer设计，成功解决了VLA模型在空间推理方面的不足，并在多个环境中取得了显著优于现有方法的性能。

Abstract: Vision-Language-Action (VLA) models have recently shown impressive
generalization and language-guided manipulation capabilities. However, their
performance degrades on tasks requiring precise spatial reasoning due to
limited spatial reasoning inherited from Vision-Language Models (VLMs).
Existing VLAs rely on extensive action-data pretraining to ground VLMs in 3D
space, which reduces training efficiency and is still insufficient for accurate
spatial understanding. In this work, we present DepthVLA, a simple yet
effective VLA architecture that explicitly incorporates spatial awareness
through a pretrained depth prediction module. DepthVLA adopts a
mixture-of-transformers design that unifies a VLM, a depth transformer, and an
action expert with fully shared attentions, forming an end-to-end model with
enhanced spatial reasoning. Extensive evaluations in both real-world and
simulated environments show that DepthVLA outperforms state-of-the-art
approaches, achieving 78.5% vs. 65.0% progress in real-world tasks, 94.9% vs.
93.6% in the LIBERO simulator, and 74.8% vs. 58.8% in the Simpler simulator.
Our code will be made publicly available.

</details>


### [35] [Spatial-DISE: A Unified Benchmark for Evaluating Spatial Reasoning in Vision-Language Models](https://arxiv.org/abs/2510.13394)
*Xinmiao Huang,Qisong He,Zhenglin Huang,Boxuan Wang,Zhuoyun Li,Guangliang Cheng,Yi Dong,Xiaowei Huang*

Main category: cs.CV

TL;DR: 这篇论文提出了Spatial-DISE，这是一个统一的基准，用于评估视觉语言模型（VLMs）在空间推理方面的能力，特别是在固有的动态空间推理方面。该研究还提供了一个名为Spatial-DISE的数据集。


<details>
  <summary>Details</summary>
Motivation: 空间推理能力对于视觉语言模型（VLMs）在现实世界的应用至关重要，但现有基准在评估空间推理能力方面存在不足，尤其是在人类空间认知的一个基本方面——固有动态空间推理方面。

Method: 本文提出了一个统一的基准Spatial-DISE，该基准基于认知基础分类法，将任务分为四个基本象限：固有-静态、固有-动态、外在-静态和外在-动态空间推理。为了解决数据稀缺问题，开发了一个可扩展的自动化流程，生成多样化和可验证的空间推理问题，从而创建了新的Spatial-DISE数据集，包括Spatial-DISE Bench（559个评估VQA对）和Spatial-DISE-12K（12K+训练VQA对）。

Result: 对28个最先进的VLM进行的综合评估表明，当前的VLM与人类能力之间存在巨大且持续的差距，特别是在多步骤多视图空间推理方面。

Conclusion: Spatial-DISE为未来VLM研究提供了一个强大的框架、有价值的数据集和明确的方向，以期实现类人空间智能。基准、数据集和代码将公开发布。

Abstract: Spatial reasoning ability is crucial for Vision Language Models (VLMs) to
support real-world applications in diverse domains including robotics,
augmented reality, and autonomous navigation. Unfortunately, existing
benchmarks are inadequate in assessing spatial reasoning ability, especially
the \emph{intrinsic-dynamic} spatial reasoning which is a fundamental aspect of
human spatial cognition. In this paper, we propose a unified benchmark,
\textbf{Spatial-DISE}, based on a cognitively grounded taxonomy that
categorizes tasks into four fundamental quadrants:
\textbf{I}ntrinsic-\textbf{S}tatic, Intrinsic-\textbf{D}ynamic,
\textbf{E}xtrinsic-Static, and Extrinsic-Dynamic spatial reasoning. Moreover,
to address the issue of data scarcity, we develop a scalable and automated
pipeline to generate diverse and verifiable spatial reasoning questions,
resulting in a new \textbf{Spatial-DISE} dataset that includes Spatial-DISE
Bench (559 evaluation VQA pairs) and Spatial-DISE-12K (12K+ training VQA
pairs). Our comprehensive evaluation across 28 state-of-the-art VLMs reveals
that, current VLMs have a large and consistent gap to human competence,
especially on multi-step multi-view spatial reasoning. Spatial-DISE offers a
robust framework, valuable dataset, and clear direction for future research
toward human-like spatial intelligence. Benchmark, dataset, and code will be
publicly released.

</details>


### [36] [Ultra High-Resolution Image Inpainting with Patch-Based Content Consistency Adapter](https://arxiv.org/abs/2510.13419)
*Jianhui Zhang,Sheng Cheng,Qirui Sun,Jia Liu,Wang Luyang,Chaoyu Feng,Chen Fang,Lei Lei,Jue Wang,Shuaicheng Liu*

Main category: cs.CV

TL;DR: Patch-Adapter 是一个用于高分辨率文本引导图像修复的框架。它克服了现有方法在分辨率上的限制，在保持内容一致性和提示对齐的同时，实现了 4K+ 分辨率的图像修复。


<details>
  <summary>Details</summary>
Motivation: 现有的图像修复方法在处理高分辨率图像时表现不佳，常常无法保持内容的精确一致性和提示的准确对齐，尤其是在图像分辨率和纹理复杂性增加时问题更加突出。

Method: Patch-Adapter 采用两阶段适配器架构，将扩散模型的图像修复分辨率从 1K 提升到 4K+：1. 双重上下文适配器：在较低分辨率下学习遮罩区域和非遮罩区域之间的一致性，以建立全局结构连贯性。 2. 参考补丁适配器：在高分辨率下，通过自适应特征融合实现补丁级别的注意力机制，从而保留局部细节。

Result: Patch-Adapter 解决了大规模图像修复中常见的伪影问题，并在 OpenImages 和 Photo-Concept-Bucket 数据集上取得了最先进的性能，在感知质量和文本提示遵循方面均优于现有方法。

Conclusion: Patch-Adapter 通过解耦全局语义与局部细化，独特地解决了高分辨率图像修复中的可伸缩性问题，提高了图像修复的质量和效率。

Abstract: In this work, we present Patch-Adapter, an effective framework for
high-resolution text-guided image inpainting. Unlike existing methods limited
to lower resolutions, our approach achieves 4K+ resolution while maintaining
precise content consistency and prompt alignment, two critical challenges in
image inpainting that intensify with increasing resolution and texture
complexity. Patch-Adapter leverages a two-stage adapter architecture to scale
the diffusion model's resolution from 1K to 4K+ without requiring structural
overhauls: (1) Dual Context Adapter learns coherence between masked and
unmasked regions at reduced resolutions to establish global structural
consistency; and (2) Reference Patch Adapter implements a patch-level attention
mechanism for full-resolution inpainting, preserving local detail fidelity
through adaptive feature fusion. This dual-stage architecture uniquely
addresses the scalability gap in high-resolution inpainting by decoupling
global semantics from localized refinement. Experiments demonstrate that
Patch-Adapter not only resolves artifacts common in large-scale inpainting but
also achieves state-of-the-art performance on the OpenImages and
Photo-Concept-Bucket datasets, outperforming existing methods in both
perceptual quality and text-prompt adherence.

</details>


### [37] [CoDS: Enhancing Collaborative Perception in Heterogeneous Scenarios via Domain Separation](https://arxiv.org/abs/2510.13432)
*Yushan Han,Hui Zhang,Honglei Zhang,Chuntao Ding,Yuanzhouhan Cao,Yidong Li*

Main category: cs.CV

TL;DR: 这篇论文提出了一种名为 CoDS 的协同感知方法，通过域分离解决异构场景中的特征差异，并在检测准确性和推理效率之间实现了权衡。


<details>
  <summary>Details</summary>
Motivation: 在自动驾驶中，协同感知能够通过多智能体交互提高个体感知能力。然而，现有方法通常假设所有智能体都使用相同的编码器，这在实际异构场景中不成立。现有的方法在实际异构场景中通常会使邻居特征与自我车辆的特征对齐，但这容易受到域间隙噪声的影响，并且不能有效解决特征差异。此外，它们采用基于 Transformer 的模块进行域适应，这导致模型在移动设备上的推理效率低下。

Method: CoDS 协同感知方法利用域分离来解决异构场景中的特征差异。它采用了两个特征对齐模块：轻量级空间通道调整器（LSCR）和通过域分离的分布对齐（DADS）。此外，它还使用域对齐互信息（DAMI）损失来确保有效的特征对齐。LSCR 使用轻量级卷积层对空间和通道维度上的邻居特征进行对齐。DADS 通过特定于编码器和独立于编码器的域分离模块来缓解特征分布差异。前者消除域相关信息，后者捕获与任务相关的信息。在训练期间，DAMI 损失最大化对齐的异构特征之间的互信息，以增强域分离过程。

Result: CoDS 有效地缓解了异构场景中的特征差异，并在检测准确性和推理效率之间实现了权衡。

Conclusion: CoDS 方法通过创新的特征对齐模块和损失函数，成功解决了异构协同感知中的特征差异和推理效率问题，为实际部署提供了有效的解决方案。

Abstract: Collaborative perception has been proven to improve individual perception in
autonomous driving through multi-agent interaction. Nevertheless, most methods
often assume identical encoders for all agents, which does not hold true when
these models are deployed in real-world applications. To realize collaborative
perception in actual heterogeneous scenarios, existing methods usually align
neighbor features to those of the ego vehicle, which is vulnerable to noise
from domain gaps and thus fails to address feature discrepancies effectively.
Moreover, they adopt transformer-based modules for domain adaptation, which
causes the model inference inefficiency on mobile devices. To tackle these
issues, we propose CoDS, a Collaborative perception method that leverages
Domain Separation to address feature discrepancies in heterogeneous scenarios.
The CoDS employs two feature alignment modules, i.e., Lightweight
Spatial-Channel Resizer (LSCR) and Distribution Alignment via Domain Separation
(DADS). Besides, it utilizes the Domain Alignment Mutual Information (DAMI)
loss to ensure effective feature alignment. Specifically, the LSCR aligns the
neighbor feature across spatial and channel dimensions using a lightweight
convolutional layer. Subsequently, the DADS mitigates feature distribution
discrepancy with encoder-specific and encoder-agnostic domain separation
modules. The former removes domain-dependent information and the latter
captures task-related information. During training, the DAMI loss maximizes the
mutual information between aligned heterogeneous features to enhance the domain
separation process. The CoDS employs a fully convolutional architecture, which
ensures high inference efficiency. Extensive experiments demonstrate that the
CoDS effectively mitigates feature discrepancies in heterogeneous scenarios and
achieves a trade-off between detection accuracy and inference efficiency.

</details>


### [38] [VIST3A: Text-to-3D by Stitching a Multi-view Reconstruction Network to a Video Generator](https://arxiv.org/abs/2510.13454)
*Hyojun Go,Dominik Narnhofer,Goutam Bhat,Prune Truong,Federico Tombari,Konrad Schindler*

Main category: cs.CV

TL;DR: VIST3A是一个结合了潜在文本到视频模型和3D重建系统的通用框架，旨在实现文本到3D场景生成。它通过模型拼接和直接奖励微调来应对组件连接和对齐的挑战，实验证明其在文本到3D生成方面优于现有模型，并支持高质量的文本到点云生成。


<details>
  <summary>Details</summary>
Motivation: 探索如何结合大型预训练模型在视觉内容生成和3D重建方面的进展，以实现文本到3D生成。

Method: VIST3A框架通过以下两种方式结合文本到视频生成器和3D重建系统：1. 模型拼接：识别3D解码器中与文本到视频生成器产生的潜在表示最匹配的层，并将两者拼接在一起，此过程仅需少量无标签数据集。2. 对齐：采用直接奖励微调技术，使生成器与拼接后的3D解码器对齐，确保生成的潜在内容可以解码为一致且视觉逼真的3D场景几何体。

Result: VIST3A显著优于之前输出高斯斑点的文本到3D模型。通过选择合适的3D基础模型，VIST3A还能够实现高质量的文本到点云生成。

Conclusion: VIST3A框架成功地结合了文本到视频生成器和3D重建系统，克服了组件连接和对齐的挑战，实现了在文本到3D生成方面的显著改进，并支持高质量的文本到点云生成。

Abstract: The rapid progress of large, pretrained models for both visual content
generation and 3D reconstruction opens up new possibilities for text-to-3D
generation. Intuitively, one could obtain a formidable 3D scene generator if
one were able to combine the power of a modern latent text-to-video model as
"generator" with the geometric abilities of a recent (feedforward) 3D
reconstruction system as "decoder". We introduce VIST3A, a general framework
that does just that, addressing two main challenges. First, the two components
must be joined in a way that preserves the rich knowledge encoded in their
weights. We revisit model stitching, i.e., we identify the layer in the 3D
decoder that best matches the latent representation produced by the
text-to-video generator and stitch the two parts together. That operation
requires only a small dataset and no labels. Second, the text-to-video
generator must be aligned with the stitched 3D decoder, to ensure that the
generated latents are decodable into consistent, perceptually convincing 3D
scene geometry. To that end, we adapt direct reward finetuning, a popular
technique for human preference alignment. We evaluate the proposed VIST3A
approach with different video generators and 3D reconstruction models. All
tested pairings markedly improve over prior text-to-3D models that output
Gaussian splats. Moreover, by choosing a suitable 3D base model, VIST3A also
enables high-quality text-to-pointmap generation.

</details>


### [39] [XD-RCDepth: Lightweight Radar-Camera Depth Estimation with Explainability-Aligned and Distribution-Aware Distillation](https://arxiv.org/abs/2510.13565)
*Huawei Sun,Zixu Wang,Xiangyuan Peng,Julius Ott,Georg Stettinger,Lorenzo Servadei,Robert Wille*

Main category: cs.CV

TL;DR: 本文提出了一种轻量级的雷达相机融合深度估计架构XD-RCDepth，在保持与最先进的轻量级基线相当的准确性的同时，参数减少了29.7%。


<details>
  <summary>Details</summary>
Motivation: 为了在恶劣条件下提供鲁棒的几何线索，雷达相机融合在自动驾驶深度估计中至关重要。

Method: 本文引入了两种知识蒸馏策略：可解释性对齐蒸馏和深度分布蒸馏。可解释性对齐蒸馏将教师模型的显著性结构转移到学生模型，而深度分布蒸馏将深度回归转换为离散化的软分类。

Result: 与直接训练相比，本文提出的方法将平均绝对误差（MAE）降低了7.97%，并在nuScenes和ZJU-4DRadarCam数据集上实现了具有实时效率的竞争性精度。

Conclusion: XD-RCDepth在参数量显著减少的情况下，通过引入两种知识蒸馏策略，在雷达相机融合深度估计任务中取得了与现有技术相当的性能，并提高了模型的解释性。

Abstract: Depth estimation remains central to autonomous driving, and radar-camera
fusion offers robustness in adverse conditions by providing complementary
geometric cues. In this paper, we present XD-RCDepth, a lightweight
architecture that reduces the parameters by 29.7% relative to the
state-of-the-art lightweight baseline while maintaining comparable accuracy. To
preserve performance under compression and enhance interpretability, we
introduce two knowledge-distillation strategies: an explainability-aligned
distillation that transfers the teacher's saliency structure to the student,
and a depth-distribution distillation that recasts depth regression as soft
classification over discretized bins. Together, these components reduce the MAE
compared with direct training with 7.97% and deliver competitive accuracy with
real-time efficiency on nuScenes and ZJU-4DRadarCam datasets.

</details>


### [40] [AVAR-Net: A Lightweight Audio-Visual Anomaly Recognition Framework with a Benchmark Dataset](https://arxiv.org/abs/2510.13630)
*Amjid Ali,Zulfiqar Ahmad Khan,Altaf Hussain,Muhammad Munsif,Adnan Hussain,Sung Wook Baik*

Main category: cs.CV

TL;DR: 该文章介紹了AVAR-Net，一個輕量高效的音頻-視覺異常識別框架，並引入了VAAR數據集，用於解決現有方法在挑戰性條件下的不可靠性以及多模態異常識別中缺乏大規模同步音頻-視覺數據集的問題。


<details>
  <summary>Details</summary>
Motivation: 現有的異常識別方法主要依賴視覺數據，在遮擋、低光照和惡劣天氣等挑戰性條件下不可靠。此外，缺乏大規模同步音頻-視覺數據集阻礙了多模態異常識別的進展。

Method: AVAR-Net框架包含四個主要模塊：音頻特徵提取器（使用Wav2Vec2）、視頻特徵提取器（使用MobileViT）、融合策略（早期融合）和序列模式學習網絡（多階段時間卷積網絡MTCN）。文章還引入了VAAR數據集，包含3,000個真實世界的視頻和同步音頻。

Result: AVAR-Net在VAAR數據集上達到了89.29%的準確率，在XD-Violence數據集上平均精度達到88.56%，比現有最先進方法提高了2.8%。

Conclusion: AVAR-Net框架在有效性、效率和泛化能力方面表現出色，並且VAAR數據集為多模態異常識別研究提供了有用的基準。

Abstract: Anomaly recognition plays a vital role in surveillance, transportation,
healthcare, and public safety. However, most existing approaches rely solely on
visual data, making them unreliable under challenging conditions such as
occlusion, low illumination, and adverse weather. Moreover, the absence of
large-scale synchronized audio-visual datasets has hindered progress in
multimodal anomaly recognition. To address these limitations, this study
presents AVAR-Net, a lightweight and efficient audio-visual anomaly recognition
framework designed for real-world environments. AVAR-Net consists of four main
modules: an audio feature extractor, a video feature extractor, fusion
strategy, and a sequential pattern learning network that models cross-modal
relationships for anomaly recognition. Specifically, the Wav2Vec2 model
extracts robust temporal features from raw audio, while MobileViT captures both
local and global visual representations from video frames. An early fusion
mechanism combines these modalities, and a Multi-Stage Temporal Convolutional
Network (MTCN) model that learns long-range temporal dependencies within the
fused representation, enabling robust spatiotemporal reasoning. A novel
Visual-Audio Anomaly Recognition (VAAR) dataset, is also introduced, serving as
a medium-scale benchmark containing 3,000 real-world videos with synchronized
audio across ten diverse anomaly classes. Experimental evaluations demonstrate
that AVAR-Net achieves 89.29% accuracy on VAAR and 88.56% Average Precision on
the XD-Violence dataset, improving Average Precision by 2.8% over existing
state-of-the-art methods. These results highlight the effectiveness,
efficiency, and generalization capability of the proposed framework, as well as
the utility of VAAR as a benchmark for advancing multimodal anomaly recognition
research.

</details>


### [41] [EditCast3D: Single-Frame-Guided 3D Editing with Video Propagation and View Selection](https://arxiv.org/abs/2510.13652)
*Huaizhi Qu,Ruichen Zhang,Shuqing Luo,Luchao Qi,Zhihao Zhang,Xiaoming Liu,Roni Sengupta,Tianlong Chen*

Main category: cs.CV

TL;DR: 本文提出了EditCast3D，一个利用视频生成基础模型，通过从单个帧传播编辑到整个数据集，从而实现高效和高质量3D编辑的流水线。


<details>
  <summary>Details</summary>
Motivation: 目前，现有基础模型在3D编辑领域的应用还未被充分探索，且将这些模型直接集成到现有的迭代编辑策略中存在计算成本高昂和API限制等问题。

Method: EditCast3D通过使用视频生成基础模型将编辑从单个首帧传播到整个数据集，并在重建之前完成此步骤。为了解决3D重建中多视角对齐的一致性问题，EditCast3D引入了视角选择策略来识别一致且有利于重建的视角，并采用前馈重建，避免了昂贵的优化过程。

Result: EditCast3D在常用的3D编辑数据集上进行了评估，并与最先进的3D编辑基线进行了比较，结果表明其在编辑质量和效率方面都表现出了卓越的性能。

Conclusion: EditCast3D为将基础模型集成到3D编辑流水线提供了一个可扩展且通用的范式。

Abstract: Recent advances in foundation models have driven remarkable progress in image
editing, yet their extension to 3D editing remains underexplored. A natural
approach is to replace the image editing modules in existing workflows with
foundation models. However, their heavy computational demands and the
restrictions and costs of closed-source APIs make plugging these models into
existing iterative editing strategies impractical. To address this limitation,
we propose EditCast3D, a pipeline that employs video generation foundation
models to propagate edits from a single first frame across the entire dataset
prior to reconstruction. While editing propagation enables dataset-level
editing via video models, its consistency remains suboptimal for 3D
reconstruction, where multi-view alignment is essential. To overcome this,
EditCast3D introduces a view selection strategy that explicitly identifies
consistent and reconstruction-friendly views and adopts feedforward
reconstruction without requiring costly refinement. In combination, the
pipeline both minimizes reliance on expensive image editing and mitigates
prompt ambiguities that arise when applying foundation models independently
across images. We evaluate EditCast3D on commonly used 3D editing datasets and
compare it against state-of-the-art 3D editing baselines, demonstrating
superior editing quality and high efficiency. These results establish
EditCast3D as a scalable and general paradigm for integrating foundation models
into 3D editing pipelines. The code is available at
https://github.com/UNITES-Lab/EditCast3D

</details>


### [42] [OmniGaze: Reward-inspired Generalizable Gaze Estimation In The Wild](https://arxiv.org/abs/2510.13660)
*Hongyu Qu,Jianan Wei,Xiangbo Shu,Yazhou Yao,Wenguan Wang,Jinhui Tang*

Main category: cs.CV

TL;DR: OmniGaze是一个半监督的3D注视估计框架，它利用大规模未标记数据来缓解域偏差并在野外泛化注视估计。


<details>
  <summary>Details</summary>
Motivation: 当前3D注视估计方法难以在不同数据域之间进行泛化，主要是由于i)带注释数据集的稀缺性，以及ii)标记数据多样性不足。

Method: 我们构建了一个多样化的未标注人脸图像集。OmniGaze采用标准的伪标记策略，并设计了一个奖励模型来评估伪标记的可靠性。奖励模型将视觉嵌入和来自注视视角的语义线索结合起来计算置信度分数，然后利用这些分数选择高质量的伪标记并对其进行加权以进行损失计算。

Result: OmniGaze在域内和跨域设置下的五个数据集上均达到了最先进的性能。OmniGaze作为注视估计的可扩展数据引擎，在四个未见过的数据集上表现出强大的零样本泛化能力。

Conclusion: OmniGaze是一种新颖的半监督学习框架，通过有效利用大规模未标记数据，显著提升了3D注视估计在真实世界场景中的泛化能力和准确性。

Abstract: Current 3D gaze estimation methods struggle to generalize across diverse data
domains, primarily due to i) the scarcity of annotated datasets, and ii) the
insufficient diversity of labeled data. In this work, we present OmniGaze, a
semi-supervised framework for 3D gaze estimation, which utilizes large-scale
unlabeled data collected from diverse and unconstrained real-world environments
to mitigate domain bias and generalize gaze estimation in the wild. First, we
build a diverse collection of unlabeled facial images, varying in facial
appearances, background environments, illumination conditions, head poses, and
eye occlusions. In order to leverage unlabeled data spanning a broader
distribution, OmniGaze adopts a standard pseudo-labeling strategy and devises a
reward model to assess the reliability of pseudo labels. Beyond pseudo labels
as 3D direction vectors, the reward model also incorporates visual embeddings
extracted by an off-the-shelf visual encoder and semantic cues from gaze
perspective generated by prompting a Multimodal Large Language Model to compute
confidence scores. Then, these scores are utilized to select high-quality
pseudo labels and weight them for loss computation. Extensive experiments
demonstrate that OmniGaze achieves state-of-the-art performance on five
datasets under both in-domain and cross-domain settings. Furthermore, we also
evaluate the efficacy of OmniGaze as a scalable data engine for gaze
estimation, which exhibits robust zero-shot generalization on four unseen
datasets.

</details>


### [43] [Seeing and Knowing in the Wild: Open-domain Visual Entity Recognition with Large-scale Knowledge Graphs via Contrastive Learning](https://arxiv.org/abs/2510.13675)
*Hongkuan Zhou,Lavdim Halilaj,Sebastian Monka,Stefan Schmid,Yuqicheng Zhu,Jingcheng Wu,Nadeem Nazer,Steffen Staab*

Main category: cs.CV

TL;DR: 这篇论文提出了一种名为KnowCoL的知识引导对比学习框架，用于开放域视觉实体识别。该框架将图像和文本描述结合到共享语义空间中，并利用维基数据中的结构化信息，显著提高了对稀有和未见实体的识别准确性，尤其是在处理未见实体时，相比现有技术，准确率提升了10.5%。


<details>
  <summary>Details</summary>
Motivation: 开放域视觉实体识别面临挑战，因为实体数量庞大且不断演变，大多在训练中未被见到，并且呈现长尾分布，导致监督信息有限、视觉模糊性高以及需要语义消歧。

Method: 本研究提出了知识引导对比学习（KnowCoL）框架，该框架将图像和文本描述整合到由维基数据结构化信息支持的共享语义空间中。通过将视觉和文本输入抽象到概念层面，模型利用实体描述、类型层级和关系上下文来支持零样本实体识别。

Result: 在OVEN基准测试集上的实验表明，结合视觉、文本和结构化知识可以显著提高准确性，特别是对于稀有和未见实体。我们最小的模型在未见实体上的准确率比现有技术提高了10.5%，尽管其模型体积小了35倍。

Conclusion: KnowCoL框架通过整合多模态和结构化知识，有效解决了开放域视觉实体识别中的挑战，尤其在处理稀有和未见实体方面表现出色，为未来的研究提供了新的方向。

Abstract: Open-domain visual entity recognition aims to identify and link entities
depicted in images to a vast and evolving set of real-world concepts, such as
those found in Wikidata. Unlike conventional classification tasks with fixed
label sets, it operates under open-set conditions, where most target entities
are unseen during training and exhibit long-tail distributions. This makes the
task inherently challenging due to limited supervision, high visual ambiguity,
and the need for semantic disambiguation. In this work, we propose a
Knowledge-guided Contrastive Learning (KnowCoL) framework that combines both
images and text descriptions into a shared semantic space grounded by
structured information from Wikidata. By abstracting visual and textual inputs
to a conceptual level, the model leverages entity descriptions, type
hierarchies, and relational context to support zero-shot entity recognition. We
evaluate our approach on the OVEN benchmark, a large-scale open-domain visual
recognition dataset with Wikidata IDs as the label space. Our experiments show
that using visual, textual, and structured knowledge greatly improves accuracy,
especially for rare and unseen entities. Our smallest model improves the
accuracy on unseen entities by 10.5% compared to the state-of-the-art, despite
being 35 times smaller.

</details>


### [44] [FlashWorld: High-quality 3D Scene Generation within Seconds](https://arxiv.org/abs/2510.13678)
*Xinyang Li,Tengfei Wang,Zixiao Gu,Shengchuan Zhang,Chunchao Guo,Liujuan Cao*

Main category: cs.CV

TL;DR: FlashWorld是一种新颖的生成模型，能够以比现有方法快10-100倍的速度，从单一图像或文本提示中在几秒钟内生成高质量的3D场景。


<details>
  <summary>Details</summary>
Motivation: 传统的3D场景生成方法通常采用多视角导向（MV-oriented）范式，即先生成多视角图像再进行3D重建，但这种方法速度较慢。而3D导向方法虽然能保证3D一致性，但视觉质量较差。因此，需要一种新的方法来解决现有方法的效率和质量问题。

Method: FlashWorld通过双模式预训练和跨模式后训练阶段，有效地结合了多视角导向和3D导向两种范式的优点。首先，利用视频扩散模型的先验知识，预训练一个双模式多视角扩散模型，支持多视角导向和3D导向两种生成模式。为了弥补3D导向生成在视觉质量上的不足，进一步提出了跨模式后训练蒸馏，通过将一致的3D导向模式与高质量的多视角导向模式的分布进行匹配。同时，该方法还利用大量的单视角图像和文本提示来增强模型对分布外输入的泛化能力。

Result: FlashWorld在保持3D一致性的同时，显著提升了视觉质量，并且减少了推理所需的去噪步骤。实验证明，该方法在速度和渲染质量方面均优于现有方法，生成3D场景的速度比以往工作快10-100倍。

Conclusion: FlashWorld通过创新的双模式预训练和跨模式后训练方法，成功地将多视角导向和3D导向的优点结合起来，极大地提高了3D场景生成的效率和质量，为从单一图像或文本生成高质量3D场景提供了一个强大的解决方案。

Abstract: We propose FlashWorld, a generative model that produces 3D scenes from a
single image or text prompt in seconds, 10~100$\times$ faster than previous
works while possessing superior rendering quality. Our approach shifts from the
conventional multi-view-oriented (MV-oriented) paradigm, which generates
multi-view images for subsequent 3D reconstruction, to a 3D-oriented approach
where the model directly produces 3D Gaussian representations during multi-view
generation. While ensuring 3D consistency, 3D-oriented method typically suffers
poor visual quality. FlashWorld includes a dual-mode pre-training phase
followed by a cross-mode post-training phase, effectively integrating the
strengths of both paradigms. Specifically, leveraging the prior from a video
diffusion model, we first pre-train a dual-mode multi-view diffusion model,
which jointly supports MV-oriented and 3D-oriented generation modes. To bridge
the quality gap in 3D-oriented generation, we further propose a cross-mode
post-training distillation by matching distribution from consistent 3D-oriented
mode to high-quality MV-oriented mode. This not only enhances visual quality
while maintaining 3D consistency, but also reduces the required denoising steps
for inference. Also, we propose a strategy to leverage massive single-view
images and text prompts during this process to enhance the model's
generalization to out-of-distribution inputs. Extensive experiments demonstrate
the superiority and efficiency of our method.

</details>


### [45] [Risk-adaptive Activation Steering for Safe Multimodal Large Language Models](https://arxiv.org/abs/2510.13698)
*Jonghyun Park,Minhyuk Seo,Jonghyun Choi*

Main category: cs.CV

TL;DR: 本文提出了一种名为RAS（Risk-adaptive Activation Steering）的方法，旨在解决多模态AI模型在安全性和实用性方面的挑战。RAS通过重新构建查询，强化对安全关键图像区域的跨模态关注，从而实现准确的查询级别风险评估，并根据评估结果自适应地引导激活，生成安全且有用的响应，且无需迭代输出调整的开销。


<details>
  <summary>Details</summary>
Motivation: 现代AI模型面临的关键挑战是如何确保它们对良性查询提供有益的回复，同时拒绝恶意查询。然而，模型通常容易受到图像中嵌入有害意图的多模态查询的攻击。现有的安全对齐方法存在数据集构建和训练成本高昂、推理时对齐可能导致过度拒绝和推理速度变慢等问题。

Method: 本文提出了一种风险自适应激活引导（RAS）方法。该方法通过重新构建查询，以强化跨模态对安全关键图像区域的关注，从而在查询级别实现准确的风险评估。然后，根据评估的风险，它自适应地引导激活，以生成安全且有帮助的响应，而不会产生迭代输出调整的开销。

Result: 在多模态安全性和实用性的多个基准测试中进行的广泛实验表明，RAS显著降低了攻击成功率，保持了通用任务性能，并提高了推理速度。

Conclusion: RAS方法有效地解决了多模态AI模型在安全对齐方面的挑战，在降低攻击成功率、保持性能和提高推理速度方面取得了显著效果。

Abstract: One of the key challenges of modern AI models is ensuring that they provide
helpful responses to benign queries while refusing malicious ones. But often,
the models are vulnerable to multimodal queries with harmful intent embedded in
images. One approach for safety alignment is training with extensive safety
datasets at the significant costs in both dataset curation and training.
Inference-time alignment mitigates these costs, but introduces two drawbacks:
excessive refusals from misclassified benign queries and slower inference speed
due to iterative output adjustments. To overcome these limitations, we propose
to reformulate queries to strengthen cross-modal attention to safety-critical
image regions, enabling accurate risk assessment at the query level. Using the
assessed risk, it adaptively steers activations to generate responses that are
safe and helpful without overhead from iterative output adjustments. We call
this Risk-adaptive Activation Steering (RAS). Extensive experiments across
multiple benchmarks on multimodal safety and utility demonstrate that the RAS
significantly reduces attack success rates, preserves general task performance,
and improves inference speed over prior inference-time defenses.

</details>


### [46] [LiFMCR: Dataset and Benchmark for Light Field Multi-Camera Registration](https://arxiv.org/abs/2510.13729)
*Aymeric Fleith,Julian Zirbel,Daniel Cremers,Niclas Zeller*

Main category: cs.CV

TL;DR: LiFMCR是一个用于多微透镜阵列（MLA）光场相机配准的新颖数据集，它提供同步图像序列和高精度6自由度姿态，并集成了光场相机模型以实现准确高效的多相机配准。


<details>
  <summary>Details</summary>
Motivation: 现有的光场数据集通常仅限于单相机设置且缺乏外部真值，这限制了多相机光场配准方法的研究与评估。

Method: LiFMCR数据集提供来自两个高分辨率Raytrix R32全光相机的同步图像序列，以及由Vicon运动捕捉系统记录的高精度6自由度（DoF）姿态。该论文提供了两种互补的配准方法作为基线：一种是通过基于RANSAC的方法使用交叉视图点云进行鲁棒的3D变换估计；另一种是全光PnP算法，从单个光场图像估计外部6自由度姿态。两种方法都明确集成了全光相机模型。

Result: 实验结果表明，该方法与真值具有很强的一致性，支持可靠的多视图光场处理。

Conclusion: LiFMCR数据集及其提供的基线方法能够有效促进多相机光场配准技术的发展和评估。

Abstract: We present LiFMCR, a novel dataset for the registration of multiple micro
lens array (MLA)-based light field cameras. While existing light field datasets
are limited to single-camera setups and typically lack external ground truth,
LiFMCR provides synchronized image sequences from two high-resolution Raytrix
R32 plenoptic cameras, together with high-precision 6-degrees of freedom (DoF)
poses recorded by a Vicon motion capture system. This unique combination
enables rigorous evaluation of multi-camera light field registration methods.
  As a baseline, we provide two complementary registration approaches: a robust
3D transformation estimation via a RANSAC-based method using cross-view point
clouds, and a plenoptic PnP algorithm estimating extrinsic 6-DoF poses from
single light field images. Both explicitly integrate the plenoptic camera
model, enabling accurate and scalable multi-camera registration. Experiments
show strong alignment with the ground truth, supporting reliable multi-view
light field processing.
  Project page: https://lifmcr.github.io/

</details>


### [47] [InteractiveOmni: A Unified Omni-modal Model for Audio-Visual Multi-turn Dialogue](https://arxiv.org/abs/2510.13747)
*Wenwen Tong,Hewei Guo,Dongchuan Ran,Jiangnan Chen,Jiefan Lu,Kaibin Wang,Keqiang Li,Xiaoxu Zhu,Jiakui Li,Kehan Li,Xueheng Li,Lumin Li,Chenxu Guo,Jiasheng Zhou,Jiandong Chen,Xianye Wu,Jiahao Wang,Silei Wu,Lei Chen,Hanming Deng,Yuxuan Song,Dinghao Zhou,Guiping Zhong,Ken Zheng,Shiyin Kang,Lewei Lu*

Main category: cs.CV

TL;DR: InteractiveOmni是一个统一的、开源的、全模态大语言模型，支持音视频多轮交互，参数范围从4B到8B。它通过集成视觉编码器、音频编码器、大语言模型和语音解码器，实现了全面的全模态理解和语音生成能力。模型采用多阶段训练策略，并通过精心策划的多轮训练数据集提升了长程对话能力。在评估方面，构建了多模态多轮内存基准和多轮语音交互基准。实验证明，InteractiveOmni在性能上显著优于领先的开源模型，尤其在长程记忆能力方面表现出色，其中4B版本在通用基准测试上可与7B模型媲美，且在同等规模模型中达到了最先进的水平。


<details>
  <summary>Details</summary>
Motivation: 目前缺乏一个统一的、开源的、轻量级的全模态大语言模型，能够同时提供全面的全模态理解和语音生成能力，并支持长程、多轮的音视频交互。

Method: InteractiveOmni通过整合视觉编码器、音频编码器、大型语言模型和语音解码器，构建了一个统一模型。模型采用多阶段训练策略，包括全模态理解的预训练以及语音对话和音视频交互的后训练。为了增强模型的长程对话能力，项目组精心整理了多轮训练数据集。为了有效评估多轮记忆和语音交互能力，项目组还构建了多模态多轮记忆基准和多轮语音交互基准。

Result: InteractiveOmni在性能上显著优于领先的开源模型，提供了更智能的多轮音视频交互体验，尤其在长程记忆能力方面表现突出。InteractiveOmni-4B在通用基准上与更大的模型（如Qwen2.5-Omni-7B）性能相当，并且在模型尺寸减半的情况下，仍能保持InteractiveOmni-8B 97%的性能。在图像、音频、视频理解和语音生成任务上，InteractiveOmni在同等规模的模型中取得了最先进的成果。

Conclusion: InteractiveOmni是一个统一的、开源的、轻量级全模态大语言模型，通过创新的架构和训练策略，在全模态理解、语音生成和长程多轮交互方面取得了显著进展。其卓越的性能和高效性使其成为下一代智能交互系统的可及且开源的基础。

Abstract: We introduce InteractiveOmni, a unified and open-source omni-modal large
language model for audio-visual multi-turn interaction, ranging from 4B to 8B
parameters, designed to lead the field of lightweight models by offering
comprehensive omni-modal understanding and speech generation capabilities. To
achieve this, we integrate the vision encoder, audio encoder, large language
model, and speech decoder into a unified model for understanding and generation
tasks. We design a multi-stage training strategy to ensure robust cross-modal
capabilities, including pre-training for omni-modal understanding, followed by
post-training with speech conversation and audio-visual interaction. To enable
human-like long-term conversational ability, we meticulously curate a
multi-turn training dataset that enhances the model's ability to handle complex
and multi-turn interactions. To effectively evaluate the multi-turn memory and
speech interaction capabilities, we construct the multi-modal multi-turn memory
benchmark and the multi-turn speech interaction benchmark. Experiments
demonstrate that InteractiveOmni significantly outperforms leading open-source
models and provides a more intelligent multi-turn audio-visual experience,
particularly in its long-term memory capabilities. Notably, InteractiveOmni-4B
is comparable to the much larger model like Qwen2.5-Omni-7B on general
benchmarks, and it can retain 97% of the performance of the InteractiveOmni-8B
while utilizing only 50% of the model size. Achieving state-of-the-art results
against similarly sized models across image, audio, video understanding, and
speech generation tasks, InteractiveOmni is an accessible, open-source
foundation for next-generation intelligent interactive systems.

</details>


### [48] [Uni-MMMU: A Massive Multi-discipline Multimodal Unified Benchmark](https://arxiv.org/abs/2510.13759)
*Kai Zou,Ziqi Huang,Yuhao Dong,Shulin Tian,Dian Zheng,Hongbo Liu,Jingwen He,Bin Liu,Yu Qiao,Ziwei Liu*

Main category: cs.CV

TL;DR: Uni-MMMU 是一个全面的、跨学科的基准测试集，旨在评估统一多模态模型在视觉理解和生成双向协同方面的能力。它包含八个以推理为中心的领域，每个任务都双向耦合，要求模型利用概念理解来指导精确的视觉合成，或利用生成作为分析推理的认知支架。


<details>
  <summary>Details</summary>
Motivation: 当前基准测试很少能真正检验统一多模态模型的视觉理解和生成能力的整合，现有评估要么孤立地处理这两种能力，要么忽略了那些本质上将它们耦合在一起的任务。

Method: Uni-MMMU 基准测试集通过八个以推理为中心的领域（包括科学、编码、数学和谜题）系统地揭示了生成和理解之间的双向协同作用。每个任务都是双向耦合的，要求模型利用概念理解来指导精确的视觉合成，或利用生成作为分析推理的认知支架。Uni-MMMU 包含可验证的中间推理步骤、独特的基准事实以及针对文本和视觉输出的可重现评分协议。

Result: 通过对最先进的统一模型、仅生成模型和仅理解模型进行广泛评估，研究揭示了显著的性能差异和跨模态依赖性。

Conclusion: Uni-MMMU 为推进统一多模态模型奠定了可靠的基础，并为这些能力何时以及如何相互增强提供了新的见解。

Abstract: Unified multimodal models aim to jointly enable visual understanding and
generation, yet current benchmarks rarely examine their true integration.
Existing evaluations either treat the two abilities in isolation or overlook
tasks that inherently couple them. To address this gap, we present Uni-MMMU, a
comprehensive and discipline-aware benchmark that systematically unfolds the
bidirectional synergy between generation and understanding across eight
reasoning-centric domains, including science, coding, mathematics, and puzzles.
Each task is bidirectionally coupled, demanding models to (i) leverage
conceptual understanding to guide precise visual synthesis, or (ii) utilize
generation as a cognitive scaffold for analytical reasoning. Uni-MMMU
incorporates verifiable intermediate reasoning steps, unique ground truths, and
a reproducible scoring protocol for both textual and visual outputs. Through
extensive evaluation of state-of-the-art unified, generation-only, and
understanding-only models, we reveal substantial performance disparities and
cross-modal dependencies, offering new insights into when and how these
abilities reinforce one another, and establishing a reliable foundation for
advancing unified models.

</details>


### [49] [NoisePrints: Distortion-Free Watermarks for Authorship in Private Diffusion Models](https://arxiv.org/abs/2510.13793)
*Nir Goren,Oren Katzir,Abhinav Nakarmi,Eyal Ronen,Mahmood Sharif,Or Patashnik*

Main category: cs.CV

TL;DR: 这篇论文提出了一种轻量级水印方案NoisePrints，它利用扩散模型中用于初始化扩散过程的随机种子作为作者身份证明，而无需修改生成过程，从而解决了现有方法需要访问模型权重且计算成本高昂的问题。


<details>
  <summary>Details</summary>
Motivation: 随着扩散模型在视觉内容生成中的广泛应用，证明作品归属权和保护版权变得至关重要。当模型所有者不愿或无法处理作者身份问题时，第三方验证变得必不可少。现有水印方法需要访问模型权重且计算量大，不切实际且不可扩展。

Method: NoisePrints方案的核心思想是利用随机种子初始化扩散过程，并通过在噪声采样过程中加入哈希函数，确保从内容中恢复有效种子或伪造通过验证的替代种子是不可行的。此外，该方法还利用加密零知识证明在不泄露种子的情况下证明所有权。

Result: 实验证明NoisePrints在多种先进的图像和视频扩散模型上实现了高效验证，仅需种子和输出，无需访问模型权重。

Conclusion: NoisePrints通过利用扩散模型的随机种子作为作者身份证明，提供了一种轻量级且鲁棒的水印方案，解决了当前内容生成领域作者身份验证的挑战，并在保护版权方面具有重要意义。

Abstract: With the rapid adoption of diffusion models for visual content generation,
proving authorship and protecting copyright have become critical. This
challenge is particularly important when model owners keep their models private
and may be unwilling or unable to handle authorship issues, making third-party
verification essential. A natural solution is to embed watermarks for later
verification. However, existing methods require access to model weights and
rely on computationally heavy procedures, rendering them impractical and
non-scalable. To address these challenges, we propose , a lightweight
watermarking scheme that utilizes the random seed used to initialize the
diffusion process as a proof of authorship without modifying the generation
process. Our key observation is that the initial noise derived from a seed is
highly correlated with the generated visual content. By incorporating a hash
function into the noise sampling process, we further ensure that recovering a
valid seed from the content is infeasible. We also show that sampling an
alternative seed that passes verification is infeasible, and demonstrate the
robustness of our method under various manipulations. Finally, we show how to
use cryptographic zero-knowledge proofs to prove ownership without revealing
the seed. By keeping the seed secret, we increase the difficulty of watermark
removal. In our experiments, we validate NoisePrints on multiple
state-of-the-art diffusion models for images and videos, demonstrating
efficient verification using only the seed and output, without requiring access
to model weights.

</details>


### [50] [Reasoning in Space via Grounding in the World](https://arxiv.org/abs/2510.13800)
*Yiming Chen,Zekun Qi,Wenyao Zhang,Xin Jin,Li Zhang,Peidong Liu*

Main category: cs.CV

TL;DR: 本文介绍了一种名为GS-Reasoner的3D大语言模型，它通过双路径池化机制统一了3D表示，实现了无外部模块的自回归视觉接地，并显著提升了3D空间推理能力。


<details>
  <summary>Details</summary>
Motivation: 现有的3D大语言模型在统一捕获语义和几何信息的3D表示方面存在不足，导致接地性能不佳或过度依赖外部模块，阻碍了接地与空间推理的无缝集成。

Method: 本文提出了一种简单而有效的双路径池化机制，将几何特征与语义和位置信息紧密对齐，构建了统一的基于图像块的3D表示，在不增加输入token数量的情况下包含了所有必要信息。在此基础上，GS-Reasoner是第一个实现完全自回归接地的3D大语言模型。此外，本文还引入了Grounded Chain-of-Thought (GCoT) 数据集，包含了3D边界框标注和分步推理路径，将接地作为解决问题的核心组成部分。

Result: GS-Reasoner在3D视觉接地方面取得了显著成果，反过来又显著增强了其空间推理能力，达到了最先进的性能。

Conclusion: GS-Reasoner通过其统一的3D表示和自回归接地能力，为3D空间推理提供了一个统一的、自包含的框架，并为未来的研究奠定了基础。

Abstract: In this paper, we claim that 3D visual grounding is the cornerstone of
spatial reasoning and introduce the Grounded-Spatial Reasoner (GS-Reasoner) to
explore the effective spatial representations that bridge the gap between them.
Existing 3D LLMs suffer from the absence of a unified 3D representation capable
of jointly capturing semantic and geometric information. This deficiency is
manifested either in poor performance on grounding or in an excessive reliance
on external modules, ultimately hindering the seamless integration of grounding
and spatial reasoning. To address this, we propose a simple yet effective
dual-path pooling mechanism that tightly aligns geometric features with both
semantic and positional cues, constructing a unified image patch-based 3D
representation that encapsulates all essential information without increasing
the number of input tokens. Leveraging this holistic representation,
GS-Reasoner is the first 3D LLM that achieves autoregressive grounding entirely
without external modules while delivering performance comparable to
state-of-the-art models, establishing a unified and self-contained framework
for 3D spatial reasoning. To further bridge grounding and spatial reasoning, we
introduce the Grounded Chain-of-Thought (GCoT) dataset. This dataset is
meticulously curated to include both 3D bounding box annotations for objects
referenced in reasoning questions and step-by-step reasoning paths that
integrate grounding as a core component of the problem-solving process.
Extensive experiments demonstrate that GS-Reasoner achieves impressive results
on 3D visual grounding, which in turn significantly enhances its spatial
reasoning capabilities, leading to state-of-the-art performance.

</details>


### [51] [VisCoP: Visual Probing for Video Domain Adaptation of Vision Language Models](https://arxiv.org/abs/2510.13808)
*Dominick Reilly,Manish Kumar Govind,Le Xue,Srijan Das*

Main category: cs.CV

TL;DR: VisCoP通过引入一组可学习的视觉探测器来增强VLM的视觉编码器，实现高效的特定领域适应，同时保持预训练模型的性能。


<details>
  <summary>Details</summary>
Motivation: 解决大型视觉语言模型（VLMs）在面对与预训练数据存在显著分布偏移的新领域时性能急剧下降的问题。现有的域适应方法在特定领域特征学习方面表现有限，或导致灾难性遗忘。

Method: 本文引入了Vision Contextualized Probing (VisCoP)方法，通过在VLM的视觉编码器中添加一组紧凑的可学习的视觉探测器，以最小的预训练参数修改实现高效的领域特定适应。

Result: VisCoP在交叉视角（从以自我为中心到以他人为中心）、交叉模态（RGB到深度）和交叉任务（人类理解到机器人控制）三种域适应设置中，均优于现有适应策略。

Conclusion: VisCoP在目标领域实现了卓越的性能，同时有效保留了源领域知识。

Abstract: Large Vision-Language Models (VLMs) excel at general visual reasoning tasks
but exhibit sharp performance degradation when applied to novel domains with
substantial distribution shifts from pretraining data. Existing domain
adaptation approaches finetune different VLM components, but this often results
in limited domain-specific feature learning or catastrophic forgetting of prior
capabilities. To address these issues, we introduce Vision Contextualized
Probing (VisCoP), which augments the VLM's vision encoder with a compact set of
learnable visual probes. These probes enable efficient domain-specific
adaptation with minimal modification to pretrained parameters. We evaluate
VisCoP across three challenging domain adaptation settings-cross-view
(exocentric to egocentric), cross-modal (RGB to depth), and cross-task (human
understanding to robot control). Experiments show that VisCoP consistently
outperforms existing adaptation strategies, achieving superior performance on
target domains while effectively retaining source-domain knowledge.

</details>


<div id='cs.AI'></div>

# cs.AI [[Back]](#toc)

### [52] [DeepPlanner: Scaling Planning Capability for Deep Research Agents via Advantage Shaping](https://arxiv.org/abs/2510.12979)
*Wei Fan,Wenlin Yao,Zheng Li,Feng Yao,Xin Liu,Liang Qiu,Qingyu Yin,Yangqiu Song,Bing Yin*

Main category: cs.AI

TL;DR: DeepPlanner是一个端到端的强化学习框架，它通过熵基项优化规划阶段，并选择性地增加规划密集型rollout的样本级优势，从而显著提升了大型语言模型的研究规划能力，并在多个基准测试中以更低的训练成本取得了最先进的结果。


<details>
  <summary>Details</summary>
Motivation: 现有的大型语言模型在处理需要长期规划的复杂任务时，其规划阶段存在不足，主要体现在：要么依赖隐式规划，要么引入显式规划但未能系统性地优化该阶段。证据表明，在vanilla强化学习下，规划token的熵明显高于其他行动token，这揭示了未被优化的不确定决策点。

Method: DeepPlanner是一个端到端的强化学习框架。它通过以下方式优化规划阶段：1. 使用一个基于熵的项来塑造token级别的优势，从而为高熵token分配更大的更新。2. 选择性地增加规划密集型rollout的样本级别优势。

Result: DeepPlanner在七个深度研究基准测试中，提高了规划质量，并在显著降低训练预算的情况下取得了最先进的结果。

Conclusion: DeepPlanner通过其独特的优化策略，有效提升了大型语言模型的研究规划能力，解决了现有方法在规划阶段的不足，并展现出卓越的性能和训练效率。

Abstract: Large language models (LLMs) augmented with multi-step reasoning and action
generation abilities have shown promise in leveraging external tools to tackle
complex tasks that require long-horizon planning. However, existing approaches
either rely on implicit planning in the reasoning stage or introduce explicit
planners without systematically addressing how to optimize the planning stage.
As evidence, we observe that under vanilla reinforcement learning (RL),
planning tokens exhibit significantly higher entropy than other action tokens,
revealing uncertain decision points that remain under-optimized. To address
this, we propose DeepPlanner, an end-to-end RL framework that effectively
enhances the planning capabilities of deep research agents. Our approach shapes
token-level advantage with an entropy-based term to allocate larger updates to
high entropy tokens, and selectively upweights sample-level advantages for
planning-intensive rollouts. Extensive experiments across seven deep research
benchmarks demonstrate that DeepPlanner improves planning quality and achieves
state-of-the-art results under a substantially lower training budget.

</details>


### [53] [From Narratives to Probabilistic Reasoning: Predicting and Interpreting Drivers' Hazardous Actions in Crashes Using Large Language Model](https://arxiv.org/abs/2510.13002)
*Boyou Chen,Gerui Xu,Zifei Wang,Huizhong Guo,Ananna Ahmed,Zhaonan Sun,Zhen Hu,Kaihan Zhang,Shan Bao*

Main category: cs.AI

TL;DR: 该研究提出了一个利用大型语言模型自动识别文本碰撞 GHA 的框架，该框架在两车事故数据上取得了 80% 的准确率，超过了所有基线模型。


<details>
  <summary>Details</summary>
Motivation: 以往 DHAs 的识别方法存在数据可靠性低、人工编码效率低等问题，导致对碰撞原因的理解受限。

Method: 该研究提出了一种创新框架，利用微调的大型语言模型自动从文本碰撞叙述中推断 DHA。该模型是一个微调的 Llama 3.2 1B 模型，并在 MTCF 的五年两车碰撞数据上进行训练。研究还开发了一种概率推理方法来分析模型输出。

Result: 微调后的 LLM 取得了 80% 的总准确率，超过了所有基线模型，并在数据不平衡的情况下表现出显著的改进。研究发现，引入驾驶员分心会显著增加“一般不安全驾驶”的可能性；双驾驶员分心会最大化“TBDHA”的概率；青少年驾驶员会显著增加“超速和停车违规”的可能性。

Conclusion: 该框架为大规模自动化 DHA 检测提供了一个稳健且可解释的解决方案，为交通安全分析和干预提供了新的机会。

Abstract: Vehicle crashes involve complex interactions between road users, split-second
decisions, and challenging environmental conditions. Among these, two-vehicle
crashes are the most prevalent, accounting for approximately 70% of roadway
crashes and posing a significant challenge to traffic safety. Identifying
Driver Hazardous Action (DHA) is essential for understanding crash causation,
yet the reliability of DHA data in large-scale databases is limited by
inconsistent and labor-intensive manual coding practices. Here, we present an
innovative framework that leverages a fine-tuned large language model to
automatically infer DHAs from textual crash narratives, thereby improving the
validity and interpretability of DHA classifications. Using five years of
two-vehicle crash data from MTCF, we fine-tuned the Llama 3.2 1B model on
detailed crash narratives and benchmarked its performance against conventional
machine learning classifiers, including Random Forest, XGBoost, CatBoost, and a
neural network. The fine-tuned LLM achieved an overall accuracy of 80%,
surpassing all baseline models and demonstrating pronounced improvements in
scenarios with imbalanced data. To increase interpretability, we developed a
probabilistic reasoning approach, analyzing model output shifts across original
test sets and three targeted counterfactual scenarios: variations in driver
distraction and age. Our analysis revealed that introducing distraction for one
driver substantially increased the likelihood of "General Unsafe Driving";
distraction for both drivers maximized the probability of "Both Drivers Took
Hazardous Actions"; and assigning a teen driver markedly elevated the
probability of "Speed and Stopping Violations." Our framework and analytical
methods provide a robust and interpretable solution for large-scale automated
DHA detection, offering new opportunities for traffic safety analysis and
intervention.

</details>


### [54] [Toward Reasoning-Centric Time-Series Analysis](https://arxiv.org/abs/2510.13029)
*Xinlei Wang,Mingtian Tan,Jing Qiu,Junhua Zhao,Jinjin Gu*

Main category: cs.AI

TL;DR: 本文提出将时间序列分析与大型语言模型（LLMs）结合起来，以应对现实世界中复杂的时间序列数据。


<details>
  <summary>Details</summary>
Motivation: 传统的时序分析方法依赖静态基准和模式识别，难以捕捉现实世界中政策变化、人类行为适应和意外事件等深层驱动因素。大型语言模型（LLMs）的兴起为时序分析带来了新机遇，但现有方法多忽视其深层推理潜力。

Method: 本文提出将时间序列分析视为一个推理任务，重点关注因果结构和可解释性，而非仅仅利用LLMs的数值回归能力。

Result: 通过将时序分析与LLMs结合，并侧重于推理任务，可以实现对复杂现实世界环境中透明且上下文感知的洞察。

Conclusion: 将时间序列分析与大型语言模型（LLMs）结合，并将其视为一个推理任务，可以更好地理解现实世界中复杂的时间序列模式，并提供更具可解释性的结果。

Abstract: Traditional time series analysis has long relied on pattern recognition,
trained on static and well-established benchmarks. However, in real-world
settings -- where policies shift, human behavior adapts, and unexpected events
unfold -- effective analysis must go beyond surface-level trends to uncover the
actual forces driving them. The recent rise of Large Language Models (LLMs)
presents new opportunities for rethinking time series analysis by integrating
multimodal inputs. However, as the use of LLMs becomes popular, we must remain
cautious, asking why we use LLMs and how to exploit them effectively. Most
existing LLM-based methods still employ their numerical regression ability and
ignore their deeper reasoning potential. This paper argues for rethinking time
series with LLMs as a reasoning task that prioritizes causal structure and
explainability. This shift brings time series analysis closer to human-aligned
understanding, enabling transparent and context-aware insights in complex
real-world environments.

</details>


### [55] [Personalized Learning Path Planning with Goal-Driven Learner State Modeling](https://arxiv.org/abs/2510.13215)
*Joy Jia Yin Lim,Ye He,Jifan Yu,Xin Cong,Daniel Zhang-Li,Zhiyuan Liu,Huiqin Liu,Lei Hou,Juanzi Li,Bin Xu*

Main category: cs.AI

TL;DR: Pxpore是一个新颖的、基于LLM的、强化学习训练的个性化学习路径规划框架。


<details>
  <summary>Details</summary>
Motivation: 现有的个性化学习路径规划方法通常缺乏与学习目标对齐的机制，而大型语言模型（LLMs）在个性化学习体验方面表现出潜力。

Method: 本文提出了Pxplore框架，集成了基于强化学习的训练范式和LLM驱动的教育架构。设计了一个结构化的学习者状态模型和自动奖励函数。策略训练结合了监督微调（SFT）和Group Relative Policy Optimization（GRPO）。

Result: Pxplore在实际学习平台中部署，并在一系列实验中验证了其在生成连贯、个性化和目标驱动的学习路径方面的有效性。

Conclusion: Pxplore能够有效地生成连贯、个性化和目标驱动的学习路径，并有望促进未来在该领域的研究。

Abstract: Personalized Learning Path Planning (PLPP) aims to design adaptive learning
paths that align with individual goals. While large language models (LLMs) show
potential in personalizing learning experiences, existing approaches often lack
mechanisms for goal-aligned planning. We introduce Pxplore, a novel framework
for PLPP that integrates a reinforcement-based training paradigm and an
LLM-driven educational architecture. We design a structured learner state model
and an automated reward function that transforms abstract objectives into
computable signals. We train the policy combining supervised fine-tuning (SFT)
and Group Relative Policy Optimization (GRPO), and deploy it within a
real-world learning platform. Extensive experiments validate Pxplore's
effectiveness in producing coherent, personalized, and goal-driven learning
paths. We release our code and dataset to facilitate future research.

</details>


### [56] [Learnable Game-theoretic Policy Optimization for Data-centric Self-explanation Rationalization](https://arxiv.org/abs/2510.13393)
*Yunxiao Zhao,Zhiqiang Wang,Xingtong Yu,Xiaoli Li,Jiye Liang,Ru Li*

Main category: cs.AI

TL;DR: 本文提出了一种新的方法PORAT，以解决现有合理化方法中模式崩溃的问题。


<details>
  <summary>Details</summary>
Motivation: 传统的合理化方法通过正则化项来校准或惩罚不期望的生成，但存在模式崩溃问题，导致预测器产生正确预测，而生成器持续输出模式崩溃的合理性。现有研究通常针对特定崩溃模式单独设计，缺乏统一的考虑。

Method: 本文从新颖的博弈论角度重新审视合作合理化，并确定了模式崩溃的根本原因：生成器不再倾向于探索新策略，从而导致系统收敛到次优博弈均衡。为解决此问题，本文提出了一种新颖的方法——面向博弈论策略优化的合理化（PORAT），通过逐步引入策略干预来解决合作博弈过程中的博弈均衡问题，从而引导模型走向更优的解决方案。

Result: PORAT在九个广泛使用的真实世界数据集和两个合成设置上，比现有最先进的方法实现了高达8.1%的性能提升。

Conclusion: PORAT方法通过博弈论策略优化，有效解决了合作合理化中的模式崩溃问题，提高了模型性能。

Abstract: Rationalization, a data-centric framework, aims to build self-explanatory
models to explain the prediction outcome by generating a subset of
human-intelligible pieces of the input data. It involves a cooperative game
model where a generator generates the most human-intelligible parts of the
input (i.e., rationales), followed by a predictor that makes predictions based
on these generated rationales. Conventional rationalization methods typically
impose constraints via regularization terms to calibrate or penalize undesired
generation. However, these methods are suffering from a problem called mode
collapse, in which the predictor produces correct predictions yet the generator
consistently outputs rationales with collapsed patterns. Moreover, existing
studies are typically designed separately for specific collapsed patterns,
lacking a unified consideration. In this paper, we systematically revisit
cooperative rationalization from a novel game-theoretic perspective and
identify the fundamental cause of this problem: the generator no longer tends
to explore new strategies to uncover informative rationales, ultimately leading
the system to converge to a suboptimal game equilibrium (correct predictions
v.s collapsed rationales). To solve this problem, we then propose a novel
approach, Game-theoretic Policy Optimization oriented RATionalization (PORAT),
which progressively introduces policy interventions to address the game
equilibrium in the cooperative game process, thereby guiding the model toward a
more optimal solution state. We theoretically analyse the cause of such a
suboptimal equilibrium and prove the feasibility of the proposed method.
Furthermore, we validate our method on nine widely used real-world datasets and
two synthetic settings, where PORAT achieves up to 8.1% performance
improvements over existing state-of-the-art methods.

</details>


### [57] [Assessing LLM Reasoning Through Implicit Causal Chain Discovery in Climate Discourse](https://arxiv.org/abs/2510.13417)
*Liesbeth Allein,Nataly Pineda-Castañeda,Andrea Rocci,Marie-Francine Moens*

Main category: cs.AI

TL;DR: 本文分析了大型语言模型（LLM）在隐式因果链发现方面的能力，发现它们在生成因果步骤的数量和粒度上存在差异。LLMs的判断主要基于关联模式匹配而非真正的因果推理，但生成链的逻辑连贯性得到了人类评估的证实。


<details>
  <summary>Details</summary>
Motivation: 探究大型语言模型（LLM）的机械因果推理能力，以回答“原因如何导致结果”以及“哪些中间因果步骤能够解释它们之间的联系”这两个问题。

Method: 本文通过隐式因果链发现任务，在诊断性评估框架下，指示九个大型语言模型生成连接给定因果对的所有可能的中间因果步骤。这些因果对来源于气候变化两极分化讨论的论证研究资源。

Result: LLM在生成因果步骤的数量和粒度上存在差异。尽管LLM对生成链中的中间因果连接具有自洽性和信心，但其判断主要由关联模式匹配驱动，而非真正的因果推理。然而，人类评估证实了生成链的逻辑一致性和完整性。

Conclusion: 本文的基线因果链发现方法、诊断性评估的见解以及带有因果链的基准数据集为未来在论证环境中进行隐式、机械因果推理的研究奠定了坚实基础。

Abstract: How does a cause lead to an effect, and which intermediate causal steps
explain their connection? This work scrutinizes the mechanistic causal
reasoning capabilities of large language models (LLMs) to answer these
questions through the task of implicit causal chain discovery. In a diagnostic
evaluation framework, we instruct nine LLMs to generate all possible
intermediate causal steps linking given cause-effect pairs in causal chain
structures. These pairs are drawn from recent resources in argumentation
studies featuring polarized discussion on climate change. Our analysis reveals
that LLMs vary in the number and granularity of causal steps they produce.
Although they are generally self-consistent and confident about the
intermediate causal connections in the generated chains, their judgments are
mainly driven by associative pattern matching rather than genuine causal
reasoning. Nonetheless, human evaluations confirmed the logical coherence and
integrity of the generated chains. Our baseline causal chain discovery
approach, insights from our diagnostic evaluation, and benchmark dataset with
causal chains lay a solid foundation for advancing future work in implicit,
mechanistic causal reasoning in argumentation settings.

</details>


### [58] [Confidence as a Reward: Transforming LLMs into Reward Models](https://arxiv.org/abs/2510.13501)
*He Du,Bowen Li,Chengxing Xie,Chang Gao,Kai Chen,Dacheng Tao*

Main category: cs.AI

TL;DR: 本文提出了一种名为 CRew 的无训练方法，它利用大型语言模型在封闭式任务中输出的token级置信度作为奖励信号。


<details>
  <summary>Details</summary>
Motivation: 奖励模型可以显著提高大型语言模型的推理能力，但它们需要大量的策划数据和昂贵的训练。为了缓解这些挑战，无训练方法（如LLM-as-a-Judge）利用LLM的内在推理能力来评估响应，取得了有希望的结果。

Method: 本文提出了一种名为 CRew 的无训练方法，它利用大型语言模型在封闭式任务中输出的token级置信度作为奖励信号。在此基础上，本文提出了CRew-DPO，这是一种训练策略，它将置信度分数与正确性信号相结合，构建偏好数据。

Result: 在数学推理任务上进行的大量实验表明，CRew 在 MATH500 和 RewardMATH 基准测试中优于现有的无训练奖励方法，甚至超过了大多数经过训练的奖励模型。研究者发现 CRew 分数与模型的实际推理性能之间存在很强的相关性。CRew 可以有效地筛选高质量的训练数据。使用 CRew-DPO 进行微调进一步增强了模型的判断能力，并始终优于现有的自训练方法。

Conclusion: CRew是一种简单而强大的无训练奖励方法，尤其适用于封闭式任务。CRew能够有效地筛选高质量的训练数据，CRew-DPO通过结合置信度分数和正确性信号，进一步提高了模型的判断能力。

Abstract: Reward models can significantly enhance the reasoning capabilities of large
language models (LLMs), but they typically require extensive curated data and
costly training. To mitigate these challenges, training-free approaches such as
LLM-as-a-Judge leverage the intrinsic reasoning abilities of LLMs to evaluate
responses, achieving promising results. Recent works have also indicated that
model confidence can serve effectively as a reward metric, distinguishing
between chain-of-thought (CoT) and non-CoT paths. However, the concept of using
confidence as a reward has not been comprehensively studied. In this work, we
systematically investigate Confidence-as-a-Reward (CRew), a simple yet powerful
training-free method that utilizes token-level confidence in the model's final
answers as a proxy for reward, especially suitable for close-ended tasks.
Through extensive experiments on mathematical reasoning tasks, we demonstrate
that CRew outperforms existing training-free reward approaches on the MATH500
and RewardMATH benchmarks, and even surpasses most trained reward models. We
further identify a strong correlation between CRew scores and the actual
reasoning performance of the model. Additionally, we find that CRew can
effectively filter high-quality training data. Building upon these insights, we
propose CRew-DPO, a training strategy that constructs preference data from
confidence scores combined with correctness signals. Finetuning with CRew-DPO
further enhances the model's judging capabilities and consistently outperforms
existing self-training methods.

</details>


### [59] [A Modal Logic for Temporal and Jurisdictional Classifier Models](https://arxiv.org/abs/2510.13691)
*Cecilia Di Florio,Huimin Dong,Antonino Rotolo*

Main category: cs.AI

TL;DR: 本文介绍了一种法律分类器的模态逻辑，旨在形式化法律领域中的案例推理（CBR）。


<details>
  <summary>Details</summary>
Motivation: 机器学习分类器通过基于先前案例预测新案例的结果来执行案例推理（CBR），因此可以使用基于逻辑的模型来构建机器学习分类器的验证工具，以用于法律领域。

Method: 将案件的时间维度和法律体系中法院的层级结构引入逻辑中，从而解决了判例之间的冲突。

Result: 没有提及具体的实验结果，但该方法旨在形式化法律CBR，并提供了一种解决判例冲突的原则。

Conclusion: 通过引入案件的时间维度和法院层级结构，本文提出的分类器模态逻辑，为解决法律CBR中的判例冲突提供了一种形式化方法。

Abstract: Logic-based models can be used to build verification tools for machine
learning classifiers employed in the legal field. ML classifiers predict the
outcomes of new cases based on previous ones, thereby performing a form of
case-based reasoning (CBR). In this paper, we introduce a modal logic of
classifiers designed to formally capture legal CBR. We incorporate principles
for resolving conflicts between precedents, by introducing into the logic the
temporal dimension of cases and the hierarchy of courts within the legal
system.

</details>


### [60] [Training LLM Agents to Empower Humans](https://arxiv.org/abs/2510.13709)
*Evan Ellis,Vivek Myers,Jens Tuyls,Sergey Levine,Anca Dragan,Benjamin Eysenbach*

Main category: cs.AI

TL;DR: 该论文提出了一种新的辅助语言模型微调方法Empower，旨在最大化人类的赋权，从而使智能体更好地协助人类实现目标，而非独立完成任务。该方法仅需要离线文本数据进行自监督训练，并在用户研究和模拟环境中均表现出显著优于基线的效果。


<details>
  <summary>Details</summary>
Motivation: 当前构建辅助智能体的方法（无论是模仿专家人类还是通过推断奖励进行强化学习微调）通常鼓励智能体独立完成任务，而非真正地协助人类实现目标。此外，这些方法通常需要昂贵且明确的人类反馈来提供训练信号。

Method: 本研究提出了一种新的辅助语言模型微调方法Empower，该方法基于最大化人类的赋权，即人类在环境中实现预期改变的能力。Empower方法仅需要离线文本数据进行自监督训练，无需额外的人类反馈或可验证的奖励。

Result: 在18人的用户研究中，参与者在78%的时间里更喜欢Empower辅助系统（p=0.015），接受率高出31%，建议数量减少38%。此外，在一个用于评估多轮代码辅助的新环境中，Empower训练的智能体使模拟程序员在具有挑战性的编码问题上的成功率比SFT基线平均提高了192%。

Conclusion: 本研究提出了一种以赋权为目标的框架，仅利用离线数据即可构建大规模、有用且对齐的AI智能体，无需任何额外的人类反馈或可验证的奖励。

Abstract: Assistive agents should not only take actions on behalf of a human, but also
step out of the way and cede control when there are important decisions to be
made. However, current methods for building assistive agents, whether via
mimicking expert humans or via RL finetuning on an inferred reward, often
encourage agents to complete tasks on their own rather than truly assisting the
human attain their objectives. Additionally, these methods often require costly
explicit human feedback to provide a training signal. We propose a new approach
to tuning assistive language models based on maximizing the human's
empowerment, their ability to effect desired changes in the environment. Our
empowerment-maximizing method, Empower, only requires offline text data,
providing a self-supervised method for fine-tuning language models to better
assist humans. To study the efficacy of our approach, we conducted an 18-person
user study comparing our empowerment assistant with a strong baseline.
Participants preferred our assistant 78% of the time (p=0.015), with a 31%
higher acceptance rate and 38% fewer suggestions. Additionally, we introduce a
new environment for evaluating multi-turn code assistance using simulated
humans. Using this environment, we show that agents trained with Empower
increase the success rate of a simulated human programmer on challenging coding
questions by an average of 192% over an SFT baseline. With this empowerment
objective, we provide a framework for useful aligned AI agents at scale using
only offline data without the need for any additional human feedback or
verifiable rewards.

</details>


<div id='cs.MA'></div>

# cs.MA [[Back]](#toc)

### [61] [KVCOMM: Online Cross-context KV-cache Communication for Efficient LLM-based Multi-agent Systems](https://arxiv.org/abs/2510.12872)
*Hancheng Ye,Zhengqi Gao,Mingyuan Ma,Qinsi Wang,Yuzhe Fu,Ming-Yu Chung,Yueqian Lin,Zhijian Liu,Jianyi Zhang,Danyang Zhuo,Yiran Chen*

Main category: cs.MA

TL;DR: KVCOMM是一种无需训练的框架，通过重用KV缓存和对齐重叠上下文的缓存偏移量，实现多智能体LLM系统中的高效预填充。


<details>
  <summary>Details</summary>
Motivation: 多智能体大型语言模型（LLM）系统在处理复杂语言任务时，由于智能体之间需要频繁通信和协调，导致重复处理重叠上下文的开销巨大。传统的KV缓存技术在单智能体场景中有效，但在多智能体场景中由于前缀上下文不同而无法直接应用。

Method: KVCOMM通过引入“锚点”来解决多智能体KV缓存的偏移量差异问题。“锚点”存储了在不同前缀下观察到的缓存偏差，用于估计和调整共享内容的KV缓存。锚点池在线维护和更新，以适应不同的用户请求和上下文结构。

Result: KVCOMM在多种多智能体工作负载（包括检索增强生成、数学推理和协同编码任务）中实现了超过70%的重用率，且没有质量下降。在特定五智能体设置下（每个智能体接收1K输入token，512前缀token和512输出token），KVCOMM比标准预填充管道提速高达7.8倍，将TTFT从约430毫秒减少到约55毫秒。

Conclusion: KVCOMM通过创新的KV缓存重用机制，显著提升了多智能体LLM系统的预填充效率，解决了传统KV缓存技术在多智能体场景中的局限性，实现了显著的性能提升。

Abstract: Multi-agent large language model (LLM) systems are increasingly adopted for
complex language processing tasks that require communication and coordination
among agents. However, these systems often suffer substantial overhead from
repeated reprocessing of overlapping contexts across agents. In typical
pipelines, once an agent receives a message from its predecessor, the full
context-including prior turns-must be reprocessed from scratch, leading to
inefficient processing. While key-value (KV) caching is an effective solution
for avoiding redundant computation in single-agent settings where prefixes
remain unchanged, it cannot be directly reused in multi-agent scenarios due to
diverging prefixes introduced by agent-specific context extensions. We identify
that the core challenge lies in the offset variance of KV-caches across agents.
To address this, we propose KVCOMM, a training-free framework that enables
efficient prefilling in multi-agent inference by reusing KV-caches and aligning
cache offsets of overlapping contexts under diverse prefix contexts. KVCOMM
estimates and adjusts KV-caches for shared content by referencing a pool of
cached examples-termed anchors-that store observed cache deviations under
varying prefixes. The anchor pool is maintained and updated online, allowing
dynamic adaptation to distinct user requests and context structures. KVCOMM
achieves over 70% reuse rate across diverse multi-agent workloads, including
retrieval-augmented generation, math reasoning, and collaborative coding tasks,
all without quality degradation. Particularly, when each fully-connected agent
receives 1K input tokens with 512 prefix tokens and 512 output tokens under a
five-agent setting, KVCOMM achieves up to 7.8x speedup compared to the standard
prefill pipeline, reducing TTFT from ~430 ms to ~55 ms.

</details>


### [62] [Altruistic Ride Sharing: A Community-Driven Approach to Short-Distance Mobility](https://arxiv.org/abs/2510.13227)
*Divyanshu Singh,Ashman Mehra,Snehanshu Saha,Santonu Sarkar*

Main category: cs.MA

TL;DR: Altruistic Ride-Sharing（ARS）是一种去中心化的点对点出行框架，通过利他主义积分而非金钱激励来匹配驾乘者。它利用多智能体强化学习进行动态匹配，并通过博弈论均衡确保公平性。ARS能有效减少出行距离和排放，提高车辆利用率，并促进公平参与。


<details>
  <summary>Details</summary>
Motivation: 传统的营利性共享出行平台侧重于收入而非公平性和可持续性。城市交通面临拥堵和燃料消耗的挑战，尤其是在人们选择私人点对点通勤方式时。

Method: 本文提出了利他共享出行（ARS）系统。该系统整合了：1. 多智能体强化学习（MADDPG）实现动态行程匹配。2. 博弈论均衡保证公平性。3. 人口模型维持长期平衡。

Result: 与无共享和基于优化的基线相比，ARS显著减少了出行距离和排放，提高了车辆利用率，并促进了公平参与。

Conclusion: ARS系统为传统共享出行提供了一种可扩展的、社区驱动的替代方案，将个体行为与城市可持续发展目标相结合。

Abstract: Urban mobility faces persistent challenges of congestion and fuel
consumption, specifically when people choose a private, point-to-point commute
option. Profit-driven ride-sharing platforms prioritize revenue over fairness
and sustainability. This paper introduces Altruistic Ride-Sharing (ARS), a
decentralized, peer-to-peer mobility framework where participants alternate
between driver and rider roles based on altruism points rather than monetary
incentives. The system integrates multi-agent reinforcement learning (MADDPG)
for dynamic ride-matching, game-theoretic equilibrium guarantees for fairness,
and a population model to sustain long-term balance. Using real-world New York
City taxi data, we demonstrate that ARS reduces travel distance and emissions,
increases vehicle utilization, and promotes equitable participation compared to
both no-sharing and optimization-based baselines. These results establish ARS
as a scalable, community-driven alternative to conventional ride-sharing,
aligning individual behavior with collective urban sustainability goals.

</details>


### [63] [AOAD-MAT: Transformer-based multi-agent deep reinforcement learning model considering agents' order of action decisions](https://arxiv.org/abs/2510.13343)
*Shota Takayama,Katsuhide Fujita*

Main category: cs.MA

TL;DR: 本文提出了AOAD-MAT模型，一个在MARL环境中通过动态调整智能体动作顺序来提升性能的新型MAT模型。


<details>
  <summary>Details</summary>
Motivation: 现有的多智能体强化学习模型（如MAT和ACE）在处理多智能体决策时，虽然性能有所提升，但并未显式考虑智能体决策顺序的重要性。

Method: AOAD-MAT模型将动作决策序列显式地融入学习过程，通过一个基于Transformer的Actor-Critic架构动态调整智能体动作的顺序。此外，模型引入了一个协同的子任务，用于预测下一个行动的智能体，并整合到基于近端策略优化的损失函数中，以最大化序列决策的优势。

Result: 在StarCraft多智能体挑战和多智能体MuJoCo基准测试中进行的广泛实验表明，AOAD-MAT模型优于现有的MAT模型和其他基线模型。

Conclusion: AOAD-MAT模型通过调整多智能体强化学习中的动作决策顺序，有效提升了模型性能。

Abstract: Multi-agent reinforcement learning focuses on training the behaviors of
multiple learning agents that coexist in a shared environment. Recently, MARL
models, such as the Multi-Agent Transformer (MAT) and ACtion dEpendent deep
Q-learning (ACE), have significantly improved performance by leveraging
sequential decision-making processes. Although these models can enhance
performance, they do not explicitly consider the importance of the order in
which agents make decisions. In this paper, we propose an Agent Order of Action
Decisions-MAT (AOAD-MAT), a novel MAT model that considers the order in which
agents make decisions. The proposed model explicitly incorporates the sequence
of action decisions into the learning process, allowing the model to learn and
predict the optimal order of agent actions. The AOAD-MAT model leverages a
Transformer-based actor-critic architecture that dynamically adjusts the
sequence of agent actions. To achieve this, we introduce a novel MARL
architecture that cooperates with a subtask focused on predicting the next
agent to act, integrated into a Proximal Policy Optimization based loss
function to synergistically maximize the advantage of the sequential
decision-making. The proposed method was validated through extensive
experiments on the StarCraft Multi-Agent Challenge and Multi-Agent MuJoCo
benchmarks. The experimental results show that the proposed AOAD-MAT model
outperforms existing MAT and other baseline models, demonstrating the
effectiveness of adjusting the AOAD order in MARL.

</details>


<div id='cs.IT'></div>

# cs.IT [[Back]](#toc)

### [64] [On the performance of Active STAR-RIS-Assisted Cell-Free Massive MIMO Systems with Phase Errors and Channel Aging](https://arxiv.org/abs/2510.13171)
*Jun Qian,Ross Murch,Khaled B. Letaief*

Main category: cs.IT

TL;DR: 本文分析了有源STAR-RIS辅助的无蜂窝大规模MIMO系统中，相位误差和信道老化的影响，并提出了相应的性能提升策略。


<details>
  <summary>Details</summary>
Motivation: 分析相位误差和信道老化对有源STAR-RIS辅助的无蜂窝大规模MIMO系统性能的影响，并提出克服这些不利影响的方法。

Method: 利用空间相关的瑞利衰落模型，推导了基于最小均方误差估计的信道估计，并建立了下行链路频谱效率的闭式表达式。

Result: 有源STAR-RIS能有效补偿相位误差和信道老化的不利影响。增加AP和STAR-RIS单元数量以及更大的放大因子可以减轻性能下降。

Conclusion: 有源STAR-RISs在补偿相位误差和信道老化方面表现出色，为未来6G系统中的智能表面部署提供了指导原则。

Abstract: Active reconfigurable intelligent surfaces (RISs) employ amplification to
overcome attenuation caused by the RIS cascaded link. In this paper, we analyze
the effects of phase errors and channel aging in active simultaneously
transmitting and reflecting (STAR) RIS-assisted cell-free massive
multiple-input multiple-output (MIMO) systems. By leveraging a spatially
correlated Rayleigh fading model, this paper derives minimum mean square error
estimate-based channel estimates and formulates closed-form expressions for
downlink spectral efficiency. This analytical framework enables a comprehensive
evaluation of the effects of channel aging and uniformly distributed phase
errors on system performance. The results demonstrate that active STAR-RISs can
effectively compensate for the adverse effects of phase errors and channel
aging. To counteract the impact of channel aging, we propose practical
guidelines for resource-block-length design. Also, an increase in APs and
STAR-RIS elements, along with a larger amplification factor, can alleviate
performance degradation.

</details>


### [65] [Simulating Mediumband Wireless Communication Systems: A Concise Description](https://arxiv.org/abs/2510.13532)
*Dushyantha A Basnayaka*

Main category: cs.IT

TL;DR: 这篇论文描述了准确模拟中频数字无线通信系统所需的方法和步骤，适用于初学者和专家。


<details>
  <summary>Details</summary>
Motivation: 在现有研究中，数字无线通信系统通常在离散时间复基带域进行模拟，并忽略了一些重要的物理层操作。然而，为了捕捉中频通信的本质，需要详细模拟某些物理层操作。

Method: 本论文详细描述了如何在MATLAB中模拟一个从中频无线通信场景，具体阐述了关键物理层子系统的工作。

Result: 通过所描述的方法，模拟系统能够捕捉中频无线通信的精细动态，包括避免深度衰落的影响。

Conclusion: 模拟中频无线通信系统需要详细考虑物理层操作，这对于准确捕捉通信精髓至关重要。

Abstract: In this paper, we describe the necessary procedures for accurately simulating
digital wireless communication systems operating in the mediumband, aimed at
both beginners and experts. In the research literature, digital wireless
communication systems are typically simulated in the discrete-time complex
baseband domain, where pulse shaping, upconversion, mixing, carrier
synchronization, and symbol timing synchronization are often ignored. These
assumptions are indeed sufficient in most cases, but to capture the essence of
communication in the mediumband, certain physical layer (PHY) operations should
be simulated in detail. In this paper, we concisely describe how to simulate a
mediumband wireless communication scenario from a single transmitter (TX) to a
single receiver (RX) in MATLAB, elaborating the operation of key PHY
subsystems. The approach described here ensures that the simulated system
captures the delicate dynamics of mediumband wireless communication, including
the effect of deep fading avoidance.

</details>


### [66] [Local Information-Theoretic Security via Euclidean Geometry](https://arxiv.org/abs/2510.13661)
*Emmanouil M. Athanasakos,Nicholas Kalouptsidis,Hariprasad Manjunath*

Main category: cs.IT

TL;DR: 本文提出了一种基于欧几里得信息论的方法，用于研究离散无记忆窃听信道上的安全通信的局部特性。


<details>
  <summary>Details</summary>
Motivation: 在离散无记忆窃听信道上，制定了一个受限优化问题，以最大化合法用户的信息速率，同时对窃听者的信息泄露和秘密消息编码的信息成本施加明确的上限。

Method: 通过利用局部几何近似，将非凸问题转化为可处理的二次规划结构。通过求解线性程序可以找到控制近似问题的最优拉格朗日乘数。该线性程序的约束条件源自 Karush-Kuhn-Tucker 条件，并用信道导出矩阵的广义特征值表示。

Result: 推导出了近似局部保密容量的解析公式。定义并分析了一类新的秘密局部收缩系数，这些系数被表征为矩阵束的最大广义特征值，量化了近似效用与近似泄露的最大可实现比率，从而衡量了信道固有的局部泄露效率。建立了连接这些局部系数与真实互信息度量定义的全局对应物的界限。

Conclusion: 该框架促进了近似局部保密容量的解析公式的推导，并定义和分析了一类新的秘密局部收缩系数。并通过详细分析和数值说明了该框架的有效性。

Abstract: This paper introduces a methodology based on Euclidean information theory to
investigate local properties of secure communication over discrete memoryless
wiretap channels. We formulate a constrained optimization problem that
maximizes a legitimate user's information rate while imposing explicit upper
bounds on both the information leakage to an eavesdropper and the informational
cost of encoding the secret message. By leveraging local geometric
approximations, this inherently non-convex problem is transformed into a
tractable quadratic programming structure. It is demonstrated that the optimal
Lagrange multipliers governing this approximated problem can be found by
solving a linear program. The constraints of this linear program are derived
from Karush-Kuhn-Tucker conditions and are expressed in terms of the
generalized eigenvalues of channel-derived matrices. This framework facilitates
the derivation of an analytical formula for an approximate local secrecy
capacity. Furthermore, we define and analyze a new class of secret local
contraction coefficients. These coefficients, characterized as the largest
generalized eigenvalues of a matrix pencil, quantify the maximum achievable
ratio of approximate utility to approximate leakage, thus measuring the
intrinsic local leakage efficiency of the channel. We establish bounds
connecting these local coefficients to their global counterparts defined over
true mutual information measures. The efficacy of the proposed framework is
demonstrated through detailed analysis and numerical illustrations for both
general multi-mode channels and the canonical binary symmetric wiretap channel.

</details>


<div id='cs.GT'></div>

# cs.GT [[Back]](#toc)

### [67] [Finding a Nash equilibrium of a random win-lose game in expected polynomial time](https://arxiv.org/abs/2510.12846)
*Andrea Collevecchio,Gabor Lugosi,Adrian Vetta,Rui-Ray Zhang*

Main category: cs.GT

TL;DR: 本文探讨了在随机二人博弈中计算纳什均衡的多项式时间算法问题。


<details>
  <summary>Details</summary>
Motivation: 在算法博弈论中，一个长期存在的开放问题是是否存在一个多项式时间算法来计算随机二人博弈中的纳什均衡。

Method: 本文研究了随机胜负博弈，其中 $n \times n$ 收益矩阵的条目是独立同分布 (i.i.d.) 的伯努利随机变量，参数为 $p=p(n)$。

Result: 本文证明了，对于几乎所有参数 $p=p(n)$ 的值，都存在一个期望多项式时间算法来找到随机胜负博弈中的纳什均衡。更具体地说，如果 $p\sim cn^{-a}$（其中 $a,c\ge 0$），那么当 $a\not\in \{1/2, 1\}$ 时，存在一个期望多项式时间算法。此外，如果 $a=1/2$，当 $c \le e^{-52} 2^{-8}$ 或 $c\ge 0.977$ 时，存在一个有效算法。如果 $a=1$，当 $c\le 0.3849$ 或 $c\ge \log^9 n$ 时，存在一个期望多项式时间算法。

Conclusion: 本文在随机胜负博弈中找到了一个期望多项式时间算法来计算纳什均衡，并为算法的适用范围提供了详细的参数条件。

Abstract: A long-standing open problem in algorithmic game theory asks whether or not
there is a polynomial time algorithm to compute a Nash equilibrium in a random
bimatrix game. We study random win-lose games, where the entries of the
$n\times n$ payoff matrices are independent and identically distributed
(i.i.d.) Bernoulli random variables with parameter $p=p(n)$. We prove that, for
nearly all values of the parameter $p=p(n)$, there is an expected
polynomial-time algorithm to find a Nash equilibrium in a random win-lose game.
More precisely, if $p\sim cn^{-a}$ for some parameters $a,c\ge 0$, then there
is an expected polynomial-time algorithm whenever $a\not\in \{1/2, 1\}$. In
addition, if $a = 1/2$ there is an efficient algorithm if either $c \le e^{-52}
2^{-8} $ or $c\ge 0.977$. If $a=1$, then there is an expected polynomial-time
algorithm if either $c\le 0.3849$ or $c\ge \log^9 n$.

</details>


### [68] [Efficiency of Constant Log Utility Market Makers](https://arxiv.org/abs/2510.12952)
*Maneesha Papireddygari,Xintong Wang,Bo Waggoner,David M. Pennock*

Main category: cs.GT

TL;DR: 本文探讨了自动化做市商（AMM）在组合预测市场中的应用，重点关注Constant Log Utility Market Maker (CLUM)。文章证明了CLUM的定价问题是#P-hard的，并提出了一种近似算法来解决这个问题，该算法在特定条件下可以多项式时间内实现。


<details>
  <summary>Details</summary>
Motivation: 组合预测市场中的流动性不足是一个普遍问题，特别是对于数量巨大的证券。LMSR等传统AMM的定价问题与最坏情况损失（worst-case loss）限制了其可扩展性，尤其是在处理大量甚至可数无限结果时。因此，需要更有效且可扩展的AMM。

Method: 本文主要关注CLUM，证明了其证券定价问题是#P-hard的，并通过模型计数2-SAT问题进行了归约。为了提高CLUM的实用性，作者提出了一种高概率的近似定价算法。该算法依赖于一个能够确定最大购买份额和具有该最大份额的结果总数的预言机。作者还证明，在受限于区间证券时，该预言机可以在多项式时间内实现。

Result: 首先，本文证明了CLUM的证券定价问题是#P-hard的，与LMSR面临的计算复杂性相似。其次，提出了一种针对CLUM的近似定价算法，能够在高概率下工作。最后，对于区间证券，论文表明所需的预言机可以在多项式时间内实现，从而增强了CLUM在实际金融应用中的可行性。

Conclusion: CLUM作为一种具有恒定最坏损失的AMM，在处理大量甚至无限结果方面具有显著优势。尽管其定价问题是#P-hard的，但通过提出的近似算法和针对区间证券的多项式时间预言机实现，CLUM在实际应用中，特别是在去中心化金融（DeFi）领域展现出巨大的潜力，这对于提高组合预测市场的流动性和效率至关重要。

Abstract: Automated Market Makers (AMMs) are used to provide liquidity for
combinatorial prediction markets that would otherwise be too thinly traded.
They offer both buy and sell prices for any of the doubly exponential many
possible securities that the market can offer. The problem of setting those
prices is known to be #P-hard for the original and most well-known AMM, the
logarithmic market scoring rule (LMSR) market maker [Chen et al., 2008]. We
focus on another natural AMM, the Constant Log Utility Market Maker (CLUM).
Unlike LMSR, whose worst-case loss bound grows with the number of outcomes,
CLUM has constant worst-case loss, allowing the market to add outcomes on the
fly and even operate over countably infinite many outcomes, among other
features. Simpler versions of CLUM underpin several Decentralized Finance
(DeFi) mechanisms including the Uniswap protocol that handles billions of
dollars of cryptocurrency trades daily. We first establish the computational
complexity of the problem: we prove that pricing securities is #P-hard for
CLUM, via a reduction from the model counting 2-SAT problem. In order to make
CLUM more practically viable, we propose an approximation algorithm for pricing
securities that works with high probability. This algorithm assumes access to
an oracle capable of determining the maximum shares purchased of any one
outcome and the total number of outcomes that has that maximum amount
purchased. We then show that this oracle can be implemented in polynomial time
when restricted to interval securities, which are used in designing financial
options.

</details>


### [69] [Repeated Sales with Heterogeneous Buyer Sophistication](https://arxiv.org/abs/2510.13088)
*Rishi Patel,Emmanouil Pountourakis,Samuel Taggart*

Main category: cs.GT

TL;DR: 本文分析了行为定价对重复销售耐用品的影响，并发现买方类型和时间范围对销售商的收益有显著影响。


<details>
  <summary>Details</summary>
Motivation: 探索重复销售非耐用品时，行为定价对卖方学习能力和收益的影响。

Method: 本文通过分析两阶段模型来理解短期影响，并通过分析无限期模型来研究长期影响。

Result: 在短期中，天真买家的引入会加剧需求减少，损害卖方收益。在长期中，天真买家会减弱极端的需求减少，卖方可以在有意义的学习中获得高收益。

Conclusion: 买家是“老练”还是“天真”以及互动的时限，对卖方进行行为定价和学习能力具有决定性的影响。

Abstract: This paper considers behavior-based price discrimination in the repeated sale
of a non-durable good to a single long-lived buyer, by a seller without
commitment power. We assume that there is a mixed population of forward-looking
``sophisticated'' buyers and myopic ``naive'' buyers. We investigate the impact
of these dynamics on the seller's ability to learn about the buyer and exploit
this learning for revenue. We obtain conclusions that differ dramatically with
the time horizon of the interactions. To understand short time horizons, we
analyze a two-period model, and find that the strategic demand reduction
observed with fully sophisticated buyers is robust to the introduction of naive
types. In fact, despite the inability of naive buyers to game the pricing
algorithm, their introduction can further harm the seller's revenue, due to
more intense demand reduction overall. For long horizons, we consider an
infinite-horizon model with time discounting. We find that the extreme demand
reduction predicted by previous work does not survive the introduction of naive
buyers. Instead, we observe equilibria where the seller learns meaningfully
despite the sophisticated buyers' demand reduction. We prove that for a natural
family of such equilibria, the seller's revenue is not just high, but
approximates the revenue attainable with commitment power, even when the
fraction of naive types is vanishingly small.

</details>


### [70] [Online Fair Division With Subsidy: When Do Envy-Free Allocations Exist, and at What Cost?](https://arxiv.org/abs/2510.13633)
*Pooja Kulkarni,Ruta Mehta,Vishnu V. Narayan,Tomasz Ponitka*

Main category: cs.GT

TL;DR: 本文研究了在线环境中不可分割物品的公平分配问题。主要贡献在于：1. 在线维持无嫉妒性：我们设计了在线算法，在每一步都保持可实现无嫉妒性，适用于可加估值及其超类。2. 确保低补贴：我们探讨了在线环境下保证无嫉妒性所需的补贴量，并确定了在某些估值类别中，最低补贴量可以很小。


<details>
  <summary>Details</summary>
Motivation: 在不可分割物品的分配中，无嫉妒（EF）分配不一定存在。为了解决这个问题，先前的研究表明，通过允许补贴可以实现无嫉妒的分配。然而，在线分配场景下，物品逐个到达并且必须立即分配，现有的离线方法不再适用。

Method: 本文首先分析了在子模或超模估值情况下，在线维持可实现无嫉妒性的困难。随后，针对可加估值及其超类，设计了在线算法以在每一步保持可实现无嫉妒性。在补贴方面，文章分析了即使对于可加估值，在线环境下的最低补贴量也可能显著增加。但同时，作者也识别了一些估值类别（如k-valued、rank-one、restricted additive和identical valuations），在这些类别中最低补贴量可以很小，并推导出了紧密的补贴界限。

Result: 1. 对于子模或超模估值，即使是二元边际，也无法总是在线保持可实现无嫉妒性。2. 设计了在线算法，对于可加估值以及k-demand和SPLC估值，可以在线保持可实现无嫉妒性。3. 对于可加估值，在线环境下所需的最低补贴可能高达$\\Omega(mn)$，远高于离线设置的$O(n)$。4. 对于k-valued、rank-one、restricted additive和identical valuations等估值类别，所需的最低补贴很小，且不依赖于m，并获得了（大部分）紧密的补贴界限。

Conclusion: 本文将离线环境下的可实现无嫉妒分配扩展到了在线环境。尽管在线分配带来了新的挑战，例如在特定估值下的高补贴需求，但通过设计特定的在线算法和识别某些估值类别，仍能有效地处理该问题。研究结果表明，在线公平分配的研究需要针对不同估值类型和具体场景进行细致分析和算法设计。

Abstract: We study the problem of fairly allocating $m$ indivisible items arriving
online, among $n$ (offline) agents. Although envy-freeness has emerged as the
archetypal fairness notion, envy-free (EF) allocations need not exist with
indivisible items. To bypass this, a prominent line of research demonstrates
that there exist allocations that can be made envy-free by allowing a subsidy.
Extensive work in the offline setting has focused on finding such envy-freeable
allocations with bounded subsidy. We extend this literature to an online
setting where items arrive one at a time and must be immediately and
irrevocably allocated. Our contributions are two-fold:
  1. Maintaining EF Online: We show that envy-freeability cannot always be
preserved online when the valuations are submodular or supermodular, even with
binary marginals. In contrast, we design online algorithms that maintain
envy-freeability at every step for the class of additive valuations, and for
its superclasses including $k$-demand and SPLC valuations.
  2. Ensuring Low Subsidy: We investigate the quantity of subsidy required to
guarantee envy-freeness online. Surprisingly, even for additive valuations, the
minimum subsidy may be as large as $\Omega(mn)$, in contrast to the offline
setting, where the bound is $O(n)$. On the positive side, we identify valuation
classes where the minimum subsidy is small (i.e., does not depend on $m$),
including $k$-valued, rank-one, restricted additive, and identical valuations,
and we obtain (mostly) tight subsidy bounds for these classes.

</details>
