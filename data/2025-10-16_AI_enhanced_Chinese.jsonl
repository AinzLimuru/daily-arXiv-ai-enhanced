{"id": "2510.12901", "categories": ["cs.CV", "cs.GR", "cs.LG", "cs.RO"], "pdf": "https://arxiv.org/pdf/2510.12901", "abs": "https://arxiv.org/abs/2510.12901", "authors": ["Haithem Turki", "Qi Wu", "Xin Kang", "Janick Martinez Esturo", "Shengyu Huang", "Ruilong Li", "Zan Gojcic", "Riccardo de Lutio"], "title": "SimULi: Real-Time LiDAR and Camera Simulation with Unscented Transforms", "comment": "Project page: https://research.nvidia.com/labs/sil/projects/simuli", "summary": "Rigorous testing of autonomous robots, such as self-driving vehicles, is\nessential to ensure their safety in real-world deployments. This requires\nbuilding high-fidelity simulators to test scenarios beyond those that can be\nsafely or exhaustively collected in the real-world. Existing neural rendering\nmethods based on NeRF and 3DGS hold promise but suffer from low rendering\nspeeds or can only render pinhole camera models, hindering their suitability to\napplications that commonly require high-distortion lenses and LiDAR data.\nMulti-sensor simulation poses additional challenges as existing methods handle\ncross-sensor inconsistencies by favoring the quality of one modality at the\nexpense of others. To overcome these limitations, we propose SimULi, the first\nmethod capable of rendering arbitrary camera models and LiDAR data in\nreal-time. Our method extends 3DGUT, which natively supports complex camera\nmodels, with LiDAR support, via an automated tiling strategy for arbitrary\nspinning LiDAR models and ray-based culling. To address cross-sensor\ninconsistencies, we design a factorized 3D Gaussian representation and\nanchoring strategy that reduces mean camera and depth error by up to 40%\ncompared to existing methods. SimULi renders 10-20x faster than ray tracing\napproaches and 1.5-10x faster than prior rasterization-based work (and handles\na wider range of camera models). When evaluated on two widely benchmarked\nautonomous driving datasets, SimULi matches or exceeds the fidelity of existing\nstate-of-the-art methods across numerous camera and LiDAR metrics.", "AI": {"tldr": "SimULi \u662f\u4e00\u79cd\u65b0\u7684\u591a\u4f20\u611f\u5668\u6a21\u62df\u65b9\u6cd5\uff0c\u80fd\u591f\u5b9e\u65f6\u6e32\u67d3\u4efb\u610f\u76f8\u673a\u6a21\u578b\u548c LiDAR \u6570\u636e\uff0c\u89e3\u51b3\u4e86\u73b0\u6709\u65b9\u6cd5\u6e32\u67d3\u901f\u5ea6\u6162\u3001\u65e0\u6cd5\u5904\u7406\u9ad8\u7578\u53d8\u955c\u5934\u548c\u8de8\u4f20\u611f\u5668\u4e0d\u4e00\u81f4\u7684\u95ee\u9898\uff0c\u5e76\u663e\u8457\u63d0\u9ad8\u4e86\u6e32\u67d3\u901f\u5ea6\u548c\u7cbe\u5ea6\u3002", "motivation": "\u73b0\u6709\u795e\u7ecf\u6e32\u67d3\u65b9\u6cd5\uff08\u5982 NeRF \u548c 3DGS\uff09\u5728\u81ea\u52a8\u9a7e\u9a76\u6c7d\u8f66\u7b49\u81ea\u4e3b\u673a\u5668\u4eba\u7684\u4e25\u683c\u6d4b\u8bd5\u4e2d\u5b58\u5728\u5c40\u9650\u6027\uff0c\u4f8b\u5982\u6e32\u67d3\u901f\u5ea6\u6162\u3001\u4ec5\u652f\u6301\u9488\u5b54\u76f8\u673a\u6a21\u578b\uff0c\u4ee5\u53ca\u5728\u5904\u7406\u591a\u4f20\u611f\u5668\u6570\u636e\u65f6\u5b58\u5728\u8de8\u4f20\u611f\u5668\u4e0d\u4e00\u81f4\u3002", "method": "SimULi \u901a\u8fc7\u6269\u5c55 3DGUT \u4ee5\u652f\u6301 LiDAR \u6570\u636e\uff0c\u5e76\u91c7\u7528\u81ea\u52a8\u5207\u7247\u7b56\u7565\u548c\u57fa\u4e8e\u5c04\u7ebf\u7684\u5254\u9664\u6280\u672f\uff0c\u5b9e\u73b0\u4e86\u4efb\u610f\u76f8\u673a\u6a21\u578b\u548c LiDAR \u6570\u636e\u7684\u5b9e\u65f6\u6e32\u67d3\u3002\u4e3a\u4e86\u89e3\u51b3\u8de8\u4f20\u611f\u5668\u4e0d\u4e00\u81f4\u95ee\u9898\uff0cSimULi \u8bbe\u8ba1\u4e86\u4e00\u79cd\u5206\u89e3\u7684 3D \u9ad8\u65af\u8868\u793a\u548c\u951a\u5b9a\u7b56\u7565\u3002", "result": "SimULi \u7684\u6e32\u67d3\u901f\u5ea6\u6bd4\u5149\u7ebf\u8ffd\u8e2a\u65b9\u6cd5\u5feb 10-20 \u500d\uff0c\u6bd4\u4e4b\u524d\u7684\u57fa\u4e8e\u5149\u6805\u5316\u7684\u65b9\u6cd5\u5feb 1.5-10 \u500d\u3002\u5b83\u5c06\u5e73\u5747\u76f8\u673a\u548c\u6df1\u5ea6\u8bef\u5dee\u964d\u4f4e\u4e86\u9ad8\u8fbe 40%\u3002\u5728\u4e24\u4e2a\u81ea\u52a8\u9a7e\u9a76\u6570\u636e\u96c6\u4e0a\uff0cSimULi \u5728\u591a\u9879\u76f8\u673a\u548c LiDAR \u6307\u6807\u4e0a\u8fbe\u5230\u6216\u8d85\u8fc7\u4e86\u73b0\u6709\u6700\u5148\u8fdb\u65b9\u6cd5\u7684\u7cbe\u5ea6\u3002", "conclusion": "SimULi \u9996\u6b21\u5b9e\u73b0\u4e86\u5bf9\u4efb\u610f\u76f8\u673a\u6a21\u578b\u548c LiDAR \u6570\u636e\u7684\u5b9e\u65f6\u6e32\u67d3\uff0c\u89e3\u51b3\u4e86\u73b0\u6709\u65b9\u6cd5\u7684\u5173\u952e\u9650\u5236\uff0c\u4e3a\u81ea\u52a8\u9a7e\u9a76\u7b49\u9886\u57df\u7684\u81ea\u4e3b\u673a\u5668\u4eba\u6d4b\u8bd5\u63d0\u4f9b\u4e86\u9ad8\u6548\u3001\u9ad8\u4fdd\u771f\u7684\u6a21\u62df\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2510.12909", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2510.12909", "abs": "https://arxiv.org/abs/2510.12909", "authors": ["Takafumi Nogami", "Satoshi Kagiwada", "Hitoshi Iyatomi"], "title": "Robust Plant Disease Diagnosis with Few Target-Domain Samples", "comment": "7 pages, 2 figures. Accepted at the IEEE International Conference on\n  Visual Communications and Image Processing (VCIP) 2025. Extended version", "summary": "Various deep learning-based systems have been proposed for accurate and\nconvenient plant disease diagnosis, achieving impressive performance. However,\nrecent studies show that these systems often fail to maintain diagnostic\naccuracy on images captured under different conditions from the training\nenvironment -- an essential criterion for model robustness. Many deep learning\nmethods have shown high accuracy in plant disease diagnosis. However, they\noften struggle to generalize to images taken in conditions that differ from the\ntraining setting. This drop in performance stems from the subtle variability of\ndisease symptoms and domain gaps -- differences in image context and\nenvironment. The root cause is the limited diversity of training data relative\nto task complexity, making even advanced models vulnerable in unseen domains.\nTo tackle this challenge, we propose a simple yet highly adaptable learning\nframework called Target-Aware Metric Learning with Prioritized Sampling (TMPS),\ngrounded in metric learning. TMPS operates under the assumption of access to a\nlimited number of labeled samples from the target (deployment) domain and\nleverages these samples effectively to improve diagnostic robustness. We assess\nTMPS on a large-scale automated plant disease diagnostic task using a dataset\ncomprising 223,073 leaf images sourced from 23 agricultural fields, spanning 21\ndiseases and healthy instances across three crop species. By incorporating just\n10 target domain samples per disease into training, TMPS surpasses models\ntrained using the same combined source and target samples, and those fine-tuned\nwith these target samples after pre-training on source data. It achieves\naverage macro F1 score improvements of 7.3 and 3.6 points, respectively, and a\nremarkable 18.7 and 17.1 point improvement over the baseline and conventional\nmetric learning.", "AI": {"tldr": "\u690d\u7269\u75c5\u5bb3\u8bca\u65ad\u4e2d\u7684\u6df1\u5ea6\u5b66\u4e60\u6a21\u578b\u5728\u8bad\u7ec3\u73af\u5883\u53d1\u751f\u53d8\u5316\u65f6\u5176\u51c6\u786e\u6027\u4f1a\u5927\u5e45\u4e0b\u964d\u3002\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3aTMPS\u7684\u5ea6\u91cf\u5b66\u4e60\u6846\u67b6\uff0c\u901a\u8fc7\u5c11\u91cf\u76ee\u6807\u57df\u6837\u672c\u663e\u8457\u63d0\u9ad8\u4e86\u8bca\u65ad\u7684\u9c81\u68d2\u6027\uff0c\u5e76\u5728\u5b9e\u9645\u6570\u636e\u4e0a\u53d6\u5f97\u4e86\u663e\u8457\u7684\u6027\u80fd\u63d0\u5347\u3002", "motivation": "\u76ee\u524d\u57fa\u4e8e\u6df1\u5ea6\u5b66\u4e60\u7684\u690d\u7269\u75c5\u5bb3\u8bca\u65ad\u7cfb\u7edf\u5728\u8bad\u7ec3\u73af\u5883\u53d1\u751f\u53d8\u5316\u65f6\u8bca\u65ad\u51c6\u786e\u6027\u4e0d\u4f73\uff0c\u8fd9\u4e3b\u8981\u662f\u7531\u4e8e\u75c5\u5bb3\u75c7\u72b6\u7684\u7ec6\u5fae\u53d8\u5316\u4ee5\u53ca\u4e0d\u540c\u56fe\u50cf\u4e0a\u4e0b\u6587\u548c\u73af\u5883\u9020\u6210\u7684\u57df\u95f4\u9699\u3002\u6839\u672c\u539f\u56e0\u662f\u8bad\u7ec3\u6570\u636e\u7684\u591a\u6837\u6027\u76f8\u5bf9\u4e8e\u4efb\u52a1\u590d\u6742\u6027\u6709\u9650\u3002", "method": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u7b80\u5355\u4f46\u9ad8\u5ea6\u9002\u5e94\u6027\u7684\u5b66\u4e60\u6846\u67b6\uff0c\u540d\u4e3a\u201c\u76ee\u6807\u611f\u77e5\u5ea6\u91cf\u5b66\u4e60\u4e0e\u4f18\u5148\u7ea7\u91c7\u6837\u201d\uff08TMPS\uff09\uff0c\u8be5\u65b9\u6cd5\u4ee5\u5ea6\u91cf\u5b66\u4e60\u4e3a\u57fa\u7840\u3002TMPS\u5047\u8bbe\u53ef\u4ee5\u8bbf\u95ee\u76ee\u6807\uff08\u90e8\u7f72\uff09\u57df\u4e2d\u6570\u91cf\u6709\u9650\u7684\u5e26\u6807\u7b7e\u6837\u672c\uff0c\u5e76\u6709\u6548\u5229\u7528\u8fd9\u4e9b\u6837\u672c\u6765\u63d0\u9ad8\u8bca\u65ad\u7684\u9c81\u68d2\u6027\u3002", "result": "TMPS\u5728\u5305\u542b223,073\u5f20\u53f6\u7247\u56fe\u50cf\u7684\u5927\u89c4\u6a21\u81ea\u52a8\u5316\u690d\u7269\u75c5\u5bb3\u8bca\u65ad\u4efb\u52a1\u4e0a\u8fdb\u884c\u4e86\u8bc4\u4f30\u3002\u901a\u8fc7\u5728\u8bad\u7ec3\u4e2d\u52a0\u5165\u6bcf\u4e2a\u75c5\u5bb3\u4ec510\u4e2a\u76ee\u6807\u57df\u6837\u672c\uff0cTMPS\u8d85\u8d8a\u4e86\u4f7f\u7528\u76f8\u540c\u7ec4\u5408\u7684\u6e90\u6837\u672c\u548c\u76ee\u6807\u6837\u672c\u8bad\u7ec3\u7684\u6a21\u578b\uff0c\u4ee5\u53ca\u5728\u6e90\u6570\u636e\u4e0a\u9884\u8bad\u7ec3\u540e\u7528\u8fd9\u4e9b\u76ee\u6807\u6837\u672c\u8fdb\u884c\u5fae\u8c03\u7684\u6a21\u578b\u3002\u5176\u5b8f\u89c2F1\u5206\u6570\u5e73\u5747\u63d0\u9ad8\u4e867.3\u548c3.6\u4e2a\u767e\u5206\u70b9\uff0c\u5e76\u4e14\u6bd4\u57fa\u7ebf\u548c\u4f20\u7edf\u5ea6\u91cf\u5b66\u4e60\u5206\u522b\u63d0\u9ad8\u4e8618.7\u548c17.1\u4e2a\u767e\u5206\u70b9\u3002", "conclusion": "TMPS\u6846\u67b6\u901a\u8fc7\u6709\u6548\u5229\u7528\u5c11\u91cf\u76ee\u6807\u57df\u7684\u5e26\u6807\u7b7e\u6837\u672c\uff0c\u663e\u8457\u63d0\u9ad8\u4e86\u6df1\u5ea6\u5b66\u4e60\u6a21\u578b\u5728\u690d\u7269\u75c5\u5bb3\u8bca\u65ad\u4efb\u52a1\u4e0a\u7684\u9c81\u68d2\u6027\u548c\u6cdb\u5316\u80fd\u529b\uff0c\u89e3\u51b3\u4e86\u6a21\u578b\u5728\u4e0d\u540c\u90e8\u7f72\u73af\u5883\u4e0b\u6027\u80fd\u4e0b\u964d\u7684\u6311\u6218\u3002"}}
{"id": "2510.13171", "categories": ["cs.IT", "math.IT"], "pdf": "https://arxiv.org/pdf/2510.13171", "abs": "https://arxiv.org/abs/2510.13171", "authors": ["Jun Qian", "Ross Murch", "Khaled B. Letaief"], "title": "On the performance of Active STAR-RIS-Assisted Cell-Free Massive MIMO Systems with Phase Errors and Channel Aging", "comment": "5 pages, 3 figures, accepted by IEEE WCL", "summary": "Active reconfigurable intelligent surfaces (RISs) employ amplification to\novercome attenuation caused by the RIS cascaded link. In this paper, we analyze\nthe effects of phase errors and channel aging in active simultaneously\ntransmitting and reflecting (STAR) RIS-assisted cell-free massive\nmultiple-input multiple-output (MIMO) systems. By leveraging a spatially\ncorrelated Rayleigh fading model, this paper derives minimum mean square error\nestimate-based channel estimates and formulates closed-form expressions for\ndownlink spectral efficiency. This analytical framework enables a comprehensive\nevaluation of the effects of channel aging and uniformly distributed phase\nerrors on system performance. The results demonstrate that active STAR-RISs can\neffectively compensate for the adverse effects of phase errors and channel\naging. To counteract the impact of channel aging, we propose practical\nguidelines for resource-block-length design. Also, an increase in APs and\nSTAR-RIS elements, along with a larger amplification factor, can alleviate\nperformance degradation.", "AI": {"tldr": "\u672c\u6587\u5206\u6790\u4e86\u6709\u6e90STAR-RIS\u8f85\u52a9\u7684\u65e0\u8702\u7a9d\u5927\u89c4\u6a21MIMO\u7cfb\u7edf\u4e2d\uff0c\u76f8\u4f4d\u8bef\u5dee\u548c\u4fe1\u9053\u8001\u5316\u7684\u5f71\u54cd\uff0c\u5e76\u63d0\u51fa\u4e86\u76f8\u5e94\u7684\u6027\u80fd\u63d0\u5347\u7b56\u7565\u3002", "motivation": "\u5206\u6790\u76f8\u4f4d\u8bef\u5dee\u548c\u4fe1\u9053\u8001\u5316\u5bf9\u6709\u6e90STAR-RIS\u8f85\u52a9\u7684\u65e0\u8702\u7a9d\u5927\u89c4\u6a21MIMO\u7cfb\u7edf\u6027\u80fd\u7684\u5f71\u54cd\uff0c\u5e76\u63d0\u51fa\u514b\u670d\u8fd9\u4e9b\u4e0d\u5229\u5f71\u54cd\u7684\u65b9\u6cd5\u3002", "method": "\u5229\u7528\u7a7a\u95f4\u76f8\u5173\u7684\u745e\u5229\u8870\u843d\u6a21\u578b\uff0c\u63a8\u5bfc\u4e86\u57fa\u4e8e\u6700\u5c0f\u5747\u65b9\u8bef\u5dee\u4f30\u8ba1\u7684\u4fe1\u9053\u4f30\u8ba1\uff0c\u5e76\u5efa\u7acb\u4e86\u4e0b\u884c\u94fe\u8def\u9891\u8c31\u6548\u7387\u7684\u95ed\u5f0f\u8868\u8fbe\u5f0f\u3002", "result": "\u6709\u6e90STAR-RIS\u80fd\u6709\u6548\u8865\u507f\u76f8\u4f4d\u8bef\u5dee\u548c\u4fe1\u9053\u8001\u5316\u7684\u4e0d\u5229\u5f71\u54cd\u3002\u589e\u52a0AP\u548cSTAR-RIS\u5355\u5143\u6570\u91cf\u4ee5\u53ca\u66f4\u5927\u7684\u653e\u5927\u56e0\u5b50\u53ef\u4ee5\u51cf\u8f7b\u6027\u80fd\u4e0b\u964d\u3002", "conclusion": "\u6709\u6e90STAR-RISs\u5728\u8865\u507f\u76f8\u4f4d\u8bef\u5dee\u548c\u4fe1\u9053\u8001\u5316\u65b9\u9762\u8868\u73b0\u51fa\u8272\uff0c\u4e3a\u672a\u67656G\u7cfb\u7edf\u4e2d\u7684\u667a\u80fd\u8868\u9762\u90e8\u7f72\u63d0\u4f9b\u4e86\u6307\u5bfc\u539f\u5219\u3002"}}
{"id": "2510.12846", "categories": ["cs.GT", "math.PR", "91A05"], "pdf": "https://arxiv.org/pdf/2510.12846", "abs": "https://arxiv.org/abs/2510.12846", "authors": ["Andrea Collevecchio", "Gabor Lugosi", "Adrian Vetta", "Rui-Ray Zhang"], "title": "Finding a Nash equilibrium of a random win-lose game in expected polynomial time", "comment": null, "summary": "A long-standing open problem in algorithmic game theory asks whether or not\nthere is a polynomial time algorithm to compute a Nash equilibrium in a random\nbimatrix game. We study random win-lose games, where the entries of the\n$n\\times n$ payoff matrices are independent and identically distributed\n(i.i.d.) Bernoulli random variables with parameter $p=p(n)$. We prove that, for\nnearly all values of the parameter $p=p(n)$, there is an expected\npolynomial-time algorithm to find a Nash equilibrium in a random win-lose game.\nMore precisely, if $p\\sim cn^{-a}$ for some parameters $a,c\\ge 0$, then there\nis an expected polynomial-time algorithm whenever $a\\not\\in \\{1/2, 1\\}$. In\naddition, if $a = 1/2$ there is an efficient algorithm if either $c \\le e^{-52}\n2^{-8} $ or $c\\ge 0.977$. If $a=1$, then there is an expected polynomial-time\nalgorithm if either $c\\le 0.3849$ or $c\\ge \\log^9 n$.", "AI": {"tldr": "\u672c\u6587\u63a2\u8ba8\u4e86\u5728\u968f\u673a\u4e8c\u4eba\u535a\u5f08\u4e2d\u8ba1\u7b97\u7eb3\u4ec0\u5747\u8861\u7684\u591a\u9879\u5f0f\u65f6\u95f4\u7b97\u6cd5\u95ee\u9898\u3002", "motivation": "\u5728\u7b97\u6cd5\u535a\u5f08\u8bba\u4e2d\uff0c\u4e00\u4e2a\u957f\u671f\u5b58\u5728\u7684\u5f00\u653e\u95ee\u9898\u662f\u662f\u5426\u5b58\u5728\u4e00\u4e2a\u591a\u9879\u5f0f\u65f6\u95f4\u7b97\u6cd5\u6765\u8ba1\u7b97\u968f\u673a\u4e8c\u4eba\u535a\u5f08\u4e2d\u7684\u7eb3\u4ec0\u5747\u8861\u3002", "method": "\u672c\u6587\u7814\u7a76\u4e86\u968f\u673a\u80dc\u8d1f\u535a\u5f08\uff0c\u5176\u4e2d $n \\times n$ \u6536\u76ca\u77e9\u9635\u7684\u6761\u76ee\u662f\u72ec\u7acb\u540c\u5206\u5e03 (i.i.d.) \u7684\u4f2f\u52aa\u5229\u968f\u673a\u53d8\u91cf\uff0c\u53c2\u6570\u4e3a $p=p(n)$\u3002", "result": "\u672c\u6587\u8bc1\u660e\u4e86\uff0c\u5bf9\u4e8e\u51e0\u4e4e\u6240\u6709\u53c2\u6570 $p=p(n)$ \u7684\u503c\uff0c\u90fd\u5b58\u5728\u4e00\u4e2a\u671f\u671b\u591a\u9879\u5f0f\u65f6\u95f4\u7b97\u6cd5\u6765\u627e\u5230\u968f\u673a\u80dc\u8d1f\u535a\u5f08\u4e2d\u7684\u7eb3\u4ec0\u5747\u8861\u3002\u66f4\u5177\u4f53\u5730\u8bf4\uff0c\u5982\u679c $p\\sim cn^{-a}$\uff08\u5176\u4e2d $a,c\\ge 0$\uff09\uff0c\u90a3\u4e48\u5f53 $a\\not\\in \\{1/2, 1\\}$ \u65f6\uff0c\u5b58\u5728\u4e00\u4e2a\u671f\u671b\u591a\u9879\u5f0f\u65f6\u95f4\u7b97\u6cd5\u3002\u6b64\u5916\uff0c\u5982\u679c $a=1/2$\uff0c\u5f53 $c \\le e^{-52} 2^{-8}$ \u6216 $c\\ge 0.977$ \u65f6\uff0c\u5b58\u5728\u4e00\u4e2a\u6709\u6548\u7b97\u6cd5\u3002\u5982\u679c $a=1$\uff0c\u5f53 $c\\le 0.3849$ \u6216 $c\\ge \\log^9 n$ \u65f6\uff0c\u5b58\u5728\u4e00\u4e2a\u671f\u671b\u591a\u9879\u5f0f\u65f6\u95f4\u7b97\u6cd5\u3002", "conclusion": "\u672c\u6587\u5728\u968f\u673a\u80dc\u8d1f\u535a\u5f08\u4e2d\u627e\u5230\u4e86\u4e00\u4e2a\u671f\u671b\u591a\u9879\u5f0f\u65f6\u95f4\u7b97\u6cd5\u6765\u8ba1\u7b97\u7eb3\u4ec0\u5747\u8861\uff0c\u5e76\u4e3a\u7b97\u6cd5\u7684\u9002\u7528\u8303\u56f4\u63d0\u4f9b\u4e86\u8be6\u7ec6\u7684\u53c2\u6570\u6761\u4ef6\u3002"}}
{"id": "2510.12979", "categories": ["cs.AI", "cs.CL"], "pdf": "https://arxiv.org/pdf/2510.12979", "abs": "https://arxiv.org/abs/2510.12979", "authors": ["Wei Fan", "Wenlin Yao", "Zheng Li", "Feng Yao", "Xin Liu", "Liang Qiu", "Qingyu Yin", "Yangqiu Song", "Bing Yin"], "title": "DeepPlanner: Scaling Planning Capability for Deep Research Agents via Advantage Shaping", "comment": "Under Review", "summary": "Large language models (LLMs) augmented with multi-step reasoning and action\ngeneration abilities have shown promise in leveraging external tools to tackle\ncomplex tasks that require long-horizon planning. However, existing approaches\neither rely on implicit planning in the reasoning stage or introduce explicit\nplanners without systematically addressing how to optimize the planning stage.\nAs evidence, we observe that under vanilla reinforcement learning (RL),\nplanning tokens exhibit significantly higher entropy than other action tokens,\nrevealing uncertain decision points that remain under-optimized. To address\nthis, we propose DeepPlanner, an end-to-end RL framework that effectively\nenhances the planning capabilities of deep research agents. Our approach shapes\ntoken-level advantage with an entropy-based term to allocate larger updates to\nhigh entropy tokens, and selectively upweights sample-level advantages for\nplanning-intensive rollouts. Extensive experiments across seven deep research\nbenchmarks demonstrate that DeepPlanner improves planning quality and achieves\nstate-of-the-art results under a substantially lower training budget.", "AI": {"tldr": "DeepPlanner\u662f\u4e00\u4e2a\u7aef\u5230\u7aef\u7684\u5f3a\u5316\u5b66\u4e60\u6846\u67b6\uff0c\u5b83\u901a\u8fc7\u71b5\u57fa\u9879\u4f18\u5316\u89c4\u5212\u9636\u6bb5\uff0c\u5e76\u9009\u62e9\u6027\u5730\u589e\u52a0\u89c4\u5212\u5bc6\u96c6\u578brollout\u7684\u6837\u672c\u7ea7\u4f18\u52bf\uff0c\u4ece\u800c\u663e\u8457\u63d0\u5347\u4e86\u5927\u578b\u8bed\u8a00\u6a21\u578b\u7684\u7814\u7a76\u89c4\u5212\u80fd\u529b\uff0c\u5e76\u5728\u591a\u4e2a\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u4ee5\u66f4\u4f4e\u7684\u8bad\u7ec3\u6210\u672c\u53d6\u5f97\u4e86\u6700\u5148\u8fdb\u7684\u7ed3\u679c\u3002", "motivation": "\u73b0\u6709\u7684\u5927\u578b\u8bed\u8a00\u6a21\u578b\u5728\u5904\u7406\u9700\u8981\u957f\u671f\u89c4\u5212\u7684\u590d\u6742\u4efb\u52a1\u65f6\uff0c\u5176\u89c4\u5212\u9636\u6bb5\u5b58\u5728\u4e0d\u8db3\uff0c\u4e3b\u8981\u4f53\u73b0\u5728\uff1a\u8981\u4e48\u4f9d\u8d56\u9690\u5f0f\u89c4\u5212\uff0c\u8981\u4e48\u5f15\u5165\u663e\u5f0f\u89c4\u5212\u4f46\u672a\u80fd\u7cfb\u7edf\u6027\u5730\u4f18\u5316\u8be5\u9636\u6bb5\u3002\u8bc1\u636e\u8868\u660e\uff0c\u5728vanilla\u5f3a\u5316\u5b66\u4e60\u4e0b\uff0c\u89c4\u5212token\u7684\u71b5\u660e\u663e\u9ad8\u4e8e\u5176\u4ed6\u884c\u52a8token\uff0c\u8fd9\u63ed\u793a\u4e86\u672a\u88ab\u4f18\u5316\u7684\u4e0d\u786e\u5b9a\u51b3\u7b56\u70b9\u3002", "method": "DeepPlanner\u662f\u4e00\u4e2a\u7aef\u5230\u7aef\u7684\u5f3a\u5316\u5b66\u4e60\u6846\u67b6\u3002\u5b83\u901a\u8fc7\u4ee5\u4e0b\u65b9\u5f0f\u4f18\u5316\u89c4\u5212\u9636\u6bb5\uff1a1. \u4f7f\u7528\u4e00\u4e2a\u57fa\u4e8e\u71b5\u7684\u9879\u6765\u5851\u9020token\u7ea7\u522b\u7684\u4f18\u52bf\uff0c\u4ece\u800c\u4e3a\u9ad8\u71b5token\u5206\u914d\u66f4\u5927\u7684\u66f4\u65b0\u30022. \u9009\u62e9\u6027\u5730\u589e\u52a0\u89c4\u5212\u5bc6\u96c6\u578brollout\u7684\u6837\u672c\u7ea7\u522b\u4f18\u52bf\u3002", "result": "DeepPlanner\u5728\u4e03\u4e2a\u6df1\u5ea6\u7814\u7a76\u57fa\u51c6\u6d4b\u8bd5\u4e2d\uff0c\u63d0\u9ad8\u4e86\u89c4\u5212\u8d28\u91cf\uff0c\u5e76\u5728\u663e\u8457\u964d\u4f4e\u8bad\u7ec3\u9884\u7b97\u7684\u60c5\u51b5\u4e0b\u53d6\u5f97\u4e86\u6700\u5148\u8fdb\u7684\u7ed3\u679c\u3002", "conclusion": "DeepPlanner\u901a\u8fc7\u5176\u72ec\u7279\u7684\u4f18\u5316\u7b56\u7565\uff0c\u6709\u6548\u63d0\u5347\u4e86\u5927\u578b\u8bed\u8a00\u6a21\u578b\u7684\u7814\u7a76\u89c4\u5212\u80fd\u529b\uff0c\u89e3\u51b3\u4e86\u73b0\u6709\u65b9\u6cd5\u5728\u89c4\u5212\u9636\u6bb5\u7684\u4e0d\u8db3\uff0c\u5e76\u5c55\u73b0\u51fa\u5353\u8d8a\u7684\u6027\u80fd\u548c\u8bad\u7ec3\u6548\u7387\u3002"}}
{"id": "2510.12872", "categories": ["cs.MA", "cs.AI", "stat.ML"], "pdf": "https://arxiv.org/pdf/2510.12872", "abs": "https://arxiv.org/abs/2510.12872", "authors": ["Hancheng Ye", "Zhengqi Gao", "Mingyuan Ma", "Qinsi Wang", "Yuzhe Fu", "Ming-Yu Chung", "Yueqian Lin", "Zhijian Liu", "Jianyi Zhang", "Danyang Zhuo", "Yiran Chen"], "title": "KVCOMM: Online Cross-context KV-cache Communication for Efficient LLM-based Multi-agent Systems", "comment": "Accepted for publication in NeurIPS2025. Code is available at\n  \\url{https://github.com/HankYe/KVCOMM}", "summary": "Multi-agent large language model (LLM) systems are increasingly adopted for\ncomplex language processing tasks that require communication and coordination\namong agents. However, these systems often suffer substantial overhead from\nrepeated reprocessing of overlapping contexts across agents. In typical\npipelines, once an agent receives a message from its predecessor, the full\ncontext-including prior turns-must be reprocessed from scratch, leading to\ninefficient processing. While key-value (KV) caching is an effective solution\nfor avoiding redundant computation in single-agent settings where prefixes\nremain unchanged, it cannot be directly reused in multi-agent scenarios due to\ndiverging prefixes introduced by agent-specific context extensions. We identify\nthat the core challenge lies in the offset variance of KV-caches across agents.\nTo address this, we propose KVCOMM, a training-free framework that enables\nefficient prefilling in multi-agent inference by reusing KV-caches and aligning\ncache offsets of overlapping contexts under diverse prefix contexts. KVCOMM\nestimates and adjusts KV-caches for shared content by referencing a pool of\ncached examples-termed anchors-that store observed cache deviations under\nvarying prefixes. The anchor pool is maintained and updated online, allowing\ndynamic adaptation to distinct user requests and context structures. KVCOMM\nachieves over 70% reuse rate across diverse multi-agent workloads, including\nretrieval-augmented generation, math reasoning, and collaborative coding tasks,\nall without quality degradation. Particularly, when each fully-connected agent\nreceives 1K input tokens with 512 prefix tokens and 512 output tokens under a\nfive-agent setting, KVCOMM achieves up to 7.8x speedup compared to the standard\nprefill pipeline, reducing TTFT from ~430 ms to ~55 ms.", "AI": {"tldr": "KVCOMM\u662f\u4e00\u79cd\u65e0\u9700\u8bad\u7ec3\u7684\u6846\u67b6\uff0c\u901a\u8fc7\u91cd\u7528KV\u7f13\u5b58\u548c\u5bf9\u9f50\u91cd\u53e0\u4e0a\u4e0b\u6587\u7684\u7f13\u5b58\u504f\u79fb\u91cf\uff0c\u5b9e\u73b0\u591a\u667a\u80fd\u4f53LLM\u7cfb\u7edf\u4e2d\u7684\u9ad8\u6548\u9884\u586b\u5145\u3002", "motivation": "\u591a\u667a\u80fd\u4f53\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08LLM\uff09\u7cfb\u7edf\u5728\u5904\u7406\u590d\u6742\u8bed\u8a00\u4efb\u52a1\u65f6\uff0c\u7531\u4e8e\u667a\u80fd\u4f53\u4e4b\u95f4\u9700\u8981\u9891\u7e41\u901a\u4fe1\u548c\u534f\u8c03\uff0c\u5bfc\u81f4\u91cd\u590d\u5904\u7406\u91cd\u53e0\u4e0a\u4e0b\u6587\u7684\u5f00\u9500\u5de8\u5927\u3002\u4f20\u7edf\u7684KV\u7f13\u5b58\u6280\u672f\u5728\u5355\u667a\u80fd\u4f53\u573a\u666f\u4e2d\u6709\u6548\uff0c\u4f46\u5728\u591a\u667a\u80fd\u4f53\u573a\u666f\u4e2d\u7531\u4e8e\u524d\u7f00\u4e0a\u4e0b\u6587\u4e0d\u540c\u800c\u65e0\u6cd5\u76f4\u63a5\u5e94\u7528\u3002", "method": "KVCOMM\u901a\u8fc7\u5f15\u5165\u201c\u951a\u70b9\u201d\u6765\u89e3\u51b3\u591a\u667a\u80fd\u4f53KV\u7f13\u5b58\u7684\u504f\u79fb\u91cf\u5dee\u5f02\u95ee\u9898\u3002\u201c\u951a\u70b9\u201d\u5b58\u50a8\u4e86\u5728\u4e0d\u540c\u524d\u7f00\u4e0b\u89c2\u5bdf\u5230\u7684\u7f13\u5b58\u504f\u5dee\uff0c\u7528\u4e8e\u4f30\u8ba1\u548c\u8c03\u6574\u5171\u4eab\u5185\u5bb9\u7684KV\u7f13\u5b58\u3002\u951a\u70b9\u6c60\u5728\u7ebf\u7ef4\u62a4\u548c\u66f4\u65b0\uff0c\u4ee5\u9002\u5e94\u4e0d\u540c\u7684\u7528\u6237\u8bf7\u6c42\u548c\u4e0a\u4e0b\u6587\u7ed3\u6784\u3002", "result": "KVCOMM\u5728\u591a\u79cd\u591a\u667a\u80fd\u4f53\u5de5\u4f5c\u8d1f\u8f7d\uff08\u5305\u62ec\u68c0\u7d22\u589e\u5f3a\u751f\u6210\u3001\u6570\u5b66\u63a8\u7406\u548c\u534f\u540c\u7f16\u7801\u4efb\u52a1\uff09\u4e2d\u5b9e\u73b0\u4e86\u8d85\u8fc770%\u7684\u91cd\u7528\u7387\uff0c\u4e14\u6ca1\u6709\u8d28\u91cf\u4e0b\u964d\u3002\u5728\u7279\u5b9a\u4e94\u667a\u80fd\u4f53\u8bbe\u7f6e\u4e0b\uff08\u6bcf\u4e2a\u667a\u80fd\u4f53\u63a5\u65361K\u8f93\u5165token\uff0c512\u524d\u7f00token\u548c512\u8f93\u51fatoken\uff09\uff0cKVCOMM\u6bd4\u6807\u51c6\u9884\u586b\u5145\u7ba1\u9053\u63d0\u901f\u9ad8\u8fbe7.8\u500d\uff0c\u5c06TTFT\u4ece\u7ea6430\u6beb\u79d2\u51cf\u5c11\u5230\u7ea655\u6beb\u79d2\u3002", "conclusion": "KVCOMM\u901a\u8fc7\u521b\u65b0\u7684KV\u7f13\u5b58\u91cd\u7528\u673a\u5236\uff0c\u663e\u8457\u63d0\u5347\u4e86\u591a\u667a\u80fd\u4f53LLM\u7cfb\u7edf\u7684\u9884\u586b\u5145\u6548\u7387\uff0c\u89e3\u51b3\u4e86\u4f20\u7edfKV\u7f13\u5b58\u6280\u672f\u5728\u591a\u667a\u80fd\u4f53\u573a\u666f\u4e2d\u7684\u5c40\u9650\u6027\uff0c\u5b9e\u73b0\u4e86\u663e\u8457\u7684\u6027\u80fd\u63d0\u5347\u3002"}}
{"id": "2510.12954", "categories": ["cs.CV", "68T07, 68U10", "I.2.10; I.4.8; I.4.9"], "pdf": "https://arxiv.org/pdf/2510.12954", "abs": "https://arxiv.org/abs/2510.12954", "authors": ["Denis Rychkovskiy", "GPT-5"], "title": "CADE 2.5 - ZeResFDG: Frequency-Decoupled, Rescaled and Zero-Projected Guidance for SD/SDXL Latent Diffusion Models", "comment": "8 pages, 3 figures. Endorsed by Dr. Seyedmorteza Sadat (ETH Zurich).\n  The work introduces CADE 2.5 with ZeResFDG as a practical inference-time\n  guidance stack for SD/SDXL. Code and visual examples to be released on GitHub\n  and Hugging Face", "summary": "We introduce CADE 2.5 (Comfy Adaptive Detail Enhancer), a sampler-level\nguidance stack for SD/SDXL latent diffusion models. The central module,\nZeResFDG, unifies (i) frequency-decoupled guidance that reweights low- and\nhigh-frequency components of the guidance signal, (ii) energy rescaling that\nmatches the per-sample magnitude of the guided prediction to the positive\nbranch, and (iii) zero-projection that removes the component parallel to the\nunconditional direction. A lightweight spectral EMA with hysteresis switches\nbetween a conservative and a detail-seeking mode as structure crystallizes\nduring sampling. Across SD/SDXL samplers, ZeResFDG improves sharpness, prompt\nadherence, and artifact control at moderate guidance scales without any\nretraining. In addition, we employ a training-free inference-time stabilizer,\nQSilk Micrograin Stabilizer (quantile clamp + depth/edge-gated micro-detail\ninjection), which improves robustness and yields natural high-frequency\nmicro-texture at high resolutions with negligible overhead. For completeness we\nnote that the same rule is compatible with alternative parameterizations (e.g.,\nvelocity), which we briefly discuss in the Appendix; however, this paper\nfocuses on SD/SDXL latent diffusion models.", "AI": {"tldr": "CADE 2.5 \u662f\u4e00\u79cd\u7528\u4e8e SD/SDXL \u6f5c\u5728\u6269\u6563\u6a21\u578b\u7684\u91c7\u6837\u5668\u7ea7\u522b\u5f15\u5bfc\u5806\u6808\uff0c\u5b83\u901a\u8fc7\u6574\u5408\u9891\u7387\u89e3\u8026\u5f15\u5bfc\u3001\u80fd\u91cf\u91cd\u7f29\u653e\u548c\u96f6\u6295\u5f71\u6765\u7edf\u4e00 ZeResFDG\uff0c\u5e76\u901a\u8fc7\u8f7b\u91cf\u7ea7\u5149\u8c31 EMA \u6765\u5207\u6362\u6a21\u5f0f\u3002\u540c\u65f6\uff0c\u5b83\u8fd8\u5305\u542b\u4e00\u4e2a\u514d\u8bad\u7ec3\u7684\u63a8\u7406\u65f6\u95f4\u7a33\u5b9a\u5668 QSilk Micrograin Stabilizer\uff0c\u4ee5\u63d0\u9ad8\u9c81\u68d2\u6027\u5e76\u751f\u6210\u81ea\u7136\u7684\u9ad8\u9891\u5fae\u7eb9\u7406\u3002", "motivation": "\u5728\u4e0d\u8fdb\u884c\u518d\u8bad\u7ec3\u7684\u60c5\u51b5\u4e0b\uff0c\u63d0\u9ad8 SD/SDXL \u91c7\u6837\u5668\u7684\u56fe\u50cf\u6e05\u6670\u5ea6\u3001\u63d0\u793a\u9075\u5faa\u5ea6\u4ee5\u53ca\u5bf9\u4f2a\u5f71\u7684\u63a7\u5236\uff0c\u5e76\u5728\u9ad8\u5206\u8fa8\u7387\u4e0b\u751f\u6210\u81ea\u7136\u7684\u9ad8\u9891\u5fae\u7eb9\u7406\u3002", "method": "CADE 2.5 \u5305\u542b\u4e24\u4e2a\u6838\u5fc3\u6a21\u5757\uff1a1. ZeResFDG\uff1a\u7edf\u4e00\u4e86\u9891\u7387\u89e3\u8026\u5f15\u5bfc\uff08\u91cd\u65b0\u52a0\u6743\u5f15\u5bfc\u4fe1\u53f7\u7684\u4f4e\u9891\u548c\u9ad8\u9891\u5206\u91cf\uff09\uff0c\u80fd\u91cf\u91cd\u7f29\u653e\uff08\u5c06\u5f15\u5bfc\u9884\u6d4b\u7684\u6bcf\u6837\u672c\u5e45\u5ea6\u4e0e\u6b63\u5206\u652f\u5339\u914d\uff09\uff0c\u4ee5\u53ca\u96f6\u6295\u5f71\uff08\u79fb\u9664\u4e0e\u65e0\u6761\u4ef6\u65b9\u5411\u5e73\u884c\u7684\u5206\u91cf\uff09\u3002\u5b83\u8fd8\u5305\u542b\u4e00\u4e2a\u5e26\u6709\u6ede\u540e\u6027\u7684\u8f7b\u91cf\u7ea7\u5149\u8c31 EMA\uff0c\u53ef\u4ee5\u5728\u91c7\u6837\u8fc7\u7a0b\u4e2d\u7ed3\u6784\u9010\u6e10\u6e05\u6670\u65f6\u5728\u4fdd\u5b88\u6a21\u5f0f\u548c\u7ec6\u8282\u5bfb\u627e\u6a21\u5f0f\u4e4b\u95f4\u5207\u6362\u30022. QSilk Micrograin Stabilizer\uff1a\u4e00\u4e2a\u514d\u8bad\u7ec3\u7684\u63a8\u7406\u65f6\u95f4\u7a33\u5b9a\u5668\uff0c\u5b83\u7ed3\u5408\u4e86\u5206\u4f4d\u6570\u94b3\u4f4d\u548c\u6df1\u5ea6/\u8fb9\u7f18\u95e8\u63a7\u7684\u5fae\u7ec6\u8282\u6ce8\u5165\uff0c\u4ee5\u63d0\u9ad8\u9c81\u68d2\u6027\u5e76\u751f\u6210\u81ea\u7136\u7684\u9ad8\u9891\u5fae\u7eb9\u7406\u3002", "result": "ZeResFDG \u5728\u4e2d\u7b49\u5f15\u5bfc\u5c3a\u5ea6\u4e0b\uff0c\u63d0\u9ad8\u4e86 SD/SDXL \u91c7\u6837\u5668\u7684\u6e05\u6670\u5ea6\u3001\u63d0\u793a\u9075\u5faa\u5ea6\u4ee5\u53ca\u5bf9\u4f2a\u5f71\u7684\u63a7\u5236\uff0c\u65e0\u9700\u4efb\u4f55\u518d\u8bad\u7ec3\u3002QSilk Micrograin Stabilizer \u63d0\u9ad8\u4e86\u9c81\u68d2\u6027\uff0c\u5e76\u5728\u9ad8\u5206\u8fa8\u7387\u4e0b\u4ee5\u53ef\u5ffd\u7565\u7684\u5f00\u9500\u4ea7\u751f\u4e86\u81ea\u7136\u7684\u9ad8\u9891\u5fae\u7eb9\u7406\u3002", "conclusion": "CADE 2.5 \u901a\u8fc7\u5176\u521b\u65b0\u7684 ZeResFDG \u548c QSilk Micrograin Stabilizer \u6a21\u5757\uff0c\u663e\u8457\u63d0\u5347\u4e86 SD/SDXL \u6f5c\u5728\u6269\u6563\u6a21\u578b\u7684\u6027\u80fd\uff0c\u5728\u4e0d\u8fdb\u884c\u518d\u8bad\u7ec3\u7684\u60c5\u51b5\u4e0b\u6539\u5584\u4e86\u56fe\u50cf\u8d28\u91cf\u548c\u7ec6\u8282\uff0c\u5e76\u589e\u5f3a\u4e86\u751f\u6210\u9ad8\u5206\u8fa8\u7387\u81ea\u7136\u5fae\u7eb9\u7406\u7684\u80fd\u529b\u3002"}}
{"id": "2510.12952", "categories": ["cs.GT"], "pdf": "https://arxiv.org/pdf/2510.12952", "abs": "https://arxiv.org/abs/2510.12952", "authors": ["Maneesha Papireddygari", "Xintong Wang", "Bo Waggoner", "David M. Pennock"], "title": "Efficiency of Constant Log Utility Market Makers", "comment": null, "summary": "Automated Market Makers (AMMs) are used to provide liquidity for\ncombinatorial prediction markets that would otherwise be too thinly traded.\nThey offer both buy and sell prices for any of the doubly exponential many\npossible securities that the market can offer. The problem of setting those\nprices is known to be #P-hard for the original and most well-known AMM, the\nlogarithmic market scoring rule (LMSR) market maker [Chen et al., 2008]. We\nfocus on another natural AMM, the Constant Log Utility Market Maker (CLUM).\nUnlike LMSR, whose worst-case loss bound grows with the number of outcomes,\nCLUM has constant worst-case loss, allowing the market to add outcomes on the\nfly and even operate over countably infinite many outcomes, among other\nfeatures. Simpler versions of CLUM underpin several Decentralized Finance\n(DeFi) mechanisms including the Uniswap protocol that handles billions of\ndollars of cryptocurrency trades daily. We first establish the computational\ncomplexity of the problem: we prove that pricing securities is #P-hard for\nCLUM, via a reduction from the model counting 2-SAT problem. In order to make\nCLUM more practically viable, we propose an approximation algorithm for pricing\nsecurities that works with high probability. This algorithm assumes access to\nan oracle capable of determining the maximum shares purchased of any one\noutcome and the total number of outcomes that has that maximum amount\npurchased. We then show that this oracle can be implemented in polynomial time\nwhen restricted to interval securities, which are used in designing financial\noptions.", "AI": {"tldr": "\u672c\u6587\u63a2\u8ba8\u4e86\u81ea\u52a8\u5316\u505a\u5e02\u5546\uff08AMM\uff09\u5728\u7ec4\u5408\u9884\u6d4b\u5e02\u573a\u4e2d\u7684\u5e94\u7528\uff0c\u91cd\u70b9\u5173\u6ce8Constant Log Utility Market Maker (CLUM)\u3002\u6587\u7ae0\u8bc1\u660e\u4e86CLUM\u7684\u5b9a\u4ef7\u95ee\u9898\u662f#P-hard\u7684\uff0c\u5e76\u63d0\u51fa\u4e86\u4e00\u79cd\u8fd1\u4f3c\u7b97\u6cd5\u6765\u89e3\u51b3\u8fd9\u4e2a\u95ee\u9898\uff0c\u8be5\u7b97\u6cd5\u5728\u7279\u5b9a\u6761\u4ef6\u4e0b\u53ef\u4ee5\u591a\u9879\u5f0f\u65f6\u95f4\u5185\u5b9e\u73b0\u3002", "motivation": "\u7ec4\u5408\u9884\u6d4b\u5e02\u573a\u4e2d\u7684\u6d41\u52a8\u6027\u4e0d\u8db3\u662f\u4e00\u4e2a\u666e\u904d\u95ee\u9898\uff0c\u7279\u522b\u662f\u5bf9\u4e8e\u6570\u91cf\u5de8\u5927\u7684\u8bc1\u5238\u3002LMSR\u7b49\u4f20\u7edfAMM\u7684\u5b9a\u4ef7\u95ee\u9898\u4e0e\u6700\u574f\u60c5\u51b5\u635f\u5931\uff08worst-case loss\uff09\u9650\u5236\u4e86\u5176\u53ef\u6269\u5c55\u6027\uff0c\u5c24\u5176\u662f\u5728\u5904\u7406\u5927\u91cf\u751a\u81f3\u53ef\u6570\u65e0\u9650\u7ed3\u679c\u65f6\u3002\u56e0\u6b64\uff0c\u9700\u8981\u66f4\u6709\u6548\u4e14\u53ef\u6269\u5c55\u7684AMM\u3002", "method": "\u672c\u6587\u4e3b\u8981\u5173\u6ce8CLUM\uff0c\u8bc1\u660e\u4e86\u5176\u8bc1\u5238\u5b9a\u4ef7\u95ee\u9898\u662f#P-hard\u7684\uff0c\u5e76\u901a\u8fc7\u6a21\u578b\u8ba1\u65702-SAT\u95ee\u9898\u8fdb\u884c\u4e86\u5f52\u7ea6\u3002\u4e3a\u4e86\u63d0\u9ad8CLUM\u7684\u5b9e\u7528\u6027\uff0c\u4f5c\u8005\u63d0\u51fa\u4e86\u4e00\u79cd\u9ad8\u6982\u7387\u7684\u8fd1\u4f3c\u5b9a\u4ef7\u7b97\u6cd5\u3002\u8be5\u7b97\u6cd5\u4f9d\u8d56\u4e8e\u4e00\u4e2a\u80fd\u591f\u786e\u5b9a\u6700\u5927\u8d2d\u4e70\u4efd\u989d\u548c\u5177\u6709\u8be5\u6700\u5927\u4efd\u989d\u7684\u7ed3\u679c\u603b\u6570\u7684\u9884\u8a00\u673a\u3002\u4f5c\u8005\u8fd8\u8bc1\u660e\uff0c\u5728\u53d7\u9650\u4e8e\u533a\u95f4\u8bc1\u5238\u65f6\uff0c\u8be5\u9884\u8a00\u673a\u53ef\u4ee5\u5728\u591a\u9879\u5f0f\u65f6\u95f4\u5185\u5b9e\u73b0\u3002", "result": "\u9996\u5148\uff0c\u672c\u6587\u8bc1\u660e\u4e86CLUM\u7684\u8bc1\u5238\u5b9a\u4ef7\u95ee\u9898\u662f#P-hard\u7684\uff0c\u4e0eLMSR\u9762\u4e34\u7684\u8ba1\u7b97\u590d\u6742\u6027\u76f8\u4f3c\u3002\u5176\u6b21\uff0c\u63d0\u51fa\u4e86\u4e00\u79cd\u9488\u5bf9CLUM\u7684\u8fd1\u4f3c\u5b9a\u4ef7\u7b97\u6cd5\uff0c\u80fd\u591f\u5728\u9ad8\u6982\u7387\u4e0b\u5de5\u4f5c\u3002\u6700\u540e\uff0c\u5bf9\u4e8e\u533a\u95f4\u8bc1\u5238\uff0c\u8bba\u6587\u8868\u660e\u6240\u9700\u7684\u9884\u8a00\u673a\u53ef\u4ee5\u5728\u591a\u9879\u5f0f\u65f6\u95f4\u5185\u5b9e\u73b0\uff0c\u4ece\u800c\u589e\u5f3a\u4e86CLUM\u5728\u5b9e\u9645\u91d1\u878d\u5e94\u7528\u4e2d\u7684\u53ef\u884c\u6027\u3002", "conclusion": "CLUM\u4f5c\u4e3a\u4e00\u79cd\u5177\u6709\u6052\u5b9a\u6700\u574f\u635f\u5931\u7684AMM\uff0c\u5728\u5904\u7406\u5927\u91cf\u751a\u81f3\u65e0\u9650\u7ed3\u679c\u65b9\u9762\u5177\u6709\u663e\u8457\u4f18\u52bf\u3002\u5c3d\u7ba1\u5176\u5b9a\u4ef7\u95ee\u9898\u662f#P-hard\u7684\uff0c\u4f46\u901a\u8fc7\u63d0\u51fa\u7684\u8fd1\u4f3c\u7b97\u6cd5\u548c\u9488\u5bf9\u533a\u95f4\u8bc1\u5238\u7684\u591a\u9879\u5f0f\u65f6\u95f4\u9884\u8a00\u673a\u5b9e\u73b0\uff0cCLUM\u5728\u5b9e\u9645\u5e94\u7528\u4e2d\uff0c\u7279\u522b\u662f\u5728\u53bb\u4e2d\u5fc3\u5316\u91d1\u878d\uff08DeFi\uff09\u9886\u57df\u5c55\u73b0\u51fa\u5de8\u5927\u7684\u6f5c\u529b\uff0c\u8fd9\u5bf9\u4e8e\u63d0\u9ad8\u7ec4\u5408\u9884\u6d4b\u5e02\u573a\u7684\u6d41\u52a8\u6027\u548c\u6548\u7387\u81f3\u5173\u91cd\u8981\u3002"}}
{"id": "2510.12974", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2510.12974", "abs": "https://arxiv.org/abs/2510.12974", "authors": ["Tianyu Zhang", "Suyuchen Wang", "Chao Wang", "Juan Rodriguez", "Ahmed Masry", "Xiangru Jian", "Yoshua Bengio", "Perouz Taslakian"], "title": "Scope: Selective Cross-modal Orchestration of Visual Perception Experts", "comment": "14 pages, 2 figures", "summary": "Vision-language models (VLMs) benefit from multiple vision encoders, but\nnaively stacking them yields diminishing returns while multiplying inference\ncosts. We propose SCOPE, a Mixture-of-Encoders (MoEnc) framework that\ndynamically selects one specialized encoder per image-text pair via\ninstance-level routing, unlike token-level routing in traditional MoE. SCOPE\nmaintains a shared encoder and a pool of routed encoders. A lightweight router\nuses cross-attention between text prompts and shared visual features to select\nthe optimal encoder from the routed encoders. To train this router, we\nintroduce dual entropy regularization with auxiliary losses to balance\ndataset-level load distribution with instance-level routing confidence.\nRemarkably, SCOPE with one shared plus one routed encoder outperforms models\nusing all four extra encoders simultaneously, while reducing compute by\n24-49\\%. This demonstrates that intelligent encoder selection beats brute-force\naggregation, challenging the prevailing paradigm in multi-encoder VLMs.", "AI": {"tldr": "\u672c\u6587\u63d0\u51faSCOPE\uff0c\u4e00\u4e2a\u6df7\u5408\u7f16\u7801\u5668\uff08MoEnc\uff09\u6846\u67b6\uff0c\u5b83\u901a\u8fc7\u5b9e\u4f8b\u7ea7\u8def\u7531\u52a8\u6001\u9009\u62e9\u6bcf\u4e2a\u56fe\u50cf-\u6587\u672c\u5bf9\u7684\u4e13\u4e1a\u7f16\u7801\u5668\uff0c\u89e3\u51b3\u4e86\u73b0\u6709\u89c6\u89c9-\u8bed\u8a00\u6a21\u578b\u4e2d\u591a\u4e2a\u89c6\u89c9\u7f16\u7801\u5668\u5806\u53e0\u6548\u7387\u4f4e\u4e0b\u7684\u95ee\u9898\u3002", "motivation": "\u73b0\u6709\u7684\u89c6\u89c9-\u8bed\u8a00\u6a21\u578b\uff08VLMs\uff09\u901a\u8fc7\u5806\u53e0\u591a\u4e2a\u89c6\u89c9\u7f16\u7801\u5668\u6765\u63d0\u5347\u6027\u80fd\uff0c\u4f46\u8fd9\u4f1a\u5bfc\u81f4\u6536\u76ca\u9012\u51cf\u5e76\u5927\u5e45\u589e\u52a0\u63a8\u7406\u6210\u672c\u3002", "method": "SCOPE\u6846\u67b6\u5305\u542b\u4e00\u4e2a\u5171\u4eab\u7f16\u7801\u5668\u548c\u4e00\u7ec4\u8def\u7531\u7f16\u7801\u5668\u3002\u4e00\u4e2a\u8f7b\u91cf\u7ea7\u8def\u7531\u5668\u5229\u7528\u6587\u672c\u63d0\u793a\u548c\u5171\u4eab\u89c6\u89c9\u7279\u5f81\u4e4b\u95f4\u7684\u4ea4\u53c9\u6ce8\u610f\u529b\uff0c\u4e3a\u6bcf\u4e2a\u56fe\u50cf-\u6587\u672c\u5bf9\u52a8\u6001\u9009\u62e9\u6700\u4f73\u7684\u8def\u7531\u7f16\u7801\u5668\u3002\u4e3a\u4e86\u8bad\u7ec3\u8def\u7531\u5668\uff0c\u5f15\u5165\u4e86\u53cc\u91cd\u71b5\u6b63\u5219\u5316\u548c\u8f85\u52a9\u635f\u5931\uff0c\u4ee5\u5e73\u8861\u6570\u636e\u96c6\u7ea7\u522b\u7684\u8d1f\u8f7d\u5206\u914d\u548c\u5b9e\u4f8b\u7ea7\u522b\u7684\u8def\u7531\u7f6e\u4fe1\u5ea6\u3002", "result": "SCOPE\u5728\u53ea\u7528\u4e00\u4e2a\u5171\u4eab\u7f16\u7801\u5668\u548c\u4e00\u4e2a\u8def\u7531\u7f16\u7801\u5668\u7684\u60c5\u51b5\u4e0b\uff0c\u6027\u80fd\u8d85\u8d8a\u4e86\u540c\u65f6\u4f7f\u7528\u6240\u6709\u56db\u4e2a\u989d\u5916\u7f16\u7801\u5668\u7684\u6a21\u578b\uff0c\u540c\u65f6\u5c06\u8ba1\u7b97\u91cf\u51cf\u5c11\u4e8624-49%\u3002", "conclusion": "SCOPE\u901a\u8fc7\u667a\u80fd\u7684\u7f16\u7801\u5668\u9009\u62e9\u800c\u975e\u66b4\u529b\u805a\u5408\uff0c\u98a0\u8986\u4e86\u591a\u7f16\u7801\u5668\u89c6\u89c9-\u8bed\u8a00\u6a21\u578b\u7684\u73b0\u6709\u8303\u5f0f\uff0c\u8bc1\u660e\u4e86\u5728\u4fdd\u6301\u751a\u81f3\u8d85\u8d8a\u6027\u80fd\u7684\u540c\u65f6\uff0c\u53ef\u4ee5\u663e\u8457\u964d\u4f4e\u8ba1\u7b97\u6210\u672c\u3002"}}
{"id": "2510.13002", "categories": ["cs.AI", "cs.LG"], "pdf": "https://arxiv.org/pdf/2510.13002", "abs": "https://arxiv.org/abs/2510.13002", "authors": ["Boyou Chen", "Gerui Xu", "Zifei Wang", "Huizhong Guo", "Ananna Ahmed", "Zhaonan Sun", "Zhen Hu", "Kaihan Zhang", "Shan Bao"], "title": "From Narratives to Probabilistic Reasoning: Predicting and Interpreting Drivers' Hazardous Actions in Crashes Using Large Language Model", "comment": null, "summary": "Vehicle crashes involve complex interactions between road users, split-second\ndecisions, and challenging environmental conditions. Among these, two-vehicle\ncrashes are the most prevalent, accounting for approximately 70% of roadway\ncrashes and posing a significant challenge to traffic safety. Identifying\nDriver Hazardous Action (DHA) is essential for understanding crash causation,\nyet the reliability of DHA data in large-scale databases is limited by\ninconsistent and labor-intensive manual coding practices. Here, we present an\ninnovative framework that leverages a fine-tuned large language model to\nautomatically infer DHAs from textual crash narratives, thereby improving the\nvalidity and interpretability of DHA classifications. Using five years of\ntwo-vehicle crash data from MTCF, we fine-tuned the Llama 3.2 1B model on\ndetailed crash narratives and benchmarked its performance against conventional\nmachine learning classifiers, including Random Forest, XGBoost, CatBoost, and a\nneural network. The fine-tuned LLM achieved an overall accuracy of 80%,\nsurpassing all baseline models and demonstrating pronounced improvements in\nscenarios with imbalanced data. To increase interpretability, we developed a\nprobabilistic reasoning approach, analyzing model output shifts across original\ntest sets and three targeted counterfactual scenarios: variations in driver\ndistraction and age. Our analysis revealed that introducing distraction for one\ndriver substantially increased the likelihood of \"General Unsafe Driving\";\ndistraction for both drivers maximized the probability of \"Both Drivers Took\nHazardous Actions\"; and assigning a teen driver markedly elevated the\nprobability of \"Speed and Stopping Violations.\" Our framework and analytical\nmethods provide a robust and interpretable solution for large-scale automated\nDHA detection, offering new opportunities for traffic safety analysis and\nintervention.", "AI": {"tldr": "\u8be5\u7814\u7a76\u63d0\u51fa\u4e86\u4e00\u4e2a\u5229\u7528\u5927\u578b\u8bed\u8a00\u6a21\u578b\u81ea\u52a8\u8bc6\u522b\u6587\u672c\u78b0\u649e GHA \u7684\u6846\u67b6\uff0c\u8be5\u6846\u67b6\u5728\u4e24\u8f66\u4e8b\u6545\u6570\u636e\u4e0a\u53d6\u5f97\u4e86 80% \u7684\u51c6\u786e\u7387\uff0c\u8d85\u8fc7\u4e86\u6240\u6709\u57fa\u7ebf\u6a21\u578b\u3002", "motivation": "\u4ee5\u5f80 DHAs \u7684\u8bc6\u522b\u65b9\u6cd5\u5b58\u5728\u6570\u636e\u53ef\u9760\u6027\u4f4e\u3001\u4eba\u5de5\u7f16\u7801\u6548\u7387\u4f4e\u7b49\u95ee\u9898\uff0c\u5bfc\u81f4\u5bf9\u78b0\u649e\u539f\u56e0\u7684\u7406\u89e3\u53d7\u9650\u3002", "method": "\u8be5\u7814\u7a76\u63d0\u51fa\u4e86\u4e00\u79cd\u521b\u65b0\u6846\u67b6\uff0c\u5229\u7528\u5fae\u8c03\u7684\u5927\u578b\u8bed\u8a00\u6a21\u578b\u81ea\u52a8\u4ece\u6587\u672c\u78b0\u649e\u53d9\u8ff0\u4e2d\u63a8\u65ad DHA\u3002\u8be5\u6a21\u578b\u662f\u4e00\u4e2a\u5fae\u8c03\u7684 Llama 3.2 1B \u6a21\u578b\uff0c\u5e76\u5728 MTCF \u7684\u4e94\u5e74\u4e24\u8f66\u78b0\u649e\u6570\u636e\u4e0a\u8fdb\u884c\u8bad\u7ec3\u3002\u7814\u7a76\u8fd8\u5f00\u53d1\u4e86\u4e00\u79cd\u6982\u7387\u63a8\u7406\u65b9\u6cd5\u6765\u5206\u6790\u6a21\u578b\u8f93\u51fa\u3002", "result": "\u5fae\u8c03\u540e\u7684 LLM \u53d6\u5f97\u4e86 80% \u7684\u603b\u51c6\u786e\u7387\uff0c\u8d85\u8fc7\u4e86\u6240\u6709\u57fa\u7ebf\u6a21\u578b\uff0c\u5e76\u5728\u6570\u636e\u4e0d\u5e73\u8861\u7684\u60c5\u51b5\u4e0b\u8868\u73b0\u51fa\u663e\u8457\u7684\u6539\u8fdb\u3002\u7814\u7a76\u53d1\u73b0\uff0c\u5f15\u5165\u9a7e\u9a76\u5458\u5206\u5fc3\u4f1a\u663e\u8457\u589e\u52a0\u201c\u4e00\u822c\u4e0d\u5b89\u5168\u9a7e\u9a76\u201d\u7684\u53ef\u80fd\u6027\uff1b\u53cc\u9a7e\u9a76\u5458\u5206\u5fc3\u4f1a\u6700\u5927\u5316\u201cTBDHA\u201d\u7684\u6982\u7387\uff1b\u9752\u5c11\u5e74\u9a7e\u9a76\u5458\u4f1a\u663e\u8457\u589e\u52a0\u201c\u8d85\u901f\u548c\u505c\u8f66\u8fdd\u89c4\u201d\u7684\u53ef\u80fd\u6027\u3002", "conclusion": "\u8be5\u6846\u67b6\u4e3a\u5927\u89c4\u6a21\u81ea\u52a8\u5316 DHA \u68c0\u6d4b\u63d0\u4f9b\u4e86\u4e00\u4e2a\u7a33\u5065\u4e14\u53ef\u89e3\u91ca\u7684\u89e3\u51b3\u65b9\u6848\uff0c\u4e3a\u4ea4\u901a\u5b89\u5168\u5206\u6790\u548c\u5e72\u9884\u63d0\u4f9b\u4e86\u65b0\u7684\u673a\u4f1a\u3002"}}
{"id": "2510.13088", "categories": ["cs.GT"], "pdf": "https://arxiv.org/pdf/2510.13088", "abs": "https://arxiv.org/abs/2510.13088", "authors": ["Rishi Patel", "Emmanouil Pountourakis", "Samuel Taggart"], "title": "Repeated Sales with Heterogeneous Buyer Sophistication", "comment": "To appear at WINE 2025", "summary": "This paper considers behavior-based price discrimination in the repeated sale\nof a non-durable good to a single long-lived buyer, by a seller without\ncommitment power. We assume that there is a mixed population of forward-looking\n``sophisticated'' buyers and myopic ``naive'' buyers. We investigate the impact\nof these dynamics on the seller's ability to learn about the buyer and exploit\nthis learning for revenue. We obtain conclusions that differ dramatically with\nthe time horizon of the interactions. To understand short time horizons, we\nanalyze a two-period model, and find that the strategic demand reduction\nobserved with fully sophisticated buyers is robust to the introduction of naive\ntypes. In fact, despite the inability of naive buyers to game the pricing\nalgorithm, their introduction can further harm the seller's revenue, due to\nmore intense demand reduction overall. For long horizons, we consider an\ninfinite-horizon model with time discounting. We find that the extreme demand\nreduction predicted by previous work does not survive the introduction of naive\nbuyers. Instead, we observe equilibria where the seller learns meaningfully\ndespite the sophisticated buyers' demand reduction. We prove that for a natural\nfamily of such equilibria, the seller's revenue is not just high, but\napproximates the revenue attainable with commitment power, even when the\nfraction of naive types is vanishingly small.", "AI": {"tldr": "\u672c\u6587\u5206\u6790\u4e86\u884c\u4e3a\u5b9a\u4ef7\u5bf9\u91cd\u590d\u9500\u552e\u8010\u7528\u54c1\u7684\u5f71\u54cd\uff0c\u5e76\u53d1\u73b0\u4e70\u65b9\u7c7b\u578b\u548c\u65f6\u95f4\u8303\u56f4\u5bf9\u9500\u552e\u5546\u7684\u6536\u76ca\u6709\u663e\u8457\u5f71\u54cd\u3002", "motivation": "\u63a2\u7d22\u91cd\u590d\u9500\u552e\u975e\u8010\u7528\u54c1\u65f6\uff0c\u884c\u4e3a\u5b9a\u4ef7\u5bf9\u5356\u65b9\u5b66\u4e60\u80fd\u529b\u548c\u6536\u76ca\u7684\u5f71\u54cd\u3002", "method": "\u672c\u6587\u901a\u8fc7\u5206\u6790\u4e24\u9636\u6bb5\u6a21\u578b\u6765\u7406\u89e3\u77ed\u671f\u5f71\u54cd\uff0c\u5e76\u901a\u8fc7\u5206\u6790\u65e0\u9650\u671f\u6a21\u578b\u6765\u7814\u7a76\u957f\u671f\u5f71\u54cd\u3002", "result": "\u5728\u77ed\u671f\u4e2d\uff0c\u5929\u771f\u4e70\u5bb6\u7684\u5f15\u5165\u4f1a\u52a0\u5267\u9700\u6c42\u51cf\u5c11\uff0c\u635f\u5bb3\u5356\u65b9\u6536\u76ca\u3002\u5728\u957f\u671f\u4e2d\uff0c\u5929\u771f\u4e70\u5bb6\u4f1a\u51cf\u5f31\u6781\u7aef\u7684\u9700\u6c42\u51cf\u5c11\uff0c\u5356\u65b9\u53ef\u4ee5\u5728\u6709\u610f\u4e49\u7684\u5b66\u4e60\u4e2d\u83b7\u5f97\u9ad8\u6536\u76ca\u3002", "conclusion": "\u4e70\u5bb6\u662f\u201c\u8001\u7ec3\u201d\u8fd8\u662f\u201c\u5929\u771f\u201d\u4ee5\u53ca\u4e92\u52a8\u7684\u65f6\u9650\uff0c\u5bf9\u5356\u65b9\u8fdb\u884c\u884c\u4e3a\u5b9a\u4ef7\u548c\u5b66\u4e60\u80fd\u529b\u5177\u6709\u51b3\u5b9a\u6027\u7684\u5f71\u54cd\u3002"}}
{"id": "2510.13227", "categories": ["cs.MA", "cs.ET", "cs.LG", "I.2"], "pdf": "https://arxiv.org/pdf/2510.13227", "abs": "https://arxiv.org/abs/2510.13227", "authors": ["Divyanshu Singh", "Ashman Mehra", "Snehanshu Saha", "Santonu Sarkar"], "title": "Altruistic Ride Sharing: A Community-Driven Approach to Short-Distance Mobility", "comment": "Submitted to IEEE Transactions on Intelligent Transportation Systems", "summary": "Urban mobility faces persistent challenges of congestion and fuel\nconsumption, specifically when people choose a private, point-to-point commute\noption. Profit-driven ride-sharing platforms prioritize revenue over fairness\nand sustainability. This paper introduces Altruistic Ride-Sharing (ARS), a\ndecentralized, peer-to-peer mobility framework where participants alternate\nbetween driver and rider roles based on altruism points rather than monetary\nincentives. The system integrates multi-agent reinforcement learning (MADDPG)\nfor dynamic ride-matching, game-theoretic equilibrium guarantees for fairness,\nand a population model to sustain long-term balance. Using real-world New York\nCity taxi data, we demonstrate that ARS reduces travel distance and emissions,\nincreases vehicle utilization, and promotes equitable participation compared to\nboth no-sharing and optimization-based baselines. These results establish ARS\nas a scalable, community-driven alternative to conventional ride-sharing,\naligning individual behavior with collective urban sustainability goals.", "AI": {"tldr": "Altruistic Ride-Sharing\uff08ARS\uff09\u662f\u4e00\u79cd\u53bb\u4e2d\u5fc3\u5316\u7684\u70b9\u5bf9\u70b9\u51fa\u884c\u6846\u67b6\uff0c\u901a\u8fc7\u5229\u4ed6\u4e3b\u4e49\u79ef\u5206\u800c\u975e\u91d1\u94b1\u6fc0\u52b1\u6765\u5339\u914d\u9a7e\u4e58\u8005\u3002\u5b83\u5229\u7528\u591a\u667a\u80fd\u4f53\u5f3a\u5316\u5b66\u4e60\u8fdb\u884c\u52a8\u6001\u5339\u914d\uff0c\u5e76\u901a\u8fc7\u535a\u5f08\u8bba\u5747\u8861\u786e\u4fdd\u516c\u5e73\u6027\u3002ARS\u80fd\u6709\u6548\u51cf\u5c11\u51fa\u884c\u8ddd\u79bb\u548c\u6392\u653e\uff0c\u63d0\u9ad8\u8f66\u8f86\u5229\u7528\u7387\uff0c\u5e76\u4fc3\u8fdb\u516c\u5e73\u53c2\u4e0e\u3002", "motivation": "\u4f20\u7edf\u7684\u8425\u5229\u6027\u5171\u4eab\u51fa\u884c\u5e73\u53f0\u4fa7\u91cd\u4e8e\u6536\u5165\u800c\u975e\u516c\u5e73\u6027\u548c\u53ef\u6301\u7eed\u6027\u3002\u57ce\u5e02\u4ea4\u901a\u9762\u4e34\u62e5\u5835\u548c\u71c3\u6599\u6d88\u8017\u7684\u6311\u6218\uff0c\u5c24\u5176\u662f\u5728\u4eba\u4eec\u9009\u62e9\u79c1\u4eba\u70b9\u5bf9\u70b9\u901a\u52e4\u65b9\u5f0f\u65f6\u3002", "method": "\u672c\u6587\u63d0\u51fa\u4e86\u5229\u4ed6\u5171\u4eab\u51fa\u884c\uff08ARS\uff09\u7cfb\u7edf\u3002\u8be5\u7cfb\u7edf\u6574\u5408\u4e86\uff1a1. \u591a\u667a\u80fd\u4f53\u5f3a\u5316\u5b66\u4e60\uff08MADDPG\uff09\u5b9e\u73b0\u52a8\u6001\u884c\u7a0b\u5339\u914d\u30022. \u535a\u5f08\u8bba\u5747\u8861\u4fdd\u8bc1\u516c\u5e73\u6027\u30023. \u4eba\u53e3\u6a21\u578b\u7ef4\u6301\u957f\u671f\u5e73\u8861\u3002", "result": "\u4e0e\u65e0\u5171\u4eab\u548c\u57fa\u4e8e\u4f18\u5316\u7684\u57fa\u7ebf\u76f8\u6bd4\uff0cARS\u663e\u8457\u51cf\u5c11\u4e86\u51fa\u884c\u8ddd\u79bb\u548c\u6392\u653e\uff0c\u63d0\u9ad8\u4e86\u8f66\u8f86\u5229\u7528\u7387\uff0c\u5e76\u4fc3\u8fdb\u4e86\u516c\u5e73\u53c2\u4e0e\u3002", "conclusion": "ARS\u7cfb\u7edf\u4e3a\u4f20\u7edf\u5171\u4eab\u51fa\u884c\u63d0\u4f9b\u4e86\u4e00\u79cd\u53ef\u6269\u5c55\u7684\u3001\u793e\u533a\u9a71\u52a8\u7684\u66ff\u4ee3\u65b9\u6848\uff0c\u5c06\u4e2a\u4f53\u884c\u4e3a\u4e0e\u57ce\u5e02\u53ef\u6301\u7eed\u53d1\u5c55\u76ee\u6807\u76f8\u7ed3\u5408\u3002"}}
{"id": "2510.13029", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2510.13029", "abs": "https://arxiv.org/abs/2510.13029", "authors": ["Xinlei Wang", "Mingtian Tan", "Jing Qiu", "Junhua Zhao", "Jinjin Gu"], "title": "Toward Reasoning-Centric Time-Series Analysis", "comment": null, "summary": "Traditional time series analysis has long relied on pattern recognition,\ntrained on static and well-established benchmarks. However, in real-world\nsettings -- where policies shift, human behavior adapts, and unexpected events\nunfold -- effective analysis must go beyond surface-level trends to uncover the\nactual forces driving them. The recent rise of Large Language Models (LLMs)\npresents new opportunities for rethinking time series analysis by integrating\nmultimodal inputs. However, as the use of LLMs becomes popular, we must remain\ncautious, asking why we use LLMs and how to exploit them effectively. Most\nexisting LLM-based methods still employ their numerical regression ability and\nignore their deeper reasoning potential. This paper argues for rethinking time\nseries with LLMs as a reasoning task that prioritizes causal structure and\nexplainability. This shift brings time series analysis closer to human-aligned\nunderstanding, enabling transparent and context-aware insights in complex\nreal-world environments.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u5c06\u65f6\u95f4\u5e8f\u5217\u5206\u6790\u4e0e\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08LLMs\uff09\u7ed3\u5408\u8d77\u6765\uff0c\u4ee5\u5e94\u5bf9\u73b0\u5b9e\u4e16\u754c\u4e2d\u590d\u6742\u7684\u65f6\u95f4\u5e8f\u5217\u6570\u636e\u3002", "motivation": "\u4f20\u7edf\u7684\u65f6\u5e8f\u5206\u6790\u65b9\u6cd5\u4f9d\u8d56\u9759\u6001\u57fa\u51c6\u548c\u6a21\u5f0f\u8bc6\u522b\uff0c\u96be\u4ee5\u6355\u6349\u73b0\u5b9e\u4e16\u754c\u4e2d\u653f\u7b56\u53d8\u5316\u3001\u4eba\u7c7b\u884c\u4e3a\u9002\u5e94\u548c\u610f\u5916\u4e8b\u4ef6\u7b49\u6df1\u5c42\u9a71\u52a8\u56e0\u7d20\u3002\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08LLMs\uff09\u7684\u5174\u8d77\u4e3a\u65f6\u5e8f\u5206\u6790\u5e26\u6765\u4e86\u65b0\u673a\u9047\uff0c\u4f46\u73b0\u6709\u65b9\u6cd5\u591a\u5ffd\u89c6\u5176\u6df1\u5c42\u63a8\u7406\u6f5c\u529b\u3002", "method": "\u672c\u6587\u63d0\u51fa\u5c06\u65f6\u95f4\u5e8f\u5217\u5206\u6790\u89c6\u4e3a\u4e00\u4e2a\u63a8\u7406\u4efb\u52a1\uff0c\u91cd\u70b9\u5173\u6ce8\u56e0\u679c\u7ed3\u6784\u548c\u53ef\u89e3\u91ca\u6027\uff0c\u800c\u975e\u4ec5\u4ec5\u5229\u7528LLMs\u7684\u6570\u503c\u56de\u5f52\u80fd\u529b\u3002", "result": "\u901a\u8fc7\u5c06\u65f6\u5e8f\u5206\u6790\u4e0eLLMs\u7ed3\u5408\uff0c\u5e76\u4fa7\u91cd\u4e8e\u63a8\u7406\u4efb\u52a1\uff0c\u53ef\u4ee5\u5b9e\u73b0\u5bf9\u590d\u6742\u73b0\u5b9e\u4e16\u754c\u73af\u5883\u4e2d\u900f\u660e\u4e14\u4e0a\u4e0b\u6587\u611f\u77e5\u7684\u6d1e\u5bdf\u3002", "conclusion": "\u5c06\u65f6\u95f4\u5e8f\u5217\u5206\u6790\u4e0e\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08LLMs\uff09\u7ed3\u5408\uff0c\u5e76\u5c06\u5176\u89c6\u4e3a\u4e00\u4e2a\u63a8\u7406\u4efb\u52a1\uff0c\u53ef\u4ee5\u66f4\u597d\u5730\u7406\u89e3\u73b0\u5b9e\u4e16\u754c\u4e2d\u590d\u6742\u7684\u65f6\u95f4\u5e8f\u5217\u6a21\u5f0f\uff0c\u5e76\u63d0\u4f9b\u66f4\u5177\u53ef\u89e3\u91ca\u6027\u7684\u7ed3\u679c\u3002"}}
{"id": "2510.13532", "categories": ["cs.IT", "math.IT"], "pdf": "https://arxiv.org/pdf/2510.13532", "abs": "https://arxiv.org/abs/2510.13532", "authors": ["Dushyantha A Basnayaka"], "title": "Simulating Mediumband Wireless Communication Systems: A Concise Description", "comment": "10 pages, 4 figures, and a MATLAB code included", "summary": "In this paper, we describe the necessary procedures for accurately simulating\ndigital wireless communication systems operating in the mediumband, aimed at\nboth beginners and experts. In the research literature, digital wireless\ncommunication systems are typically simulated in the discrete-time complex\nbaseband domain, where pulse shaping, upconversion, mixing, carrier\nsynchronization, and symbol timing synchronization are often ignored. These\nassumptions are indeed sufficient in most cases, but to capture the essence of\ncommunication in the mediumband, certain physical layer (PHY) operations should\nbe simulated in detail. In this paper, we concisely describe how to simulate a\nmediumband wireless communication scenario from a single transmitter (TX) to a\nsingle receiver (RX) in MATLAB, elaborating the operation of key PHY\nsubsystems. The approach described here ensures that the simulated system\ncaptures the delicate dynamics of mediumband wireless communication, including\nthe effect of deep fading avoidance.", "AI": {"tldr": "\u8fd9\u7bc7\u8bba\u6587\u63cf\u8ff0\u4e86\u51c6\u786e\u6a21\u62df\u4e2d\u9891\u6570\u5b57\u65e0\u7ebf\u901a\u4fe1\u7cfb\u7edf\u6240\u9700\u7684\u65b9\u6cd5\u548c\u6b65\u9aa4\uff0c\u9002\u7528\u4e8e\u521d\u5b66\u8005\u548c\u4e13\u5bb6\u3002", "motivation": "\u5728\u73b0\u6709\u7814\u7a76\u4e2d\uff0c\u6570\u5b57\u65e0\u7ebf\u901a\u4fe1\u7cfb\u7edf\u901a\u5e38\u5728\u79bb\u6563\u65f6\u95f4\u590d\u57fa\u5e26\u57df\u8fdb\u884c\u6a21\u62df\uff0c\u5e76\u5ffd\u7565\u4e86\u4e00\u4e9b\u91cd\u8981\u7684\u7269\u7406\u5c42\u64cd\u4f5c\u3002\u7136\u800c\uff0c\u4e3a\u4e86\u6355\u6349\u4e2d\u9891\u901a\u4fe1\u7684\u672c\u8d28\uff0c\u9700\u8981\u8be6\u7ec6\u6a21\u62df\u67d0\u4e9b\u7269\u7406\u5c42\u64cd\u4f5c\u3002", "method": "\u672c\u8bba\u6587\u8be6\u7ec6\u63cf\u8ff0\u4e86\u5982\u4f55\u5728MATLAB\u4e2d\u6a21\u62df\u4e00\u4e2a\u4ece\u4e2d\u9891\u65e0\u7ebf\u901a\u4fe1\u573a\u666f\uff0c\u5177\u4f53\u9610\u8ff0\u4e86\u5173\u952e\u7269\u7406\u5c42\u5b50\u7cfb\u7edf\u7684\u5de5\u4f5c\u3002", "result": "\u901a\u8fc7\u6240\u63cf\u8ff0\u7684\u65b9\u6cd5\uff0c\u6a21\u62df\u7cfb\u7edf\u80fd\u591f\u6355\u6349\u4e2d\u9891\u65e0\u7ebf\u901a\u4fe1\u7684\u7cbe\u7ec6\u52a8\u6001\uff0c\u5305\u62ec\u907f\u514d\u6df1\u5ea6\u8870\u843d\u7684\u5f71\u54cd\u3002", "conclusion": "\u6a21\u62df\u4e2d\u9891\u65e0\u7ebf\u901a\u4fe1\u7cfb\u7edf\u9700\u8981\u8be6\u7ec6\u8003\u8651\u7269\u7406\u5c42\u64cd\u4f5c\uff0c\u8fd9\u5bf9\u4e8e\u51c6\u786e\u6355\u6349\u901a\u4fe1\u7cbe\u9ad3\u81f3\u5173\u91cd\u8981\u3002"}}
{"id": "2510.13343", "categories": ["cs.MA", "cs.AI", "cs.LG"], "pdf": "https://arxiv.org/pdf/2510.13343", "abs": "https://arxiv.org/abs/2510.13343", "authors": ["Shota Takayama", "Katsuhide Fujita"], "title": "AOAD-MAT: Transformer-based multi-agent deep reinforcement learning model considering agents' order of action decisions", "comment": "This manuscript is an extended version of the work accepted as a\n  short paper at the 26th International Conference on Principles and Practice\n  of Multi-Agent Systems (PRIMA 2025). The Version of Record of this\n  contribution is published in Springer's Lecture Notes in Artificial\n  Intelligence series (LNCS/LNAI)", "summary": "Multi-agent reinforcement learning focuses on training the behaviors of\nmultiple learning agents that coexist in a shared environment. Recently, MARL\nmodels, such as the Multi-Agent Transformer (MAT) and ACtion dEpendent deep\nQ-learning (ACE), have significantly improved performance by leveraging\nsequential decision-making processes. Although these models can enhance\nperformance, they do not explicitly consider the importance of the order in\nwhich agents make decisions. In this paper, we propose an Agent Order of Action\nDecisions-MAT (AOAD-MAT), a novel MAT model that considers the order in which\nagents make decisions. The proposed model explicitly incorporates the sequence\nof action decisions into the learning process, allowing the model to learn and\npredict the optimal order of agent actions. The AOAD-MAT model leverages a\nTransformer-based actor-critic architecture that dynamically adjusts the\nsequence of agent actions. To achieve this, we introduce a novel MARL\narchitecture that cooperates with a subtask focused on predicting the next\nagent to act, integrated into a Proximal Policy Optimization based loss\nfunction to synergistically maximize the advantage of the sequential\ndecision-making. The proposed method was validated through extensive\nexperiments on the StarCraft Multi-Agent Challenge and Multi-Agent MuJoCo\nbenchmarks. The experimental results show that the proposed AOAD-MAT model\noutperforms existing MAT and other baseline models, demonstrating the\neffectiveness of adjusting the AOAD order in MARL.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86AOAD-MAT\u6a21\u578b\uff0c\u4e00\u4e2a\u5728MARL\u73af\u5883\u4e2d\u901a\u8fc7\u52a8\u6001\u8c03\u6574\u667a\u80fd\u4f53\u52a8\u4f5c\u987a\u5e8f\u6765\u63d0\u5347\u6027\u80fd\u7684\u65b0\u578bMAT\u6a21\u578b\u3002", "motivation": "\u73b0\u6709\u7684\u591a\u667a\u80fd\u4f53\u5f3a\u5316\u5b66\u4e60\u6a21\u578b\uff08\u5982MAT\u548cACE\uff09\u5728\u5904\u7406\u591a\u667a\u80fd\u4f53\u51b3\u7b56\u65f6\uff0c\u867d\u7136\u6027\u80fd\u6709\u6240\u63d0\u5347\uff0c\u4f46\u5e76\u672a\u663e\u5f0f\u8003\u8651\u667a\u80fd\u4f53\u51b3\u7b56\u987a\u5e8f\u7684\u91cd\u8981\u6027\u3002", "method": "AOAD-MAT\u6a21\u578b\u5c06\u52a8\u4f5c\u51b3\u7b56\u5e8f\u5217\u663e\u5f0f\u5730\u878d\u5165\u5b66\u4e60\u8fc7\u7a0b\uff0c\u901a\u8fc7\u4e00\u4e2a\u57fa\u4e8eTransformer\u7684Actor-Critic\u67b6\u6784\u52a8\u6001\u8c03\u6574\u667a\u80fd\u4f53\u52a8\u4f5c\u7684\u987a\u5e8f\u3002\u6b64\u5916\uff0c\u6a21\u578b\u5f15\u5165\u4e86\u4e00\u4e2a\u534f\u540c\u7684\u5b50\u4efb\u52a1\uff0c\u7528\u4e8e\u9884\u6d4b\u4e0b\u4e00\u4e2a\u884c\u52a8\u7684\u667a\u80fd\u4f53\uff0c\u5e76\u6574\u5408\u5230\u57fa\u4e8e\u8fd1\u7aef\u7b56\u7565\u4f18\u5316\u7684\u635f\u5931\u51fd\u6570\u4e2d\uff0c\u4ee5\u6700\u5927\u5316\u5e8f\u5217\u51b3\u7b56\u7684\u4f18\u52bf\u3002", "result": "\u5728StarCraft\u591a\u667a\u80fd\u4f53\u6311\u6218\u548c\u591a\u667a\u80fd\u4f53MuJoCo\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u8fdb\u884c\u7684\u5e7f\u6cdb\u5b9e\u9a8c\u8868\u660e\uff0cAOAD-MAT\u6a21\u578b\u4f18\u4e8e\u73b0\u6709\u7684MAT\u6a21\u578b\u548c\u5176\u4ed6\u57fa\u7ebf\u6a21\u578b\u3002", "conclusion": "AOAD-MAT\u6a21\u578b\u901a\u8fc7\u8c03\u6574\u591a\u667a\u80fd\u4f53\u5f3a\u5316\u5b66\u4e60\u4e2d\u7684\u52a8\u4f5c\u51b3\u7b56\u987a\u5e8f\uff0c\u6709\u6548\u63d0\u5347\u4e86\u6a21\u578b\u6027\u80fd\u3002"}}
{"id": "2510.13042", "categories": ["cs.CV", "cs.AI"], "pdf": "https://arxiv.org/pdf/2510.13042", "abs": "https://arxiv.org/abs/2510.13042", "authors": ["Zhengxu Tang", "Zizheng Wang", "Luning Wang", "Zitao Shuai", "Chenhao Zhang", "Siyu Qian", "Yirui Wu", "Bohao Wang", "Haosong Rao", "Zhenyu Yang", "Chenwei Wu"], "title": "SeqBench: Benchmarking Sequential Narrative Generation in Text-to-Video Models", "comment": null, "summary": "Text-to-video (T2V) generation models have made significant progress in\ncreating visually appealing videos. However, they struggle with generating\ncoherent sequential narratives that require logical progression through\nmultiple events. Existing T2V benchmarks primarily focus on visual quality\nmetrics but fail to evaluate narrative coherence over extended sequences. To\nbridge this gap, we present SeqBench, a comprehensive benchmark for evaluating\nsequential narrative coherence in T2V generation. SeqBench includes a carefully\ndesigned dataset of 320 prompts spanning various narrative complexities, with\n2,560 human-annotated videos generated from 8 state-of-the-art T2V models.\nAdditionally, we design a Dynamic Temporal Graphs (DTG)-based automatic\nevaluation metric, which can efficiently capture long-range dependencies and\ntemporal ordering while maintaining computational efficiency. Our DTG-based\nmetric demonstrates a strong correlation with human annotations. Through\nsystematic evaluation using SeqBench, we reveal critical limitations in current\nT2V models: failure to maintain consistent object states across multi-action\nsequences, physically implausible results in multi-object scenarios, and\ndifficulties in preserving realistic timing and ordering relationships between\nsequential actions. SeqBench provides the first systematic framework for\nevaluating narrative coherence in T2V generation and offers concrete insights\nfor improving sequential reasoning capabilities in future models. Please refer\nto https://videobench.github.io/SeqBench.github.io/ for more details.", "AI": {"tldr": "SeqBench\u662f\u4e00\u4e2a\u7528\u4e8e\u8bc4\u4f30T2V\u6a21\u578b\u5728\u751f\u6210\u8fde\u8d2f\u7684\u987a\u5e8f\u53d9\u4e8b\u65b9\u9762\u7684\u57fa\u51c6\uff0c\u5b83\u63ed\u793a\u4e86\u5f53\u524dT2V\u6a21\u578b\u5728\u5904\u7406\u591a\u4e8b\u4ef6\u5e8f\u5217\u65f6\u7684\u5c40\u9650\u6027\u3002", "motivation": "\u73b0\u6709\u7684T2V\u6a21\u578b\u5728\u751f\u6210\u89c6\u89c9\u4e0a\u5438\u5f15\u4eba\u7684\u89c6\u9891\u65b9\u9762\u53d6\u5f97\u4e86\u663e\u8457\u8fdb\u5c55\uff0c\u4f46\u96be\u4ee5\u751f\u6210\u9700\u8981\u901a\u8fc7\u591a\u4e2a\u4e8b\u4ef6\u8fdb\u884c\u903b\u8f91\u63a8\u8fdb\u7684\u8fde\u8d2f\u987a\u5e8f\u53d9\u4e8b\u3002\u73b0\u6709T2V\u57fa\u51c6\u4e3b\u8981\u5173\u6ce8\u89c6\u89c9\u8d28\u91cf\u6307\u6807\uff0c\u4f46\u672a\u80fd\u8bc4\u4f30\u6269\u5c55\u5e8f\u5217\u4e0a\u7684\u53d9\u4e8b\u8fde\u8d2f\u6027\u3002", "method": "\u6211\u4eec\u63d0\u51fa\u4e86SeqBench\uff0c\u8fd9\u662f\u4e00\u4e2a\u7528\u4e8e\u8bc4\u4f30T2V\u751f\u6210\u4e2d\u987a\u5e8f\u53d9\u4e8b\u8fde\u8d2f\u6027\u7684\u7efc\u5408\u57fa\u51c6\u3002SeqBench\u5305\u542b\u4e00\u4e2a\u7cbe\u5fc3\u8bbe\u8ba1\u7684320\u4e2a\u63d0\u793a\u6570\u636e\u96c6\uff0c\u6db5\u76d6\u5404\u79cd\u53d9\u4e8b\u590d\u6742\u6027\uff0c\u4ee5\u53ca\u4ece8\u4e2a\u6700\u5148\u8fdb\u7684T2V\u6a21\u578b\u751f\u6210\u76842,560\u4e2a\u7ecf\u8fc7\u4eba\u5de5\u6807\u6ce8\u7684\u89c6\u9891\u3002\u6b64\u5916\uff0c\u6211\u4eec\u8bbe\u8ba1\u4e86\u4e00\u4e2a\u57fa\u4e8e\u52a8\u6001\u65f6\u95f4\u56fe\uff08DTG\uff09\u7684\u81ea\u52a8\u8bc4\u4f30\u6307\u6807\uff0c\u5b83\u53ef\u4ee5\u6709\u6548\u5730\u6355\u83b7\u957f\u7a0b\u4f9d\u8d56\u548c\u65f6\u95f4\u987a\u5e8f\uff0c\u540c\u65f6\u4fdd\u6301\u8ba1\u7b97\u6548\u7387\u3002", "result": "\u6211\u4eec\u7684DTG\u8bc4\u4f30\u6307\u6807\u4e0e\u4eba\u5de5\u6807\u6ce8\u663e\u793a\u51fa\u5f88\u5f3a\u7684\u76f8\u5173\u6027\u3002\u901a\u8fc7\u4f7f\u7528SeqBench\u8fdb\u884c\u7684\u7cfb\u7edf\u8bc4\u4f30\uff0c\u6211\u4eec\u63ed\u793a\u4e86\u5f53\u524dT2V\u6a21\u578b\u7684\u5173\u952e\u5c40\u9650\u6027\uff1a\u5728\u591a\u52a8\u4f5c\u5e8f\u5217\u4e2d\u672a\u80fd\u4fdd\u6301\u4e00\u81f4\u7684\u5bf9\u8c61\u72b6\u6001\uff0c\u5728\u591a\u5bf9\u8c61\u573a\u666f\u4e2d\u4ea7\u751f\u7269\u7406\u4e0a\u4e0d\u5408\u7406\u7684\u7ed3\u679c\uff0c\u4ee5\u53ca\u96be\u4ee5\u4fdd\u6301\u987a\u5e8f\u52a8\u4f5c\u4e4b\u95f4\u7684\u73b0\u5b9e\u65f6\u95f4\u5b89\u6392\u548c\u6392\u5e8f\u5173\u7cfb\u3002", "conclusion": "SeqBench\u4e3a\u8bc4\u4f30T2V\u751f\u6210\u4e2d\u7684\u53d9\u4e8b\u8fde\u8d2f\u6027\u63d0\u4f9b\u4e86\u7b2c\u4e00\u4e2a\u7cfb\u7edf\u6846\u67b6\uff0c\u5e76\u4e3a\u6539\u8fdb\u672a\u6765\u6a21\u578b\u4e2d\u7684\u987a\u5e8f\u63a8\u7406\u80fd\u529b\u63d0\u4f9b\u4e86\u5177\u4f53\u7684\u89c1\u89e3\u3002"}}
{"id": "2510.13661", "categories": ["cs.IT", "cs.CR", "math.IT"], "pdf": "https://arxiv.org/pdf/2510.13661", "abs": "https://arxiv.org/abs/2510.13661", "authors": ["Emmanouil M. Athanasakos", "Nicholas Kalouptsidis", "Hariprasad Manjunath"], "title": "Local Information-Theoretic Security via Euclidean Geometry", "comment": "48 pages, 12 figures, submitted to IEEE Transactions on Information\n  Theory", "summary": "This paper introduces a methodology based on Euclidean information theory to\ninvestigate local properties of secure communication over discrete memoryless\nwiretap channels. We formulate a constrained optimization problem that\nmaximizes a legitimate user's information rate while imposing explicit upper\nbounds on both the information leakage to an eavesdropper and the informational\ncost of encoding the secret message. By leveraging local geometric\napproximations, this inherently non-convex problem is transformed into a\ntractable quadratic programming structure. It is demonstrated that the optimal\nLagrange multipliers governing this approximated problem can be found by\nsolving a linear program. The constraints of this linear program are derived\nfrom Karush-Kuhn-Tucker conditions and are expressed in terms of the\ngeneralized eigenvalues of channel-derived matrices. This framework facilitates\nthe derivation of an analytical formula for an approximate local secrecy\ncapacity. Furthermore, we define and analyze a new class of secret local\ncontraction coefficients. These coefficients, characterized as the largest\ngeneralized eigenvalues of a matrix pencil, quantify the maximum achievable\nratio of approximate utility to approximate leakage, thus measuring the\nintrinsic local leakage efficiency of the channel. We establish bounds\nconnecting these local coefficients to their global counterparts defined over\ntrue mutual information measures. The efficacy of the proposed framework is\ndemonstrated through detailed analysis and numerical illustrations for both\ngeneral multi-mode channels and the canonical binary symmetric wiretap channel.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u6b27\u51e0\u91cc\u5f97\u4fe1\u606f\u8bba\u7684\u65b9\u6cd5\uff0c\u7528\u4e8e\u7814\u7a76\u79bb\u6563\u65e0\u8bb0\u5fc6\u7a83\u542c\u4fe1\u9053\u4e0a\u7684\u5b89\u5168\u901a\u4fe1\u7684\u5c40\u90e8\u7279\u6027\u3002", "motivation": "\u5728\u79bb\u6563\u65e0\u8bb0\u5fc6\u7a83\u542c\u4fe1\u9053\u4e0a\uff0c\u5236\u5b9a\u4e86\u4e00\u4e2a\u53d7\u9650\u4f18\u5316\u95ee\u9898\uff0c\u4ee5\u6700\u5927\u5316\u5408\u6cd5\u7528\u6237\u7684\u4fe1\u606f\u901f\u7387\uff0c\u540c\u65f6\u5bf9\u7a83\u542c\u8005\u7684\u4fe1\u606f\u6cc4\u9732\u548c\u79d8\u5bc6\u6d88\u606f\u7f16\u7801\u7684\u4fe1\u606f\u6210\u672c\u65bd\u52a0\u660e\u786e\u7684\u4e0a\u9650\u3002", "method": "\u901a\u8fc7\u5229\u7528\u5c40\u90e8\u51e0\u4f55\u8fd1\u4f3c\uff0c\u5c06\u975e\u51f8\u95ee\u9898\u8f6c\u5316\u4e3a\u53ef\u5904\u7406\u7684\u4e8c\u6b21\u89c4\u5212\u7ed3\u6784\u3002\u901a\u8fc7\u6c42\u89e3\u7ebf\u6027\u7a0b\u5e8f\u53ef\u4ee5\u627e\u5230\u63a7\u5236\u8fd1\u4f3c\u95ee\u9898\u7684\u6700\u4f18\u62c9\u683c\u6717\u65e5\u4e58\u6570\u3002\u8be5\u7ebf\u6027\u7a0b\u5e8f\u7684\u7ea6\u675f\u6761\u4ef6\u6e90\u81ea Karush-Kuhn-Tucker \u6761\u4ef6\uff0c\u5e76\u7528\u4fe1\u9053\u5bfc\u51fa\u77e9\u9635\u7684\u5e7f\u4e49\u7279\u5f81\u503c\u8868\u793a\u3002", "result": "\u63a8\u5bfc\u51fa\u4e86\u8fd1\u4f3c\u5c40\u90e8\u4fdd\u5bc6\u5bb9\u91cf\u7684\u89e3\u6790\u516c\u5f0f\u3002\u5b9a\u4e49\u5e76\u5206\u6790\u4e86\u4e00\u7c7b\u65b0\u7684\u79d8\u5bc6\u5c40\u90e8\u6536\u7f29\u7cfb\u6570\uff0c\u8fd9\u4e9b\u7cfb\u6570\u88ab\u8868\u5f81\u4e3a\u77e9\u9635\u675f\u7684\u6700\u5927\u5e7f\u4e49\u7279\u5f81\u503c\uff0c\u91cf\u5316\u4e86\u8fd1\u4f3c\u6548\u7528\u4e0e\u8fd1\u4f3c\u6cc4\u9732\u7684\u6700\u5927\u53ef\u5b9e\u73b0\u6bd4\u7387\uff0c\u4ece\u800c\u8861\u91cf\u4e86\u4fe1\u9053\u56fa\u6709\u7684\u5c40\u90e8\u6cc4\u9732\u6548\u7387\u3002\u5efa\u7acb\u4e86\u8fde\u63a5\u8fd9\u4e9b\u5c40\u90e8\u7cfb\u6570\u4e0e\u771f\u5b9e\u4e92\u4fe1\u606f\u5ea6\u91cf\u5b9a\u4e49\u7684\u5168\u5c40\u5bf9\u5e94\u7269\u7684\u754c\u9650\u3002", "conclusion": "\u8be5\u6846\u67b6\u4fc3\u8fdb\u4e86\u8fd1\u4f3c\u5c40\u90e8\u4fdd\u5bc6\u5bb9\u91cf\u7684\u89e3\u6790\u516c\u5f0f\u7684\u63a8\u5bfc\uff0c\u5e76\u5b9a\u4e49\u548c\u5206\u6790\u4e86\u4e00\u7c7b\u65b0\u7684\u79d8\u5bc6\u5c40\u90e8\u6536\u7f29\u7cfb\u6570\u3002\u5e76\u901a\u8fc7\u8be6\u7ec6\u5206\u6790\u548c\u6570\u503c\u8bf4\u660e\u4e86\u8be5\u6846\u67b6\u7684\u6709\u6548\u6027\u3002"}}
{"id": "2510.13633", "categories": ["cs.GT"], "pdf": "https://arxiv.org/pdf/2510.13633", "abs": "https://arxiv.org/abs/2510.13633", "authors": ["Pooja Kulkarni", "Ruta Mehta", "Vishnu V. Narayan", "Tomasz Ponitka"], "title": "Online Fair Division With Subsidy: When Do Envy-Free Allocations Exist, and at What Cost?", "comment": "20 pages", "summary": "We study the problem of fairly allocating $m$ indivisible items arriving\nonline, among $n$ (offline) agents. Although envy-freeness has emerged as the\narchetypal fairness notion, envy-free (EF) allocations need not exist with\nindivisible items. To bypass this, a prominent line of research demonstrates\nthat there exist allocations that can be made envy-free by allowing a subsidy.\nExtensive work in the offline setting has focused on finding such envy-freeable\nallocations with bounded subsidy. We extend this literature to an online\nsetting where items arrive one at a time and must be immediately and\nirrevocably allocated. Our contributions are two-fold:\n  1. Maintaining EF Online: We show that envy-freeability cannot always be\npreserved online when the valuations are submodular or supermodular, even with\nbinary marginals. In contrast, we design online algorithms that maintain\nenvy-freeability at every step for the class of additive valuations, and for\nits superclasses including $k$-demand and SPLC valuations.\n  2. Ensuring Low Subsidy: We investigate the quantity of subsidy required to\nguarantee envy-freeness online. Surprisingly, even for additive valuations, the\nminimum subsidy may be as large as $\\Omega(mn)$, in contrast to the offline\nsetting, where the bound is $O(n)$. On the positive side, we identify valuation\nclasses where the minimum subsidy is small (i.e., does not depend on $m$),\nincluding $k$-valued, rank-one, restricted additive, and identical valuations,\nand we obtain (mostly) tight subsidy bounds for these classes.", "AI": {"tldr": "\u672c\u6587\u7814\u7a76\u4e86\u5728\u7ebf\u73af\u5883\u4e2d\u4e0d\u53ef\u5206\u5272\u7269\u54c1\u7684\u516c\u5e73\u5206\u914d\u95ee\u9898\u3002\u4e3b\u8981\u8d21\u732e\u5728\u4e8e\uff1a1. \u5728\u7ebf\u7ef4\u6301\u65e0\u5ac9\u5992\u6027\uff1a\u6211\u4eec\u8bbe\u8ba1\u4e86\u5728\u7ebf\u7b97\u6cd5\uff0c\u5728\u6bcf\u4e00\u6b65\u90fd\u4fdd\u6301\u53ef\u5b9e\u73b0\u65e0\u5ac9\u5992\u6027\uff0c\u9002\u7528\u4e8e\u53ef\u52a0\u4f30\u503c\u53ca\u5176\u8d85\u7c7b\u30022. \u786e\u4fdd\u4f4e\u8865\u8d34\uff1a\u6211\u4eec\u63a2\u8ba8\u4e86\u5728\u7ebf\u73af\u5883\u4e0b\u4fdd\u8bc1\u65e0\u5ac9\u5992\u6027\u6240\u9700\u7684\u8865\u8d34\u91cf\uff0c\u5e76\u786e\u5b9a\u4e86\u5728\u67d0\u4e9b\u4f30\u503c\u7c7b\u522b\u4e2d\uff0c\u6700\u4f4e\u8865\u8d34\u91cf\u53ef\u4ee5\u5f88\u5c0f\u3002", "motivation": "\u5728\u4e0d\u53ef\u5206\u5272\u7269\u54c1\u7684\u5206\u914d\u4e2d\uff0c\u65e0\u5ac9\u5992\uff08EF\uff09\u5206\u914d\u4e0d\u4e00\u5b9a\u5b58\u5728\u3002\u4e3a\u4e86\u89e3\u51b3\u8fd9\u4e2a\u95ee\u9898\uff0c\u5148\u524d\u7684\u7814\u7a76\u8868\u660e\uff0c\u901a\u8fc7\u5141\u8bb8\u8865\u8d34\u53ef\u4ee5\u5b9e\u73b0\u65e0\u5ac9\u5992\u7684\u5206\u914d\u3002\u7136\u800c\uff0c\u5728\u7ebf\u5206\u914d\u573a\u666f\u4e0b\uff0c\u7269\u54c1\u9010\u4e2a\u5230\u8fbe\u5e76\u4e14\u5fc5\u987b\u7acb\u5373\u5206\u914d\uff0c\u73b0\u6709\u7684\u79bb\u7ebf\u65b9\u6cd5\u4e0d\u518d\u9002\u7528\u3002", "method": "\u672c\u6587\u9996\u5148\u5206\u6790\u4e86\u5728\u5b50\u6a21\u6216\u8d85\u6a21\u4f30\u503c\u60c5\u51b5\u4e0b\uff0c\u5728\u7ebf\u7ef4\u6301\u53ef\u5b9e\u73b0\u65e0\u5ac9\u5992\u6027\u7684\u56f0\u96be\u3002\u968f\u540e\uff0c\u9488\u5bf9\u53ef\u52a0\u4f30\u503c\u53ca\u5176\u8d85\u7c7b\uff0c\u8bbe\u8ba1\u4e86\u5728\u7ebf\u7b97\u6cd5\u4ee5\u5728\u6bcf\u4e00\u6b65\u4fdd\u6301\u53ef\u5b9e\u73b0\u65e0\u5ac9\u5992\u6027\u3002\u5728\u8865\u8d34\u65b9\u9762\uff0c\u6587\u7ae0\u5206\u6790\u4e86\u5373\u4f7f\u5bf9\u4e8e\u53ef\u52a0\u4f30\u503c\uff0c\u5728\u7ebf\u73af\u5883\u4e0b\u7684\u6700\u4f4e\u8865\u8d34\u91cf\u4e5f\u53ef\u80fd\u663e\u8457\u589e\u52a0\u3002\u4f46\u540c\u65f6\uff0c\u4f5c\u8005\u4e5f\u8bc6\u522b\u4e86\u4e00\u4e9b\u4f30\u503c\u7c7b\u522b\uff08\u5982k-valued\u3001rank-one\u3001restricted additive\u548cidentical valuations\uff09\uff0c\u5728\u8fd9\u4e9b\u7c7b\u522b\u4e2d\u6700\u4f4e\u8865\u8d34\u91cf\u53ef\u4ee5\u5f88\u5c0f\uff0c\u5e76\u63a8\u5bfc\u51fa\u4e86\u7d27\u5bc6\u7684\u8865\u8d34\u754c\u9650\u3002", "result": "1. \u5bf9\u4e8e\u5b50\u6a21\u6216\u8d85\u6a21\u4f30\u503c\uff0c\u5373\u4f7f\u662f\u4e8c\u5143\u8fb9\u9645\uff0c\u4e5f\u65e0\u6cd5\u603b\u662f\u5728\u7ebf\u4fdd\u6301\u53ef\u5b9e\u73b0\u65e0\u5ac9\u5992\u6027\u30022. \u8bbe\u8ba1\u4e86\u5728\u7ebf\u7b97\u6cd5\uff0c\u5bf9\u4e8e\u53ef\u52a0\u4f30\u503c\u4ee5\u53cak-demand\u548cSPLC\u4f30\u503c\uff0c\u53ef\u4ee5\u5728\u7ebf\u4fdd\u6301\u53ef\u5b9e\u73b0\u65e0\u5ac9\u5992\u6027\u30023. \u5bf9\u4e8e\u53ef\u52a0\u4f30\u503c\uff0c\u5728\u7ebf\u73af\u5883\u4e0b\u6240\u9700\u7684\u6700\u4f4e\u8865\u8d34\u53ef\u80fd\u9ad8\u8fbe$\\\\Omega(mn)$\uff0c\u8fdc\u9ad8\u4e8e\u79bb\u7ebf\u8bbe\u7f6e\u7684$O(n)$\u30024. \u5bf9\u4e8ek-valued\u3001rank-one\u3001restricted additive\u548cidentical valuations\u7b49\u4f30\u503c\u7c7b\u522b\uff0c\u6240\u9700\u7684\u6700\u4f4e\u8865\u8d34\u5f88\u5c0f\uff0c\u4e14\u4e0d\u4f9d\u8d56\u4e8em\uff0c\u5e76\u83b7\u5f97\u4e86\uff08\u5927\u90e8\u5206\uff09\u7d27\u5bc6\u7684\u8865\u8d34\u754c\u9650\u3002", "conclusion": "\u672c\u6587\u5c06\u79bb\u7ebf\u73af\u5883\u4e0b\u7684\u53ef\u5b9e\u73b0\u65e0\u5ac9\u5992\u5206\u914d\u6269\u5c55\u5230\u4e86\u5728\u7ebf\u73af\u5883\u3002\u5c3d\u7ba1\u5728\u7ebf\u5206\u914d\u5e26\u6765\u4e86\u65b0\u7684\u6311\u6218\uff0c\u4f8b\u5982\u5728\u7279\u5b9a\u4f30\u503c\u4e0b\u7684\u9ad8\u8865\u8d34\u9700\u6c42\uff0c\u4f46\u901a\u8fc7\u8bbe\u8ba1\u7279\u5b9a\u7684\u5728\u7ebf\u7b97\u6cd5\u548c\u8bc6\u522b\u67d0\u4e9b\u4f30\u503c\u7c7b\u522b\uff0c\u4ecd\u80fd\u6709\u6548\u5730\u5904\u7406\u8be5\u95ee\u9898\u3002\u7814\u7a76\u7ed3\u679c\u8868\u660e\uff0c\u5728\u7ebf\u516c\u5e73\u5206\u914d\u7684\u7814\u7a76\u9700\u8981\u9488\u5bf9\u4e0d\u540c\u4f30\u503c\u7c7b\u578b\u548c\u5177\u4f53\u573a\u666f\u8fdb\u884c\u7ec6\u81f4\u5206\u6790\u548c\u7b97\u6cd5\u8bbe\u8ba1\u3002"}}
{"id": "2510.13046", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2510.13046", "abs": "https://arxiv.org/abs/2510.13046", "authors": ["Huawei Jiang", "Husna Mutahira", "Gan Huang", "Mannan Saeed Muhammad"], "title": "One Dimensional CNN ECG Mamba for Multilabel Abnormality Classification in 12 Lead ECG", "comment": "6 Pages, 2 figures", "summary": "Accurate detection of cardiac abnormalities from electrocardiogram recordings\nis regarded as essential for clinical diagnostics and decision support.\nTraditional deep learning models such as residual networks and transformer\narchitectures have been applied successfully to this task, but their\nperformance has been limited when long sequential signals are processed.\nRecently, state space models have been introduced as an efficient alternative.\nIn this study, a hybrid framework named One Dimensional Convolutional Neural\nNetwork Electrocardiogram Mamba is introduced, in which convolutional feature\nextraction is combined with Mamba, a selective state space model designed for\neffective sequence modeling. The model is built upon Vision Mamba, a\nbidirectional variant through which the representation of temporal dependencies\nin electrocardiogram data is enhanced. Comprehensive experiments on the\nPhysioNet Computing in Cardiology Challenges of 2020 and 2021 were conducted,\nand superior performance compared with existing methods was achieved.\nSpecifically, the proposed model achieved substantially higher AUPRC and AUROC\nscores than those reported by the best previously published algorithms on\ntwelve lead electrocardiograms. These results demonstrate the potential of\nMamba-based architectures to advance reliable ECG classification. This\ncapability supports early diagnosis and personalized treatment, while enhancing\naccessibility in telemedicine and resource-constrained healthcare systems.", "AI": {"tldr": "\u8be5\u7814\u7a76\u4ecb\u7ecd\u4e86\u4e00\u79cd\u540d\u4e3aOne Dimensional Convolutional Neural Network Electrocardiogram Mamba\u7684\u6df7\u5408\u6846\u67b6\uff0c\u5b83\u7ed3\u5408\u4e86\u5377\u79ef\u7279\u5f81\u63d0\u53d6\u548cMamba\uff08\u4e00\u79cd\u9009\u62e9\u6027\u72b6\u6001\u7a7a\u95f4\u6a21\u578b\uff09\uff0c\u7528\u4e8e\u5fc3\u7535\u56fe\u5f02\u5e38\u68c0\u6d4b\u3002\u8be5\u6a21\u578b\u5728PhysioNet\u6311\u6218\u8d5b\u6570\u636e\u96c6\u4e0a\u53d6\u5f97\u4e86\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\u7684\u6027\u80fd\uff0c\u8bc1\u660e\u4e86Mamba\u57fa\u67b6\u6784\u5728\u5fc3\u7535\u56fe\u5206\u7c7b\u4e2d\u7684\u6f5c\u529b\uff0c\u5e76\u652f\u6301\u65e9\u671f\u8bca\u65ad\u548c\u4e2a\u6027\u5316\u6cbb\u7597\u3002", "motivation": "\u4f20\u7edf\u7684\u6df1\u5ea6\u5b66\u4e60\u6a21\u578b\u5728\u5904\u7406\u957f\u5e8f\u5217\u5fc3\u7535\u56fe\u4fe1\u53f7\u65f6\u6027\u80fd\u6709\u9650\uff0c\u800c\u5fc3\u810f\u5f02\u5e38\u7684\u51c6\u786e\u68c0\u6d4b\u5bf9\u4e34\u5e8a\u8bca\u65ad\u548c\u51b3\u7b56\u652f\u6301\u81f3\u5173\u91cd\u8981\u3002", "method": "\u672c\u7814\u7a76\u5f15\u5165\u4e86\u4e00\u79cd\u540d\u4e3aOne Dimensional Convolutional Neural Network Electrocardiogram Mamba\uff081D CNN-ECG Mamba\uff09\u7684\u6df7\u5408\u6846\u67b6\u3002\u8be5\u6846\u67b6\u5c06\u5377\u79ef\u7279\u5f81\u63d0\u53d6\u4e0eMamba\uff08\u4e00\u79cd\u4e13\u4e3a\u6709\u6548\u5e8f\u5217\u5efa\u6a21\u800c\u8bbe\u8ba1\u7684\u9009\u62e9\u6027\u72b6\u6001\u7a7a\u95f4\u6a21\u578b\uff09\u76f8\u7ed3\u5408\u3002\u6a21\u578b\u6784\u5efa\u4e8eVision Mamba\u7684\u57fa\u7840\u4e4b\u4e0a\uff0cVision Mamba\u662f\u4e00\u79cd\u53cc\u5411\u53d8\u4f53\uff0c\u5b83\u589e\u5f3a\u4e86\u5fc3\u7535\u56fe\u6570\u636e\u4e2d\u65f6\u95f4\u4f9d\u8d56\u6027\u7684\u8868\u793a\u3002", "result": "\u5728PhysioNet Computing in Cardiology Challenges 2020\u548c2021\u7684\u7efc\u5408\u5b9e\u9a8c\u4e2d\uff0c\u6240\u63d0\u51fa\u7684\u6a21\u578b\u53d6\u5f97\u4e86\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\u7684\u6027\u80fd\u3002\u5177\u4f53\u800c\u8a00\uff0c\u8be5\u6a21\u578b\u5728\u5341\u4e8c\u5bfc\u8054\u5fc3\u7535\u56fe\u4e0a\u7684AUPRC\u548cAUROC\u5206\u6570\u5747\u663e\u8457\u9ad8\u4e8e\u6b64\u524d\u8868\u73b0\u6700\u4f73\u7684\u7b97\u6cd5\u3002", "conclusion": "\u8fd9\u4e9b\u7ed3\u679c\u8bc1\u660e\u4e86\u57fa\u4e8eMamba\u7684\u67b6\u6784\u5728\u63a8\u8fdb\u53ef\u9760\u5fc3\u7535\u56fe\u5206\u7c7b\u65b9\u9762\u7684\u6f5c\u529b\u3002\u8fd9\u79cd\u80fd\u529b\u652f\u6301\u65e9\u671f\u8bca\u65ad\u548c\u4e2a\u6027\u5316\u6cbb\u7597\uff0c\u540c\u65f6\u589e\u5f3a\u4e86\u8fdc\u7a0b\u533b\u7597\u548c\u8d44\u6e90\u53d7\u9650\u533b\u7597\u7cfb\u7edf\u7684\u53ef\u53ca\u6027\u3002"}}
{"id": "2510.13063", "categories": ["cs.CV", "cs.AI", "cs.LG"], "pdf": "https://arxiv.org/pdf/2510.13063", "abs": "https://arxiv.org/abs/2510.13063", "authors": ["Thomas W. Mitchel", "Hyunwoo Ryu", "Vincent Sitzmann"], "title": "True Self-Supervised Novel View Synthesis is Transferable", "comment": null, "summary": "In this paper, we identify that the key criterion for determining whether a\nmodel is truly capable of novel view synthesis (NVS) is transferability:\nWhether any pose representation extracted from one video sequence can be used\nto re-render the same camera trajectory in another. We analyze prior work on\nself-supervised NVS and find that their predicted poses do not transfer: The\nsame set of poses lead to different camera trajectories in different 3D scenes.\nHere, we present XFactor, the first geometry-free self-supervised model capable\nof true NVS. XFactor combines pair-wise pose estimation with a simple\naugmentation scheme of the inputs and outputs that jointly enables\ndisentangling camera pose from scene content and facilitates geometric\nreasoning. Remarkably, we show that XFactor achieves transferability with\nunconstrained latent pose variables, without any 3D inductive biases or\nconcepts from multi-view geometry -- such as an explicit parameterization of\nposes as elements of SE(3). We introduce a new metric to quantify\ntransferability, and through large-scale experiments, we demonstrate that\nXFactor significantly outperforms prior pose-free NVS transformers, and show\nthat latent poses are highly correlated with real-world poses through probing\nexperiments.", "AI": {"tldr": "\u8fd9\u7bc7\u8bba\u6587\u4ecb\u7ecd\u4e86\u4e00\u79cd\u540d\u4e3aXFactor\u7684\u65b0\u578b\u65e0\u51e0\u4f55\u81ea\u76d1\u7763\u6a21\u578b\uff0c\u7528\u4e8e\u5b9e\u73b0\u771f\u6b63\u7684\u5168\u65b0\u89c6\u89d2\u5408\u6210\u3002", "motivation": "\u4ee5\u5f80\u7684\u81ea\u76d1\u7763\u5168\u65b0\u89c6\u89d2\u5408\u6210\u6a21\u578b\u90fd\u65e0\u6cd5\u5c06\u59ff\u6001\u4ece\u4e00\u4e2a\u89c6\u9891\u5e8f\u5217\u8f6c\u79fb\u5230\u53e6\u4e00\u4e2a\u89c6\u9891\u5e8f\u5217\uff0c\u672c\u6587\u65e8\u5728\u89e3\u51b3\u8fd9\u4e00\u95ee\u9898\u3002", "method": "XFactor\u7ed3\u5408\u4e86\u6210\u5bf9\u59ff\u6001\u4f30\u8ba1\u548c\u7b80\u5355\u7684\u8f93\u5165\u8f93\u51fa\u589e\u5f3a\u65b9\u6848\uff0c\u4ece\u800c\u89e3\u8026\u76f8\u673a\u59ff\u6001\u548c\u573a\u666f\u5185\u5bb9\uff0c\u5e76\u4fc3\u8fdb\u51e0\u4f55\u63a8\u7406\u3002", "result": "XFactor\u5b9e\u73b0\u4e86\u5177\u6709\u65e0\u7ea6\u675f\u6f5c\u5728\u59ff\u6001\u53d8\u91cf\u7684\u53ef\u8f6c\u79fb\u6027\uff0c\u5e76\u4e14\u65e0\u9700\u4efb\u4f553D\u5f52\u7eb3\u504f\u5dee\u6216\u591a\u89c6\u89d2\u51e0\u4f55\u6982\u5ff5\u3002\u5b83\u663e\u8457\u4f18\u4e8e\u5148\u524d\u7684\u65e0\u59ff\u6001NVS\u8f6c\u6362\u5668\u3002", "conclusion": "XFactor\u662f\u7b2c\u4e00\u4e2a\u80fd\u591f\u5b9e\u73b0\u771f\u6b63NVS\u7684\u65e0\u51e0\u4f55\u81ea\u76d1\u7763\u6a21\u578b\uff0c\u5e76\u901a\u8fc7\u5b9e\u9a8c\u8bc1\u660e\u4e86\u5176\u59ff\u6001\u7684\u53ef\u8f6c\u79fb\u6027\u548c\u4e0e\u771f\u5b9e\u4e16\u754c\u59ff\u6001\u7684\u9ad8\u5ea6\u76f8\u5173\u6027\u3002"}}
{"id": "2510.13215", "categories": ["cs.AI", "cs.CL"], "pdf": "https://arxiv.org/pdf/2510.13215", "abs": "https://arxiv.org/abs/2510.13215", "authors": ["Joy Jia Yin Lim", "Ye He", "Jifan Yu", "Xin Cong", "Daniel Zhang-Li", "Zhiyuan Liu", "Huiqin Liu", "Lei Hou", "Juanzi Li", "Bin Xu"], "title": "Personalized Learning Path Planning with Goal-Driven Learner State Modeling", "comment": null, "summary": "Personalized Learning Path Planning (PLPP) aims to design adaptive learning\npaths that align with individual goals. While large language models (LLMs) show\npotential in personalizing learning experiences, existing approaches often lack\nmechanisms for goal-aligned planning. We introduce Pxplore, a novel framework\nfor PLPP that integrates a reinforcement-based training paradigm and an\nLLM-driven educational architecture. We design a structured learner state model\nand an automated reward function that transforms abstract objectives into\ncomputable signals. We train the policy combining supervised fine-tuning (SFT)\nand Group Relative Policy Optimization (GRPO), and deploy it within a\nreal-world learning platform. Extensive experiments validate Pxplore's\neffectiveness in producing coherent, personalized, and goal-driven learning\npaths. We release our code and dataset to facilitate future research.", "AI": {"tldr": "Pxpore\u662f\u4e00\u4e2a\u65b0\u9896\u7684\u3001\u57fa\u4e8eLLM\u7684\u3001\u5f3a\u5316\u5b66\u4e60\u8bad\u7ec3\u7684\u4e2a\u6027\u5316\u5b66\u4e60\u8def\u5f84\u89c4\u5212\u6846\u67b6\u3002", "motivation": "\u73b0\u6709\u7684\u4e2a\u6027\u5316\u5b66\u4e60\u8def\u5f84\u89c4\u5212\u65b9\u6cd5\u901a\u5e38\u7f3a\u4e4f\u4e0e\u5b66\u4e60\u76ee\u6807\u5bf9\u9f50\u7684\u673a\u5236\uff0c\u800c\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08LLMs\uff09\u5728\u4e2a\u6027\u5316\u5b66\u4e60\u4f53\u9a8c\u65b9\u9762\u8868\u73b0\u51fa\u6f5c\u529b\u3002", "method": "\u672c\u6587\u63d0\u51fa\u4e86Pxplore\u6846\u67b6\uff0c\u96c6\u6210\u4e86\u57fa\u4e8e\u5f3a\u5316\u5b66\u4e60\u7684\u8bad\u7ec3\u8303\u5f0f\u548cLLM\u9a71\u52a8\u7684\u6559\u80b2\u67b6\u6784\u3002\u8bbe\u8ba1\u4e86\u4e00\u4e2a\u7ed3\u6784\u5316\u7684\u5b66\u4e60\u8005\u72b6\u6001\u6a21\u578b\u548c\u81ea\u52a8\u5956\u52b1\u51fd\u6570\u3002\u7b56\u7565\u8bad\u7ec3\u7ed3\u5408\u4e86\u76d1\u7763\u5fae\u8c03\uff08SFT\uff09\u548cGroup Relative Policy Optimization\uff08GRPO\uff09\u3002", "result": "Pxplore\u5728\u5b9e\u9645\u5b66\u4e60\u5e73\u53f0\u4e2d\u90e8\u7f72\uff0c\u5e76\u5728\u4e00\u7cfb\u5217\u5b9e\u9a8c\u4e2d\u9a8c\u8bc1\u4e86\u5176\u5728\u751f\u6210\u8fde\u8d2f\u3001\u4e2a\u6027\u5316\u548c\u76ee\u6807\u9a71\u52a8\u7684\u5b66\u4e60\u8def\u5f84\u65b9\u9762\u7684\u6709\u6548\u6027\u3002", "conclusion": "Pxplore\u80fd\u591f\u6709\u6548\u5730\u751f\u6210\u8fde\u8d2f\u3001\u4e2a\u6027\u5316\u548c\u76ee\u6807\u9a71\u52a8\u7684\u5b66\u4e60\u8def\u5f84\uff0c\u5e76\u6709\u671b\u4fc3\u8fdb\u672a\u6765\u5728\u8be5\u9886\u57df\u7684\u7814\u7a76\u3002"}}
{"id": "2510.13080", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2510.13080", "abs": "https://arxiv.org/abs/2510.13080", "authors": ["Shuai Fu", "Jian Zhou", "Qi Chen", "Huang Jing", "Huy Anh Nguyen", "Xiaohan Liu", "Zhixiong Zeng", "Lin Ma", "Quanshi Zhang", "Qi Wu"], "title": "Counting Hallucinations in Diffusion Models", "comment": null, "summary": "Diffusion probabilistic models (DPMs) have demonstrated remarkable progress\nin generative tasks, such as image and video synthesis. However, they still\noften produce hallucinated samples (hallucinations) that conflict with\nreal-world knowledge, such as generating an implausible duplicate cup floating\nbeside another cup. Despite their prevalence, the lack of feasible\nmethodologies for systematically quantifying such hallucinations hinders\nprogress in addressing this challenge and obscures potential pathways for\ndesigning next-generation generative models under factual constraints. In this\nwork, we bridge this gap by focusing on a specific form of hallucination, which\nwe term counting hallucination, referring to the generation of an incorrect\nnumber of instances or structured objects, such as a hand image with six\nfingers, despite such patterns being absent from the training data. To this\nend, we construct a dataset suite CountHalluSet, with well-defined counting\ncriteria, comprising ToyShape, SimObject, and RealHand. Using these datasets,\nwe develop a standardized evaluation protocol for quantifying counting\nhallucinations, and systematically examine how different sampling conditions in\nDPMs, including solver type, ODE solver order, sampling steps, and initial\nnoise, affect counting hallucination levels. Furthermore, we analyze their\ncorrelation with common evaluation metrics such as FID, revealing that this\nwidely used image quality metric fails to capture counting hallucinations\nconsistently. This work aims to take the first step toward systematically\nquantifying hallucinations in diffusion models and offer new insights into the\ninvestigation of hallucination phenomena in image generation.", "AI": {"tldr": "\u8fd9\u7bc7\u8bba\u6587\u4ecb\u7ecd\u4e86\u4e00\u79cd\u91cf\u5316\u6269\u6563\u6a21\u578b\u4e2d\u201c\u8ba1\u6570\u5e7b\u89c9\u201d\u7684\u65b9\u6cd5\uff0c\u5e76\u521b\u5efa\u4e86\u4e00\u4e2a\u540d\u4e3aCountHalluSet\u7684\u6570\u636e\u96c6\u6765\u7cfb\u7edf\u8bc4\u4f30\u4e0d\u540c\u91c7\u6837\u6761\u4ef6\u4e0b\u6a21\u578b\u7684\u5e7b\u89c9\u6c34\u5e73\uff0c\u53d1\u73b0\u5e38\u7528\u56fe\u50cf\u8d28\u91cf\u8bc4\u4f30\u6307\u6807FID\u672a\u80fd\u6709\u6548\u6355\u6349\u8ba1\u6570\u5e7b\u89c9\u3002", "motivation": "\u5c3d\u7ba1\u6269\u6563\u6982\u7387\u6a21\u578b\u5728\u751f\u6210\u4efb\u52a1\u4e2d\u8868\u73b0\u51fa\u8272\uff0c\u4f46\u4ecd\u4f1a\u4ea7\u751f\u4e0e\u73b0\u5b9e\u4e16\u754c\u77e5\u8bc6\u76f8\u6096\u7684\u201c\u5e7b\u89c9\u201d\u6837\u672c\uff0c\u4f8b\u5982\u751f\u6210\u591a\u4f59\u6216\u4e0d\u5408\u7406\u7684\u7269\u4f53\u3002\u76ee\u524d\u7f3a\u4e4f\u7cfb\u7edf\u91cf\u5316\u8fd9\u4e9b\u5e7b\u89c9\u7684\u53ef\u884c\u65b9\u6cd5\uff0c\u963b\u788d\u4e86\u89e3\u51b3\u6b64\u95ee\u9898\u7684\u8fdb\u5c55\u5e76\u63a9\u76d6\u4e86\u8bbe\u8ba1\u53d7\u4e8b\u5b9e\u7ea6\u675f\u7684\u4e0b\u4e00\u4ee3\u751f\u6210\u6a21\u578b\u7684\u6f5c\u529b\u3002", "method": "\u4f5c\u8005\u4e13\u6ce8\u4e8e\u4e00\u79cd\u7279\u5b9a\u5f62\u5f0f\u7684\u5e7b\u89c9\uff0c\u79f0\u4e4b\u4e3a\u201c\u8ba1\u6570\u5e7b\u89c9\u201d\uff0c\u5373\u751f\u6210\u4e0d\u6b63\u786e\u6570\u91cf\u7684\u5b9e\u4f8b\u6216\u7ed3\u6784\u5316\u5bf9\u8c61\uff08\u4f8b\u5982\u516d\u6307\u624b\uff09\u3002\u4e3a\u6b64\uff0c\u4ed6\u4eec\u6784\u5efa\u4e86\u4e00\u4e2a\u540d\u4e3aCountHalluSet\u7684\u6570\u636e\u96c6\u5957\u4ef6\uff0c\u5305\u542bToyShape\u3001SimObject\u548cRealHand\uff0c\u5e76\u5b9a\u4e49\u4e86\u660e\u786e\u7684\u8ba1\u6570\u6807\u51c6\u3002\u5229\u7528\u8fd9\u4e9b\u6570\u636e\u96c6\uff0c\u4ed6\u4eec\u5f00\u53d1\u4e86\u4e00\u4e2a\u6807\u51c6\u5316\u7684\u8bc4\u4f30\u534f\u8bae\u6765\u91cf\u5316\u8ba1\u6570\u5e7b\u89c9\uff0c\u5e76\u7cfb\u7edf\u5730\u7814\u7a76\u4e86DPM\u4e2d\u4e0d\u540c\u7684\u91c7\u6837\u6761\u4ef6\uff08\u5305\u62ec\u6c42\u89e3\u5668\u7c7b\u578b\u3001ODE\u6c42\u89e3\u5668\u9636\u6570\u3001\u91c7\u6837\u6b65\u957f\u548c\u521d\u59cb\u566a\u58f0\uff09\u5982\u4f55\u5f71\u54cd\u8ba1\u6570\u5e7b\u89c9\u6c34\u5e73\u3002", "result": "\u7814\u7a76\u7ed3\u679c\u8868\u660e\uff0c\u4e0d\u540c\u7684\u91c7\u6837\u6761\u4ef6\u4f1a\u5f71\u54cd\u8ba1\u6570\u5e7b\u89c9\u7684\u6c34\u5e73\u3002\u6b64\u5916\uff0c\u4ed6\u4eec\u53d1\u73b0\u5e7f\u6cdb\u4f7f\u7528\u7684\u56fe\u50cf\u8d28\u91cf\u8bc4\u4f30\u6307\u6807FID\u672a\u80fd\u59cb\u7ec8\u5982\u4e00\u5730\u6355\u6349\u5230\u8ba1\u6570\u5e7b\u89c9\u3002", "conclusion": "\u8fd9\u9879\u5de5\u4f5c\u65e8\u5728\u8fc8\u51fa\u7cfb\u7edf\u91cf\u5316\u6269\u6563\u6a21\u578b\u4e2d\u5e7b\u89c9\u7684\u7b2c\u4e00\u6b65\uff0c\u5e76\u4e3a\u7814\u7a76\u56fe\u50cf\u751f\u6210\u4e2d\u7684\u5e7b\u89c9\u73b0\u8c61\u63d0\u4f9b\u4e86\u65b0\u7684\u89c1\u89e3\u3002"}}
{"id": "2510.13393", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2510.13393", "abs": "https://arxiv.org/abs/2510.13393", "authors": ["Yunxiao Zhao", "Zhiqiang Wang", "Xingtong Yu", "Xiaoli Li", "Jiye Liang", "Ru Li"], "title": "Learnable Game-theoretic Policy Optimization for Data-centric Self-explanation Rationalization", "comment": "14 pages, 7 figures, 11 tables. Under review by IEEE", "summary": "Rationalization, a data-centric framework, aims to build self-explanatory\nmodels to explain the prediction outcome by generating a subset of\nhuman-intelligible pieces of the input data. It involves a cooperative game\nmodel where a generator generates the most human-intelligible parts of the\ninput (i.e., rationales), followed by a predictor that makes predictions based\non these generated rationales. Conventional rationalization methods typically\nimpose constraints via regularization terms to calibrate or penalize undesired\ngeneration. However, these methods are suffering from a problem called mode\ncollapse, in which the predictor produces correct predictions yet the generator\nconsistently outputs rationales with collapsed patterns. Moreover, existing\nstudies are typically designed separately for specific collapsed patterns,\nlacking a unified consideration. In this paper, we systematically revisit\ncooperative rationalization from a novel game-theoretic perspective and\nidentify the fundamental cause of this problem: the generator no longer tends\nto explore new strategies to uncover informative rationales, ultimately leading\nthe system to converge to a suboptimal game equilibrium (correct predictions\nv.s collapsed rationales). To solve this problem, we then propose a novel\napproach, Game-theoretic Policy Optimization oriented RATionalization (PORAT),\nwhich progressively introduces policy interventions to address the game\nequilibrium in the cooperative game process, thereby guiding the model toward a\nmore optimal solution state. We theoretically analyse the cause of such a\nsuboptimal equilibrium and prove the feasibility of the proposed method.\nFurthermore, we validate our method on nine widely used real-world datasets and\ntwo synthetic settings, where PORAT achieves up to 8.1% performance\nimprovements over existing state-of-the-art methods.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u7684\u65b9\u6cd5PORAT\uff0c\u4ee5\u89e3\u51b3\u73b0\u6709\u5408\u7406\u5316\u65b9\u6cd5\u4e2d\u6a21\u5f0f\u5d29\u6e83\u7684\u95ee\u9898\u3002", "motivation": "\u4f20\u7edf\u7684\u5408\u7406\u5316\u65b9\u6cd5\u901a\u8fc7\u6b63\u5219\u5316\u9879\u6765\u6821\u51c6\u6216\u60e9\u7f5a\u4e0d\u671f\u671b\u7684\u751f\u6210\uff0c\u4f46\u5b58\u5728\u6a21\u5f0f\u5d29\u6e83\u95ee\u9898\uff0c\u5bfc\u81f4\u9884\u6d4b\u5668\u4ea7\u751f\u6b63\u786e\u9884\u6d4b\uff0c\u800c\u751f\u6210\u5668\u6301\u7eed\u8f93\u51fa\u6a21\u5f0f\u5d29\u6e83\u7684\u5408\u7406\u6027\u3002\u73b0\u6709\u7814\u7a76\u901a\u5e38\u9488\u5bf9\u7279\u5b9a\u5d29\u6e83\u6a21\u5f0f\u5355\u72ec\u8bbe\u8ba1\uff0c\u7f3a\u4e4f\u7edf\u4e00\u7684\u8003\u8651\u3002", "method": "\u672c\u6587\u4ece\u65b0\u9896\u7684\u535a\u5f08\u8bba\u89d2\u5ea6\u91cd\u65b0\u5ba1\u89c6\u5408\u4f5c\u5408\u7406\u5316\uff0c\u5e76\u786e\u5b9a\u4e86\u6a21\u5f0f\u5d29\u6e83\u7684\u6839\u672c\u539f\u56e0\uff1a\u751f\u6210\u5668\u4e0d\u518d\u503e\u5411\u4e8e\u63a2\u7d22\u65b0\u7b56\u7565\uff0c\u4ece\u800c\u5bfc\u81f4\u7cfb\u7edf\u6536\u655b\u5230\u6b21\u4f18\u535a\u5f08\u5747\u8861\u3002\u4e3a\u89e3\u51b3\u6b64\u95ee\u9898\uff0c\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u9896\u7684\u65b9\u6cd5\u2014\u2014\u9762\u5411\u535a\u5f08\u8bba\u7b56\u7565\u4f18\u5316\u7684\u5408\u7406\u5316\uff08PORAT\uff09\uff0c\u901a\u8fc7\u9010\u6b65\u5f15\u5165\u7b56\u7565\u5e72\u9884\u6765\u89e3\u51b3\u5408\u4f5c\u535a\u5f08\u8fc7\u7a0b\u4e2d\u7684\u535a\u5f08\u5747\u8861\u95ee\u9898\uff0c\u4ece\u800c\u5f15\u5bfc\u6a21\u578b\u8d70\u5411\u66f4\u4f18\u7684\u89e3\u51b3\u65b9\u6848\u3002", "result": "PORAT\u5728\u4e5d\u4e2a\u5e7f\u6cdb\u4f7f\u7528\u7684\u771f\u5b9e\u4e16\u754c\u6570\u636e\u96c6\u548c\u4e24\u4e2a\u5408\u6210\u8bbe\u7f6e\u4e0a\uff0c\u6bd4\u73b0\u6709\u6700\u5148\u8fdb\u7684\u65b9\u6cd5\u5b9e\u73b0\u4e86\u9ad8\u8fbe8.1%\u7684\u6027\u80fd\u63d0\u5347\u3002", "conclusion": "PORAT\u65b9\u6cd5\u901a\u8fc7\u535a\u5f08\u8bba\u7b56\u7565\u4f18\u5316\uff0c\u6709\u6548\u89e3\u51b3\u4e86\u5408\u4f5c\u5408\u7406\u5316\u4e2d\u7684\u6a21\u5f0f\u5d29\u6e83\u95ee\u9898\uff0c\u63d0\u9ad8\u4e86\u6a21\u578b\u6027\u80fd\u3002"}}
{"id": "2510.13417", "categories": ["cs.AI", "cs.CL"], "pdf": "https://arxiv.org/pdf/2510.13417", "abs": "https://arxiv.org/abs/2510.13417", "authors": ["Liesbeth Allein", "Nataly Pineda-Casta\u00f1eda", "Andrea Rocci", "Marie-Francine Moens"], "title": "Assessing LLM Reasoning Through Implicit Causal Chain Discovery in Climate Discourse", "comment": null, "summary": "How does a cause lead to an effect, and which intermediate causal steps\nexplain their connection? This work scrutinizes the mechanistic causal\nreasoning capabilities of large language models (LLMs) to answer these\nquestions through the task of implicit causal chain discovery. In a diagnostic\nevaluation framework, we instruct nine LLMs to generate all possible\nintermediate causal steps linking given cause-effect pairs in causal chain\nstructures. These pairs are drawn from recent resources in argumentation\nstudies featuring polarized discussion on climate change. Our analysis reveals\nthat LLMs vary in the number and granularity of causal steps they produce.\nAlthough they are generally self-consistent and confident about the\nintermediate causal connections in the generated chains, their judgments are\nmainly driven by associative pattern matching rather than genuine causal\nreasoning. Nonetheless, human evaluations confirmed the logical coherence and\nintegrity of the generated chains. Our baseline causal chain discovery\napproach, insights from our diagnostic evaluation, and benchmark dataset with\ncausal chains lay a solid foundation for advancing future work in implicit,\nmechanistic causal reasoning in argumentation settings.", "AI": {"tldr": "\u672c\u6587\u5206\u6790\u4e86\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08LLM\uff09\u5728\u9690\u5f0f\u56e0\u679c\u94fe\u53d1\u73b0\u65b9\u9762\u7684\u80fd\u529b\uff0c\u53d1\u73b0\u5b83\u4eec\u5728\u751f\u6210\u56e0\u679c\u6b65\u9aa4\u7684\u6570\u91cf\u548c\u7c92\u5ea6\u4e0a\u5b58\u5728\u5dee\u5f02\u3002LLMs\u7684\u5224\u65ad\u4e3b\u8981\u57fa\u4e8e\u5173\u8054\u6a21\u5f0f\u5339\u914d\u800c\u975e\u771f\u6b63\u7684\u56e0\u679c\u63a8\u7406\uff0c\u4f46\u751f\u6210\u94fe\u7684\u903b\u8f91\u8fde\u8d2f\u6027\u5f97\u5230\u4e86\u4eba\u7c7b\u8bc4\u4f30\u7684\u8bc1\u5b9e\u3002", "motivation": "\u63a2\u7a76\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08LLM\uff09\u7684\u673a\u68b0\u56e0\u679c\u63a8\u7406\u80fd\u529b\uff0c\u4ee5\u56de\u7b54\u201c\u539f\u56e0\u5982\u4f55\u5bfc\u81f4\u7ed3\u679c\u201d\u4ee5\u53ca\u201c\u54ea\u4e9b\u4e2d\u95f4\u56e0\u679c\u6b65\u9aa4\u80fd\u591f\u89e3\u91ca\u5b83\u4eec\u4e4b\u95f4\u7684\u8054\u7cfb\u201d\u8fd9\u4e24\u4e2a\u95ee\u9898\u3002", "method": "\u672c\u6587\u901a\u8fc7\u9690\u5f0f\u56e0\u679c\u94fe\u53d1\u73b0\u4efb\u52a1\uff0c\u5728\u8bca\u65ad\u6027\u8bc4\u4f30\u6846\u67b6\u4e0b\uff0c\u6307\u793a\u4e5d\u4e2a\u5927\u578b\u8bed\u8a00\u6a21\u578b\u751f\u6210\u8fde\u63a5\u7ed9\u5b9a\u56e0\u679c\u5bf9\u7684\u6240\u6709\u53ef\u80fd\u7684\u4e2d\u95f4\u56e0\u679c\u6b65\u9aa4\u3002\u8fd9\u4e9b\u56e0\u679c\u5bf9\u6765\u6e90\u4e8e\u6c14\u5019\u53d8\u5316\u4e24\u6781\u5206\u5316\u8ba8\u8bba\u7684\u8bba\u8bc1\u7814\u7a76\u8d44\u6e90\u3002", "result": "LLM\u5728\u751f\u6210\u56e0\u679c\u6b65\u9aa4\u7684\u6570\u91cf\u548c\u7c92\u5ea6\u4e0a\u5b58\u5728\u5dee\u5f02\u3002\u5c3d\u7ba1LLM\u5bf9\u751f\u6210\u94fe\u4e2d\u7684\u4e2d\u95f4\u56e0\u679c\u8fde\u63a5\u5177\u6709\u81ea\u6d3d\u6027\u548c\u4fe1\u5fc3\uff0c\u4f46\u5176\u5224\u65ad\u4e3b\u8981\u7531\u5173\u8054\u6a21\u5f0f\u5339\u914d\u9a71\u52a8\uff0c\u800c\u975e\u771f\u6b63\u7684\u56e0\u679c\u63a8\u7406\u3002\u7136\u800c\uff0c\u4eba\u7c7b\u8bc4\u4f30\u8bc1\u5b9e\u4e86\u751f\u6210\u94fe\u7684\u903b\u8f91\u4e00\u81f4\u6027\u548c\u5b8c\u6574\u6027\u3002", "conclusion": "\u672c\u6587\u7684\u57fa\u7ebf\u56e0\u679c\u94fe\u53d1\u73b0\u65b9\u6cd5\u3001\u8bca\u65ad\u6027\u8bc4\u4f30\u7684\u89c1\u89e3\u4ee5\u53ca\u5e26\u6709\u56e0\u679c\u94fe\u7684\u57fa\u51c6\u6570\u636e\u96c6\u4e3a\u672a\u6765\u5728\u8bba\u8bc1\u73af\u5883\u4e2d\u8fdb\u884c\u9690\u5f0f\u3001\u673a\u68b0\u56e0\u679c\u63a8\u7406\u7684\u7814\u7a76\u5960\u5b9a\u4e86\u575a\u5b9e\u57fa\u7840\u3002"}}
{"id": "2510.13108", "categories": ["cs.CV", "cs.AI", "cs.RO"], "pdf": "https://arxiv.org/pdf/2510.13108", "abs": "https://arxiv.org/abs/2510.13108", "authors": ["Jingyu Song", "Zhenxin Li", "Shiyi Lan", "Xinglong Sun", "Nadine Chang", "Maying Shen", "Joshua Chen", "Katherine A. Skinner", "Jose M. Alvarez"], "title": "DriveCritic: Towards Context-Aware, Human-Aligned Evaluation for Autonomous Driving with Vision-Language Models", "comment": "9 pages, 3 figures", "summary": "Benchmarking autonomous driving planners to align with human judgment remains\na critical challenge, as state-of-the-art metrics like the Extended Predictive\nDriver Model Score (EPDMS) lack context awareness in nuanced scenarios. To\naddress this, we introduce DriveCritic, a novel framework featuring two key\ncontributions: the DriveCritic dataset, a curated collection of challenging\nscenarios where context is critical for correct judgment and annotated with\npairwise human preferences, and the DriveCritic model, a Vision-Language Model\n(VLM) based evaluator. Fine-tuned using a two-stage supervised and\nreinforcement learning pipeline, the DriveCritic model learns to adjudicate\nbetween trajectory pairs by integrating visual and symbolic context.\nExperiments show DriveCritic significantly outperforms existing metrics and\nbaselines in matching human preferences and demonstrates strong context\nawareness. Overall, our work provides a more reliable, human-aligned foundation\nto evaluating autonomous driving systems.", "AI": {"tldr": "\u4e3a\u4e86\u89e3\u51b3\u81ea\u52a8\u9a7e\u9a76\u89c4\u5212\u5668\u5728\u590d\u6742\u573a\u666f\u4e2d\u4e0e\u4eba\u7c7b\u5224\u65ad\u5bf9\u9f50\u7684\u6311\u6218\uff0c\u6211\u4eec\u63d0\u51fa\u4e86DriveCritic\u6846\u67b6\uff0c\u5b83\u5305\u542b\u4e00\u4e2a\u6807\u6ce8\u4e86\u4eba\u7c7b\u504f\u597d\u7684\u6570\u636e\u96c6\u548c\u4e00\u4e2a\u57fa\u4e8e\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\u7684\u8bc4\u4f30\u5668\uff0c\u8be5\u8bc4\u4f30\u5668\u901a\u8fc7\u6574\u5408\u89c6\u89c9\u548c\u7b26\u53f7\u4e0a\u4e0b\u6587\u6765\u5224\u65ad\u8f68\u8ff9\u5bf9\u3002\u5b9e\u9a8c\u8bc1\u660eDriveCritic\u5728\u5339\u914d\u4eba\u7c7b\u504f\u597d\u548c\u4e0a\u4e0b\u6587\u611f\u77e5\u65b9\u9762\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\u3002", "motivation": "\u73b0\u6709\u7684\u81ea\u52a8\u9a7e\u9a76\u89c4\u5212\u5668\u8bc4\u4f30\u6307\u6807\uff0c\u5982EPDMS\uff0c\u5728\u5904\u7406\u590d\u6742\u60c5\u5883\u65f6\u7f3a\u4e4f\u4e0a\u4e0b\u6587\u611f\u77e5\u80fd\u529b\uff0c\u5bfc\u81f4\u8bc4\u4f30\u7ed3\u679c\u4e0e\u4eba\u7c7b\u5224\u65ad\u4e0d\u7b26\u3002", "method": "\u6211\u4eec\u63d0\u51fa\u4e86DriveCritic\u6846\u67b6\uff0c\u5305\u542b\u4e24\u4e2a\u4e3b\u8981\u8d21\u732e\uff1a\n1. DriveCritic\u6570\u636e\u96c6\uff1a\u4e00\u4e2a\u7cbe\u5fc3\u7b56\u5212\u7684\u6311\u6218\u6027\u573a\u666f\u96c6\u5408\uff0c\u8fd9\u4e9b\u573a\u666f\u7684\u6b63\u786e\u5224\u65ad\u9700\u8981\u5173\u952e\u7684\u4e0a\u4e0b\u6587\u4fe1\u606f\uff0c\u5e76\u6807\u6ce8\u4e86\u6210\u5bf9\u7684\u4eba\u7c7b\u504f\u597d\u3002\n2. DriveCritic\u6a21\u578b\uff1a\u4e00\u4e2a\u57fa\u4e8e\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\uff08VLM\uff09\u7684\u8bc4\u4f30\u5668\u3002\u8be5\u6a21\u578b\u901a\u8fc7\u4e24\u9636\u6bb5\u7684\u76d1\u7763\u5b66\u4e60\u548c\u5f3a\u5316\u5b66\u4e60\u6d41\u7a0b\u8fdb\u884c\u5fae\u8c03\uff0c\u5b66\u4e60\u901a\u8fc7\u6574\u5408\u89c6\u89c9\u548c\u7b26\u53f7\u4e0a\u4e0b\u6587\u6765\u88c1\u51b3\u8f68\u8ff9\u5bf9\u3002", "result": "DriveCritic\u5728\u5339\u914d\u4eba\u7c7b\u504f\u597d\u548c\u5c55\u793a\u5f3a\u5927\u7684\u4e0a\u4e0b\u6587\u611f\u77e5\u80fd\u529b\u65b9\u9762\u663e\u8457\u4f18\u4e8e\u73b0\u6709\u6307\u6807\u548c\u57fa\u7ebf\u65b9\u6cd5\u3002", "conclusion": "DriveCritic\u6846\u67b6\u4e3a\u8bc4\u4f30\u81ea\u52a8\u9a7e\u9a76\u7cfb\u7edf\u63d0\u4f9b\u4e86\u4e00\u4e2a\u66f4\u53ef\u9760\u3001\u4e0e\u4eba\u7c7b\u5224\u65ad\u66f4\u4e00\u81f4\u7684\u57fa\u7840\uff0c\u7279\u522b\u662f\u5728\u9700\u8981\u4e0a\u4e0b\u6587\u611f\u77e5\u7684\u590d\u6742\u573a\u666f\u4e2d\u3002"}}
{"id": "2510.13501", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2510.13501", "abs": "https://arxiv.org/abs/2510.13501", "authors": ["He Du", "Bowen Li", "Chengxing Xie", "Chang Gao", "Kai Chen", "Dacheng Tao"], "title": "Confidence as a Reward: Transforming LLMs into Reward Models", "comment": null, "summary": "Reward models can significantly enhance the reasoning capabilities of large\nlanguage models (LLMs), but they typically require extensive curated data and\ncostly training. To mitigate these challenges, training-free approaches such as\nLLM-as-a-Judge leverage the intrinsic reasoning abilities of LLMs to evaluate\nresponses, achieving promising results. Recent works have also indicated that\nmodel confidence can serve effectively as a reward metric, distinguishing\nbetween chain-of-thought (CoT) and non-CoT paths. However, the concept of using\nconfidence as a reward has not been comprehensively studied. In this work, we\nsystematically investigate Confidence-as-a-Reward (CRew), a simple yet powerful\ntraining-free method that utilizes token-level confidence in the model's final\nanswers as a proxy for reward, especially suitable for close-ended tasks.\nThrough extensive experiments on mathematical reasoning tasks, we demonstrate\nthat CRew outperforms existing training-free reward approaches on the MATH500\nand RewardMATH benchmarks, and even surpasses most trained reward models. We\nfurther identify a strong correlation between CRew scores and the actual\nreasoning performance of the model. Additionally, we find that CRew can\neffectively filter high-quality training data. Building upon these insights, we\npropose CRew-DPO, a training strategy that constructs preference data from\nconfidence scores combined with correctness signals. Finetuning with CRew-DPO\nfurther enhances the model's judging capabilities and consistently outperforms\nexisting self-training methods.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3a CRew \u7684\u65e0\u8bad\u7ec3\u65b9\u6cd5\uff0c\u5b83\u5229\u7528\u5927\u578b\u8bed\u8a00\u6a21\u578b\u5728\u5c01\u95ed\u5f0f\u4efb\u52a1\u4e2d\u8f93\u51fa\u7684token\u7ea7\u7f6e\u4fe1\u5ea6\u4f5c\u4e3a\u5956\u52b1\u4fe1\u53f7\u3002", "motivation": "\u5956\u52b1\u6a21\u578b\u53ef\u4ee5\u663e\u8457\u63d0\u9ad8\u5927\u578b\u8bed\u8a00\u6a21\u578b\u7684\u63a8\u7406\u80fd\u529b\uff0c\u4f46\u5b83\u4eec\u9700\u8981\u5927\u91cf\u7684\u7b56\u5212\u6570\u636e\u548c\u6602\u8d35\u7684\u8bad\u7ec3\u3002\u4e3a\u4e86\u7f13\u89e3\u8fd9\u4e9b\u6311\u6218\uff0c\u65e0\u8bad\u7ec3\u65b9\u6cd5\uff08\u5982LLM-as-a-Judge\uff09\u5229\u7528LLM\u7684\u5185\u5728\u63a8\u7406\u80fd\u529b\u6765\u8bc4\u4f30\u54cd\u5e94\uff0c\u53d6\u5f97\u4e86\u6709\u5e0c\u671b\u7684\u7ed3\u679c\u3002", "method": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3a CRew \u7684\u65e0\u8bad\u7ec3\u65b9\u6cd5\uff0c\u5b83\u5229\u7528\u5927\u578b\u8bed\u8a00\u6a21\u578b\u5728\u5c01\u95ed\u5f0f\u4efb\u52a1\u4e2d\u8f93\u51fa\u7684token\u7ea7\u7f6e\u4fe1\u5ea6\u4f5c\u4e3a\u5956\u52b1\u4fe1\u53f7\u3002\u5728\u6b64\u57fa\u7840\u4e0a\uff0c\u672c\u6587\u63d0\u51fa\u4e86CRew-DPO\uff0c\u8fd9\u662f\u4e00\u79cd\u8bad\u7ec3\u7b56\u7565\uff0c\u5b83\u5c06\u7f6e\u4fe1\u5ea6\u5206\u6570\u4e0e\u6b63\u786e\u6027\u4fe1\u53f7\u76f8\u7ed3\u5408\uff0c\u6784\u5efa\u504f\u597d\u6570\u636e\u3002", "result": "\u5728\u6570\u5b66\u63a8\u7406\u4efb\u52a1\u4e0a\u8fdb\u884c\u7684\u5927\u91cf\u5b9e\u9a8c\u8868\u660e\uff0cCRew \u5728 MATH500 \u548c RewardMATH \u57fa\u51c6\u6d4b\u8bd5\u4e2d\u4f18\u4e8e\u73b0\u6709\u7684\u65e0\u8bad\u7ec3\u5956\u52b1\u65b9\u6cd5\uff0c\u751a\u81f3\u8d85\u8fc7\u4e86\u5927\u591a\u6570\u7ecf\u8fc7\u8bad\u7ec3\u7684\u5956\u52b1\u6a21\u578b\u3002\u7814\u7a76\u8005\u53d1\u73b0 CRew \u5206\u6570\u4e0e\u6a21\u578b\u7684\u5b9e\u9645\u63a8\u7406\u6027\u80fd\u4e4b\u95f4\u5b58\u5728\u5f88\u5f3a\u7684\u76f8\u5173\u6027\u3002CRew \u53ef\u4ee5\u6709\u6548\u5730\u7b5b\u9009\u9ad8\u8d28\u91cf\u7684\u8bad\u7ec3\u6570\u636e\u3002\u4f7f\u7528 CRew-DPO \u8fdb\u884c\u5fae\u8c03\u8fdb\u4e00\u6b65\u589e\u5f3a\u4e86\u6a21\u578b\u7684\u5224\u65ad\u80fd\u529b\uff0c\u5e76\u59cb\u7ec8\u4f18\u4e8e\u73b0\u6709\u7684\u81ea\u8bad\u7ec3\u65b9\u6cd5\u3002", "conclusion": "CRew\u662f\u4e00\u79cd\u7b80\u5355\u800c\u5f3a\u5927\u7684\u65e0\u8bad\u7ec3\u5956\u52b1\u65b9\u6cd5\uff0c\u5c24\u5176\u9002\u7528\u4e8e\u5c01\u95ed\u5f0f\u4efb\u52a1\u3002CRew\u80fd\u591f\u6709\u6548\u5730\u7b5b\u9009\u9ad8\u8d28\u91cf\u7684\u8bad\u7ec3\u6570\u636e\uff0cCRew-DPO\u901a\u8fc7\u7ed3\u5408\u7f6e\u4fe1\u5ea6\u5206\u6570\u548c\u6b63\u786e\u6027\u4fe1\u53f7\uff0c\u8fdb\u4e00\u6b65\u63d0\u9ad8\u4e86\u6a21\u578b\u7684\u5224\u65ad\u80fd\u529b\u3002"}}
{"id": "2510.13131", "categories": ["cs.CV", "cs.MM"], "pdf": "https://arxiv.org/pdf/2510.13131", "abs": "https://arxiv.org/abs/2510.13131", "authors": ["Rongjun Chen", "Chengsi Yao", "Jinchang Ren", "Xianxian Zeng", "Peixian Wang", "Jun Yuan", "Jiawen Li", "Huimin Zhao", "Xu Lu"], "title": "OS-HGAdapter: Open Semantic Hypergraph Adapter for Large Language Models Assisted Entropy-Enhanced Image-Text Alignment", "comment": null, "summary": "Text-image alignment constitutes a foundational challenge in multimedia\ncontent understanding, where effective modeling of cross-modal semantic\ncorrespondences critically enhances retrieval system performance through joint\nembedding space optimization. Given the inherent difference in information\nentropy between texts and images, conventional approaches often show an\nimbalance in the mutual retrieval of these two modalities. To address this\nparticular challenge, we propose to use the open semantic knowledge of Large\nLanguage Model (LLM) to fill for the entropy gap and reproduce the alignment\nability of humans in these tasks. Our entropy-enhancing alignment is achieved\nthrough a two-step process: 1) a new prompt template that does not rely on\nexplicit knowledge in the task domain is designed to use LLM to enhance the\npolysemy description of the text modality. By analogy, the information entropy\nof the text modality relative to the visual modality is increased; 2) A\nhypergraph adapter is used to construct multilateral connections between the\ntext and image modalities, which can correct the positive and negative matching\nerrors for synonymous semantics in the same fixed embedding space, whilst\nreducing the noise caused by open semantic entropy by mapping the reduced\ndimensions back to the original dimensions. Comprehensive evaluations on the\nFlickr30K and MS-COCO benchmarks validate the superiority of our Open Semantic\nHypergraph Adapter (OS-HGAdapter), showcasing 16.8\\% (text-to-image) and 40.1\\%\n(image-to-text) cross-modal retrieval gains over existing methods while\nestablishing new state-of-the-art performance in semantic alignment tasks.", "AI": {"tldr": "\u8be5\u6587\u7ae0\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3aOS-HGAdapter\u7684\u65b0\u65b9\u6cd5\uff0c\u901a\u8fc7\u5229\u7528\u5927\u578b\u8bed\u8a00\u6a21\u578b\u7684\u5f00\u653e\u8bed\u4e49\u77e5\u8bc6\u548c\u8d85\u56fe\u9002\u914d\u5668\u6765\u89e3\u51b3\u6587\u672c-\u56fe\u50cf\u8de8\u6a21\u6001\u68c0\u7d22\u4e2d\u4fe1\u606f\u71b5\u4e0d\u5e73\u8861\u7684\u95ee\u9898\uff0c\u5e76\u5728Flickr30K\u548cMS-COCO\u6570\u636e\u96c6\u4e0a\u53d6\u5f97\u4e86\u6700\u5148\u8fdb\u7684\u6027\u80fd\u3002", "motivation": "\u6587\u672c\u548c\u56fe\u50cf\u4e4b\u95f4\u7684\u4fe1\u606f\u71b5\u5dee\u5f02\u5bfc\u81f4\u4e86\u591a\u6a21\u6001\u68c0\u7d22\u4e2d\u5e38\u89c1\u7684\u76f8\u4e92\u68c0\u7d22\u4e0d\u5e73\u8861\u95ee\u9898\u3002", "method": "1. \u8bbe\u8ba1\u4e86\u4e00\u79cd\u65b0\u7684\u63d0\u793a\u6a21\u677f\uff0c\u5229\u7528\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08LLM\uff09\u6765\u589e\u5f3a\u6587\u672c\u6a21\u6001\u7684\u591a\u4e49\u6027\u63cf\u8ff0\uff0c\u589e\u52a0\u6587\u672c\u6a21\u6001\u76f8\u5bf9\u4e8e\u89c6\u89c9\u6a21\u6001\u7684\u4fe1\u606f\u71b5\u30022. \u4f7f\u7528\u8d85\u56fe\u9002\u914d\u5668\u6784\u5efa\u6587\u672c\u548c\u56fe\u50cf\u6a21\u6001\u4e4b\u95f4\u7684\u591a\u8fb9\u8fde\u63a5\uff0c\u4ee5\u7ea0\u6b63\u540c\u4e49\u8bed\u4e49\u7684\u6b63\u8d1f\u5339\u914d\u9519\u8bef\uff0c\u5e76\u901a\u8fc7\u5c06\u964d\u7ef4\u6620\u5c04\u56de\u539f\u59cb\u7ef4\u5ea6\u6765\u51cf\u5c11\u5f00\u653e\u8bed\u4e49\u71b5\u5f15\u8d77\u7684\u566a\u58f0\u3002", "result": "\u5728Flickr30K\u548cMS-COCO\u57fa\u51c6\u6d4b\u8bd5\u4e2d\uff0cOS-HGAdapter\u5728\u6587\u672c\u5230\u56fe\u50cf\u68c0\u7d22\u65b9\u9762\u53d6\u5f97\u4e8616.8%\u7684\u63d0\u5347\uff0c\u5728\u56fe\u50cf\u5230\u6587\u672c\u68c0\u7d22\u65b9\u9762\u53d6\u5f97\u4e8640.1%\u7684\u63d0\u5347\uff0c\u8d85\u8d8a\u4e86\u73b0\u6709\u65b9\u6cd5\u3002", "conclusion": "OS-HGAdapter\u901a\u8fc7\u5f25\u8865\u6587\u672c\u548c\u56fe\u50cf\u4e4b\u95f4\u7684\u4fe1\u606f\u71b5\u5dee\u8ddd\uff0c\u663e\u8457\u63d0\u9ad8\u4e86\u8de8\u6a21\u6001\u68c0\u7d22\u7684\u6027\u80fd\uff0c\u5e76\u5728\u8bed\u4e49\u5bf9\u9f50\u4efb\u52a1\u4e2d\u8fbe\u5230\u4e86\u6700\u5148\u8fdb\u7684\u6c34\u5e73\u3002"}}
{"id": "2510.13151", "categories": ["cs.CV", "cs.GR", "I.2.10; I.4"], "pdf": "https://arxiv.org/pdf/2510.13151", "abs": "https://arxiv.org/abs/2510.13151", "authors": ["Lifeng Qiu Lin", "Henry Kam", "Qi Sun", "Kaan Ak\u015fit"], "title": "Foveation Improves Payload Capacity in Steganography", "comment": "SIGGRAPH Asia 2025 Posters Proceedings", "summary": "Steganography finds its use in visual medium such as providing metadata and\nwatermarking. With support of efficient latent representations and foveated\nrendering, we trained models that improve existing capacity limits from 100 to\n500 bits, while achieving better accuracy of up to 1 failure bit out of 2000,\nat 200K test bits. Finally, we achieve a comparable visual quality of 31.47 dB\nPSNR and 0.13 LPIPS, showing the effectiveness of novel perceptual design in\ncreating multi-modal latent representations in steganography.", "AI": {"tldr": "\u8fd9\u7bc7\u8bba\u6587\u4ecb\u7ecd\u4e86\u4e00\u79cd\u65b0\u7684\u9690\u5199\u6280\u672f\uff0c\u8be5\u6280\u672f\u901a\u8fc7\u4f7f\u7528\u9ad8\u6548\u7684\u6f5c\u5728\u8868\u793a\u548c\u51f9\u70b9\u6e32\u67d3\uff0c\u5c06\u73b0\u6709\u5bb9\u91cf\u9650\u5236\u4ece100\u6bd4\u7279\u63d0\u9ad8\u5230500\u6bd4\u7279\u3002", "motivation": "\u5728\u9690\u5199\u672f\u4e2d\uff0c\u5c06\u5143\u6570\u636e\u548c\u6c34\u5370\u5d4c\u5165\u5230\u89c6\u89c9\u5a92\u4f53\u4e2d\u662f\u4e00\u4e2a\u91cd\u8981\u7684\u5e94\u7528\u3002\u7136\u800c\uff0c\u73b0\u6709\u7684\u65b9\u6cd5\u5728\u5bb9\u91cf\u548c\u51c6\u786e\u6027\u65b9\u9762\u5b58\u5728\u5c40\u9650\u6027\u3002", "method": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u7684\u6a21\u578b\uff0c\u8be5\u6a21\u578b\u5229\u7528\u9ad8\u6548\u7684\u6f5c\u5728\u8868\u793a\u548c\u51f9\u70b9\u6e32\u67d3\u6765\u63d0\u9ad8\u9690\u5199\u5bb9\u91cf\u3002", "result": "\u8be5\u6a21\u578b\u5c06\u73b0\u6709\u5bb9\u91cf\u9650\u5236\u4ece100\u6bd4\u7279\u63d0\u9ad8\u5230500\u6bd4\u7279\uff0c\u5e76\u4e14\u572820\u4e07\u6d4b\u8bd5\u6bd4\u7279\u4e2d\uff0c\u6bcf2000\u6bd4\u7279\u53ea\u51fa\u73b01\u4e2a\u6545\u969c\u6bd4\u7279\uff0c\u51c6\u786e\u6027\u66f4\u9ad8\u3002\u6b64\u5916\uff0c\u8be5\u6a21\u578b\u5728\u89c6\u89c9\u8d28\u91cf\u65b9\u9762\u4e5f\u8fbe\u5230\u4e8631.47 dB PSNR\u548c0.13 LPIPS\uff0c\u5177\u6709\u53ef\u6bd4\u6027\u3002", "conclusion": "\u6240\u63d0\u51fa\u7684\u65b9\u6cd5\u901a\u8fc7\u5229\u7528\u65b0\u9896\u7684\u611f\u77e5\u8bbe\u8ba1\u6765\u521b\u5efa\u591a\u6a21\u6001\u6f5c\u5728\u8868\u793a\uff0c\u6709\u6548\u5730\u63d0\u9ad8\u4e86\u9690\u5199\u5bb9\u91cf\u3001\u51c6\u786e\u6027\u548c\u89c6\u89c9\u8d28\u91cf\u3002"}}
{"id": "2510.13691", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2510.13691", "abs": "https://arxiv.org/abs/2510.13691", "authors": ["Cecilia Di Florio", "Huimin Dong", "Antonino Rotolo"], "title": "A Modal Logic for Temporal and Jurisdictional Classifier Models", "comment": "18 pages, 2 figures. Extended version of a short paper accepted at\n  PRIMA 2025. This is the authors' version of the work. It is posted here for\n  your personal use", "summary": "Logic-based models can be used to build verification tools for machine\nlearning classifiers employed in the legal field. ML classifiers predict the\noutcomes of new cases based on previous ones, thereby performing a form of\ncase-based reasoning (CBR). In this paper, we introduce a modal logic of\nclassifiers designed to formally capture legal CBR. We incorporate principles\nfor resolving conflicts between precedents, by introducing into the logic the\ntemporal dimension of cases and the hierarchy of courts within the legal\nsystem.", "AI": {"tldr": "\u672c\u6587\u4ecb\u7ecd\u4e86\u4e00\u79cd\u6cd5\u5f8b\u5206\u7c7b\u5668\u7684\u6a21\u6001\u903b\u8f91\uff0c\u65e8\u5728\u5f62\u5f0f\u5316\u6cd5\u5f8b\u9886\u57df\u4e2d\u7684\u6848\u4f8b\u63a8\u7406\uff08CBR\uff09\u3002", "motivation": "\u673a\u5668\u5b66\u4e60\u5206\u7c7b\u5668\u901a\u8fc7\u57fa\u4e8e\u5148\u524d\u6848\u4f8b\u9884\u6d4b\u65b0\u6848\u4f8b\u7684\u7ed3\u679c\u6765\u6267\u884c\u6848\u4f8b\u63a8\u7406\uff08CBR\uff09\uff0c\u56e0\u6b64\u53ef\u4ee5\u4f7f\u7528\u57fa\u4e8e\u903b\u8f91\u7684\u6a21\u578b\u6765\u6784\u5efa\u673a\u5668\u5b66\u4e60\u5206\u7c7b\u5668\u7684\u9a8c\u8bc1\u5de5\u5177\uff0c\u4ee5\u7528\u4e8e\u6cd5\u5f8b\u9886\u57df\u3002", "method": "\u5c06\u6848\u4ef6\u7684\u65f6\u95f4\u7ef4\u5ea6\u548c\u6cd5\u5f8b\u4f53\u7cfb\u4e2d\u6cd5\u9662\u7684\u5c42\u7ea7\u7ed3\u6784\u5f15\u5165\u903b\u8f91\u4e2d\uff0c\u4ece\u800c\u89e3\u51b3\u4e86\u5224\u4f8b\u4e4b\u95f4\u7684\u51b2\u7a81\u3002", "result": "\u6ca1\u6709\u63d0\u53ca\u5177\u4f53\u7684\u5b9e\u9a8c\u7ed3\u679c\uff0c\u4f46\u8be5\u65b9\u6cd5\u65e8\u5728\u5f62\u5f0f\u5316\u6cd5\u5f8bCBR\uff0c\u5e76\u63d0\u4f9b\u4e86\u4e00\u79cd\u89e3\u51b3\u5224\u4f8b\u51b2\u7a81\u7684\u539f\u5219\u3002", "conclusion": "\u901a\u8fc7\u5f15\u5165\u6848\u4ef6\u7684\u65f6\u95f4\u7ef4\u5ea6\u548c\u6cd5\u9662\u5c42\u7ea7\u7ed3\u6784\uff0c\u672c\u6587\u63d0\u51fa\u7684\u5206\u7c7b\u5668\u6a21\u6001\u903b\u8f91\uff0c\u4e3a\u89e3\u51b3\u6cd5\u5f8bCBR\u4e2d\u7684\u5224\u4f8b\u51b2\u7a81\u63d0\u4f9b\u4e86\u4e00\u79cd\u5f62\u5f0f\u5316\u65b9\u6cd5\u3002"}}
{"id": "2510.13709", "categories": ["cs.AI", "cs.LG"], "pdf": "https://arxiv.org/pdf/2510.13709", "abs": "https://arxiv.org/abs/2510.13709", "authors": ["Evan Ellis", "Vivek Myers", "Jens Tuyls", "Sergey Levine", "Anca Dragan", "Benjamin Eysenbach"], "title": "Training LLM Agents to Empower Humans", "comment": null, "summary": "Assistive agents should not only take actions on behalf of a human, but also\nstep out of the way and cede control when there are important decisions to be\nmade. However, current methods for building assistive agents, whether via\nmimicking expert humans or via RL finetuning on an inferred reward, often\nencourage agents to complete tasks on their own rather than truly assisting the\nhuman attain their objectives. Additionally, these methods often require costly\nexplicit human feedback to provide a training signal. We propose a new approach\nto tuning assistive language models based on maximizing the human's\nempowerment, their ability to effect desired changes in the environment. Our\nempowerment-maximizing method, Empower, only requires offline text data,\nproviding a self-supervised method for fine-tuning language models to better\nassist humans. To study the efficacy of our approach, we conducted an 18-person\nuser study comparing our empowerment assistant with a strong baseline.\nParticipants preferred our assistant 78% of the time (p=0.015), with a 31%\nhigher acceptance rate and 38% fewer suggestions. Additionally, we introduce a\nnew environment for evaluating multi-turn code assistance using simulated\nhumans. Using this environment, we show that agents trained with Empower\nincrease the success rate of a simulated human programmer on challenging coding\nquestions by an average of 192% over an SFT baseline. With this empowerment\nobjective, we provide a framework for useful aligned AI agents at scale using\nonly offline data without the need for any additional human feedback or\nverifiable rewards.", "AI": {"tldr": "\u8be5\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u7684\u8f85\u52a9\u8bed\u8a00\u6a21\u578b\u5fae\u8c03\u65b9\u6cd5Empower\uff0c\u65e8\u5728\u6700\u5927\u5316\u4eba\u7c7b\u7684\u8d4b\u6743\uff0c\u4ece\u800c\u4f7f\u667a\u80fd\u4f53\u66f4\u597d\u5730\u534f\u52a9\u4eba\u7c7b\u5b9e\u73b0\u76ee\u6807\uff0c\u800c\u975e\u72ec\u7acb\u5b8c\u6210\u4efb\u52a1\u3002\u8be5\u65b9\u6cd5\u4ec5\u9700\u8981\u79bb\u7ebf\u6587\u672c\u6570\u636e\u8fdb\u884c\u81ea\u76d1\u7763\u8bad\u7ec3\uff0c\u5e76\u5728\u7528\u6237\u7814\u7a76\u548c\u6a21\u62df\u73af\u5883\u4e2d\u5747\u8868\u73b0\u51fa\u663e\u8457\u4f18\u4e8e\u57fa\u7ebf\u7684\u6548\u679c\u3002", "motivation": "\u5f53\u524d\u6784\u5efa\u8f85\u52a9\u667a\u80fd\u4f53\u7684\u65b9\u6cd5\uff08\u65e0\u8bba\u662f\u6a21\u4eff\u4e13\u5bb6\u4eba\u7c7b\u8fd8\u662f\u901a\u8fc7\u63a8\u65ad\u5956\u52b1\u8fdb\u884c\u5f3a\u5316\u5b66\u4e60\u5fae\u8c03\uff09\u901a\u5e38\u9f13\u52b1\u667a\u80fd\u4f53\u72ec\u7acb\u5b8c\u6210\u4efb\u52a1\uff0c\u800c\u975e\u771f\u6b63\u5730\u534f\u52a9\u4eba\u7c7b\u5b9e\u73b0\u76ee\u6807\u3002\u6b64\u5916\uff0c\u8fd9\u4e9b\u65b9\u6cd5\u901a\u5e38\u9700\u8981\u6602\u8d35\u4e14\u660e\u786e\u7684\u4eba\u7c7b\u53cd\u9988\u6765\u63d0\u4f9b\u8bad\u7ec3\u4fe1\u53f7\u3002", "method": "\u672c\u7814\u7a76\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u7684\u8f85\u52a9\u8bed\u8a00\u6a21\u578b\u5fae\u8c03\u65b9\u6cd5Empower\uff0c\u8be5\u65b9\u6cd5\u57fa\u4e8e\u6700\u5927\u5316\u4eba\u7c7b\u7684\u8d4b\u6743\uff0c\u5373\u4eba\u7c7b\u5728\u73af\u5883\u4e2d\u5b9e\u73b0\u9884\u671f\u6539\u53d8\u7684\u80fd\u529b\u3002Empower\u65b9\u6cd5\u4ec5\u9700\u8981\u79bb\u7ebf\u6587\u672c\u6570\u636e\u8fdb\u884c\u81ea\u76d1\u7763\u8bad\u7ec3\uff0c\u65e0\u9700\u989d\u5916\u7684\u4eba\u7c7b\u53cd\u9988\u6216\u53ef\u9a8c\u8bc1\u7684\u5956\u52b1\u3002", "result": "\u572818\u4eba\u7684\u7528\u6237\u7814\u7a76\u4e2d\uff0c\u53c2\u4e0e\u8005\u572878%\u7684\u65f6\u95f4\u91cc\u66f4\u559c\u6b22Empower\u8f85\u52a9\u7cfb\u7edf\uff08p=0.015\uff09\uff0c\u63a5\u53d7\u7387\u9ad8\u51fa31%\uff0c\u5efa\u8bae\u6570\u91cf\u51cf\u5c1138%\u3002\u6b64\u5916\uff0c\u5728\u4e00\u4e2a\u7528\u4e8e\u8bc4\u4f30\u591a\u8f6e\u4ee3\u7801\u8f85\u52a9\u7684\u65b0\u73af\u5883\u4e2d\uff0cEmpower\u8bad\u7ec3\u7684\u667a\u80fd\u4f53\u4f7f\u6a21\u62df\u7a0b\u5e8f\u5458\u5728\u5177\u6709\u6311\u6218\u6027\u7684\u7f16\u7801\u95ee\u9898\u4e0a\u7684\u6210\u529f\u7387\u6bd4SFT\u57fa\u7ebf\u5e73\u5747\u63d0\u9ad8\u4e86192%\u3002", "conclusion": "\u672c\u7814\u7a76\u63d0\u51fa\u4e86\u4e00\u79cd\u4ee5\u8d4b\u6743\u4e3a\u76ee\u6807\u7684\u6846\u67b6\uff0c\u4ec5\u5229\u7528\u79bb\u7ebf\u6570\u636e\u5373\u53ef\u6784\u5efa\u5927\u89c4\u6a21\u3001\u6709\u7528\u4e14\u5bf9\u9f50\u7684AI\u667a\u80fd\u4f53\uff0c\u65e0\u9700\u4efb\u4f55\u989d\u5916\u7684\u4eba\u7c7b\u53cd\u9988\u6216\u53ef\u9a8c\u8bc1\u7684\u5956\u52b1\u3002"}}
{"id": "2510.13186", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2510.13186", "abs": "https://arxiv.org/abs/2510.13186", "authors": ["Zhen Li", "Xibin Jin", "Guoliang Li", "Shuai Wang", "Miaowen Wen", "Huseyin Arslan", "Derrick Wing Kwan Ng", "Chengzhong Xu"], "title": "STT-GS: Sample-Then-Transmit Edge Gaussian Splatting with Joint Client Selection and Power Control", "comment": null, "summary": "Edge Gaussian splatting (EGS), which aggregates data from distributed clients\nand trains a global GS model at the edge server, is an emerging paradigm for\nscene reconstruction. Unlike traditional edge resource management methods that\nemphasize communication throughput or general-purpose learning performance, EGS\nexplicitly aims to maximize the GS qualities, rendering existing approaches\ninapplicable. To address this problem, this paper formulates a novel\nGS-oriented objective function that distinguishes the heterogeneous view\ncontributions of different clients. However, evaluating this function in turn\nrequires clients' images, leading to a causality dilemma. To this end, this\npaper further proposes a sample-then-transmit EGS (or STT-GS for short)\nstrategy, which first samples a subset of images as pilot data from each client\nfor loss prediction. Based on the first-stage evaluation, communication\nresources are then prioritized towards more valuable clients. To achieve\nefficient sampling, a feature-domain clustering (FDC) scheme is proposed to\nselect the most representative data and pilot transmission time minimization\n(PTTM) is adopted to reduce the pilot overhead.Subsequently, we develop a joint\nclient selection and power control (JCSPC) framework to maximize the\nGS-oriented function under communication resource constraints. Despite the\nnonconvexity of the problem, we propose a low-complexity efficient solution\nbased on the penalty alternating majorization minimization (PAMM) algorithm.\nExperiments unveil that the proposed scheme significantly outperforms existing\nbenchmarks on real-world datasets. It is found that the GS-oriented objective\ncan be accurately predicted with low sampling ratios (e.g.,10%), and our method\nachieves an excellent tradeoff between view contributions and communication\ncosts.", "AI": {"tldr": "\u8be5\u6587\u7ae0\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3a STT-GS \u7684\u65b0\u578b\u8fb9\u7f18\u9ad8\u65af\u6cfc\u6e85 (EGS) \u7b56\u7565\uff0c\u7528\u4e8e\u5728\u8fb9\u7f18\u670d\u52a1\u5668\u4e0a\u91cd\u5efa\u573a\u666f\uff0c\u65e8\u5728\u6700\u5927\u9650\u5ea6\u5730\u63d0\u9ad8\u9ad8\u65af\u6cfc\u6e85 (GS) \u8d28\u91cf\u3002", "motivation": "\u4f20\u7edf\u7684\u8fb9\u7f18\u8d44\u6e90\u7ba1\u7406\u65b9\u6cd5\u4e0d\u9002\u7528\u4e8e EGS\uff0c\u56e0\u4e3a\u5b83\u4eec\u4fa7\u91cd\u4e8e\u901a\u4fe1\u541e\u5410\u91cf\u6216\u901a\u7528\u5b66\u4e60\u6027\u80fd\uff0c\u800c EGS \u660e\u786e\u65e8\u5728\u6700\u5927\u5316 GS \u8d28\u91cf\u3002\u56e0\u6b64\uff0c\u9700\u8981\u4e00\u79cd\u65b0\u7684 GS \u5bfc\u5411\u7684\u76ee\u6807\u51fd\u6570\uff0c\u4f46\u8bc4\u4f30\u6b64\u51fd\u6570\u53c8\u9700\u8981\u5ba2\u6237\u7aef\u7684\u56fe\u50cf\uff0c\u5bfc\u81f4\u56e0\u679c\u5173\u7cfb\u56f0\u5883\u3002", "method": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u201c\u5148\u91c7\u6837\u540e\u4f20\u8f93\u201d\u7684 EGS (STT-GS) \u7b56\u7565\uff0c\u8be5\u7b56\u7565\u9996\u5148\u4ece\u6bcf\u4e2a\u5ba2\u6237\u7aef\u91c7\u6837\u4e00\u4e2a\u56fe\u50cf\u5b50\u96c6\u4f5c\u4e3a\u8bd5\u70b9\u6570\u636e\u8fdb\u884c\u635f\u5931\u9884\u6d4b\u3002\u57fa\u4e8e\u7b2c\u4e00\u9636\u6bb5\u8bc4\u4f30\uff0c\u901a\u4fe1\u8d44\u6e90\u4f18\u5148\u5206\u914d\u7ed9\u66f4\u6709\u4ef7\u503c\u7684\u5ba2\u6237\u7aef\u3002\u4e3a\u4e86\u5b9e\u73b0\u9ad8\u6548\u91c7\u6837\uff0c\u672c\u6587\u63d0\u51fa\u4e86\u7279\u5f81\u57df\u805a\u7c7b (FDC) \u65b9\u6848\u6765\u9009\u62e9\u6700\u5177\u4ee3\u8868\u6027\u7684\u6570\u636e\uff0c\u5e76\u91c7\u7528\u8bd5\u70b9\u4f20\u8f93\u65f6\u95f4\u6700\u5c0f\u5316 (PTTM) \u6765\u51cf\u5c11\u8bd5\u70b9\u5f00\u9500\u3002\u968f\u540e\uff0c\u5f00\u53d1\u4e86\u4e00\u4e2a\u8054\u5408\u5ba2\u6237\u7aef\u9009\u62e9\u548c\u529f\u7387\u63a7\u5236 (JCSPC) \u6846\u67b6\uff0c\u4ee5\u5728\u901a\u4fe1\u8d44\u6e90\u9650\u5236\u4e0b\u6700\u5927\u5316 GS \u5bfc\u5411\u51fd\u6570\u3002\u9488\u5bf9\u8be5\u975e\u51f8\u95ee\u9898\uff0c\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u60e9\u7f5a\u4ea4\u66ff\u4e3b\u5316\u6700\u5c0f\u5316 (PAMM) \u7b97\u6cd5\u7684\u4f4e\u590d\u6742\u5ea6\u9ad8\u6548\u89e3\u51b3\u65b9\u6848\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\uff0c\u6240\u63d0\u51fa\u7684\u65b9\u6848\u5728\u771f\u5b9e\u4e16\u754c\u6570\u636e\u96c6\u4e0a\u663e\u8457\u4f18\u4e8e\u73b0\u6709\u57fa\u51c6\u3002\u7814\u7a76\u53d1\u73b0\uff0cGS \u5bfc\u5411\u76ee\u6807\u53ef\u4ee5\u901a\u8fc7\u4f4e\u91c7\u6837\u7387\uff08\u4f8b\u5982 10%\uff09\u51c6\u786e\u9884\u6d4b\uff0c\u5e76\u4e14\u8be5\u65b9\u6cd5\u5728\u89c6\u56fe\u8d21\u732e\u548c\u901a\u4fe1\u6210\u672c\u4e4b\u95f4\u53d6\u5f97\u4e86\u51fa\u8272\u7684\u6743\u8861\u3002", "conclusion": "\u6240\u63d0\u51fa\u7684 STT-GS \u7b56\u7565\u6709\u6548\u89e3\u51b3\u4e86\u8fb9\u7f18\u9ad8\u65af\u6cfc\u6e85\u573a\u666f\u91cd\u5efa\u4e2d\u7684\u6311\u6218\uff0c\u901a\u8fc7\u521b\u65b0\u7684\u91c7\u6837\u548c\u8d44\u6e90\u7ba1\u7406\u673a\u5236\uff0c\u5728\u4fdd\u8bc1 GS \u8d28\u91cf\u7684\u540c\u65f6\uff0c\u4f18\u5316\u4e86\u901a\u4fe1\u6548\u7387\u3002"}}
{"id": "2510.13198", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2510.13198", "abs": "https://arxiv.org/abs/2510.13198", "authors": ["Rongtao Xu", "Jinzhou Lin", "Jialei Zhou", "Jiahua Dong", "Changwei Wang", "Ruisheng Wang", "Li Guo", "Shibiao Xu", "Xiaodan Liang"], "title": "Complementary Information Guided Occupancy Prediction via Multi-Level Representation Fusion", "comment": null, "summary": "Camera-based occupancy prediction is a mainstream approach for 3D perception\nin autonomous driving, aiming to infer complete 3D scene geometry and semantics\nfrom 2D images. Almost existing methods focus on improving performance through\nstructural modifications, such as lightweight backbones and complex cascaded\nframeworks, with good yet limited performance. Few studies explore from the\nperspective of representation fusion, leaving the rich diversity of features in\n2D images underutilized. Motivated by this, we propose \\textbf{CIGOcc, a\ntwo-stage occupancy prediction framework based on multi-level representation\nfusion. \\textbf{CIGOcc extracts segmentation, graphics, and depth features from\nan input image and introduces a deformable multi-level fusion mechanism to fuse\nthese three multi-level features. Additionally, CIGOcc incorporates knowledge\ndistilled from SAM to further enhance prediction accuracy. Without increasing\ntraining costs, CIGOcc achieves state-of-the-art performance on the\nSemanticKITTI benchmark. The code is provided in the supplementary material and\nwill be released https://github.com/VitaLemonTea1/CIGOcc", "AI": {"tldr": "CIGOcc\u662f\u4e00\u79cd\u57fa\u4e8e\u591a\u7ea7\u8868\u793a\u878d\u5408\u7684\u4e24\u9636\u6bb5\u5360\u7528\u9884\u6d4b\u6846\u67b6\uff0c\u5b83\u63d0\u53d6\u5206\u5272\u3001\u56fe\u5f62\u548c\u6df1\u5ea6\u7279\u5f81\u5e76\u878d\u5408\u5b83\u4eec\uff0c\u540c\u65f6\u7ed3\u5408\u4eceSAM\u4e2d\u63d0\u53d6\u7684\u77e5\u8bc6\uff0c\u5b9e\u73b0\u4e86\u6700\u5148\u8fdb\u7684\u6027\u80fd\u3002", "motivation": "\u73b0\u6709\u7684\u76f8\u673a\u5360\u7528\u9884\u6d4b\u65b9\u6cd5\u4e3b\u8981\u901a\u8fc7\u7ed3\u6784\u4fee\u6539\u6765\u63d0\u9ad8\u6027\u80fd\uff0c\u4f46\u6027\u80fd\u63d0\u5347\u6709\u9650\u3002\u5f88\u5c11\u6709\u7814\u7a76\u4ece\u8868\u793a\u878d\u5408\u7684\u89d2\u5ea6\u8fdb\u884c\u63a2\u7d22\uff0c\u5bfc\u81f42D\u56fe\u50cf\u4e2d\u4e30\u5bcc\u7684\u7279\u5f81\u591a\u6837\u6027\u672a\u88ab\u5145\u5206\u5229\u7528\u3002", "method": "CIGOcc\u662f\u4e00\u4e2a\u4e24\u9636\u6bb5\u5360\u7528\u9884\u6d4b\u6846\u67b6\u3002\u5b83\u4ece\u8f93\u5165\u56fe\u50cf\u4e2d\u63d0\u53d6\u5206\u5272\u3001\u56fe\u5f62\u548c\u6df1\u5ea6\u7279\u5f81\uff0c\u5e76\u5f15\u5165\u4e00\u4e2a\u53ef\u53d8\u5f62\u7684\u591a\u7ea7\u878d\u5408\u673a\u5236\u6765\u878d\u5408\u8fd9\u4e09\u79cd\u591a\u7ea7\u7279\u5f81\u3002\u6b64\u5916\uff0cCIGOcc\u8fd8\u7ed3\u5408\u4e86\u4eceSAM\u4e2d\u63d0\u53d6\u7684\u77e5\u8bc6\uff0c\u4ee5\u8fdb\u4e00\u6b65\u63d0\u9ad8\u9884\u6d4b\u7cbe\u5ea6\u3002", "result": "CIGOcc\u5728\u4e0d\u589e\u52a0\u8bad\u7ec3\u6210\u672c\u7684\u60c5\u51b5\u4e0b\uff0c\u5728SemanticKITTI\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u5b9e\u73b0\u4e86\u6700\u5148\u8fdb\u7684\u6027\u80fd\u3002", "conclusion": "CIGOcc\u901a\u8fc7\u591a\u7ea7\u8868\u793a\u878d\u5408\u548c\u77e5\u8bc6\u84b8\u998f\uff0c\u6709\u6548\u63d0\u5347\u4e86\u76f8\u673a\u5360\u7528\u9884\u6d4b\u7684\u51c6\u786e\u6027\uff0c\u4e3a\u81ea\u52a8\u9a7e\u9a76\u4e2d\u76843D\u611f\u77e5\u63d0\u4f9b\u4e86\u65b0\u7684\u601d\u8def\u3002"}}
{"id": "2510.13201", "categories": ["cs.CV", "cs.AI", "cs.DL", "cs.LG"], "pdf": "https://arxiv.org/pdf/2510.13201", "abs": "https://arxiv.org/abs/2510.13201", "authors": ["Jing Yang", "Qiyao Wei", "Jiaxin Pei"], "title": "Paper Copilot: Tracking the Evolution of Peer Review in AI Conferences", "comment": null, "summary": "The rapid growth of AI conferences is straining an already fragile\npeer-review system, leading to heavy reviewer workloads, expertise mismatches,\ninconsistent evaluation standards, superficial or templated reviews, and\nlimited accountability under compressed timelines. In response, conference\norganizers have introduced new policies and interventions to preserve review\nstandards. Yet these ad-hoc changes often create further concerns and confusion\nabout the review process, leaving how papers are ultimately accepted - and how\npractices evolve across years - largely opaque. We present Paper Copilot, a\nsystem that creates durable digital archives of peer reviews across a wide\nrange of computer-science venues, an open dataset that enables researchers to\nstudy peer review at scale, and a large-scale empirical analysis of ICLR\nreviews spanning multiple years. By releasing both the infrastructure and the\ndataset, Paper Copilot supports reproducible research on the evolution of peer\nreview. We hope these resources help the community track changes, diagnose\nfailure modes, and inform evidence-based improvements toward a more robust,\ntransparent, and reliable peer-review system.", "AI": {"tldr": "\u8fd9\u7bc7\u8bba\u6587\u4ecb\u7ecd\u4e86Paper Copilot\uff0c\u4e00\u4e2a\u65e8\u5728\u901a\u8fc7\u521b\u5efa\u6301\u4e45\u7684\u6570\u5b57\u540c\u884c\u8bc4\u5ba1\u6863\u6848\u548c\u5f00\u653e\u6570\u636e\u96c6\u6765\u6539\u8fdbAI\u4f1a\u8bae\u540c\u884c\u8bc4\u5ba1\u7cfb\u7edf\u7684\u9879\u76ee\u3002", "motivation": "AI\u4f1a\u8bae\u6570\u91cf\u7684\u5feb\u901f\u589e\u957f\u7ed9\u540c\u884c\u8bc4\u5ba1\u7cfb\u7edf\u5e26\u6765\u4e86\u5de8\u5927\u538b\u529b\uff0c\u5bfc\u81f4\u5ba1\u7a3f\u4eba\u5de5\u4f5c\u91cf\u5927\u3001\u4e13\u4e1a\u4e0d\u5bf9\u53e3\u3001\u8bc4\u4f30\u6807\u51c6\u4e0d\u4e00\u3001\u5ba1\u7a3f\u80a4\u6d45\u4ee5\u53ca\u95ee\u8d23\u5236\u6709\u9650\u3002\u73b0\u6709\u7684\u4e34\u65f6\u6027\u653f\u7b56\u548c\u5e72\u9884\u63aa\u65bd\u53cd\u800c\u5e26\u6765\u4e86\u66f4\u591a\u95ee\u9898\u3002", "method": "Paper Copilot\u901a\u8fc7\u4ee5\u4e0b\u65b9\u5f0f\u89e3\u51b3\u95ee\u9898\uff1a1. \u5efa\u7acb\u4e86\u8de8\u591a\u79cd\u8ba1\u7b97\u673a\u79d1\u5b66\u4f1a\u8bae\u7684\u540c\u884c\u8bc4\u5ba1\u6301\u4e45\u6570\u5b57\u6863\u6848\uff1b2. \u521b\u5efa\u4e86\u4e00\u4e2a\u5f00\u653e\u6570\u636e\u96c6\uff0c\u4f7f\u7814\u7a76\u4eba\u5458\u80fd\u591f\u5927\u89c4\u6a21\u7814\u7a76\u540c\u884c\u8bc4\u5ba1\uff1b3. \u5bf9ICLR\u4f1a\u8bae\u8de8\u5e74\u5ea6\u7684\u5ba1\u7a3f\u8fdb\u884c\u4e86\u5927\u89c4\u6a21\u5b9e\u8bc1\u5206\u6790\u3002", "result": "Paper Copilot\u53d1\u5e03\u4e86\u57fa\u7840\u8bbe\u65bd\u548c\u6570\u636e\u96c6\uff0c\u652f\u6301\u540c\u884c\u8bc4\u5ba1\u6f14\u53d8\u7684\u53ef\u590d\u73b0\u7814\u7a76\u3002", "conclusion": "Paper Coper\u5e0c\u671b\u8fd9\u4e9b\u8d44\u6e90\u80fd\u5e2e\u52a9\u793e\u533a\u8ddf\u8e2a\u53d8\u5316\u3001\u8bca\u65ad\u6545\u969c\u6a21\u5f0f\uff0c\u5e76\u4e3a\u6539\u8fdb\u540c\u884c\u8bc4\u5ba1\u7cfb\u7edf\u63d0\u4f9b\u5faa\u8bc1\u4f9d\u636e\uff0c\u4f7f\u5176\u66f4\u52a0\u5065\u58ee\u3001\u900f\u660e\u548c\u53ef\u9760\u3002"}}
{"id": "2510.13226", "categories": ["cs.CV", "cs.LG"], "pdf": "https://arxiv.org/pdf/2510.13226", "abs": "https://arxiv.org/abs/2510.13226", "authors": ["Hang-Cheng Dong", "Yibo Jiao", "Fupeng Wei", "Guodong Liu", "Dong Ye", "Bingguo Liu"], "title": "Sample-Centric Multi-Task Learning for Detection and Segmentation of Industrial Surface Defects", "comment": null, "summary": "Industrial surface defect inspection for sample-wise quality control (QC)\nmust simultaneously decide whether a given sample contains defects and localize\nthose defects spatially. In real production lines, extreme\nforeground-background imbalance, defect sparsity with a long-tailed scale\ndistribution, and low contrast are common. As a result, pixel-centric training\nand evaluation are easily dominated by large homogeneous regions, making it\ndifficult to drive models to attend to small or low-contrast defects-one of the\nmain bottlenecks for deployment. Empirically, existing models achieve strong\npixel-overlap metrics (e.g., mIoU) but exhibit insufficient stability at the\nsample level, especially for sparse or slender defects. The root cause is a\nmismatch between the optimization objective and the granularity of QC\ndecisions. To address this, we propose a sample-centric multi-task learning\nframework and evaluation suite. Built on a shared-encoder architecture, the\nmethod jointly learns sample-level defect classification and pixel-level mask\nlocalization. Sample-level supervision modulates the feature distribution and,\nat the gradient level, continually boosts recall for small and low-contrast\ndefects, while the segmentation branch preserves boundary and shape details to\nenhance per-sample decision stability and reduce misses. For evaluation, we\npropose decision-linked metrics, Seg_mIoU and Seg_Recall, which remove the bias\nof classical mIoU caused by empty or true-negative samples and tightly couple\nlocalization quality with sample-level decisions. Experiments on two benchmark\ndatasets demonstrate that our approach substantially improves the reliability\nof sample-level decisions and the completeness of defect localization.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u4ee5\u6837\u672c\u4e3a\u4e2d\u5fc3\u7684\u591a\u4efb\u52a1\u5b66\u4e60\u6846\u67b6\uff0c\u7528\u4e8e\u5de5\u4e1a\u8868\u9762\u7f3a\u9677\u68c0\u6d4b\uff0c\u901a\u8fc7\u6837\u672c\u7ea7\u76d1\u7763\u548c\u50cf\u7d20\u7ea7\u5b9a\u4f4d\uff0c\u89e3\u51b3\u4e86\u7f3a\u9677\u68c0\u6d4b\u4e2d\u524d\u666f-\u80cc\u666f\u4e0d\u5e73\u8861\u3001\u7f3a\u9677\u7a00\u758f\u6027\u548c\u4f4e\u5bf9\u6bd4\u5ea6\u7b49\u6311\u6218\uff0c\u663e\u8457\u63d0\u9ad8\u4e86\u6837\u672c\u7ea7\u51b3\u7b56\u7684\u53ef\u9760\u6027\u548c\u7f3a\u9677\u5b9a\u4f4d\u7684\u5b8c\u6574\u6027\u3002", "motivation": "\u5de5\u4e1a\u8868\u9762\u7f3a\u9677\u68c0\u6d4b\u5728\u5b9e\u9645\u751f\u4ea7\u7ebf\u4e2d\u9762\u4e34\u4e25\u5cfb\u6311\u6218\uff0c\u5305\u62ec\u524d\u666f-\u80cc\u666f\u6781\u7aef\u4e0d\u5e73\u8861\u3001\u7f3a\u9677\u7a00\u758f\u4e14\u5c3a\u5ea6\u5206\u5e03\u957f\u5c3e\u3001\u5bf9\u6bd4\u5ea6\u4f4e\u7b49\u95ee\u9898\u3002\u4f20\u7edf\u7684\u50cf\u7d20\u7ea7\u8bad\u7ec3\u548c\u8bc4\u4f30\u5bb9\u6613\u88ab\u5927\u7684\u540c\u8d28\u533a\u57df\u4e3b\u5bfc\uff0c\u5bfc\u81f4\u6a21\u578b\u96be\u4ee5\u5173\u6ce8\u5c0f\u578b\u6216\u4f4e\u5bf9\u6bd4\u5ea6\u7f3a\u9677\uff0c\u4ece\u800c\u5f71\u54cd\u90e8\u7f72\u3002\u73b0\u6709\u6a21\u578b\u5728\u50cf\u7d20\u91cd\u53e0\u5ea6\u6307\u6807\u4e0a\u8868\u73b0\u826f\u597d\uff0c\u4f46\u5728\u6837\u672c\u7ea7\u522b\uff0c\u7279\u522b\u5bf9\u4e8e\u7a00\u758f\u6216\u7ec6\u957f\u7f3a\u9677\uff0c\u7a33\u5b9a\u6027\u4e0d\u8db3\uff0c\u6839\u672c\u539f\u56e0\u662f\u4f18\u5316\u76ee\u6807\u4e0e\u8d28\u91cf\u63a7\u5236\u51b3\u7b56\u7c92\u5ea6\u4e0d\u5339\u914d\u3002", "method": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u4ee5\u6837\u672c\u4e3a\u4e2d\u5fc3\u7684\u591a\u4efb\u52a1\u5b66\u4e60\u6846\u67b6\u548c\u8bc4\u4f30\u5957\u4ef6\u3002\u8be5\u65b9\u6cd5\u57fa\u4e8e\u5171\u4eab\u7f16\u7801\u5668\u67b6\u6784\uff0c\u5171\u540c\u5b66\u4e60\u6837\u672c\u7ea7\u7f3a\u9677\u5206\u7c7b\u548c\u50cf\u7d20\u7ea7\u63a9\u6a21\u5b9a\u4f4d\u3002\u6837\u672c\u7ea7\u76d1\u7763\u8c03\u8282\u7279\u5f81\u5206\u5e03\uff0c\u5e76\u5728\u68af\u5ea6\u5c42\u9762\u6301\u7eed\u63d0\u5347\u5bf9\u5c0f\u578b\u548c\u4f4e\u5bf9\u6bd4\u5ea6\u7f3a\u9677\u7684\u53ec\u56de\u7387\uff0c\u800c\u5206\u5272\u5206\u652f\u5219\u4fdd\u7559\u8fb9\u754c\u548c\u5f62\u72b6\u7ec6\u8282\uff0c\u4ee5\u589e\u5f3a\u6bcf\u4e2a\u6837\u672c\u7684\u51b3\u7b56\u7a33\u5b9a\u6027\u5e76\u51cf\u5c11\u6f0f\u68c0\u3002\u672c\u6587\u8fd8\u63d0\u51fa\u4e86\u4e0e\u51b3\u7b56\u76f8\u5173\u8054\u7684\u8bc4\u4f30\u6307\u6807Seg_mIoU\u548cSeg_Recall\uff0c\u6d88\u9664\u4e86\u7ecf\u5178mIoU\u56e0\u7a7a\u6837\u672c\u6216\u771f\u9634\u6027\u6837\u672c\u5bfc\u81f4\u7684\u504f\u5dee\uff0c\u5e76\u5c06\u5b9a\u4f4d\u8d28\u91cf\u4e0e\u6837\u672c\u7ea7\u51b3\u7b56\u7d27\u5bc6\u7ed3\u5408\u3002", "result": "\u5728\u4e24\u4e2a\u57fa\u51c6\u6570\u636e\u96c6\u4e0a\u7684\u5b9e\u9a8c\u8868\u660e\uff0c\u672c\u6587\u63d0\u51fa\u7684\u65b9\u6cd5\u663e\u8457\u63d0\u9ad8\u4e86\u6837\u672c\u7ea7\u51b3\u7b56\u7684\u53ef\u9760\u6027\u548c\u7f3a\u9677\u5b9a\u4f4d\u7684\u5b8c\u6574\u6027\u3002", "conclusion": "\u672c\u6587\u901a\u8fc7\u63d0\u51fa\u4e00\u79cd\u6837\u672c\u4e2d\u5fc3\u7684\u7f3a\u9677\u68c0\u6d4b\u591a\u4efb\u52a1\u5b66\u4e60\u6846\u67b6\uff0c\u6709\u6548\u89e3\u51b3\u4e86\u5de5\u4e1a\u7f3a\u9677\u68c0\u6d4b\u4e2d\u5b58\u5728\u7684\u6311\u6218\uff0c\u63d0\u9ad8\u4e86\u6a21\u578b\u5728\u5b9e\u9645\u5e94\u7528\u4e2d\u7684\u6027\u80fd\u548c\u7a33\u5b9a\u6027\u3002"}}
{"id": "2510.13232", "categories": ["cs.CV", "cs.AI"], "pdf": "https://arxiv.org/pdf/2510.13232", "abs": "https://arxiv.org/abs/2510.13232", "authors": ["Inha Kang", "Youngsun Lim", "Seonho Lee", "Jiho Choi", "Junsuk Choe", "Hyunjung Shim"], "title": "What \"Not\" to Detect: Negation-Aware VLMs via Structured Reasoning and Token Merging", "comment": "38 pages", "summary": "State-of-the-art vision-language models (VLMs) suffer from a critical failure\nin understanding negation, often referred to as affirmative bias. This\nlimitation is particularly severe in described object detection (DOD) tasks. To\naddress this, we propose two primary contributions: (1) a new dataset pipeline\nand (2) a novel, lightweight adaptation recipe. First, we introduce CoVAND, a\ndataset constructed with a systematic chain-of-thought (CoT) and VQA-based\npipeline to generate high-quality, instance-grounded negation data. Second, we\npropose NegToMe, a novel text token merging module that directly tackles the\narchitectural cause of affirmative bias. NegToMe fundamentally addresses the\nstructural loss of negation cues in tokenization, grouping them with attributes\ninto coherent semantic phrases. It maintains correct polarity at the input\nlevel, enabling robust negation understanding even with limited data. For\ninstance, to prevent a model from treating the fragmented tokens \"not\" and\n\"girl\" as simply \"girl\", NegToMe binds them into a single token whose meaning\nis correctly distinguished from that of \"girl\" alone. This module is integrated\nwith a parameter-efficient and strategic LoRA fine-tuning approach. Our method\nsignificantly improves performance on challenging negation benchmarks with a\nlowered false positive rate, boosting NMS-AP by up to +10.8 points on OVDEval\nand demonstrating generalization to SoTA VLMs. This work marks a crucial step\nforward in addressing negation understanding for real-world detection\napplications.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u89e3\u51b3\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\u4e2d\u5426\u5b9a\u7406\u89e3\u7f3a\u9677\uff08\u5373\u80af\u5b9a\u504f\u5dee\uff09\u7684\u65b9\u6cd5\uff0c\u7279\u522b\u662f\u5728\u63cf\u8ff0\u5bf9\u8c61\u68c0\u6d4b\u4efb\u52a1\u4e2d\u3002\u901a\u8fc7\u5f15\u5165CoVAND\u6570\u636e\u96c6\u548cNegToMe\u6a21\u5757\uff0c\u663e\u8457\u63d0\u9ad8\u4e86\u6a21\u578b\u5728\u5426\u5b9a\u57fa\u51c6\u4e0a\u7684\u6027\u80fd\uff0c\u964d\u4f4e\u4e86\u8bef\u62a5\u7387\u3002", "motivation": "\u76ee\u524d\u6700\u5148\u8fdb\u7684\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\u5728\u7406\u89e3\u5426\u5b9a\u65b9\u9762\u5b58\u5728\u4e25\u91cd\u7f3a\u9677\uff0c\u5373\u80af\u5b9a\u504f\u5dee\uff0c\u5c24\u5176\u662f\u5728\u63cf\u8ff0\u5bf9\u8c61\u68c0\u6d4b\uff08DOD\uff09\u4efb\u52a1\u4e2d\u3002", "method": "1. \u63d0\u51fa\u4e86\u4e00\u4e2a\u540d\u4e3aCoVAND\u7684\u65b0\u6570\u636e\u96c6\uff0c\u8be5\u6570\u636e\u96c6\u901a\u8fc7\u7cfb\u7edf\u7684\u601d\u7ef4\u94fe\uff08CoT\uff09\u548c\u57fa\u4e8eVQA\u7684\u6d41\u7a0b\u751f\u6210\u9ad8\u8d28\u91cf\u3001\u5b9e\u4f8b\u63a5\u5730\u7684\u5426\u5b9a\u6570\u636e\u3002\n2. \u63d0\u51fa\u4e86\u4e00\u4e2a\u540d\u4e3aNegToMe\u7684\u8f7b\u91cf\u7ea7\u9002\u5e94\u65b9\u6cd5\uff0c\u8fd9\u662f\u4e00\u4e2a\u65b0\u9896\u7684\u6587\u672c\u6807\u8bb0\u5408\u5e76\u6a21\u5757\uff0c\u901a\u8fc7\u5c06\u5426\u5b9a\u63d0\u793a\u4e0e\u5c5e\u6027\u7ec4\u5408\u6210\u8fde\u8d2f\u7684\u8bed\u4e49\u77ed\u8bed\uff0c\u4ece\u6839\u672c\u4e0a\u89e3\u51b3\u6807\u8bb0\u5316\u8fc7\u7a0b\u4e2d\u5426\u5b9a\u63d0\u793a\u7684\u7ed3\u6784\u6027\u4e22\u5931\uff0c\u4ece\u800c\u5728\u8f93\u5165\u7ea7\u522b\u4fdd\u6301\u6b63\u786e\u7684\u6781\u6027\u3002\n3. NegToMe\u6a21\u5757\u4e0e\u53c2\u6570\u9ad8\u6548\u7684LoRA\u5fae\u8c03\u65b9\u6cd5\u76f8\u7ed3\u5408\u3002", "result": "\u8be5\u65b9\u6cd5\u663e\u8457\u63d0\u9ad8\u4e86\u6a21\u578b\u5728\u5177\u6709\u6311\u6218\u6027\u7684\u5426\u5b9a\u57fa\u51c6\u4e0a\u7684\u6027\u80fd\uff0c\u964d\u4f4e\u4e86\u8bef\u62a5\u7387\u3002\u5728OVDEval\u4e0a\uff0cNMS-AP\u63d0\u9ad8\u4e8610.8\u4e2a\u70b9\uff0c\u5e76\u5c55\u793a\u4e86\u5bf9SoTA VLM\u7684\u6cdb\u5316\u80fd\u529b\u3002", "conclusion": "\u8fd9\u9879\u5de5\u4f5c\u5728\u89e3\u51b3\u73b0\u5b9e\u4e16\u754c\u68c0\u6d4b\u5e94\u7528\u4e2d\u7684\u5426\u5b9a\u7406\u89e3\u65b9\u9762\u8fc8\u51fa\u4e86\u5173\u952e\u4e00\u6b65\u3002"}}
{"id": "2510.13235", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2510.13235", "abs": "https://arxiv.org/abs/2510.13235", "authors": ["Yukuan Zhang", "Jiarui Zhao", "Shangqing Nie", "Jin Kuang", "Shengsheng Wang"], "title": "EPIPTrack: Rethinking Prompt Modeling with Explicit and Implicit Prompts for Multi-Object Tracking", "comment": null, "summary": "Multimodal semantic cues, such as textual descriptions, have shown strong\npotential in enhancing target perception for tracking. However, existing\nmethods rely on static textual descriptions from large language models, which\nlack adaptability to real-time target state changes and prone to\nhallucinations. To address these challenges, we propose a unified multimodal\nvision-language tracking framework, named EPIPTrack, which leverages explicit\nand implicit prompts for dynamic target modeling and semantic alignment.\nSpecifically, explicit prompts transform spatial motion information into\nnatural language descriptions to provide spatiotemporal guidance. Implicit\nprompts combine pseudo-words with learnable descriptors to construct\nindividualized knowledge representations capturing appearance attributes. Both\nprompts undergo dynamic adjustment via the CLIP text encoder to respond to\nchanges in target state. Furthermore, we design a Discriminative Feature\nAugmentor to enhance visual and cross-modal representations. Extensive\nexperiments on MOT17, MOT20, and DanceTrack demonstrate that EPIPTrack\noutperforms existing trackers in diverse scenarios, exhibiting robust\nadaptability and superior performance.", "AI": {"tldr": "\u8be5\u8bba\u6587\u63d0\u51fa\u4e86EPIPTrack\uff0c\u4e00\u4e2a\u5229\u7528\u663e\u5f0f\u548c\u9690\u5f0f\u63d0\u793a\u8fdb\u884c\u52a8\u6001\u76ee\u6807\u5efa\u6a21\u548c\u8bed\u4e49\u5bf9\u9f50\u7684\u7edf\u4e00\u591a\u6a21\u6001\u89c6\u89c9-\u8bed\u8a00\u8ddf\u8e2a\u6846\u67b6\uff0c\u89e3\u51b3\u4e86\u73b0\u6709\u65b9\u6cd5\u4e2d\u9759\u6001\u6587\u672c\u63cf\u8ff0\u7f3a\u4e4f\u9002\u5e94\u6027\u548c\u6613\u4ea7\u751f\u5e7b\u89c9\u7684\u95ee\u9898\u3002", "motivation": "\u73b0\u6709\u8ddf\u8e2a\u65b9\u6cd5\u4f9d\u8d56\u4e8e\u5927\u578b\u8bed\u8a00\u6a21\u578b\u7684\u9759\u6001\u6587\u672c\u63cf\u8ff0\uff0c\u8fd9\u5bfc\u81f4\u5b83\u4eec\u65e0\u6cd5\u9002\u5e94\u76ee\u6807\u72b6\u6001\u7684\u5b9e\u65f6\u53d8\u5316\uff0c\u5e76\u4e14\u5bb9\u6613\u51fa\u73b0\u5e7b\u89c9\u3002", "method": "EPIPTrack\u901a\u8fc7\u663e\u5f0f\u63d0\u793a\u5c06\u7a7a\u95f4\u8fd0\u52a8\u4fe1\u606f\u8f6c\u5316\u4e3a\u81ea\u7136\u8bed\u8a00\u63cf\u8ff0\u4ee5\u63d0\u4f9b\u65f6\u7a7a\u6307\u5bfc\uff0c\u4ee5\u53ca\u901a\u8fc7\u9690\u5f0f\u63d0\u793a\u7ed3\u5408\u4f2a\u8bcd\u548c\u53ef\u5b66\u4e60\u63cf\u8ff0\u7b26\u6784\u5efa\u4e2a\u6027\u5316\u77e5\u8bc6\u8868\u793a\u6765\u6355\u6349\u5916\u89c2\u5c5e\u6027\u3002\u4e24\u79cd\u63d0\u793a\u90fd\u901a\u8fc7CLIP\u6587\u672c\u7f16\u7801\u5668\u8fdb\u884c\u52a8\u6001\u8c03\u6574\uff0c\u4ee5\u54cd\u5e94\u76ee\u6807\u72b6\u6001\u7684\u53d8\u5316\u3002\u6b64\u5916\uff0c\u8fd8\u8bbe\u8ba1\u4e86\u4e00\u4e2a\u5224\u522b\u6027\u7279\u5f81\u589e\u5f3a\u5668\u6765\u589e\u5f3a\u89c6\u89c9\u548c\u8de8\u6a21\u6001\u8868\u793a\u3002", "result": "\u5728MOT17\u3001MOT20\u548cDanceTrack\u4e0a\u7684\u5927\u91cf\u5b9e\u9a8c\u8868\u660e\uff0cEPIPTrack\u5728\u4e0d\u540c\u573a\u666f\u4e2d\u5747\u4f18\u4e8e\u73b0\u6709\u8ddf\u8e2a\u5668\uff0c\u8868\u73b0\u51fa\u5f3a\u5927\u7684\u9002\u5e94\u6027\u548c\u5353\u8d8a\u7684\u6027\u80fd\u3002", "conclusion": "EPIPTrack\u901a\u8fc7\u52a8\u6001\u8c03\u6574\u7684\u663e\u5f0f\u548c\u9690\u5f0f\u63d0\u793a\uff0c\u6709\u6548\u89e3\u51b3\u4e86\u591a\u6a21\u6001\u76ee\u6807\u8ddf\u8e2a\u4e2d\u9759\u6001\u6587\u672c\u63cf\u8ff0\u7684\u5c40\u9650\u6027\uff0c\u663e\u8457\u63d0\u5347\u4e86\u8ddf\u8e2a\u7684\u9c81\u68d2\u6027\u548c\u6027\u80fd\u3002"}}
{"id": "2510.13243", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2510.13243", "abs": "https://arxiv.org/abs/2510.13243", "authors": ["Francesco Barbato", "Matteo Caligiuri", "Pietro Zanuttigh"], "title": "FlyAwareV2: A Multimodal Cross-Domain UAV Dataset for Urban Scene Understanding", "comment": "20 pages, 7 figures, 10 tables, data and code available", "summary": "The development of computer vision algorithms for Unmanned Aerial Vehicle\n(UAV) applications in urban environments heavily relies on the availability of\nlarge-scale datasets with accurate annotations. However, collecting and\nannotating real-world UAV data is extremely challenging and costly. To address\nthis limitation, we present FlyAwareV2, a novel multimodal dataset encompassing\nboth real and synthetic UAV imagery tailored for urban scene understanding\ntasks. Building upon the recently introduced SynDrone and FlyAware datasets,\nFlyAwareV2 introduces several new key contributions: 1) Multimodal data (RGB,\ndepth, semantic labels) across diverse environmental conditions including\nvarying weather and daytime; 2) Depth maps for real samples computed via\nstate-of-the-art monocular depth estimation; 3) Benchmarks for RGB and\nmultimodal semantic segmentation on standard architectures; 4) Studies on\nsynthetic-to-real domain adaptation to assess the generalization capabilities\nof models trained on the synthetic data. With its rich set of annotations and\nenvironmental diversity, FlyAwareV2 provides a valuable resource for research\non UAV-based 3D urban scene understanding.", "AI": {"tldr": "FlyAwareV2\u662f\u4e00\u4e2a\u5305\u542b\u771f\u5b9e\u548c\u5408\u6210\u65e0\u4eba\u673a\u56fe\u50cf\u7684\u591a\u6a21\u6001\u6570\u636e\u96c6\uff0c\u65e8\u5728\u89e3\u51b3\u65e0\u4eba\u673a\u5728\u57ce\u5e02\u73af\u5883\u4e2d\u8ba1\u7b97\u673a\u89c6\u89c9\u7b97\u6cd5\u5f00\u53d1\u4e2d\u6570\u636e\u6536\u96c6\u548c\u6807\u6ce8\u7684\u6311\u6218\u3002", "motivation": "\u6536\u96c6\u548c\u6807\u6ce8\u771f\u5b9e\u7684\u65e0\u4eba\u673a\u6570\u636e\u65e2\u56f0\u96be\u53c8\u6602\u8d35\uff0c\u9650\u5236\u4e86\u57ce\u5e02\u73af\u5883\u4e2d\u65e0\u4eba\u673a\u8ba1\u7b97\u673a\u89c6\u89c9\u7b97\u6cd5\u7684\u5f00\u53d1\u3002", "method": "FlyAwareV2\u6570\u636e\u96c6\u5728SynDrone\u548cFlyAware\u6570\u636e\u96c6\u7684\u57fa\u7840\u4e0a\uff0c\u589e\u52a0\u4e86\u591a\u6a21\u6001\u6570\u636e\uff08RGB\u3001\u6df1\u5ea6\u3001\u8bed\u4e49\u6807\u7b7e\uff09\uff0c\u6db5\u76d6\u4e86\u4e0d\u540c\u7684\u5929\u6c14\u548c\u767d\u5929\u6761\u4ef6\u3002\u5b83\u8fd8\u901a\u8fc7\u6700\u5148\u8fdb\u7684\u5355\u76ee\u6df1\u5ea6\u4f30\u8ba1\u8ba1\u7b97\u4e86\u771f\u5b9e\u6837\u672c\u7684\u6df1\u5ea6\u56fe\uff0c\u5e76\u63d0\u4f9b\u4e86RGB\u548c\u591a\u6a21\u6001\u8bed\u4e49\u5206\u5272\u7684\u57fa\u51c6\u6d4b\u8bd5\uff0c\u4ee5\u53ca\u5408\u6210\u5230\u771f\u5b9e\u57df\u9002\u5e94\u7684\u7814\u7a76\u3002", "result": "FlyAwareV2\u6570\u636e\u96c6\u63d0\u4f9b\u4e86\u4e30\u5bcc\u7684\u6807\u6ce8\u548c\u73af\u5883\u591a\u6837\u6027\uff0c\u4e3a\u57fa\u4e8e\u65e0\u4eba\u673a\u76843D\u57ce\u5e02\u573a\u666f\u7406\u89e3\u7814\u7a76\u63d0\u4f9b\u4e86\u5b9d\u8d35\u7684\u8d44\u6e90\u3002", "conclusion": "FlyAwareV2\u901a\u8fc7\u63d0\u4f9b\u4e00\u4e2a\u591a\u6a21\u6001\u3001\u591a\u6837\u5316\u4e14\u5305\u542b\u5408\u6210\u548c\u771f\u5b9e\u6570\u636e\u7684\u65e0\u4eba\u673a\u6570\u636e\u96c6\uff0c\u6709\u6548\u89e3\u51b3\u4e86\u65e0\u4eba\u673a\u8ba1\u7b97\u673a\u89c6\u89c9\u9886\u57df\u6570\u636e\u7a00\u7f3a\u548c\u6807\u6ce8\u56f0\u96be\u7684\u95ee\u9898\uff0c\u6709\u671b\u63a8\u52a83D\u57ce\u5e02\u573a\u666f\u7406\u89e3\u7b97\u6cd5\u7684\u53d1\u5c55\u3002"}}
{"id": "2510.13245", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2510.13245", "abs": "https://arxiv.org/abs/2510.13245", "authors": ["Li Liang", "Bo Miao", "Xinyu Wang", "Naveed Akhtar", "Jordan Vice", "Ajmal Mian"], "title": "CymbaDiff: Structured Spatial Diffusion for Sketch-based 3D Semantic Urban Scene Generation", "comment": "Accepted by NeurIPS 2025", "summary": "Outdoor 3D semantic scene generation produces realistic and semantically rich\nenvironments for applications such as urban simulation and autonomous driving.\nHowever, advances in this direction are constrained by the absence of publicly\navailable, well-annotated datasets. We introduce SketchSem3D, the first\nlarge-scale benchmark for generating 3D outdoor semantic scenes from abstract\nfreehand sketches and pseudo-labeled annotations of satellite images.\nSketchSem3D includes two subsets, Sketch-based SemanticKITTI and Sketch-based\nKITTI-360 (containing LiDAR voxels along with their corresponding sketches and\nannotated satellite images), to enable standardized, rigorous, and diverse\nevaluations. We also propose Cylinder Mamba Diffusion (CymbaDiff) that\nsignificantly enhances spatial coherence in outdoor 3D scene generation.\nCymbaDiff imposes structured spatial ordering, explicitly captures cylindrical\ncontinuity and vertical hierarchy, and preserves both physical neighborhood\nrelationships and global context within the generated scenes. Extensive\nexperiments on SketchSem3D demonstrate that CymbaDiff achieves superior\nsemantic consistency, spatial realism, and cross-dataset generalization. The\ncode and dataset will be available at\nhttps://github.com/Lillian-research-hub/CymbaDiff", "AI": {"tldr": "SketchSem3D\u662f\u9996\u4e2a\u5927\u89c4\u6a21\u6237\u59163D\u8bed\u4e49\u573a\u666f\u751f\u6210\u57fa\u51c6\u6570\u636e\u96c6\uff0c\u5305\u542bSketch-based SemanticKITTI\u548cSketch-based KITTI-360\u4e24\u4e2a\u5b50\u96c6\u3002\u6211\u4eec\u8fd8\u63d0\u51fa\u4e86Cylinder Mamba Diffusion (CymbaDiff)\u6a21\u578b\uff0c\u8be5\u6a21\u578b\u663e\u8457\u589e\u5f3a\u4e86\u6237\u59163D\u573a\u666f\u751f\u6210\u7684\u7a7a\u95f4\u8fde\u8d2f\u6027\uff0c\u5e76\u5728SketchSem3D\u4e0a\u53d6\u5f97\u4e86\u4f18\u5f02\u7684\u8bed\u4e49\u4e00\u81f4\u6027\u3001\u7a7a\u95f4\u771f\u5b9e\u611f\u548c\u8de8\u6570\u636e\u96c6\u6cdb\u5316\u80fd\u529b\u3002", "motivation": "\u76ee\u524d\u7f3a\u4e4f\u516c\u5f00\u7684\u3001\u6709\u826f\u597d\u6807\u6ce8\u7684\u6237\u59163D\u8bed\u4e49\u573a\u666f\u751f\u6210\u6570\u636e\u96c6\uff0c\u8fd9\u9650\u5236\u4e86\u8be5\u9886\u57df\u7684\u53d1\u5c55\u3002", "method": "\u672c\u6587\u5f15\u5165\u4e86SketchSem3D\u6570\u636e\u96c6\uff0c\u5b83\u662f\u7b2c\u4e00\u4e2a\u7528\u4e8e\u4ece\u62bd\u8c61\u624b\u7ed8\u8349\u56fe\u548c\u536b\u661f\u56fe\u50cf\u4f2a\u6807\u6ce8\u751f\u62103D\u6237\u5916\u8bed\u4e49\u573a\u666f\u7684\u5927\u89c4\u6a21\u57fa\u51c6\u6570\u636e\u96c6\u3002\u8be5\u6570\u636e\u96c6\u5305\u542bSketch-based SemanticKITTI\u548cSketch-based KITTA-360\u4e24\u4e2a\u5b50\u96c6\u3002\u540c\u65f6\uff0c\u672c\u6587\u63d0\u51fa\u4e86Cylinder Mamba Diffusion (CymbaDiff)\u6a21\u578b\uff0c\u8be5\u6a21\u578b\u901a\u8fc7\u65bd\u52a0\u7ed3\u6784\u5316\u7684\u7a7a\u95f4\u6392\u5e8f\u3001\u660e\u786e\u6355\u83b7\u5706\u67f1\u5f62\u8fde\u7eed\u6027\u548c\u5782\u76f4\u5c42\u6b21\u7ed3\u6784\uff0c\u5e76\u4fdd\u7559\u751f\u6210\u7684\u573a\u666f\u4e2d\u7684\u7269\u7406\u90bb\u57df\u5173\u7cfb\u548c\u5168\u5c40\u4e0a\u4e0b\u6587\uff0c\u663e\u8457\u589e\u5f3a\u4e86\u6237\u59163D\u573a\u666f\u751f\u6210\u7684\u7a7a\u95f4\u8fde\u8d2f\u6027\u3002", "result": "CymbaDiff\u5728SketchSem3D\u6570\u636e\u96c6\u4e0a\u901a\u8fc7\u5e7f\u6cdb\u5b9e\u9a8c\u8bc1\u660e\uff0c\u5176\u5728\u8bed\u4e49\u4e00\u81f4\u6027\u3001\u7a7a\u95f4\u771f\u5b9e\u611f\u548c\u8de8\u6570\u636e\u96c6\u6cdb\u5316\u80fd\u529b\u65b9\u9762\u5747\u8868\u73b0\u51fa\u8272\u3002", "conclusion": "SketchSem3D\u6570\u636e\u96c6\u7684\u53d1\u5e03\u548cCymbaDiff\u6a21\u578b\u7684\u63d0\u51fa\uff0c\u4e3a\u6237\u59163D\u8bed\u4e49\u573a\u666f\u751f\u6210\u9886\u57df\u7684\u7814\u7a76\u63d0\u4f9b\u4e86\u65b0\u7684\u57fa\u51c6\u548cSOTA\u6a21\u578b\uff0c\u6709\u671b\u63a8\u52a8\u8be5\u9886\u57df\u7684\u53d1\u5c55\u3002"}}
{"id": "2510.13251", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2510.13251", "abs": "https://arxiv.org/abs/2510.13251", "authors": ["Minji Kim", "Taekyung Kim", "Bohyung Han"], "title": "Map the Flow: Revealing Hidden Pathways of Information in VideoLLMs", "comment": "23 pages, 28 figures, 8 tables", "summary": "Video Large Language Models (VideoLLMs) extend the capabilities of\nvision-language models to spatiotemporal inputs, enabling tasks such as video\nquestion answering (VideoQA). Despite recent advances in VideoLLMs, their\ninternal mechanisms on where and how they extract and propagate video and\ntextual information remain less explored. In this study, we investigate the\ninternal information flow of VideoLLMs using mechanistic interpretability\ntechniques. Our analysis reveals consistent patterns across diverse VideoQA\ntasks: (1) temporal reasoning in VideoLLMs initiates with active cross-frame\ninteractions in early-to-middle layers, (2) followed by progressive\nvideo-language integration in middle layers. This is facilitated by alignment\nbetween video representations and linguistic embeddings containing temporal\nconcepts. (3) Upon completion of this integration, the model is ready to\ngenerate correct answers in middle-to-late layers. (4) Based on our analysis,\nwe show that VideoLLMs can retain their VideoQA performance by selecting these\neffective information pathways while suppressing a substantial amount of\nattention edges, e.g., 58% in LLaVA-NeXT-7B-Video-FT. These findings provide a\nblueprint on how VideoLLMs perform temporal reasoning and offer practical\ninsights for improving model interpretability and downstream generalization.\nOur project page with the source code is available at\nhttps://map-the-flow.github.io", "AI": {"tldr": "\u8fd9\u7bc7\u8bba\u6587\u901a\u8fc7\u673a\u68b0\u53ef\u89e3\u91ca\u6027\u6280\u672f\u5206\u6790\u4e86VideoLLMs\u7684\u5185\u90e8\u4fe1\u606f\u6d41\uff0c\u63ed\u793a\u4e86\u6a21\u578b\u4e2d\u65f6\u5e8f\u63a8\u7406\u548c\u89c6\u9891-\u8bed\u8a00 T\u7edf\u4e00\u7684\u89c4\u5f8b\uff0c\u5e76\u63d0\u51fa\u4e86\u4e00\u79cd\u901a\u8fc7\u9009\u62e9\u6709\u6548\u4fe1\u606f\u8def\u5f84\u6765\u63d0\u9ad8\u6a21\u578b\u6027\u80fd\u548c\u53ef\u89e3\u91ca\u6027\u7684\u65b9\u6cd5\u3002", "motivation": "\u5c3d\u7ba1VideoLLMs\u5728\u89c6\u9891\u95ee\u7b54\uff08VideoQA\uff09\u7b49\u65b9\u9762\u53d6\u5f97\u4e86\u8fdb\u5c55\uff0c\u4f46\u5176\u5185\u90e8\u63d0\u53d6\u548c\u4f20\u64ad\u89c6\u9891\u4e0e\u6587\u672c\u4fe1\u606f\u7684\u673a\u5236\u5c1a\u4e0d\u6e05\u695a\u3002", "method": "\u672c\u7814\u7a76\u91c7\u7528\u673a\u68b0\u53ef\u89e3\u91ca\u6027\u6280\u672f\uff0c\u5206\u6790\u4e86VideoLLMs\u7684\u5185\u90e8\u4fe1\u606f\u6d41\u3002", "result": "\u5206\u6790\u63ed\u793a\u4e86VideoLLMs\u4e2d\u4e00\u81f4\u7684\u6a21\u5f0f\uff1a1\uff09\u65f6\u5e8f\u63a8\u7406\u59cb\u4e8e\u65e9\u671f\u5230\u4e2d\u671f\u5c42\u4e2d\u7684\u8de8\u5e27\u4ea4\u4e92\uff1b2\uff09\u968f\u540e\u4e2d\u671f\u5c42\u4e2d\u8fdb\u884c\u6e10\u8fdb\u5f0f\u89c6\u9891-\u8bed\u8a00\u6574\u5408\uff0c\u8fd9\u901a\u8fc7\u89c6\u9891\u8868\u793a\u548c\u5305\u542b\u65f6\u5e8f\u6982\u5ff5\u7684\u8bed\u8a00\u5d4c\u5165\u4e4b\u95f4\u7684\u5bf9\u9f50\u6765\u5b9e\u73b0\uff1b3\uff09\u6574\u5408\u5b8c\u6210\u540e\uff0c\u6a21\u578b\u5728\u4e2d\u540e\u671f\u5c42\u51c6\u5907\u751f\u6210\u6b63\u786e\u7b54\u6848\uff1b4\uff09\u901a\u8fc7\u9009\u62e9\u6709\u6548\u7684\u4fe1\u606f\u8def\u5f84\uff0cVideoLLMs\u53ef\u4ee5\u4fdd\u6301\u5176VideoQA\u6027\u80fd\uff0c\u540c\u65f6\u6291\u5236\u5927\u91cf\u7684\u6ce8\u610f\u529b\uff0c\u4f8b\u5982\u5728LLaVA-NeXT-7B-Video-FT\u4e2d\u6291\u5236\u4e8658%\u7684\u6ce8\u610f\u529b\u3002", "conclusion": "\u8fd9\u4e9b\u53d1\u73b0\u63ed\u793a\u4e86VideoLLMs\u6267\u884c\u65f6\u5e8f\u63a8\u7406\u7684\u84dd\u56fe\uff0c\u5e76\u4e3a\u63d0\u9ad8\u6a21\u578b\u53ef\u89e3\u91ca\u6027\u548c\u4e0b\u6e38\u6cdb\u5316\u80fd\u529b\u63d0\u4f9b\u4e86\u5b9e\u7528\u7684\u89c1\u89e3\u3002"}}
{"id": "2510.13253", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2510.13253", "abs": "https://arxiv.org/abs/2510.13253", "authors": ["Chunhao Lu", "Qiang Lu", "Meichen Dong", "Jake Luo"], "title": "End-to-End Multi-Modal Diffusion Mamba", "comment": "Accepted by ICCV 2025", "summary": "Current end-to-end multi-modal models utilize different encoders and decoders\nto process input and output information. This separation hinders the joint\nrepresentation learning of various modalities. To unify multi-modal processing,\nwe propose a novel architecture called MDM (Multi-modal Diffusion Mamba). MDM\nutilizes a Mamba-based multi-step selection diffusion model to progressively\ngenerate and refine modality-specific information through a unified variational\nautoencoder for both encoding and decoding. This innovative approach allows MDM\nto achieve superior performance when processing high-dimensional data,\nparticularly in generating high-resolution images and extended text sequences\nsimultaneously. Our evaluations in areas such as image generation, image\ncaptioning, visual question answering, text comprehension, and reasoning tasks\ndemonstrate that MDM significantly outperforms existing end-to-end models\n(MonoFormer, LlamaGen, and Chameleon etc.) and competes effectively with SOTA\nmodels like GPT-4V, Gemini Pro, and Mistral. Our results validate MDM's\neffectiveness in unifying multi-modal processes while maintaining computational\nefficiency, establishing a new direction for end-to-end multi-modal\narchitectures.", "AI": {"tldr": "MDM\u6a21\u578b\u901a\u8fc7\u7edf\u4e00\u7684VAE\u548cMamba\u9a71\u52a8\u7684\u591a\u6b65\u9009\u62e9\u6269\u6563\u6a21\u578b\uff0c\u5b9e\u73b0\u4e86\u591a\u6a21\u6001\u5904\u7406\u7684\u7edf\u4e00\uff0c\u5e76\u5728\u9ad8\u7ef4\u6570\u636e\u5904\u7406\u65b9\u9762\u5c55\u73b0\u51fa\u5353\u8d8a\u6027\u80fd\uff0c\u8d85\u8d8a\u4e86\u73b0\u6709\u7aef\u5230\u7aef\u6a21\u578b\uff0c\u5e76\u4e0eSOTA\u6a21\u578b\u7ade\u4e89\u3002", "motivation": "\u5f53\u524d\u7aef\u5230\u7aef\u591a\u6a21\u6001\u6a21\u578b\u4e2d\u7f16\u7801\u5668\u548c\u89e3\u7801\u5668\u5206\u79bb\uff0c\u963b\u788d\u4e86\u591a\u6a21\u6001\u8054\u5408\u8868\u793a\u5b66\u4e60\u3002", "method": "\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3aMDM\uff08\u591a\u6a21\u6001\u6269\u6563Mamba\uff09\u7684\u65b0\u578b\u67b6\u6784\u3002MDM\u5229\u7528\u57fa\u4e8eMamba\u7684\u591a\u6b65\u9009\u62e9\u6269\u6563\u6a21\u578b\uff0c\u901a\u8fc7\u7edf\u4e00\u7684\u53d8\u5206\u81ea\u52a8\u7f16\u7801\u5668\uff08VAE\uff09\u9010\u6b65\u751f\u6210\u548c\u7ec6\u5316\u7279\u5b9a\u6a21\u6001\u7684\u4fe1\u606f\uff0c\u8be5VAE\u540c\u65f6\u7528\u4e8e\u7f16\u7801\u548c\u89e3\u7801\u3002", "result": "MDM\u5728\u56fe\u50cf\u751f\u6210\u3001\u56fe\u50cf\u5b57\u5e55\u3001\u89c6\u89c9\u95ee\u7b54\u3001\u6587\u672c\u7406\u89e3\u548c\u63a8\u7406\u4efb\u52a1\u7b49\u9886\u57df\u663e\u8457\u4f18\u4e8e\u73b0\u6709\u7aef\u5230\u7aef\u6a21\u578b\uff08MonoFormer\u3001LlamaGen\u548cChameleon\u7b49\uff09\uff0c\u5e76\u80fd\u6709\u6548\u4e0eGPT-4V\u3001Gemini Pro\u548cMistral\u7b49SOTA\u6a21\u578b\u7ade\u4e89\u3002", "conclusion": "MDM\u5728\u7edf\u4e00\u591a\u6a21\u6001\u5904\u7406\u7684\u540c\u65f6\u4fdd\u6301\u4e86\u8ba1\u7b97\u6548\u7387\uff0c\u4e3a\u7aef\u5230\u7aef\u591a\u6a21\u6001\u67b6\u6784\u5f00\u8f9f\u4e86\u65b0\u65b9\u5411\u3002"}}
{"id": "2510.13315", "categories": ["cs.CV", "cs.AI"], "pdf": "https://arxiv.org/pdf/2510.13315", "abs": "https://arxiv.org/abs/2510.13315", "authors": ["Eun Woo Im", "Muhammad Kashif Ali", "Vivek Gupta"], "title": "Self-Augmented Visual Contrastive Decoding", "comment": null, "summary": "Large Vision-Language Models (LVLMs) have demonstrated remarkable multimodal\ncapabilities, but they inherit the tendency to hallucinate from their\nunderlying language models. While visual contrastive decoding has been proposed\nto mitigate this issue, existing methods often apply generic visual\naugmentations that disregard the specific context provided by the text query,\nlimiting their effectiveness. This study introduces a novel training-free\ndecoding strategy that addresses these limitations, featuring two key\ncontributions. First, a self-augmentation prompting strategy that leverages the\nintrinsic knowledge of the model to dynamically align semantics between the\nquery and the visual augmentation. Second, an adaptive thresholding algorithm\nthat adaptively adjusts next token candidate size based on the output sparsity,\nutilizing full information from the logit distribution. Extensive experiments\nacross four LVLMs and seven benchmarks demonstrate that the proposed decoding\nsignificantly enhances factual consistency compared to state-of-the-art\ndecoding methods. This work highlights the importance of integrating\nquery-dependent augmentation and entropy-aware decoding for improving effective\ngeneration of LVLMs.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u65e0\u9700\u8bad\u7ec3\u7684\u89e3\u7801\u7b56\u7565\uff0c\u901a\u8fc7\u81ea\u589e\u5f3a\u63d0\u793a\u548c\u81ea\u9002\u5e94\u9608\u503c\u7b97\u6cd5\uff0c\u663e\u8457\u63d0\u9ad8\u4e86\u5927\u578b\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\uff08LVLMs\uff09\u7684\u4e8b\u5b9e\u4e00\u81f4\u6027\uff0c\u89e3\u51b3\u4e86\u5e7b\u89c9\u95ee\u9898\u3002", "motivation": "\u5927\u578b\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\uff08LVLMs\uff09\u5b58\u5728\u5e7b\u89c9\u95ee\u9898\uff0c\u73b0\u6709\u65b9\u6cd5\u4f7f\u7528\u901a\u7528\u89c6\u89c9\u589e\u5f3a\uff0c\u672a\u80fd\u5145\u5206\u5229\u7528\u6587\u672c\u67e5\u8be2\u7684\u7279\u5b9a\u4e0a\u4e0b\u6587\uff0c\u5bfc\u81f4\u6548\u679c\u53d7\u9650\u3002", "method": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u65e0\u9700\u8bad\u7ec3\u7684\u89e3\u7801\u7b56\u7565\uff0c\u5305\u542b\u4e24\u9879\u5173\u952e\u8d21\u732e\uff1a1. \u81ea\u589e\u5f3a\u63d0\u793a\u7b56\u7565\uff1a\u5229\u7528\u6a21\u578b\u5185\u5728\u77e5\u8bc6\uff0c\u52a8\u6001\u5bf9\u9f50\u67e5\u8be2\u4e0e\u89c6\u89c9\u589e\u5f3a\u4e4b\u95f4\u7684\u8bed\u4e49\u30022. \u81ea\u9002\u5e94\u9608\u503c\u7b97\u6cd5\uff1a\u6839\u636e\u8f93\u51fa\u7a00\u758f\u6027\u81ea\u9002\u5e94\u8c03\u6574\u4e0b\u4e00\u4e2a\u5019\u9009token\u7684\u5927\u5c0f\uff0c\u5229\u7528logit\u5206\u5e03\u7684\u5168\u90e8\u4fe1\u606f\u3002", "result": "\u5728\u56db\u4e2aLVLMs\u548c\u4e03\u4e2a\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u8fdb\u884c\u4e86\u5e7f\u6cdb\u5b9e\u9a8c\uff0c\u7ed3\u679c\u8868\u660e\u6240\u63d0\u51fa\u7684\u89e3\u7801\u7b56\u7565\u6bd4\u6700\u5148\u8fdb\u7684\u89e3\u7801\u65b9\u6cd5\u663e\u8457\u589e\u5f3a\u4e86\u4e8b\u5b9e\u4e00\u81f4\u6027\u3002", "conclusion": "\u672c\u7814\u7a76\u5f3a\u8c03\u4e86\u6574\u5408\u4f9d\u8d56\u4e8e\u67e5\u8be2\u7684\u589e\u5f3a\u548c\u611f\u77e5\u71b5\u7684\u89e3\u7801\u5bf9\u4e8e\u63d0\u9ad8LVLMs\u6709\u6548\u751f\u6210\u7684\u91cd\u8981\u6027\u3002"}}
{"id": "2510.13282", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2510.13282", "abs": "https://arxiv.org/abs/2510.13282", "authors": ["JiaKui Hu", "Zhengjian Yao", "Lujia Jin", "Yinghao Chen", "Yanye Lu"], "title": "Universal Image Restoration Pre-training via Masked Degradation Classification", "comment": null, "summary": "This study introduces a Masked Degradation Classification Pre-Training method\n(MaskDCPT), designed to facilitate the classification of degradation types in\ninput images, leading to comprehensive image restoration pre-training. Unlike\nconventional pre-training methods, MaskDCPT uses the degradation type of the\nimage as an extremely weak supervision, while simultaneously leveraging the\nimage reconstruction to enhance performance and robustness. MaskDCPT includes\nan encoder and two decoders: the encoder extracts features from the masked\nlow-quality input image. The classification decoder uses these features to\nidentify the degradation type, whereas the reconstruction decoder aims to\nreconstruct a corresponding high-quality image. This design allows the\npre-training to benefit from both masked image modeling and contrastive\nlearning, resulting in a generalized representation suited for restoration\ntasks. Benefit from the straightforward yet potent MaskDCPT, the pre-trained\nencoder can be used to address universal image restoration and achieve\noutstanding performance. Implementing MaskDCPT significantly improves\nperformance for both convolution neural networks (CNNs) and Transformers, with\na minimum increase in PSNR of 3.77 dB in the 5D all-in-one restoration task and\na 34.8% reduction in PIQE compared to baseline in real-world degradation\nscenarios. It also emergences strong generalization to previously unseen\ndegradation types and levels. In addition, we curate and release the UIR-2.5M\ndataset, which includes 2.5 million paired restoration samples across 19\ndegradation types and over 200 degradation levels, incorporating both synthetic\nand real-world data. The dataset, source code, and models are available at\nhttps://github.com/MILab-PKU/MaskDCPT.", "AI": {"tldr": "\u8be5\u7814\u7a76\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3a MaskDCPT \u7684\u63a9\u7801\u9000\u5316\u5206\u7c7b\u9884\u8bad\u7ec3\u65b9\u6cd5\uff0c\u7528\u4e8e\u56fe\u50cf\u6062\u590d\u4efb\u52a1\u3002", "motivation": "\u4f20\u7edf\u7684\u9884\u8bad\u7ec3\u65b9\u6cd5\u5728\u56fe\u50cf\u6062\u590d\u65b9\u9762\u5b58\u5728\u5c40\u9650\u6027\uff0cMaskDCPT \u65e8\u5728\u901a\u8fc7\u7ed3\u5408\u9000\u5316\u7c7b\u578b\u5206\u7c7b\u548c\u56fe\u50cf\u91cd\u5efa\u6765\u63d0\u9ad8\u56fe\u50cf\u6062\u590d\u7684\u6027\u80fd\u548c\u9c81\u68d2\u6027\u3002", "method": "MaskDCPT \u5305\u542b\u4e00\u4e2a\u7f16\u7801\u5668\u548c\u4e24\u4e2a\u89e3\u7801\u5668\u3002\u7f16\u7801\u5668\u4ece\u63a9\u7801\u7684\u4f4e\u8d28\u91cf\u8f93\u5165\u56fe\u50cf\u4e2d\u63d0\u53d6\u7279\u5f81\uff0c\u5206\u7c7b\u89e3\u7801\u5668\u8bc6\u522b\u9000\u5316\u7c7b\u578b\uff0c\u800c\u91cd\u5efa\u89e3\u7801\u5668\u91cd\u5efa\u76f8\u5e94\u7684\u9ad8\u8d28\u91cf\u56fe\u50cf\u3002\u8be5\u65b9\u6cd5\u7ed3\u5408\u4e86\u63a9\u7801\u56fe\u50cf\u5efa\u6a21\u548c\u5bf9\u6bd4\u5b66\u4e60\uff0c\u4ee5\u83b7\u5f97\u901a\u7528\u7684\u56fe\u50cf\u6062\u590d\u8868\u793a\u3002", "result": "MaskDCPT \u663e\u8457\u6539\u5584\u4e86 CNN \u548c Transformer \u7684\u6027\u80fd\u3002\u5728 5D all-in-one \u6062\u590d\u4efb\u52a1\u4e2d\uff0cPSNR \u81f3\u5c11\u589e\u52a0\u4e86 3.77 dB\uff1b\u5728\u771f\u5b9e\u4e16\u754c\u9000\u5316\u573a\u666f\u4e2d\uff0cPIQE \u6bd4\u57fa\u7ebf\u964d\u4f4e\u4e86 34.8%\u3002\u5b83\u5bf9\u672a\u89c1\u8fc7\u7684\u9000\u5316\u7c7b\u578b\u548c\u6c34\u5e73\u4e5f\u8868\u73b0\u51fa\u5f3a\u5927\u7684\u6cdb\u5316\u80fd\u529b\u3002\u6b64\u5916\uff0c\u8fd8\u53d1\u5e03\u4e86\u5305\u542b 250 \u4e07\u4e2a\u914d\u5bf9\u6062\u590d\u6837\u672c\u7684 UIR-2.5M \u6570\u636e\u96c6\u3002", "conclusion": "MaskDCPT \u65b9\u6cd5\u901a\u8fc7\u7ed3\u5408\u5206\u7c7b\u548c\u91cd\u5efa\u4efb\u52a1\uff0c\u6709\u6548\u5730\u63d0\u5347\u4e86\u56fe\u50cf\u6062\u590d\u7684\u6027\u80fd\u548c\u6cdb\u5316\u80fd\u529b\u3002\u6240\u63d0\u51fa\u7684\u6570\u636e\u96c6\u4e5f\u4e3a\u672a\u6765\u7684\u7814\u7a76\u63d0\u4f9b\u4e86\u5b9d\u8d35\u7684\u8d44\u6e90\u3002"}}
{"id": "2510.13364", "categories": ["cs.CV", "cs.AI"], "pdf": "https://arxiv.org/pdf/2510.13364", "abs": "https://arxiv.org/abs/2510.13364", "authors": ["MingZe Tang", "Jubal Chandy Jacob"], "title": "Language as a Label: Zero-Shot Multimodal Classification of Everyday Postures under Data Scarcity", "comment": null, "summary": "Recent Vision-Language Models (VLMs) enable zero-shot classification by\naligning images and text in a shared space, a promising approach for\ndata-scarce conditions. However, the influence of prompt design on recognizing\nvisually similar categories, such as human postures, is not well understood.\nThis study investigates how prompt specificity affects the zero-shot\nclassification of sitting, standing, and walking/running on a small, 285-image\nCOCO-derived dataset. A suite of modern VLMs, including OpenCLIP, MetaCLIP 2,\nand SigLip, were evaluated using a three-tiered prompt design that\nsystematically increases linguistic detail. Our findings reveal a compelling,\ncounter-intuitive trend: for the highest-performing models (MetaCLIP 2 and\nOpenCLIP), the simplest, most basic prompts consistently achieve the best\nresults. Adding descriptive detail significantly degrades performance for\ninstance, MetaCLIP 2's multi-class accuracy drops from 68.8\\% to 55.1\\% a\nphenomenon we term \"prompt overfitting\". Conversely, the lower-performing\nSigLip model shows improved classification on ambiguous classes when given more\ndescriptive, body-cue-based prompts.", "AI": {"tldr": "\u8be5\u7814\u7a76\u63a2\u8ba8\u4e86\u5728\u96f6\u6837\u672c\u56fe\u50cf\u5206\u7c7b\u4e2d\uff0c\u63d0\u793a\uff08prompt\uff09\u8bbe\u8ba1\u5bf9\u8bc6\u522b\u89c6\u89c9\u76f8\u4f3c\u7c7b\u522b\uff08\u5982\u4eba\u7c7b\u59ff\u6001\uff09\u7684\u5f71\u54cd\u3002", "motivation": "\u63a2\u7a76\u63d0\u793a\u8bbe\u8ba1\u5982\u4f55\u5f71\u54cd\u89c6\u89c9\u76f8\u4f3c\u7c7b\u522b\u7684\u96f6\u6837\u672c\u5206\u7c7b\uff0c\u4f8b\u5982\u4eba\u7c7b\u59ff\u6001\u8bc6\u522b\u3002", "method": "\u4f7f\u7528\u4e09\u79cd\u4e0d\u540c\u8be6\u7ec6\u7a0b\u5ea6\u7684\u63d0\u793a\uff0c\u5728\u5305\u542b285\u5f20\u56fe\u50cf\u7684COCO\u6570\u636e\u5b50\u96c6\u4e0a\uff0c\u8bc4\u4f30\u4e86OpenCLIP\u3001MetaCLIP 2\u548cSigLip\u7b49\u73b0\u4ee3\u89c6\u89c9-\u8bed\u8a00\u6a21\u578b\uff08VLMs\uff09\u3002", "result": "\u5bf9\u4e8e\u9ad8\u6027\u80fd\u6a21\u578b\uff08MetaCLIP 2\u548cOpenCLIP\uff09\uff0c\u6700\u7b80\u5355\u3001\u6700\u57fa\u7840\u7684\u63d0\u793a\u8868\u73b0\u6700\u4f73\uff0c\u589e\u52a0\u63cf\u8ff0\u6027\u7ec6\u8282\u4f1a\u5bfc\u81f4\u6027\u80fd\u663e\u8457\u4e0b\u964d\uff08\u4f8b\u5982\uff0cMetaCLIP 2\u7684\u591a\u7c7b\u51c6\u786e\u7387\u4ece68.8%\u964d\u81f355.1%\uff09\uff0c\u8fd9\u79cd\u73b0\u8c61\u88ab\u79f0\u4e3a\u201c\u63d0\u793a\u8fc7\u62df\u5408\u201d\u3002\u800c\u6027\u80fd\u8f83\u4f4e\u7684SigLip\u6a21\u578b\u5728\u9762\u5bf9\u6a21\u68f1\u4e24\u53ef\u7684\u7c7b\u522b\u65f6\uff0c\u901a\u8fc7\u66f4\u5177\u63cf\u8ff0\u6027\u3001\u57fa\u4e8e\u8eab\u4f53\u7ebf\u7d22\u7684\u63d0\u793a\u53ef\u4ee5\u63d0\u9ad8\u5206\u7c7b\u6548\u679c\u3002", "conclusion": "\u5bf9\u4e8e\u9ad8\u6027\u80fd\u7684VLMs\uff0c\u5728\u96f6\u6837\u672c\u5206\u7c7b\u4e2d\uff0c\u7b80\u6d01\u7684\u63d0\u793a\u4f18\u4e8e\u8be6\u7ec6\u7684\u63d0\u793a\uff0c\u907f\u514d\u4e86\u201c\u63d0\u793a\u8fc7\u62df\u5408\u201d\u73b0\u8c61\u3002\u800c\u5bf9\u4e8e\u6027\u80fd\u8f83\u4f4e\u7684\u6a21\u578b\uff0c\u8be6\u7ec6\u7684\u63d0\u793a\u6709\u52a9\u4e8e\u63d0\u9ad8\u5206\u7c7b\u7cbe\u5ea6\u3002"}}
{"id": "2510.13307", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2510.13307", "abs": "https://arxiv.org/abs/2510.13307", "authors": ["Yang Li", "Aming Wu", "Zihao Zhang", "Yahong Han"], "title": "Novel Class Discovery for Point Cloud Segmentation via Joint Learning of Causal Representation and Reasoning", "comment": "Accepted by NeurIPS 2025", "summary": "In this paper, we focus on Novel Class Discovery for Point Cloud Segmentation\n(3D-NCD), aiming to learn a model that can segment unlabeled (novel) 3D classes\nusing only the supervision from labeled (base) 3D classes. The key to this task\nis to setup the exact correlations between the point representations and their\nbase class labels, as well as the representation correlations between the\npoints from base and novel classes. A coarse or statistical correlation\nlearning may lead to the confusion in novel class inference. lf we impose a\ncausal relationship as a strong correlated constraint upon the learning\nprocess, the essential point cloud representations that accurately correspond\nto the classes should be uncovered. To this end, we introduce a structural\ncausal model (SCM) to re-formalize the 3D-NCD problem and propose a new method,\ni.e., Joint Learning of Causal Representation and Reasoning. Specifically, we\nfirst analyze hidden confounders in the base class representations and the\ncausal relationships between the base and novel classes through SCM. We devise\na causal representation prototype that eliminates confounders to capture the\ncausal representations of base classes. A graph structure is then used to model\nthe causal relationships between the base classes' causal representation\nprototypes and the novel class prototypes, enabling causal reasoning from base\nto novel classes. Extensive experiments and visualization results on 3D and 2D\nNCD semantic segmentation demonstrate the superiorities of our method.", "AI": {"tldr": "\u8fd9\u7bc7\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u4e2a\u9488\u5bf9\u70b9\u4e91\u5206\u5272\uff083D-NCD\uff09\u7684\u65b0\u9896\u7c7b\u53d1\u73b0\u65b9\u6cd5\uff0c\u901a\u8fc7\u5f15\u5165\u7ed3\u6784\u56e0\u679c\u6a21\u578b\uff08SCM\uff09\u6765\u5b66\u4e60\u56e0\u679c\u8868\u793a\u548c\u63a8\u7406\uff0c\u4ece\u800c\u5b9e\u73b0\u4ece\u672a\u6807\u8bb0\u76843D\u7c7b\u4e2d\u5206\u5272\u51fa\u65b0\u76843D\u7c7b\u3002", "motivation": "\u4ee5\u5f80\u7684\u65b0\u9896\u7c7b\u53d1\u73b0\u65b9\u6cd5\u5728\u70b9\u4e91\u8868\u793a\u4e0e\u57fa\u7c7b\u6807\u7b7e\u4e4b\u95f4\u4ee5\u53ca\u57fa\u7c7b\u4e0e\u65b0\u7c7b\u70b9\u4e4b\u95f4\u7684\u8868\u793a\u76f8\u5173\u6027\u5b66\u4e60\u4e0a\u53ef\u80fd\u5b58\u5728\u4e0d\u8db3\uff0c\u5bfc\u81f4\u65b0\u7c7b\u63a8\u65ad\u6df7\u6dc6\u3002", "method": "\u672c\u6587\u5f15\u5165\u4e86\u4e00\u4e2a\u7ed3\u6784\u56e0\u679c\u6a21\u578b\uff08SCM\uff09\u6765\u91cd\u65b0\u5f62\u5f0f\u53163D-NCD\u95ee\u9898\uff0c\u5e76\u63d0\u51fa\u4e86\u201c\u56e0\u679c\u8868\u793a\u4e0e\u63a8\u7406\u8054\u5408\u5b66\u4e60\u201d\u7684\u65b0\u65b9\u6cd5\u3002\u9996\u5148\uff0c\u901a\u8fc7SCM\u5206\u6790\u57fa\u7c7b\u8868\u793a\u4e2d\u7684\u6f5c\u5728\u6df7\u6742\u56e0\u7d20\u4ee5\u53ca\u57fa\u7c7b\u548c\u65b0\u7c7b\u4e4b\u95f4\u7684\u56e0\u679c\u5173\u7cfb\u3002\u7136\u540e\uff0c\u8bbe\u8ba1\u4e86\u4e00\u79cd\u56e0\u679c\u8868\u793a\u539f\u578b\u6765\u6d88\u9664\u6df7\u6742\u56e0\u7d20\uff0c\u4ee5\u6355\u83b7\u57fa\u7c7b\u7684\u56e0\u679c\u8868\u793a\u3002\u6700\u540e\uff0c\u4f7f\u7528\u56fe\u7ed3\u6784\u6765\u5efa\u6a21\u57fa\u7c7b\u56e0\u679c\u8868\u793a\u539f\u578b\u4e0e\u65b0\u7c7b\u539f\u578b\u4e4b\u95f4\u7684\u56e0\u679c\u5173\u7cfb\uff0c\u4ece\u800c\u5b9e\u73b0\u4ece\u57fa\u7c7b\u5230\u65b0\u7c7b\u7684\u56e0\u679c\u63a8\u7406\u3002", "result": "\u57283D\u548c2D NCD\u8bed\u4e49\u5206\u5272\u4e0a\u7684\u5927\u91cf\u5b9e\u9a8c\u548c\u53ef\u89c6\u5316\u7ed3\u679c\u8868\u660e\uff0c\u8be5\u65b9\u6cd5\u5177\u6709\u4f18\u8d8a\u6027\u3002", "conclusion": "\u901a\u8fc7\u5f15\u5165\u7ed3\u6784\u56e0\u679c\u6a21\u578b\u548c\u56e0\u679c\u8868\u793a\u4e0e\u63a8\u7406\u8054\u5408\u5b66\u4e60\u65b9\u6cd5\uff0c\u53ef\u4ee5\u6709\u6548\u5730\u8fdb\u884c\u70b9\u4e91\u5206\u5272\u7684\u65b0\u9896\u7c7b\u53d1\u73b0\uff0c\u6d88\u9664\u6df7\u6742\u56e0\u7d20\u5e76\u5b9e\u73b0\u4ece\u57fa\u7c7b\u5230\u65b0\u7c7b\u7684\u56e0\u679c\u63a8\u7406\u3002"}}
{"id": "2510.13310", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2510.13310", "abs": "https://arxiv.org/abs/2510.13310", "authors": ["Jiankun Zhong", "Zitong Zhan", "Quankai Gao", "Ziyu Chen", "Haozhe Lou", "Jiageng Mao", "Ulrich Neumann", "Yue Wang"], "title": "InstantSfM: Fully Sparse and Parallel Structure-from-Motion", "comment": null, "summary": "Structure-from-Motion (SfM), a method that recovers camera poses and scene\ngeometry from uncalibrated images, is a central component in robotic\nreconstruction and simulation. Despite the state-of-the-art performance of\ntraditional SfM methods such as COLMAP and its follow-up work, GLOMAP, naive\nCPU-specialized implementations of bundle adjustment (BA) or global positioning\n(GP) introduce significant computational overhead when handling large-scale\nscenarios, leading to a trade-off between accuracy and speed in SfM. Moreover,\nthe blessing of efficient C++-based implementations in COLMAP and GLOMAP comes\nwith the curse of limited flexibility, as they lack support for various\nexternal optimization options. On the other hand, while deep learning based SfM\npipelines like VGGSfM and VGGT enable feed-forward 3D reconstruction, they are\nunable to scale to thousands of input views at once as GPU memory consumption\nincreases sharply as the number of input views grows. In this paper, we unleash\nthe full potential of GPU parallel computation to accelerate each critical\nstage of the standard SfM pipeline. Building upon recent advances in\nsparse-aware bundle adjustment optimization, our design extends these\ntechniques to accelerate both BA and GP within a unified global SfM framework.\nThrough extensive experiments on datasets of varying scales (e.g. 5000 images\nwhere VGGSfM and VGGT run out of memory), our method demonstrates up to about\n40 times speedup over COLMAP while achieving consistently comparable or even\nimproved reconstruction accuracy. Our project page can be found at\nhttps://cre185.github.io/InstantSfM/.", "AI": {"tldr": "\u8fd9\u7bc7\u8bba\u6587\u4ecb\u7ecd\u4e86\u4e00\u79cd\u540d\u4e3aInstantSfM\u7684\u65b0\u65b9\u6cd5\uff0c\u5b83\u5229\u7528GPU\u5e76\u884c\u8ba1\u7b97\u52a0\u901f\u4e86Structure-from-Motion\uff08SfM\uff09\u6d41\u7a0b\uff0c\u514b\u670d\u4e86\u4f20\u7edf\u65b9\u6cd5\u548c\u57fa\u4e8e\u6df1\u5ea6\u5b66\u4e60\u7684\u65b9\u6cd5\u5728\u5904\u7406\u5927\u89c4\u6a21\u6570\u636e\u96c6\u65f6\u7684\u5c40\u9650\u6027\u3002", "motivation": "\u4f20\u7edf\u7684SfM\u65b9\u6cd5\uff08\u5982COLMAP\u548cGLOMAP\uff09\u5728\u5904\u7406\u5927\u89c4\u6a21\u573a\u666f\u65f6\u5b58\u5728\u8ba1\u7b97\u5f00\u9500\u5927\u3001\u7075\u6d3b\u6027\u6709\u9650\u7684\u95ee\u9898\u3002\u53e6\u4e00\u65b9\u9762\uff0c\u57fa\u4e8e\u6df1\u5ea6\u5b66\u4e60\u7684SfM\u65b9\u6cd5\uff08\u5982VGGSfM\u548cVGGT\uff09\u5728\u8f93\u5165\u89c6\u56fe\u6570\u91cf\u589e\u52a0\u65f6\uff0cGPU\u5185\u5b58\u6d88\u8017\u6025\u5267\u589e\u52a0\uff0c\u65e0\u6cd5\u6269\u5c55\u5230\u6570\u5343\u4e2a\u89c6\u56fe\u3002", "method": "\u672c\u8bba\u6587\u901a\u8fc7\u5229\u7528GPU\u5e76\u884c\u8ba1\u7b97\u6765\u52a0\u901f\u6807\u51c6SfM\u6d41\u7a0b\u7684\u6bcf\u4e2a\u5173\u952e\u9636\u6bb5\uff0c\u7279\u522b\u662f\u57fa\u4e8e\u7a00\u758f\u611f\u77e5\u7684BA\u4f18\u5316\uff0c\u5c06\u8fd9\u4e9b\u6280\u672f\u6269\u5c55\u5230\u7edf\u4e00\u7684\u5168\u5c40SfM\u6846\u67b6\u4e2d\u7684BA\u548cGP\u52a0\u901f\u3002", "result": "\u5728\u4e0d\u540c\u89c4\u6a21\u7684\u6570\u636e\u96c6\uff08\u4f8b\u59825000\u5f20\u56fe\u50cf\uff0cVGGSfM\u548cVGGT\u4f1a\u5185\u5b58\u4e0d\u8db3\uff09\u4e0a\u8fdb\u884c\u7684\u5e7f\u6cdb\u5b9e\u9a8c\u8868\u660e\uff0c\u8be5\u65b9\u6cd5\u6bd4COLMAP\u63d0\u901f\u9ad8\u8fbe\u7ea640\u500d\uff0c\u540c\u65f6\u5b9e\u73b0\u4e86\u59cb\u7ec8\u76f8\u5f53\u751a\u81f3\u66f4\u9ad8\u7684\u91cd\u5efa\u7cbe\u5ea6\u3002", "conclusion": "InstantSfM\u901a\u8fc7\u5145\u5206\u5229\u7528GPU\u5e76\u884c\u8ba1\u7b97\uff0c\u5728\u4fdd\u6301\u9ad8\u7cbe\u5ea6\u7684\u540c\u65f6\u663e\u8457\u63d0\u5347\u4e86SfM\u5904\u7406\u5927\u89c4\u6a21\u6570\u636e\u7684\u901f\u5ea6\u548c\u6548\u7387\uff0c\u5f25\u8865\u4e86\u73b0\u6709SfM\u65b9\u6cd5\u7684\u4e0d\u8db3\u3002"}}
{"id": "2510.13557", "categories": ["cs.CV", "cs.AI"], "pdf": "https://arxiv.org/pdf/2510.13557", "abs": "https://arxiv.org/abs/2510.13557", "authors": ["David Freire-Obreg\u00f3n", "Jos\u00e9 Salas-C\u00e1ceres", "Javier Lorenzo-Navarro", "Oliverio J. Santana", "Daniel Hern\u00e1ndez-Sosa", "Modesto Castrill\u00f3n-Santana"], "title": "Modeling Cultural Bias in Facial Expression Recognition with Adaptive Agents", "comment": "Accepted for presentation at the International Symposium on Agentic\n  Artificial Intelligence Systems (AAIS 2025)", "summary": "Facial expression recognition (FER) must remain robust under both cultural\nvariation and perceptually degraded visual conditions, yet most existing\nevaluations assume homogeneous data and high-quality imagery. We introduce an\nagent-based, streaming benchmark that reveals how cross-cultural composition\nand progressive blurring interact to shape face recognition robustness. Each\nagent operates in a frozen CLIP feature space with a lightweight residual\nadapter trained online at sigma=0 and fixed during testing. Agents move and\ninteract on a 5x5 lattice, while the environment provides inputs with\nsigma-scheduled Gaussian blur. We examine monocultural populations\n(Western-only, Asian-only) and mixed environments with balanced (5/5) and\nimbalanced (8/2, 2/8) compositions, as well as different spatial contact\nstructures. Results show clear asymmetric degradation curves between cultural\ngroups: JAFFE (Asian) populations maintain higher performance at low blur but\nexhibit sharper drops at intermediate stages, whereas KDEF (Western)\npopulations degrade more uniformly. Mixed populations exhibit intermediate\npatterns, with balanced mixtures mitigating early degradation, but imbalanced\nsettings amplify majority-group weaknesses under high blur. These findings\nquantify how cultural composition and interaction structure influence the\nrobustness of FER as perceptual conditions deteriorate.", "AI": {"tldr": "\u8be5\u6587\u7ae0\u4ecb\u7ecd\u4e86\u4e00\u79cd\u57fa\u4e8eagent\u7684\u6d41\u5f0f\u57fa\u51c6\uff0c\u7528\u4e8e\u8bc4\u4f30\u8de8\u6587\u5316\u7ec4\u6210\u548c\u6e10\u8fdb\u6a21\u7cca\u5bf9\u4eba\u8138\u8bc6\u522b\u9c81\u68d2\u6027\u7684\u5f71\u54cd\u3002", "motivation": "\u73b0\u6709\u7684\u9762\u90e8\u8868\u60c5\u8bc6\u522b\uff08FER\uff09\u8bc4\u4f30\u901a\u5e38\u5047\u8bbe\u6570\u636e\u540c\u8d28\u4e14\u56fe\u50cf\u8d28\u91cf\u9ad8\uff0c\u4f46\u5728\u6587\u5316\u5dee\u5f02\u548c\u611f\u77e5\u9000\u5316\u7684\u89c6\u89c9\u6761\u4ef6\u4e0b\uff0cFER\u5fc5\u987b\u4fdd\u6301\u9c81\u68d2\u6027\u3002", "method": "\u6587\u7ae0\u5f15\u5165\u4e86\u4e00\u4e2a\u57fa\u4e8eagent\u7684\u6d41\u5f0f\u57fa\u51c6\u3002\u6bcf\u4e2aagent\u5728\u4e00\u4e2a\u51bb\u7ed3\u7684CLIP\u7279\u5f81\u7a7a\u95f4\u4e2d\u8fd0\u884c\uff0c\u5e76\u5e26\u6709\u4e00\u4e2a\u8f7b\u91cf\u7ea7\u7684\u6b8b\u5dee\u9002\u914d\u5668\uff0c\u8be5\u9002\u914d\u5668\u5728sigma=0\u65f6\u5728\u7ebf\u8bad\u7ec3\u5e76\u5728\u6d4b\u8bd5\u671f\u95f4\u56fa\u5b9a\u3002Agent\u57285x5\u7684\u7f51\u683c\u4e0a\u79fb\u52a8\u548c\u4ea4\u4e92\uff0c\u73af\u5883\u5219\u63d0\u4f9b\u5e26\u6709sigma\u8c03\u5ea6\u9ad8\u65af\u6a21\u7cca\u7684\u8f93\u5165\u3002\u7814\u7a76\u4e86\u5355\u6587\u5316\u7fa4\u4f53\uff08\u4ec5\u897f\u65b9\u3001\u4ec5\u4e9a\u6d32\uff09\u548c\u6df7\u5408\u73af\u5883\uff0c\u5305\u62ec\u5e73\u8861\uff085/5\uff09\u548c\u4e0d\u5e73\u8861\uff088/2\u30012/8\uff09\u7684\u7ec4\u6210\uff0c\u4ee5\u53ca\u4e0d\u540c\u7684\u7a7a\u95f4\u63a5\u89e6\u7ed3\u6784\u3002", "result": "\u7ed3\u679c\u663e\u793a\uff0c\u4e0d\u540c\u6587\u5316\u7fa4\u4f53\u4e4b\u95f4\u5b58\u5728\u660e\u663e\u4e0d\u5bf9\u79f0\u7684\u6027\u80fd\u4e0b\u964d\u66f2\u7ebf\uff1aJAFFE\uff08\u4e9a\u6d32\uff09\u4eba\u7fa4\u5728\u4f4e\u6a21\u7cca\u5ea6\u4e0b\u4fdd\u6301\u8f83\u9ad8\u6027\u80fd\uff0c\u4f46\u5728\u4e2d\u7b49\u6a21\u7cca\u9636\u6bb5\u8868\u73b0\u51fa\u66f4\u6025\u5267\u7684\u4e0b\u964d\uff1b\u800cKDEF\uff08\u897f\u65b9\uff09\u4eba\u7fa4\u7684\u4e0b\u964d\u5219\u66f4\u4e3a\u5747\u5300\u3002\u6df7\u5408\u4eba\u7fa4\u8868\u73b0\u51fa\u4e2d\u95f4\u6a21\u5f0f\uff0c\u5176\u4e2d\u5e73\u8861\u7684\u6df7\u5408\u7f13\u89e3\u4e86\u65e9\u671f\u6027\u80fd\u4e0b\u964d\uff0c\u4f46\u4e0d\u5e73\u8861\u7684\u8bbe\u7f6e\u5728\u9ad8\u6a21\u7cca\u5ea6\u4e0b\u5219\u653e\u5927\u4e86\u591a\u6570\u7fa4\u4f53\u7684\u5f31\u70b9\u3002", "conclusion": "\u8fd9\u4e9b\u53d1\u73b0\u91cf\u5316\u4e86\u6587\u5316\u7ec4\u6210\u548c\u4ea4\u4e92\u7ed3\u6784\u5982\u4f55\u5f71\u54cdFER\u5728\u611f\u77e5\u6761\u4ef6\u6076\u5316\u65f6\u7684\u9c81\u68d2\u6027\u3002"}}
{"id": "2510.13316", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2510.13316", "abs": "https://arxiv.org/abs/2510.13316", "authors": ["Fitim Abdullahu", "Helmut Grabner"], "title": "Visual Interestingness Decoded: How GPT-4o Mirrors Human Interests", "comment": "ICCV 2025", "summary": "Our daily life is highly influenced by what we consume and see. Attracting\nand holding one's attention -- the definition of (visual) interestingness -- is\nessential. The rise of Large Multimodal Models (LMMs) trained on large-scale\nvisual and textual data has demonstrated impressive capabilities. We explore\nthese models' potential to understand to what extent the concepts of visual\ninterestingness are captured and examine the alignment between human\nassessments and GPT-4o's, a leading LMM, predictions through comparative\nanalysis. Our studies reveal partial alignment between humans and GPT-4o. It\nalready captures the concept as best compared to state-of-the-art methods.\nHence, this allows for the effective labeling of image pairs according to their\n(commonly) interestingness, which are used as training data to distill the\nknowledge into a learning-to-rank model. The insights pave the way for a deeper\nunderstanding of human interest.", "AI": {"tldr": "\u672c\u6587\u63a2\u8ba8\u4e86\u5927\u578b\u591a\u6a21\u6001\u6a21\u578b\uff08LMMs\uff09\u5728\u7406\u89e3\u89c6\u89c9\u8da3\u5473\u6027\u65b9\u9762\u7684\u6f5c\u529b\uff0c\u53d1\u73b0GPT-4o\u4e0e\u4eba\u7c7b\u8bc4\u4f30\u5b58\u5728\u90e8\u5206\u4e00\u81f4\u6027\uff0c\u5e76\u4f18\u4e8e\u73b0\u6709\u6700\u5148\u8fdb\u65b9\u6cd5\u3002", "motivation": "\u63a2\u7d22\u5927\u578b\u591a\u6a21\u6001\u6a21\u578b\uff08LMMs\uff09\u662f\u5426\u80fd\u591f\u7406\u89e3\u89c6\u89c9\u8da3\u5473\u6027\uff0c\u5e76\u8bc4\u4f30\u5176\u4e0e\u4eba\u7c7b\u8bc4\u4f30\u7684\u4e00\u81f4\u6027\u3002", "method": "\u901a\u8fc7\u6bd4\u8f83\u5206\u6790\uff0c\u5c06GPT-4o\uff08\u4e00\u79cd\u9886\u5148\u7684LMM\uff09\u7684\u9884\u6d4b\u4e0e\u4eba\u7c7b\u8bc4\u4f30\u8fdb\u884c\u5bf9\u6bd4\uff0c\u4ee5\u4e86\u89e3\u89c6\u89c9\u8da3\u5473\u6027\u6982\u5ff5\u5728\u6a21\u578b\u4e2d\u88ab\u6355\u83b7\u7684\u7a0b\u5ea6\u3002\u5229\u7528GPT-4o\u751f\u6210\u7684\uff08\u5171\u540c\uff09\u8da3\u5473\u6027\u6807\u7b7e\u4f5c\u4e3a\u8bad\u7ec3\u6570\u636e\uff0c\u84b8\u998f\u77e5\u8bc6\u5230\u5b66\u4e60\u6392\u5e8f\u6a21\u578b\u4e2d\u3002", "result": "\u7814\u7a76\u663e\u793aGPT-4o\u4e0e\u4eba\u7c7b\u8bc4\u4f30\u4e4b\u95f4\u5b58\u5728\u90e8\u5206\u4e00\u81f4\u6027\u3002\u4e0e\u6700\u5148\u8fdb\u7684\u65b9\u6cd5\u76f8\u6bd4\uff0cGPT-4o\u5df2\u7ecf\u80fd\u591f\u5f88\u597d\u5730\u6355\u6349\u89c6\u89c9\u8da3\u5473\u6027\u6982\u5ff5\u3002", "conclusion": "GPT-4o\u5728\u7406\u89e3\u89c6\u89c9\u8da3\u5473\u6027\u65b9\u9762\u5c55\u73b0\u51fa\u5f3a\u5927\u6f5c\u529b\uff0c\u5e76\u4e3a\u6df1\u5165\u7406\u89e3\u4eba\u7c7b\u5174\u8da3\u94fa\u5e73\u4e86\u9053\u8def\u3002\u5176\u80fd\u591f\u6709\u6548\u5730\u5bf9\u56fe\u50cf\u5bf9\u8fdb\u884c\u8da3\u5473\u6027\u6807\u6ce8\uff0c\u53ef\u7528\u4e8e\u8bad\u7ec3\u5b66\u4e60\u6392\u5e8f\u6a21\u578b\u3002"}}
{"id": "2510.13702", "categories": ["cs.CV", "cs.AI"], "pdf": "https://arxiv.org/pdf/2510.13702", "abs": "https://arxiv.org/abs/2510.13702", "authors": ["Minjung Shin", "Hyunin Cho", "Sooyeon Go", "Jin-Hwa Kim", "Youngjung Uh"], "title": "MVCustom: Multi-View Customized Diffusion via Geometric Latent Rendering and Completion", "comment": "Project page: https://minjung-s.github.io/mvcustom", "summary": "Multi-view generation with camera pose control and prompt-based customization\nare both essential elements for achieving controllable generative models.\nHowever, existing multi-view generation models do not support customization\nwith geometric consistency, whereas customization models lack explicit\nviewpoint control, making them challenging to unify. Motivated by these gaps,\nwe introduce a novel task, multi-view customization, which aims to jointly\nachieve multi-view camera pose control and customization. Due to the scarcity\nof training data in customization, existing multi-view generation models, which\ninherently rely on large-scale datasets, struggle to generalize to diverse\nprompts. To address this, we propose MVCustom, a novel diffusion-based\nframework explicitly designed to achieve both multi-view consistency and\ncustomization fidelity. In the training stage, MVCustom learns the subject's\nidentity and geometry using a feature-field representation, incorporating the\ntext-to-video diffusion backbone enhanced with dense spatio-temporal attention,\nwhich leverages temporal coherence for multi-view consistency. In the inference\nstage, we introduce two novel techniques: depth-aware feature rendering\nexplicitly enforces geometric consistency, and consistent-aware latent\ncompletion ensures accurate perspective alignment of the customized subject and\nsurrounding backgrounds. Extensive experiments demonstrate that MVCustom is the\nonly framework that simultaneously achieves faithful multi-view generation and\ncustomization.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3aMVCustom\u7684\u65b0\u578b\u6269\u6563\u6a21\u578b\u6846\u67b6\uff0c\u65e8\u5728\u89e3\u51b3\u591a\u89c6\u89d2\u751f\u6210\u4e2d\u76f8\u673a\u4f4d\u59ff\u63a7\u5236\u548c\u57fa\u4e8e\u63d0\u793a\u8bcd\u5b9a\u5236\u7684\u7edf\u4e00\u95ee\u9898\u3002MVCustom\u901a\u8fc7\u7279\u5f81\u573a\u8868\u793a\u5b66\u4e60\u4e3b\u4f53\u8eab\u4efd\u548c\u51e0\u4f55\uff0c\u5e76\u5728\u63a8\u7406\u9636\u6bb5\u5f15\u5165\u6df1\u5ea6\u611f\u77e5\u7279\u5f81\u6e32\u67d3\u548c\u4e00\u81f4\u6027\u611f\u77e5\u6f5c\u5728\u8865\u5168\u6280\u672f\uff0c\u4ee5\u5b9e\u73b0\u591a\u89c6\u89d2\u4e00\u81f4\u6027\u548c\u5b9a\u5236\u5316\u4fdd\u771f\u3002", "motivation": "\u73b0\u6709\u7684\u591a\u89c6\u89d2\u751f\u6210\u6a21\u578b\u4e0d\u652f\u6301\u51e0\u4f55\u4e00\u81f4\u6027\u7684\u5b9a\u5236\uff0c\u800c\u5b9a\u5236\u6a21\u578b\u53c8\u7f3a\u4e4f\u660e\u786e\u7684\u89c6\u89d2\u63a7\u5236\uff0c\u96be\u4ee5\u7edf\u4e00\u3002", "method": "MVCustom\u662f\u4e00\u4e2a\u57fa\u4e8e\u6269\u6563\u7684\u6846\u67b6\uff0c\u5728\u8bad\u7ec3\u9636\u6bb5\u4f7f\u7528\u7279\u5f81\u573a\u8868\u793a\u548c\u5bc6\u96c6\u65f6\u7a7a\u6ce8\u610f\u529b\u589e\u5f3a\u7684\u6587\u672c\u5230\u89c6\u9891\u6269\u6563\u9aa8\u5e72\u6765\u5b66\u4e60\u4e3b\u4f53\u7684\u8eab\u4efd\u548c\u51e0\u4f55\u3002\u5728\u63a8\u7406\u9636\u6bb5\uff0c\u5f15\u5165\u4e86\u6df1\u5ea6\u611f\u77e5\u7279\u5f81\u6e32\u67d3\u548c\u4e00\u81f4\u6027\u611f\u77e5\u6f5c\u5728\u8865\u5168\u4e24\u79cd\u65b0\u9896\u6280\u672f\u3002", "result": "MVCustom\u662f\u552f\u4e00\u4e00\u4e2a\u540c\u65f6\u5b9e\u73b0\u5fe0\u5b9e\u591a\u89c6\u89d2\u751f\u6210\u548c\u5b9a\u5236\u7684\u6846\u67b6\u3002", "conclusion": "MVCustom\u6846\u67b6\u6709\u6548\u5730\u7ed3\u5408\u4e86\u591a\u89c6\u89d2\u76f8\u673a\u4f4d\u59ff\u63a7\u5236\u548c\u5b9a\u5236\u5316\u529f\u80fd\uff0c\u586b\u8865\u4e86\u73b0\u6709\u6280\u672f\u7684\u7a7a\u767d\uff0c\u4e3a\u53ef\u63a7\u751f\u6210\u6a21\u578b\u63d0\u4f9b\u4e86\u65b0\u7684\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2510.13331", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2510.13331", "abs": "https://arxiv.org/abs/2510.13331", "authors": ["Hong-Kai Zheng", "Piji Li"], "title": "Group-Wise Optimization for Self-Extensible Codebooks in Vector Quantized Models", "comment": null, "summary": "Vector Quantized Variational Autoencoders (VQ-VAEs) leverage self-supervised\nlearning through reconstruction tasks to represent continuous vectors using the\nclosest vectors in a codebook. However, issues such as codebook collapse\npersist in the VQ model. To address these issues, existing approaches employ\nimplicit static codebooks or jointly optimize the entire codebook, but these\nmethods constrain the codebook's learning capability, leading to reduced\nreconstruction quality. In this paper, we propose Group-VQ, which performs\ngroup-wise optimization on the codebook. Each group is optimized independently,\nwith joint optimization performed within groups. This approach improves the\ntrade-off between codebook utilization and reconstruction performance.\nAdditionally, we introduce a training-free codebook resampling method, allowing\npost-training adjustment of the codebook size. In image reconstruction\nexperiments under various settings, Group-VQ demonstrates improved performance\non reconstruction metrics. And the post-training codebook sampling method\nachieves the desired flexibility in adjusting the codebook size.", "AI": {"tldr": "\u672c\u6587\u63d0\u51faGroup-VQ\uff0c\u4e00\u79cd\u5bf9VQ-VAE\u7801\u672c\u8fdb\u884c\u7ec4\u4f18\u5316\u548c\u8bad\u7ec3\u65e0\u5173\u7684\u7801\u672c\u91cd\u91c7\u6837\u65b9\u6cd5\uff0c\u4ee5\u89e3\u51b3\u7801\u672c\u5d29\u6e83\u95ee\u9898\u5e76\u63d0\u9ad8\u91cd\u5efa\u6027\u80fd\u548c\u7801\u672c\u5927\u5c0f\u8c03\u6574\u7684\u7075\u6d3b\u6027\u3002", "motivation": "VQ-VAE\u6a21\u578b\u4e2d\u5b58\u5728\u7684\u7801\u672c\u5d29\u6e83\u95ee\u9898\uff0c\u4ee5\u53ca\u73b0\u6709\u65b9\u6cd5\uff08\u9690\u5f0f\u9759\u6001\u7801\u672c\u6216\u8054\u5408\u4f18\u5316\u6574\u4e2a\u7801\u672c\uff09\u5bf9\u7801\u672c\u5b66\u4e60\u80fd\u529b\u7684\u9650\u5236\uff0c\u5bfc\u81f4\u91cd\u5efa\u8d28\u91cf\u4e0b\u964d\u3002", "method": "\u672c\u6587\u63d0\u51faGroup-VQ\u65b9\u6cd5\uff0c\u91c7\u7528\u7ec4\u5185\u72ec\u7acb\u4f18\u5316\u3001\u7ec4\u95f4\u8054\u5408\u4f18\u5316\u7684\u65b9\u5f0f\u5bf9\u7801\u672c\u8fdb\u884c\u7ec4\u4f18\u5316\u3002\u540c\u65f6\u5f15\u5165\u4e86\u4e00\u79cd\u8bad\u7ec3\u65e0\u5173\u7684\u7801\u672c\u91cd\u91c7\u6837\u65b9\u6cd5\uff0c\u53ef\u4ee5\u5728\u8bad\u7ec3\u540e\u8c03\u6574\u7801\u672c\u5927\u5c0f\u3002", "result": "\u5728\u5404\u79cd\u8bbe\u7f6e\u4e0b\u7684\u56fe\u50cf\u91cd\u5efa\u5b9e\u9a8c\u4e2d\uff0cGroup-VQ\u5728\u91cd\u5efa\u6307\u6807\u4e0a\u8868\u73b0\u51fa\u66f4\u597d\u7684\u6027\u80fd\u3002\u8bad\u7ec3\u540e\u7684\u7801\u672c\u91c7\u6837\u65b9\u6cd5\u5728\u8c03\u6574\u7801\u672c\u5927\u5c0f\u65f6\u4e5f\u5b9e\u73b0\u4e86\u9884\u671f\u7684\u7075\u6d3b\u6027\u3002", "conclusion": "Group-VQ\u901a\u8fc7\u7ec4\u4f18\u5316\u548c\u8bad\u7ec3\u65e0\u5173\u7684\u7801\u672c\u91cd\u91c7\u6837\u65b9\u6cd5\uff0c\u6709\u6548\u89e3\u51b3\u4e86VQ-VAE\u4e2d\u7684\u7801\u672c\u5d29\u6e83\u95ee\u9898\uff0c\u63d0\u9ad8\u4e86\u91cd\u5efa\u6027\u80fd\uff0c\u5e76\u589e\u52a0\u4e86\u7801\u672c\u5927\u5c0f\u8c03\u6574\u7684\u7075\u6d3b\u6027\u3002"}}
{"id": "2510.13768", "categories": ["cs.CV", "cs.AI", "q-bio.NC"], "pdf": "https://arxiv.org/pdf/2510.13768", "abs": "https://arxiv.org/abs/2510.13768", "authors": ["Connor Lane", "Daniel Z. Kaplan", "Tanishq Mathew Abraham", "Paul S. Scotti"], "title": "Scaling Vision Transformers for Functional MRI with Flat Maps", "comment": "NeurIPS 2025 Workshop, Foundation Models for the Brain and Body;\n  Code: https://github.com/MedARC-AI/fmri-fm; Discord:\n  https://discord.gg/tVR4TWnRM9", "summary": "A key question for adapting modern deep learning architectures to functional\nMRI (fMRI) is how to represent the data for model input. To bridge the modality\ngap between fMRI and natural images, we transform the 4D volumetric fMRI data\ninto videos of 2D fMRI activity flat maps. We train Vision Transformers on 2.3K\nhours of fMRI flat map videos from the Human Connectome Project using the\nspatiotemporal masked autoencoder (MAE) framework. We observe that masked fMRI\nmodeling performance improves with dataset size according to a strict power\nscaling law. Downstream classification benchmarks show that our model learns\nrich representations supporting both fine-grained state decoding across\nsubjects, as well as subject-specific trait decoding across changes in brain\nstate. This work is part of an ongoing open science project to build foundation\nmodels for fMRI data. Our code and datasets are available at\nhttps://github.com/MedARC-AI/fmri-fm.", "AI": {"tldr": "\u5c064D fMRI\u6570\u636e\u8f6c\u6362\u4e3a2D fMRI\u6d3b\u52a8\u5e73\u94fa\u56fe\u7684\u89c6\u9891\uff0c\u5e76\u4f7f\u7528\u65f6\u7a7a\u63a9\u7801\u81ea\u52a8\u7f16\u7801\u5668\uff08MAE\uff09\u6846\u67b6\u5728Vision Transformers\u4e0a\u8fdb\u884c\u8bad\u7ec3\u3002", "motivation": "\u89e3\u51b3\u73b0\u4ee3\u6df1\u5ea6\u5b66\u4e60\u67b6\u6784\u4e0e\u529f\u80fd\u6027MRI\uff08fMRI\uff09\u6570\u636e\u4e4b\u95f4\u7684\u6a21\u6001\u5dee\u5f02\uff0c\u5e76\u63a2\u7d22\u5982\u4f55\u8868\u793afMRI\u6570\u636e\u4ee5\u7528\u4e8e\u6a21\u578b\u8f93\u5165\u3002", "method": "\u5c064D\u4f53\u79effMRI\u6570\u636e\u8f6c\u6362\u4e3a2D fMRI\u6d3b\u52a8\u5e73\u94fa\u56fe\u7684\u89c6\u9891\uff0c\u5e76\u4f7f\u7528\u65f6\u7a7a\u63a9\u7801\u81ea\u52a8\u7f16\u7801\u5668\uff08MAE\uff09\u6846\u67b6\u5728\u6765\u81ea\u4eba\u7c7b\u8fde\u63a5\u7ec4\u9879\u76ee\u76842.3K\u5c0f\u65f6fMRI\u5e73\u94fa\u56fe\u89c6\u9891\u4e0a\u8bad\u7ec3Vision Transformers\u3002", "result": "\u63a9\u7801fMRI\u5efa\u6a21\u6027\u80fd\u968f\u6570\u636e\u96c6\u5927\u5c0f\u7684\u589e\u52a0\u800c\u63d0\u9ad8\uff0c\u5e76\u9075\u5faa\u4e25\u683c\u7684\u5e42\u5f8b\u7f29\u653e\u5b9a\u5f8b\u3002\u4e0b\u6e38\u5206\u7c7b\u57fa\u51c6\u6d4b\u8bd5\u8868\u660e\uff0c\u6a21\u578b\u5b66\u4e60\u4e86\u4e30\u5bcc\u7684\u8868\u793a\uff0c\u652f\u6301\u8de8\u53d7\u8bd5\u8005\u7684\u7ec6\u7c92\u5ea6\u72b6\u6001\u89e3\u7801\u4ee5\u53ca\u8de8\u5927\u8111\u72b6\u6001\u53d8\u5316\u7684\u53d7\u8bd5\u8005\u7279\u5f02\u6027\u6027\u72b6\u89e3\u7801\u3002", "conclusion": "\u672c\u7814\u7a76\u6210\u529f\u5730\u5c064D fMRI\u6570\u636e\u8f6c\u6362\u4e3a2D fMRI\u5e73\u94fa\u56fe\u89c6\u9891\uff0c\u5e76\u5229\u7528Vision Transformers\u548cMAE\u6846\u67b6\u5b66\u4e60\u4e86fMRI\u6570\u636e\u7684\u4e30\u5bcc\u8868\u793a\u3002\u8fd9\u4e00\u6210\u679c\u4e3afMRI\u6570\u636e\u6784\u5efa\u57fa\u7840\u6a21\u578b\u5960\u5b9a\u4e86\u57fa\u7840\uff0c\u5e76\u4e3a\u672a\u6765fMRI\u6570\u636e\u7684\u5206\u6790\u548c\u7406\u89e3\u63d0\u4f9b\u4e86\u65b0\u7684\u65b9\u5411\u3002"}}
{"id": "2510.13795", "categories": ["cs.CV", "cs.AI"], "pdf": "https://arxiv.org/pdf/2510.13795", "abs": "https://arxiv.org/abs/2510.13795", "authors": ["Yi Zhang", "Bolin Ni", "Xin-Sheng Chen", "Heng-Rui Zhang", "Yongming Rao", "Houwen Peng", "Qinglin Lu", "Han Hu", "Meng-Hao Guo", "Shi-Min Hu"], "title": "Bee: A High-Quality Corpus and Full-Stack Suite to Unlock Advanced Fully Open MLLMs", "comment": "homepage: https://open-bee.github.io/", "summary": "Fully open multimodal large language models (MLLMs) currently lag behind\nproprietary counterparts, primarily due to a significant gap in data quality\nfor supervised fine-tuning (SFT). Existing open-source datasets are often\nplagued by widespread noise and a critical deficit in complex reasoning data,\nsuch as Chain-of-Thought (CoT), which hinders the development of advanced model\ncapabilities. Addressing these challenges, our work makes three primary\ncontributions. First, we introduce Honey-Data-15M, a new SFT dataset comprising\napproximately 15 million QA pairs, processed through multiple cleaning\ntechniques and enhanced with a novel dual-level (short and long) CoT enrichment\nstrategy. Second, we introduce HoneyPipe, the data curation pipeline, and its\nunderlying framework DataStudio, providing the community with a transparent and\nadaptable methodology for data curation that moves beyond static dataset\nreleases. Finally, to validate our dataset and pipeline, we train Bee-8B, an 8B\nmodel on Honey-Data-15M. Experiments show that Bee-8B establishes a new\nstate-of-the-art (SOTA) for fully open MLLMs, achieving performance that is\ncompetitive with, and in some cases surpasses, recent semi-open models such as\nInternVL3.5-8B. Our work delivers to the community a suite of foundational\nresources, including: the Honey-Data-15M corpus; the full-stack suite\ncomprising HoneyPipe and DataStudio; training recipes; an evaluation harness;\nand the model weights. This effort demonstrates that a principled focus on data\nquality is a key pathway to developing fully open MLLMs that are highly\ncompetitive with their semi-open counterparts.", "AI": {"tldr": "\u672c\u6587\u4ecb\u7ecd\u4e86Honey-Data-15M\u6570\u636e\u96c6\u3001HoneyPipe\u6570\u636e\u5904\u7406\u7ba1\u9053\u548cDataStudio\u6846\u67b6\uff0c\u5e76\u63d0\u51fa\u4e86\u540d\u4e3aBee-8B\u7684\u6a21\u578b\u3002", "motivation": "\u76ee\u524d\uff0c\u5b8c\u5168\u5f00\u653e\u7684\u591a\u6a21\u6001\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08MLLMs\uff09\u5728\u6027\u80fd\u4e0a\u843d\u540e\u4e8e\u4e13\u6709\u6a21\u578b\uff0c\u4e3b\u8981\u539f\u56e0\u662f\u76d1\u7763\u5fae\u8c03\uff08SFT\uff09\u7684\u6570\u636e\u8d28\u91cf\u5b58\u5728\u663e\u8457\u5dee\u8ddd\u3002\u73b0\u6709\u7684\u5f00\u6e90\u6570\u636e\u96c6\u901a\u5e38\u5b58\u5728\u5e7f\u6cdb\u7684\u566a\u58f0\uff0c\u5e76\u4e14\u7f3a\u4e4f\u590d\u6742\u7684\u63a8\u7406\u6570\u636e\uff08\u5982\u601d\u7ef4\u94feCoT\uff09\uff0c\u8fd9\u963b\u788d\u4e86\u6a21\u578b\u9ad8\u7ea7\u80fd\u529b\u7684\u53d1\u5c55\u3002", "method": "1. \u5f15\u5165\u4e86Honey-Data-15M\u6570\u636e\u96c6\uff0c\u5305\u542b\u7ea61500\u4e07\u4e2a\u95ee\u7b54\u5bf9\uff0c\u901a\u8fc7\u591a\u79cd\u6e05\u6d17\u6280\u672f\u5904\u7406\uff0c\u5e76\u91c7\u7528\u65b0\u9896\u7684\u53cc\u5c42\uff08\u77edCoT\u548c\u957fCoT\uff09CoT\u4e30\u5bcc\u7b56\u7565\u8fdb\u884c\u589e\u5f3a\u30022. \u5f15\u5165\u4e86HoneyPipe\u6570\u636e\u6574\u7406\u7ba1\u9053\u53ca\u5176\u5e95\u5c42\u6846\u67b6DataStudio\uff0c\u4e3a\u793e\u533a\u63d0\u4f9b\u4e86\u4e00\u4e2a\u900f\u660e\u4e14\u9002\u5e94\u6027\u5f3a\u7684\u6570\u636e\u6574\u7406\u65b9\u6cd5\u30023. \u5728Honey-Data-15M\u4e0a\u8bad\u7ec3\u4e86\u4e00\u4e2a8B\u6a21\u578bBee-8B\u3002", "result": "Bee-8B\u6a21\u578b\u5728\u5b8c\u5168\u5f00\u653e\u7684MLLMs\u4e2d\u5efa\u7acb\u4e86\u65b0\u7684\u6700\u5148\u8fdb\u6c34\u5e73\uff08SOTA\uff09\uff0c\u5176\u6027\u80fd\u4e0eInternVL3.5-8B\u7b49\u534a\u5f00\u653e\u6a21\u578b\u5177\u6709\u7ade\u4e89\u529b\uff0c\u5728\u67d0\u4e9b\u60c5\u51b5\u4e0b\u751a\u81f3\u8d85\u8d8a\u5b83\u4eec\u3002", "conclusion": "\u6570\u636e\u8d28\u91cf\u7684\u539f\u5219\u6027\u5173\u6ce8\u662f\u5f00\u53d1\u5177\u6709\u9ad8\u5ea6\u7ade\u4e89\u529b\u7684\u5b8c\u5168\u5f00\u653eMLLMs\u7684\u5173\u952e\u9014\u5f84\u3002"}}
{"id": "2510.13804", "categories": ["cs.CV", "cs.AI", "cs.CL"], "pdf": "https://arxiv.org/pdf/2510.13804", "abs": "https://arxiv.org/abs/2510.13804", "authors": ["Xinchen Zhang", "Xiaoying Zhang", "Youbin Wu", "Yanbin Cao", "Renrui Zhang", "Ruihang Chu", "Ling Yang", "Yujiu Yang"], "title": "Generative Universal Verifier as Multimodal Meta-Reasoner", "comment": null, "summary": "We introduce Generative Universal Verifier, a novel concept and plugin\ndesigned for next-generation multimodal reasoning in vision-language models and\nunified multimodal models, providing the fundamental capability of reflection\nand refinement on visual outcomes during the reasoning and generation process.\nThis work makes three main contributions: (1) We build ViVerBench, a\ncomprehensive benchmark spanning 16 categories of critical tasks for evaluating\nvisual outcomes in multimodal reasoning. Results show that existing VLMs\nconsistently underperform across these tasks, underscoring a substantial gap\nfrom human-level capability in reliable visual verification. (2) We design two\nautomated pipelines to construct large-scale visual verification data and train\nOmniVerifier-7B, the first omni-capable generative verifier trained for\nuniversal visual verification and achieves notable gains on ViVerBench(+8.3).\nThrough training, we identify three atomic capabilities in visual verification\nand demonstrate how they generalize and interact synergistically. (3) We\npropose OmniVerifier-TTS, a sequential test-time scaling paradigm that\nleverages the universal verifier to bridge image generation and editing within\nunified models, enhancing the upper bound of generative ability through\niterative fine-grained optimization. Beyond generation, we extend universal\nverifier to broader world-modeling interleaved reasoning scenarios.\nEmpirically, OmniVerifier-TTS achieves improvements on T2I-ReasonBench(+3.7),\nand GenEval++(+4.3), outperforming existing parallel test-time scaling methods,\nsuch as Best-of-N. By endowing multimodal reasoning with reliable visual\nverification, OmniVerifier advances both reliable reflection during generation\nand scalable test-time refinement, marking a step toward more trustworthy and\ncontrollable next-generation reasoning systems.", "AI": {"tldr": "\u8be5\u8bba\u6587\u4ecb\u7ecd\u4e86\u4e00\u79cd\u540d\u4e3a\u201c\u751f\u6210\u5f0f\u901a\u7528\u9a8c\u8bc1\u5668\u201d\uff08GUV\uff09\u7684\u65b0\u6982\u5ff5\u548c\u63d2\u4ef6\uff0c\u65e8\u5728\u63d0\u9ad8\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\u548c\u7edf\u4e00\u591a\u6a21\u6001\u6a21\u578b\u7684\u591a\u6a21\u6001\u63a8\u7406\u80fd\u529b\uff0c\u4f7f\u5176\u80fd\u591f\u5bf9\u89c6\u89c9\u7ed3\u679c\u8fdb\u884c\u53cd\u601d\u548c\u6539\u8fdb\u3002", "motivation": "\u73b0\u6709\u7684\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\u5728\u89c6\u89c9\u9a8c\u8bc1\u65b9\u9762\u8868\u73b0\u4e0d\u4f73\uff0c\u4e0e\u4eba\u7c7b\u6c34\u5e73\u5b58\u5728\u663e\u8457\u5dee\u8ddd\u3002\u56e0\u6b64\uff0c\u5b58\u5728\u5f00\u53d1\u80fd\u591f\u53ef\u9760\u5730\u5bf9\u89c6\u89c9\u7ed3\u679c\u8fdb\u884c\u9a8c\u8bc1\u548c\u6539\u8fdb\u7684\u7cfb\u7edf\u3002", "method": "1. \u5efa\u7acb\u4e86ViVerBench\u57fa\u51c6\uff0c\u7528\u4e8e\u8bc4\u4f30\u591a\u6a21\u6001\u63a8\u7406\u4e2d\u7684\u89c6\u89c9\u7ed3\u679c\u30022. \u8bbe\u8ba1\u4e86\u81ea\u52a8\u5316\u6d41\u7a0b\u6765\u6784\u5efa\u5927\u89c4\u6a21\u89c6\u89c9\u9a8c\u8bc1\u6570\u636e\uff0c\u5e76\u8bad\u7ec3\u4e86OmniVerifier-7B\u6a21\u578b\u30023. \u63d0\u51fa\u4e86OmniVerifier-TTS\uff0c\u4e00\u79cd\u6d4b\u8bd5\u65f6\u5e8f\u7f29\u653e\u8303\u5f0f\uff0c\u5229\u7528\u901a\u7528\u9a8c\u8bc1\u5668\u589e\u5f3a\u751f\u6210\u80fd\u529b\u30024. \u5c06\u901a\u7528\u9a8c\u8bc1\u5668\u6269\u5c55\u5230\u66f4\u5e7f\u6cdb\u7684\u4e16\u754c\u5efa\u6a21\u4ea4\u9519\u63a8\u7406\u573a\u666f\u3002", "result": "1. \u73b0\u6709\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\u5728ViVerBench\u4e0a\u7684\u8868\u73b0\u4e0d\u4f73\u30022. OmniVerifier-7B\u5728ViVerBench\u4e0a\u5b9e\u73b0\u4e86\u663e\u8457\u6539\u8fdb\uff08+8.3\uff09\u30023. OmniVerifier-TTS\u5728T2I-ReasonBench\uff08+3.7\uff09\u548cGenEval++\uff08+4.3\uff09\u4e0a\u5b9e\u73b0\u4e86\u6539\u8fdb\uff0c\u4f18\u4e8e\u73b0\u6709\u7684\u5e76\u884c\u6d4b\u8bd5\u65f6\u7f29\u653e\u65b9\u6cd5\u3002", "conclusion": "\u6240\u63d0\u51fa\u7684\u751f\u6210\u5f0f\u901a\u7528\u9a8c\u8bc1\u5668\uff08GUV\uff09\u53ca\u5176\u5b9e\u73b0OmniVerifier\uff0c\u901a\u8fc7\u53ef\u9760\u7684\u89c6\u89c9\u9a8c\u8bc1\uff0c\u663e\u8457\u63d0\u9ad8\u4e86\u591a\u6a21\u6001\u63a8\u7406\u7684\u53ef\u9760\u6027\u548c\u53ef\u63a7\u6027\uff0c\u4ece\u800c\u5728\u751f\u6210\u8fc7\u7a0b\u4e2d\u5b9e\u73b0\u4e86\u53ef\u9760\u7684\u53cd\u601d\u548c\u53ef\u6269\u5c55\u7684\u6d4b\u8bd5\u65f6\u7ec6\u5316\u3002"}}
{"id": "2510.13375", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2510.13375", "abs": "https://arxiv.org/abs/2510.13375", "authors": ["Tianyuan Yuan", "Yicheng Liu", "Chenhao Lu", "Zhuoguang Chen", "Tao Jiang", "Hang Zhao"], "title": "DepthVLA: Enhancing Vision-Language-Action Models with Depth-Aware Spatial Reasoning", "comment": null, "summary": "Vision-Language-Action (VLA) models have recently shown impressive\ngeneralization and language-guided manipulation capabilities. However, their\nperformance degrades on tasks requiring precise spatial reasoning due to\nlimited spatial reasoning inherited from Vision-Language Models (VLMs).\nExisting VLAs rely on extensive action-data pretraining to ground VLMs in 3D\nspace, which reduces training efficiency and is still insufficient for accurate\nspatial understanding. In this work, we present DepthVLA, a simple yet\neffective VLA architecture that explicitly incorporates spatial awareness\nthrough a pretrained depth prediction module. DepthVLA adopts a\nmixture-of-transformers design that unifies a VLM, a depth transformer, and an\naction expert with fully shared attentions, forming an end-to-end model with\nenhanced spatial reasoning. Extensive evaluations in both real-world and\nsimulated environments show that DepthVLA outperforms state-of-the-art\napproaches, achieving 78.5% vs. 65.0% progress in real-world tasks, 94.9% vs.\n93.6% in the LIBERO simulator, and 74.8% vs. 58.8% in the Simpler simulator.\nOur code will be made publicly available.", "AI": {"tldr": "DepthVLA\u662f\u4e00\u79cd\u65b0\u578bVLA\u6a21\u578b\uff0c\u901a\u8fc7\u96c6\u6210\u9884\u8bad\u7ec3\u6df1\u5ea6\u9884\u6d4b\u6a21\u5757\u548c\u6df7\u5408Transformer\u8bbe\u8ba1\uff0c\u663e\u8457\u63d0\u5347\u4e86\u7a7a\u95f4\u63a8\u7406\u80fd\u529b\uff0c\u5e76\u5728\u771f\u5b9e\u4e16\u754c\u548c\u6a21\u62df\u73af\u5883\u4e2d\u5747\u8d85\u8d8a\u4e86\u73b0\u6709SOTA\u65b9\u6cd5\u3002", "motivation": "\u73b0\u6709\u7684\u89c6\u89c9\u8bed\u8a00\u52a8\u4f5c\uff08VLA\uff09\u6a21\u578b\u5728\u9700\u8981\u7cbe\u786e\u7a7a\u95f4\u63a8\u7406\u7684\u4efb\u52a1\u4e0a\u8868\u73b0\u4e0d\u4f73\uff0c\u56e0\u4e3a\u5b83\u4eec\u4ece\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\uff08VLM\uff09\u7ee7\u627f\u7684\u7a7a\u95f4\u63a8\u7406\u80fd\u529b\u6709\u9650\u3002\u73b0\u6709VLA\u6a21\u578b\u4f9d\u8d56\u5927\u91cf\u7684\u52a8\u4f5c\u6570\u636e\u9884\u8bad\u7ec3\u6765\u5c06VLM\u4e0e3D\u7a7a\u95f4\u5173\u8054\u8d77\u6765\uff0c\u8fd9\u4e0d\u4ec5\u964d\u4f4e\u4e86\u8bad\u7ec3\u6548\u7387\uff0c\u800c\u4e14\u5bf9\u4e8e\u7cbe\u786e\u7684\u7a7a\u95f4\u7406\u89e3\u4ecd\u7136\u4e0d\u8db3\u3002", "method": "\u672c\u6587\u63d0\u51fa\u4e86DepthVLA\uff0c\u4e00\u79cd\u7b80\u5355\u4f46\u6709\u6548\u7684VLA\u67b6\u6784\uff0c\u901a\u8fc7\u9884\u8bad\u7ec3\u7684\u6df1\u5ea6\u9884\u6d4b\u6a21\u5757\u660e\u786e\u5730\u878d\u5165\u7a7a\u95f4\u611f\u77e5\u3002DepthVLA\u91c7\u7528\u6df7\u5408Transformer\u8bbe\u8ba1\uff0c\u7edf\u4e00\u4e86VLM\u3001\u6df1\u5ea6Transformer\u548c\u52a8\u4f5c\u4e13\u5bb6\uff0c\u5177\u6709\u5b8c\u5168\u5171\u4eab\u7684\u6ce8\u610f\u529b\u673a\u5236\uff0c\u5f62\u6210\u4e86\u4e00\u4e2a\u5177\u6709\u589e\u5f3a\u7a7a\u95f4\u63a8\u7406\u80fd\u529b\u7684\u7aef\u5230\u7aef\u6a21\u578b\u3002", "result": "\u5728\u73b0\u5b9e\u4e16\u754c\u548c\u6a21\u62df\u73af\u5883\u4e2d\u7684\u5927\u91cf\u8bc4\u4f30\u8868\u660e\uff0cDepthVLA\u4f18\u4e8e\u73b0\u6709\u7684SOTA\u65b9\u6cd5\uff0c\u5728\u73b0\u5b9e\u4e16\u754c\u4efb\u52a1\u4e2d\u53d6\u5f97\u4e8678.5%\u7684\u8fdb\u5c55\uff08\u5bf9\u6bd4\u6b64\u524d\u768465.0%\uff09\uff0c\u5728LIBERO\u6a21\u62df\u5668\u4e2d\u53d6\u5f97\u4e8694.9%\u7684\u8fdb\u5c55\uff08\u5bf9\u6bd4\u6b64\u524d\u768493.6%\uff09\uff0c\u5728Simpler\u6a21\u62df\u5668\u4e2d\u53d6\u5f97\u4e8674.8%\u7684\u8fdb\u5c55\uff08\u5bf9\u6bd4\u6b64\u524d\u768458.8%\uff09\u3002", "conclusion": "DepthVLA\u901a\u8fc7\u663e\u5f0f\u5730\u6574\u5408\u7a7a\u95f4\u611f\u77e5\u548c\u6df7\u5408Transformer\u8bbe\u8ba1\uff0c\u6210\u529f\u89e3\u51b3\u4e86VLA\u6a21\u578b\u5728\u7a7a\u95f4\u63a8\u7406\u65b9\u9762\u7684\u4e0d\u8db3\uff0c\u5e76\u5728\u591a\u4e2a\u73af\u5883\u4e2d\u53d6\u5f97\u4e86\u663e\u8457\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\u7684\u6027\u80fd\u3002"}}
{"id": "2510.13394", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2510.13394", "abs": "https://arxiv.org/abs/2510.13394", "authors": ["Xinmiao Huang", "Qisong He", "Zhenglin Huang", "Boxuan Wang", "Zhuoyun Li", "Guangliang Cheng", "Yi Dong", "Xiaowei Huang"], "title": "Spatial-DISE: A Unified Benchmark for Evaluating Spatial Reasoning in Vision-Language Models", "comment": null, "summary": "Spatial reasoning ability is crucial for Vision Language Models (VLMs) to\nsupport real-world applications in diverse domains including robotics,\naugmented reality, and autonomous navigation. Unfortunately, existing\nbenchmarks are inadequate in assessing spatial reasoning ability, especially\nthe \\emph{intrinsic-dynamic} spatial reasoning which is a fundamental aspect of\nhuman spatial cognition. In this paper, we propose a unified benchmark,\n\\textbf{Spatial-DISE}, based on a cognitively grounded taxonomy that\ncategorizes tasks into four fundamental quadrants:\n\\textbf{I}ntrinsic-\\textbf{S}tatic, Intrinsic-\\textbf{D}ynamic,\n\\textbf{E}xtrinsic-Static, and Extrinsic-Dynamic spatial reasoning. Moreover,\nto address the issue of data scarcity, we develop a scalable and automated\npipeline to generate diverse and verifiable spatial reasoning questions,\nresulting in a new \\textbf{Spatial-DISE} dataset that includes Spatial-DISE\nBench (559 evaluation VQA pairs) and Spatial-DISE-12K (12K+ training VQA\npairs). Our comprehensive evaluation across 28 state-of-the-art VLMs reveals\nthat, current VLMs have a large and consistent gap to human competence,\nespecially on multi-step multi-view spatial reasoning. Spatial-DISE offers a\nrobust framework, valuable dataset, and clear direction for future research\ntoward human-like spatial intelligence. Benchmark, dataset, and code will be\npublicly released.", "AI": {"tldr": "\u8fd9\u7bc7\u8bba\u6587\u63d0\u51fa\u4e86Spatial-DISE\uff0c\u8fd9\u662f\u4e00\u4e2a\u7edf\u4e00\u7684\u57fa\u51c6\uff0c\u7528\u4e8e\u8bc4\u4f30\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\uff08VLMs\uff09\u5728\u7a7a\u95f4\u63a8\u7406\u65b9\u9762\u7684\u80fd\u529b\uff0c\u7279\u522b\u662f\u5728\u56fa\u6709\u7684\u52a8\u6001\u7a7a\u95f4\u63a8\u7406\u65b9\u9762\u3002\u8be5\u7814\u7a76\u8fd8\u63d0\u4f9b\u4e86\u4e00\u4e2a\u540d\u4e3aSpatial-DISE\u7684\u6570\u636e\u96c6\u3002", "motivation": "\u7a7a\u95f4\u63a8\u7406\u80fd\u529b\u5bf9\u4e8e\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\uff08VLMs\uff09\u5728\u73b0\u5b9e\u4e16\u754c\u7684\u5e94\u7528\u81f3\u5173\u91cd\u8981\uff0c\u4f46\u73b0\u6709\u57fa\u51c6\u5728\u8bc4\u4f30\u7a7a\u95f4\u63a8\u7406\u80fd\u529b\u65b9\u9762\u5b58\u5728\u4e0d\u8db3\uff0c\u5c24\u5176\u662f\u5728\u4eba\u7c7b\u7a7a\u95f4\u8ba4\u77e5\u7684\u4e00\u4e2a\u57fa\u672c\u65b9\u9762\u2014\u2014\u56fa\u6709\u52a8\u6001\u7a7a\u95f4\u63a8\u7406\u65b9\u9762\u3002", "method": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u4e2a\u7edf\u4e00\u7684\u57fa\u51c6Spatial-DISE\uff0c\u8be5\u57fa\u51c6\u57fa\u4e8e\u8ba4\u77e5\u57fa\u7840\u5206\u7c7b\u6cd5\uff0c\u5c06\u4efb\u52a1\u5206\u4e3a\u56db\u4e2a\u57fa\u672c\u8c61\u9650\uff1a\u56fa\u6709-\u9759\u6001\u3001\u56fa\u6709-\u52a8\u6001\u3001\u5916\u5728-\u9759\u6001\u548c\u5916\u5728-\u52a8\u6001\u7a7a\u95f4\u63a8\u7406\u3002\u4e3a\u4e86\u89e3\u51b3\u6570\u636e\u7a00\u7f3a\u95ee\u9898\uff0c\u5f00\u53d1\u4e86\u4e00\u4e2a\u53ef\u6269\u5c55\u7684\u81ea\u52a8\u5316\u6d41\u7a0b\uff0c\u751f\u6210\u591a\u6837\u5316\u548c\u53ef\u9a8c\u8bc1\u7684\u7a7a\u95f4\u63a8\u7406\u95ee\u9898\uff0c\u4ece\u800c\u521b\u5efa\u4e86\u65b0\u7684Spatial-DISE\u6570\u636e\u96c6\uff0c\u5305\u62ecSpatial-DISE Bench\uff08559\u4e2a\u8bc4\u4f30VQA\u5bf9\uff09\u548cSpatial-DISE-12K\uff0812K+\u8bad\u7ec3VQA\u5bf9\uff09\u3002", "result": "\u5bf928\u4e2a\u6700\u5148\u8fdb\u7684VLM\u8fdb\u884c\u7684\u7efc\u5408\u8bc4\u4f30\u8868\u660e\uff0c\u5f53\u524d\u7684VLM\u4e0e\u4eba\u7c7b\u80fd\u529b\u4e4b\u95f4\u5b58\u5728\u5de8\u5927\u4e14\u6301\u7eed\u7684\u5dee\u8ddd\uff0c\u7279\u522b\u662f\u5728\u591a\u6b65\u9aa4\u591a\u89c6\u56fe\u7a7a\u95f4\u63a8\u7406\u65b9\u9762\u3002", "conclusion": "Spatial-DISE\u4e3a\u672a\u6765VLM\u7814\u7a76\u63d0\u4f9b\u4e86\u4e00\u4e2a\u5f3a\u5927\u7684\u6846\u67b6\u3001\u6709\u4ef7\u503c\u7684\u6570\u636e\u96c6\u548c\u660e\u786e\u7684\u65b9\u5411\uff0c\u4ee5\u671f\u5b9e\u73b0\u7c7b\u4eba\u7a7a\u95f4\u667a\u80fd\u3002\u57fa\u51c6\u3001\u6570\u636e\u96c6\u548c\u4ee3\u7801\u5c06\u516c\u5f00\u53d1\u5e03\u3002"}}
{"id": "2510.13419", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2510.13419", "abs": "https://arxiv.org/abs/2510.13419", "authors": ["Jianhui Zhang", "Sheng Cheng", "Qirui Sun", "Jia Liu", "Wang Luyang", "Chaoyu Feng", "Chen Fang", "Lei Lei", "Jue Wang", "Shuaicheng Liu"], "title": "Ultra High-Resolution Image Inpainting with Patch-Based Content Consistency Adapter", "comment": null, "summary": "In this work, we present Patch-Adapter, an effective framework for\nhigh-resolution text-guided image inpainting. Unlike existing methods limited\nto lower resolutions, our approach achieves 4K+ resolution while maintaining\nprecise content consistency and prompt alignment, two critical challenges in\nimage inpainting that intensify with increasing resolution and texture\ncomplexity. Patch-Adapter leverages a two-stage adapter architecture to scale\nthe diffusion model's resolution from 1K to 4K+ without requiring structural\noverhauls: (1) Dual Context Adapter learns coherence between masked and\nunmasked regions at reduced resolutions to establish global structural\nconsistency; and (2) Reference Patch Adapter implements a patch-level attention\nmechanism for full-resolution inpainting, preserving local detail fidelity\nthrough adaptive feature fusion. This dual-stage architecture uniquely\naddresses the scalability gap in high-resolution inpainting by decoupling\nglobal semantics from localized refinement. Experiments demonstrate that\nPatch-Adapter not only resolves artifacts common in large-scale inpainting but\nalso achieves state-of-the-art performance on the OpenImages and\nPhoto-Concept-Bucket datasets, outperforming existing methods in both\nperceptual quality and text-prompt adherence.", "AI": {"tldr": "Patch-Adapter \u662f\u4e00\u4e2a\u7528\u4e8e\u9ad8\u5206\u8fa8\u7387\u6587\u672c\u5f15\u5bfc\u56fe\u50cf\u4fee\u590d\u7684\u6846\u67b6\u3002\u5b83\u514b\u670d\u4e86\u73b0\u6709\u65b9\u6cd5\u5728\u5206\u8fa8\u7387\u4e0a\u7684\u9650\u5236\uff0c\u5728\u4fdd\u6301\u5185\u5bb9\u4e00\u81f4\u6027\u548c\u63d0\u793a\u5bf9\u9f50\u7684\u540c\u65f6\uff0c\u5b9e\u73b0\u4e86 4K+ \u5206\u8fa8\u7387\u7684\u56fe\u50cf\u4fee\u590d\u3002", "motivation": "\u73b0\u6709\u7684\u56fe\u50cf\u4fee\u590d\u65b9\u6cd5\u5728\u5904\u7406\u9ad8\u5206\u8fa8\u7387\u56fe\u50cf\u65f6\u8868\u73b0\u4e0d\u4f73\uff0c\u5e38\u5e38\u65e0\u6cd5\u4fdd\u6301\u5185\u5bb9\u7684\u7cbe\u786e\u4e00\u81f4\u6027\u548c\u63d0\u793a\u7684\u51c6\u786e\u5bf9\u9f50\uff0c\u5c24\u5176\u662f\u5728\u56fe\u50cf\u5206\u8fa8\u7387\u548c\u7eb9\u7406\u590d\u6742\u6027\u589e\u52a0\u65f6\u95ee\u9898\u66f4\u52a0\u7a81\u51fa\u3002", "method": "Patch-Adapter \u91c7\u7528\u4e24\u9636\u6bb5\u9002\u914d\u5668\u67b6\u6784\uff0c\u5c06\u6269\u6563\u6a21\u578b\u7684\u56fe\u50cf\u4fee\u590d\u5206\u8fa8\u7387\u4ece 1K \u63d0\u5347\u5230 4K+\uff1a1. \u53cc\u91cd\u4e0a\u4e0b\u6587\u9002\u914d\u5668\uff1a\u5728\u8f83\u4f4e\u5206\u8fa8\u7387\u4e0b\u5b66\u4e60\u906e\u7f69\u533a\u57df\u548c\u975e\u906e\u7f69\u533a\u57df\u4e4b\u95f4\u7684\u4e00\u81f4\u6027\uff0c\u4ee5\u5efa\u7acb\u5168\u5c40\u7ed3\u6784\u8fde\u8d2f\u6027\u3002 2. \u53c2\u8003\u8865\u4e01\u9002\u914d\u5668\uff1a\u5728\u9ad8\u5206\u8fa8\u7387\u4e0b\uff0c\u901a\u8fc7\u81ea\u9002\u5e94\u7279\u5f81\u878d\u5408\u5b9e\u73b0\u8865\u4e01\u7ea7\u522b\u7684\u6ce8\u610f\u529b\u673a\u5236\uff0c\u4ece\u800c\u4fdd\u7559\u5c40\u90e8\u7ec6\u8282\u3002", "result": "Patch-Adapter \u89e3\u51b3\u4e86\u5927\u89c4\u6a21\u56fe\u50cf\u4fee\u590d\u4e2d\u5e38\u89c1\u7684\u4f2a\u5f71\u95ee\u9898\uff0c\u5e76\u5728 OpenImages \u548c Photo-Concept-Bucket \u6570\u636e\u96c6\u4e0a\u53d6\u5f97\u4e86\u6700\u5148\u8fdb\u7684\u6027\u80fd\uff0c\u5728\u611f\u77e5\u8d28\u91cf\u548c\u6587\u672c\u63d0\u793a\u9075\u5faa\u65b9\u9762\u5747\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\u3002", "conclusion": "Patch-Adapter \u901a\u8fc7\u89e3\u8026\u5168\u5c40\u8bed\u4e49\u4e0e\u5c40\u90e8\u7ec6\u5316\uff0c\u72ec\u7279\u5730\u89e3\u51b3\u4e86\u9ad8\u5206\u8fa8\u7387\u56fe\u50cf\u4fee\u590d\u4e2d\u7684\u53ef\u4f38\u7f29\u6027\u95ee\u9898\uff0c\u63d0\u9ad8\u4e86\u56fe\u50cf\u4fee\u590d\u7684\u8d28\u91cf\u548c\u6548\u7387\u3002"}}
{"id": "2510.13432", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2510.13432", "abs": "https://arxiv.org/abs/2510.13432", "authors": ["Yushan Han", "Hui Zhang", "Honglei Zhang", "Chuntao Ding", "Yuanzhouhan Cao", "Yidong Li"], "title": "CoDS: Enhancing Collaborative Perception in Heterogeneous Scenarios via Domain Separation", "comment": "Accepted by IEEE Transactions on Mobile Computing", "summary": "Collaborative perception has been proven to improve individual perception in\nautonomous driving through multi-agent interaction. Nevertheless, most methods\noften assume identical encoders for all agents, which does not hold true when\nthese models are deployed in real-world applications. To realize collaborative\nperception in actual heterogeneous scenarios, existing methods usually align\nneighbor features to those of the ego vehicle, which is vulnerable to noise\nfrom domain gaps and thus fails to address feature discrepancies effectively.\nMoreover, they adopt transformer-based modules for domain adaptation, which\ncauses the model inference inefficiency on mobile devices. To tackle these\nissues, we propose CoDS, a Collaborative perception method that leverages\nDomain Separation to address feature discrepancies in heterogeneous scenarios.\nThe CoDS employs two feature alignment modules, i.e., Lightweight\nSpatial-Channel Resizer (LSCR) and Distribution Alignment via Domain Separation\n(DADS). Besides, it utilizes the Domain Alignment Mutual Information (DAMI)\nloss to ensure effective feature alignment. Specifically, the LSCR aligns the\nneighbor feature across spatial and channel dimensions using a lightweight\nconvolutional layer. Subsequently, the DADS mitigates feature distribution\ndiscrepancy with encoder-specific and encoder-agnostic domain separation\nmodules. The former removes domain-dependent information and the latter\ncaptures task-related information. During training, the DAMI loss maximizes the\nmutual information between aligned heterogeneous features to enhance the domain\nseparation process. The CoDS employs a fully convolutional architecture, which\nensures high inference efficiency. Extensive experiments demonstrate that the\nCoDS effectively mitigates feature discrepancies in heterogeneous scenarios and\nachieves a trade-off between detection accuracy and inference efficiency.", "AI": {"tldr": "\u8fd9\u7bc7\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3a CoDS \u7684\u534f\u540c\u611f\u77e5\u65b9\u6cd5\uff0c\u901a\u8fc7\u57df\u5206\u79bb\u89e3\u51b3\u5f02\u6784\u573a\u666f\u4e2d\u7684\u7279\u5f81\u5dee\u5f02\uff0c\u5e76\u5728\u68c0\u6d4b\u51c6\u786e\u6027\u548c\u63a8\u7406\u6548\u7387\u4e4b\u95f4\u5b9e\u73b0\u4e86\u6743\u8861\u3002", "motivation": "\u5728\u81ea\u52a8\u9a7e\u9a76\u4e2d\uff0c\u534f\u540c\u611f\u77e5\u80fd\u591f\u901a\u8fc7\u591a\u667a\u80fd\u4f53\u4ea4\u4e92\u63d0\u9ad8\u4e2a\u4f53\u611f\u77e5\u80fd\u529b\u3002\u7136\u800c\uff0c\u73b0\u6709\u65b9\u6cd5\u901a\u5e38\u5047\u8bbe\u6240\u6709\u667a\u80fd\u4f53\u90fd\u4f7f\u7528\u76f8\u540c\u7684\u7f16\u7801\u5668\uff0c\u8fd9\u5728\u5b9e\u9645\u5f02\u6784\u573a\u666f\u4e2d\u4e0d\u6210\u7acb\u3002\u73b0\u6709\u7684\u65b9\u6cd5\u5728\u5b9e\u9645\u5f02\u6784\u573a\u666f\u4e2d\u901a\u5e38\u4f1a\u4f7f\u90bb\u5c45\u7279\u5f81\u4e0e\u81ea\u6211\u8f66\u8f86\u7684\u7279\u5f81\u5bf9\u9f50\uff0c\u4f46\u8fd9\u5bb9\u6613\u53d7\u5230\u57df\u95f4\u9699\u566a\u58f0\u7684\u5f71\u54cd\uff0c\u5e76\u4e14\u4e0d\u80fd\u6709\u6548\u89e3\u51b3\u7279\u5f81\u5dee\u5f02\u3002\u6b64\u5916\uff0c\u5b83\u4eec\u91c7\u7528\u57fa\u4e8e Transformer \u7684\u6a21\u5757\u8fdb\u884c\u57df\u9002\u5e94\uff0c\u8fd9\u5bfc\u81f4\u6a21\u578b\u5728\u79fb\u52a8\u8bbe\u5907\u4e0a\u7684\u63a8\u7406\u6548\u7387\u4f4e\u4e0b\u3002", "method": "CoDS \u534f\u540c\u611f\u77e5\u65b9\u6cd5\u5229\u7528\u57df\u5206\u79bb\u6765\u89e3\u51b3\u5f02\u6784\u573a\u666f\u4e2d\u7684\u7279\u5f81\u5dee\u5f02\u3002\u5b83\u91c7\u7528\u4e86\u4e24\u4e2a\u7279\u5f81\u5bf9\u9f50\u6a21\u5757\uff1a\u8f7b\u91cf\u7ea7\u7a7a\u95f4\u901a\u9053\u8c03\u6574\u5668\uff08LSCR\uff09\u548c\u901a\u8fc7\u57df\u5206\u79bb\u7684\u5206\u5e03\u5bf9\u9f50\uff08DADS\uff09\u3002\u6b64\u5916\uff0c\u5b83\u8fd8\u4f7f\u7528\u57df\u5bf9\u9f50\u4e92\u4fe1\u606f\uff08DAMI\uff09\u635f\u5931\u6765\u786e\u4fdd\u6709\u6548\u7684\u7279\u5f81\u5bf9\u9f50\u3002LSCR \u4f7f\u7528\u8f7b\u91cf\u7ea7\u5377\u79ef\u5c42\u5bf9\u7a7a\u95f4\u548c\u901a\u9053\u7ef4\u5ea6\u4e0a\u7684\u90bb\u5c45\u7279\u5f81\u8fdb\u884c\u5bf9\u9f50\u3002DADS \u901a\u8fc7\u7279\u5b9a\u4e8e\u7f16\u7801\u5668\u548c\u72ec\u7acb\u4e8e\u7f16\u7801\u5668\u7684\u57df\u5206\u79bb\u6a21\u5757\u6765\u7f13\u89e3\u7279\u5f81\u5206\u5e03\u5dee\u5f02\u3002\u524d\u8005\u6d88\u9664\u57df\u76f8\u5173\u4fe1\u606f\uff0c\u540e\u8005\u6355\u83b7\u4e0e\u4efb\u52a1\u76f8\u5173\u7684\u4fe1\u606f\u3002\u5728\u8bad\u7ec3\u671f\u95f4\uff0cDAMI \u635f\u5931\u6700\u5927\u5316\u5bf9\u9f50\u7684\u5f02\u6784\u7279\u5f81\u4e4b\u95f4\u7684\u4e92\u4fe1\u606f\uff0c\u4ee5\u589e\u5f3a\u57df\u5206\u79bb\u8fc7\u7a0b\u3002", "result": "CoDS \u6709\u6548\u5730\u7f13\u89e3\u4e86\u5f02\u6784\u573a\u666f\u4e2d\u7684\u7279\u5f81\u5dee\u5f02\uff0c\u5e76\u5728\u68c0\u6d4b\u51c6\u786e\u6027\u548c\u63a8\u7406\u6548\u7387\u4e4b\u95f4\u5b9e\u73b0\u4e86\u6743\u8861\u3002", "conclusion": "CoDS \u65b9\u6cd5\u901a\u8fc7\u521b\u65b0\u7684\u7279\u5f81\u5bf9\u9f50\u6a21\u5757\u548c\u635f\u5931\u51fd\u6570\uff0c\u6210\u529f\u89e3\u51b3\u4e86\u5f02\u6784\u534f\u540c\u611f\u77e5\u4e2d\u7684\u7279\u5f81\u5dee\u5f02\u548c\u63a8\u7406\u6548\u7387\u95ee\u9898\uff0c\u4e3a\u5b9e\u9645\u90e8\u7f72\u63d0\u4f9b\u4e86\u6709\u6548\u7684\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2510.13454", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2510.13454", "abs": "https://arxiv.org/abs/2510.13454", "authors": ["Hyojun Go", "Dominik Narnhofer", "Goutam Bhat", "Prune Truong", "Federico Tombari", "Konrad Schindler"], "title": "VIST3A: Text-to-3D by Stitching a Multi-view Reconstruction Network to a Video Generator", "comment": "Project page: https://gohyojun15.github.io/VIST3A/", "summary": "The rapid progress of large, pretrained models for both visual content\ngeneration and 3D reconstruction opens up new possibilities for text-to-3D\ngeneration. Intuitively, one could obtain a formidable 3D scene generator if\none were able to combine the power of a modern latent text-to-video model as\n\"generator\" with the geometric abilities of a recent (feedforward) 3D\nreconstruction system as \"decoder\". We introduce VIST3A, a general framework\nthat does just that, addressing two main challenges. First, the two components\nmust be joined in a way that preserves the rich knowledge encoded in their\nweights. We revisit model stitching, i.e., we identify the layer in the 3D\ndecoder that best matches the latent representation produced by the\ntext-to-video generator and stitch the two parts together. That operation\nrequires only a small dataset and no labels. Second, the text-to-video\ngenerator must be aligned with the stitched 3D decoder, to ensure that the\ngenerated latents are decodable into consistent, perceptually convincing 3D\nscene geometry. To that end, we adapt direct reward finetuning, a popular\ntechnique for human preference alignment. We evaluate the proposed VIST3A\napproach with different video generators and 3D reconstruction models. All\ntested pairings markedly improve over prior text-to-3D models that output\nGaussian splats. Moreover, by choosing a suitable 3D base model, VIST3A also\nenables high-quality text-to-pointmap generation.", "AI": {"tldr": "VIST3A\u662f\u4e00\u4e2a\u7ed3\u5408\u4e86\u6f5c\u5728\u6587\u672c\u5230\u89c6\u9891\u6a21\u578b\u548c3D\u91cd\u5efa\u7cfb\u7edf\u7684\u901a\u7528\u6846\u67b6\uff0c\u65e8\u5728\u5b9e\u73b0\u6587\u672c\u52303D\u573a\u666f\u751f\u6210\u3002\u5b83\u901a\u8fc7\u6a21\u578b\u62fc\u63a5\u548c\u76f4\u63a5\u5956\u52b1\u5fae\u8c03\u6765\u5e94\u5bf9\u7ec4\u4ef6\u8fde\u63a5\u548c\u5bf9\u9f50\u7684\u6311\u6218\uff0c\u5b9e\u9a8c\u8bc1\u660e\u5176\u5728\u6587\u672c\u52303D\u751f\u6210\u65b9\u9762\u4f18\u4e8e\u73b0\u6709\u6a21\u578b\uff0c\u5e76\u652f\u6301\u9ad8\u8d28\u91cf\u7684\u6587\u672c\u5230\u70b9\u4e91\u751f\u6210\u3002", "motivation": "\u63a2\u7d22\u5982\u4f55\u7ed3\u5408\u5927\u578b\u9884\u8bad\u7ec3\u6a21\u578b\u5728\u89c6\u89c9\u5185\u5bb9\u751f\u6210\u548c3D\u91cd\u5efa\u65b9\u9762\u7684\u8fdb\u5c55\uff0c\u4ee5\u5b9e\u73b0\u6587\u672c\u52303D\u751f\u6210\u3002", "method": "VIST3A\u6846\u67b6\u901a\u8fc7\u4ee5\u4e0b\u4e24\u79cd\u65b9\u5f0f\u7ed3\u5408\u6587\u672c\u5230\u89c6\u9891\u751f\u6210\u5668\u548c3D\u91cd\u5efa\u7cfb\u7edf\uff1a1. \u6a21\u578b\u62fc\u63a5\uff1a\u8bc6\u522b3D\u89e3\u7801\u5668\u4e2d\u4e0e\u6587\u672c\u5230\u89c6\u9891\u751f\u6210\u5668\u4ea7\u751f\u7684\u6f5c\u5728\u8868\u793a\u6700\u5339\u914d\u7684\u5c42\uff0c\u5e76\u5c06\u4e24\u8005\u62fc\u63a5\u5728\u4e00\u8d77\uff0c\u6b64\u8fc7\u7a0b\u4ec5\u9700\u5c11\u91cf\u65e0\u6807\u7b7e\u6570\u636e\u96c6\u30022. \u5bf9\u9f50\uff1a\u91c7\u7528\u76f4\u63a5\u5956\u52b1\u5fae\u8c03\u6280\u672f\uff0c\u4f7f\u751f\u6210\u5668\u4e0e\u62fc\u63a5\u540e\u76843D\u89e3\u7801\u5668\u5bf9\u9f50\uff0c\u786e\u4fdd\u751f\u6210\u7684\u6f5c\u5728\u5185\u5bb9\u53ef\u4ee5\u89e3\u7801\u4e3a\u4e00\u81f4\u4e14\u89c6\u89c9\u903c\u771f\u76843D\u573a\u666f\u51e0\u4f55\u4f53\u3002", "result": "VIST3A\u663e\u8457\u4f18\u4e8e\u4e4b\u524d\u8f93\u51fa\u9ad8\u65af\u6591\u70b9\u7684\u6587\u672c\u52303D\u6a21\u578b\u3002\u901a\u8fc7\u9009\u62e9\u5408\u9002\u76843D\u57fa\u7840\u6a21\u578b\uff0cVIST3A\u8fd8\u80fd\u591f\u5b9e\u73b0\u9ad8\u8d28\u91cf\u7684\u6587\u672c\u5230\u70b9\u4e91\u751f\u6210\u3002", "conclusion": "VIST3A\u6846\u67b6\u6210\u529f\u5730\u7ed3\u5408\u4e86\u6587\u672c\u5230\u89c6\u9891\u751f\u6210\u5668\u548c3D\u91cd\u5efa\u7cfb\u7edf\uff0c\u514b\u670d\u4e86\u7ec4\u4ef6\u8fde\u63a5\u548c\u5bf9\u9f50\u7684\u6311\u6218\uff0c\u5b9e\u73b0\u4e86\u5728\u6587\u672c\u52303D\u751f\u6210\u65b9\u9762\u7684\u663e\u8457\u6539\u8fdb\uff0c\u5e76\u652f\u6301\u9ad8\u8d28\u91cf\u7684\u6587\u672c\u5230\u70b9\u4e91\u751f\u6210\u3002"}}
{"id": "2510.13565", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2510.13565", "abs": "https://arxiv.org/abs/2510.13565", "authors": ["Huawei Sun", "Zixu Wang", "Xiangyuan Peng", "Julius Ott", "Georg Stettinger", "Lorenzo Servadei", "Robert Wille"], "title": "XD-RCDepth: Lightweight Radar-Camera Depth Estimation with Explainability-Aligned and Distribution-Aware Distillation", "comment": "Submitted to ICASSP 2026", "summary": "Depth estimation remains central to autonomous driving, and radar-camera\nfusion offers robustness in adverse conditions by providing complementary\ngeometric cues. In this paper, we present XD-RCDepth, a lightweight\narchitecture that reduces the parameters by 29.7% relative to the\nstate-of-the-art lightweight baseline while maintaining comparable accuracy. To\npreserve performance under compression and enhance interpretability, we\nintroduce two knowledge-distillation strategies: an explainability-aligned\ndistillation that transfers the teacher's saliency structure to the student,\nand a depth-distribution distillation that recasts depth regression as soft\nclassification over discretized bins. Together, these components reduce the MAE\ncompared with direct training with 7.97% and deliver competitive accuracy with\nreal-time efficiency on nuScenes and ZJU-4DRadarCam datasets.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u8f7b\u91cf\u7ea7\u7684\u96f7\u8fbe\u76f8\u673a\u878d\u5408\u6df1\u5ea6\u4f30\u8ba1\u67b6\u6784XD-RCDepth\uff0c\u5728\u4fdd\u6301\u4e0e\u6700\u5148\u8fdb\u7684\u8f7b\u91cf\u7ea7\u57fa\u7ebf\u76f8\u5f53\u7684\u51c6\u786e\u6027\u7684\u540c\u65f6\uff0c\u53c2\u6570\u51cf\u5c11\u4e8629.7%\u3002", "motivation": "\u4e3a\u4e86\u5728\u6076\u52a3\u6761\u4ef6\u4e0b\u63d0\u4f9b\u9c81\u68d2\u7684\u51e0\u4f55\u7ebf\u7d22\uff0c\u96f7\u8fbe\u76f8\u673a\u878d\u5408\u5728\u81ea\u52a8\u9a7e\u9a76\u6df1\u5ea6\u4f30\u8ba1\u4e2d\u81f3\u5173\u91cd\u8981\u3002", "method": "\u672c\u6587\u5f15\u5165\u4e86\u4e24\u79cd\u77e5\u8bc6\u84b8\u998f\u7b56\u7565\uff1a\u53ef\u89e3\u91ca\u6027\u5bf9\u9f50\u84b8\u998f\u548c\u6df1\u5ea6\u5206\u5e03\u84b8\u998f\u3002\u53ef\u89e3\u91ca\u6027\u5bf9\u9f50\u84b8\u998f\u5c06\u6559\u5e08\u6a21\u578b\u7684\u663e\u8457\u6027\u7ed3\u6784\u8f6c\u79fb\u5230\u5b66\u751f\u6a21\u578b\uff0c\u800c\u6df1\u5ea6\u5206\u5e03\u84b8\u998f\u5c06\u6df1\u5ea6\u56de\u5f52\u8f6c\u6362\u4e3a\u79bb\u6563\u5316\u7684\u8f6f\u5206\u7c7b\u3002", "result": "\u4e0e\u76f4\u63a5\u8bad\u7ec3\u76f8\u6bd4\uff0c\u672c\u6587\u63d0\u51fa\u7684\u65b9\u6cd5\u5c06\u5e73\u5747\u7edd\u5bf9\u8bef\u5dee\uff08MAE\uff09\u964d\u4f4e\u4e867.97%\uff0c\u5e76\u5728nuScenes\u548cZJU-4DRadarCam\u6570\u636e\u96c6\u4e0a\u5b9e\u73b0\u4e86\u5177\u6709\u5b9e\u65f6\u6548\u7387\u7684\u7ade\u4e89\u6027\u7cbe\u5ea6\u3002", "conclusion": "XD-RCDepth\u5728\u53c2\u6570\u91cf\u663e\u8457\u51cf\u5c11\u7684\u60c5\u51b5\u4e0b\uff0c\u901a\u8fc7\u5f15\u5165\u4e24\u79cd\u77e5\u8bc6\u84b8\u998f\u7b56\u7565\uff0c\u5728\u96f7\u8fbe\u76f8\u673a\u878d\u5408\u6df1\u5ea6\u4f30\u8ba1\u4efb\u52a1\u4e2d\u53d6\u5f97\u4e86\u4e0e\u73b0\u6709\u6280\u672f\u76f8\u5f53\u7684\u6027\u80fd\uff0c\u5e76\u63d0\u9ad8\u4e86\u6a21\u578b\u7684\u89e3\u91ca\u6027\u3002"}}
{"id": "2510.13630", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2510.13630", "abs": "https://arxiv.org/abs/2510.13630", "authors": ["Amjid Ali", "Zulfiqar Ahmad Khan", "Altaf Hussain", "Muhammad Munsif", "Adnan Hussain", "Sung Wook Baik"], "title": "AVAR-Net: A Lightweight Audio-Visual Anomaly Recognition Framework with a Benchmark Dataset", "comment": null, "summary": "Anomaly recognition plays a vital role in surveillance, transportation,\nhealthcare, and public safety. However, most existing approaches rely solely on\nvisual data, making them unreliable under challenging conditions such as\nocclusion, low illumination, and adverse weather. Moreover, the absence of\nlarge-scale synchronized audio-visual datasets has hindered progress in\nmultimodal anomaly recognition. To address these limitations, this study\npresents AVAR-Net, a lightweight and efficient audio-visual anomaly recognition\nframework designed for real-world environments. AVAR-Net consists of four main\nmodules: an audio feature extractor, a video feature extractor, fusion\nstrategy, and a sequential pattern learning network that models cross-modal\nrelationships for anomaly recognition. Specifically, the Wav2Vec2 model\nextracts robust temporal features from raw audio, while MobileViT captures both\nlocal and global visual representations from video frames. An early fusion\nmechanism combines these modalities, and a Multi-Stage Temporal Convolutional\nNetwork (MTCN) model that learns long-range temporal dependencies within the\nfused representation, enabling robust spatiotemporal reasoning. A novel\nVisual-Audio Anomaly Recognition (VAAR) dataset, is also introduced, serving as\na medium-scale benchmark containing 3,000 real-world videos with synchronized\naudio across ten diverse anomaly classes. Experimental evaluations demonstrate\nthat AVAR-Net achieves 89.29% accuracy on VAAR and 88.56% Average Precision on\nthe XD-Violence dataset, improving Average Precision by 2.8% over existing\nstate-of-the-art methods. These results highlight the effectiveness,\nefficiency, and generalization capability of the proposed framework, as well as\nthe utility of VAAR as a benchmark for advancing multimodal anomaly recognition\nresearch.", "AI": {"tldr": "\u8be5\u6587\u7ae0\u4ecb\u7d39\u4e86AVAR-Net\uff0c\u4e00\u500b\u8f15\u91cf\u9ad8\u6548\u7684\u97f3\u983b-\u8996\u89ba\u7570\u5e38\u8b58\u5225\u6846\u67b6\uff0c\u4e26\u5f15\u5165\u4e86VAAR\u6578\u64da\u96c6\uff0c\u7528\u65bc\u89e3\u6c7a\u73fe\u6709\u65b9\u6cd5\u5728\u6311\u6230\u6027\u689d\u4ef6\u4e0b\u7684\u4e0d\u53ef\u9760\u6027\u4ee5\u53ca\u591a\u6a21\u614b\u7570\u5e38\u8b58\u5225\u4e2d\u7f3a\u4e4f\u5927\u898f\u6a21\u540c\u6b65\u97f3\u983b-\u8996\u89ba\u6578\u64da\u96c6\u7684\u554f\u984c\u3002", "motivation": "\u73fe\u6709\u7684\u7570\u5e38\u8b58\u5225\u65b9\u6cd5\u4e3b\u8981\u4f9d\u8cf4\u8996\u89ba\u6578\u64da\uff0c\u5728\u906e\u64cb\u3001\u4f4e\u5149\u7167\u548c\u60e1\u52a3\u5929\u6c23\u7b49\u6311\u6230\u6027\u689d\u4ef6\u4e0b\u4e0d\u53ef\u9760\u3002\u6b64\u5916\uff0c\u7f3a\u4e4f\u5927\u898f\u6a21\u540c\u6b65\u97f3\u983b-\u8996\u89ba\u6578\u64da\u96c6\u963b\u7919\u4e86\u591a\u6a21\u614b\u7570\u5e38\u8b58\u5225\u7684\u9032\u5c55\u3002", "method": "AVAR-Net\u6846\u67b6\u5305\u542b\u56db\u500b\u4e3b\u8981\u6a21\u584a\uff1a\u97f3\u983b\u7279\u5fb5\u63d0\u53d6\u5668\uff08\u4f7f\u7528Wav2Vec2\uff09\u3001\u8996\u983b\u7279\u5fb5\u63d0\u53d6\u5668\uff08\u4f7f\u7528MobileViT\uff09\u3001\u878d\u5408\u7b56\u7565\uff08\u65e9\u671f\u878d\u5408\uff09\u548c\u5e8f\u5217\u6a21\u5f0f\u5b78\u7fd2\u7db2\u7d61\uff08\u591a\u968e\u6bb5\u6642\u9593\u5377\u7a4d\u7db2\u7d61MTCN\uff09\u3002\u6587\u7ae0\u9084\u5f15\u5165\u4e86VAAR\u6578\u64da\u96c6\uff0c\u5305\u542b3,000\u500b\u771f\u5be6\u4e16\u754c\u7684\u8996\u983b\u548c\u540c\u6b65\u97f3\u983b\u3002", "result": "AVAR-Net\u5728VAAR\u6578\u64da\u96c6\u4e0a\u9054\u5230\u4e8689.29%\u7684\u6e96\u78ba\u7387\uff0c\u5728XD-Violence\u6578\u64da\u96c6\u4e0a\u5e73\u5747\u7cbe\u5ea6\u9054\u523088.56%\uff0c\u6bd4\u73fe\u6709\u6700\u5148\u9032\u65b9\u6cd5\u63d0\u9ad8\u4e862.8%\u3002", "conclusion": "AVAR-Net\u6846\u67b6\u5728\u6709\u6548\u6027\u3001\u6548\u7387\u548c\u6cdb\u5316\u80fd\u529b\u65b9\u9762\u8868\u73fe\u51fa\u8272\uff0c\u4e26\u4e14VAAR\u6578\u64da\u96c6\u70ba\u591a\u6a21\u614b\u7570\u5e38\u8b58\u5225\u7814\u7a76\u63d0\u4f9b\u4e86\u6709\u7528\u7684\u57fa\u6e96\u3002"}}
{"id": "2510.13652", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2510.13652", "abs": "https://arxiv.org/abs/2510.13652", "authors": ["Huaizhi Qu", "Ruichen Zhang", "Shuqing Luo", "Luchao Qi", "Zhihao Zhang", "Xiaoming Liu", "Roni Sengupta", "Tianlong Chen"], "title": "EditCast3D: Single-Frame-Guided 3D Editing with Video Propagation and View Selection", "comment": null, "summary": "Recent advances in foundation models have driven remarkable progress in image\nediting, yet their extension to 3D editing remains underexplored. A natural\napproach is to replace the image editing modules in existing workflows with\nfoundation models. However, their heavy computational demands and the\nrestrictions and costs of closed-source APIs make plugging these models into\nexisting iterative editing strategies impractical. To address this limitation,\nwe propose EditCast3D, a pipeline that employs video generation foundation\nmodels to propagate edits from a single first frame across the entire dataset\nprior to reconstruction. While editing propagation enables dataset-level\nediting via video models, its consistency remains suboptimal for 3D\nreconstruction, where multi-view alignment is essential. To overcome this,\nEditCast3D introduces a view selection strategy that explicitly identifies\nconsistent and reconstruction-friendly views and adopts feedforward\nreconstruction without requiring costly refinement. In combination, the\npipeline both minimizes reliance on expensive image editing and mitigates\nprompt ambiguities that arise when applying foundation models independently\nacross images. We evaluate EditCast3D on commonly used 3D editing datasets and\ncompare it against state-of-the-art 3D editing baselines, demonstrating\nsuperior editing quality and high efficiency. These results establish\nEditCast3D as a scalable and general paradigm for integrating foundation models\ninto 3D editing pipelines. The code is available at\nhttps://github.com/UNITES-Lab/EditCast3D", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86EditCast3D\uff0c\u4e00\u4e2a\u5229\u7528\u89c6\u9891\u751f\u6210\u57fa\u7840\u6a21\u578b\uff0c\u901a\u8fc7\u4ece\u5355\u4e2a\u5e27\u4f20\u64ad\u7f16\u8f91\u5230\u6574\u4e2a\u6570\u636e\u96c6\uff0c\u4ece\u800c\u5b9e\u73b0\u9ad8\u6548\u548c\u9ad8\u8d28\u91cf3D\u7f16\u8f91\u7684\u6d41\u6c34\u7ebf\u3002", "motivation": "\u76ee\u524d\uff0c\u73b0\u6709\u57fa\u7840\u6a21\u578b\u57283D\u7f16\u8f91\u9886\u57df\u7684\u5e94\u7528\u8fd8\u672a\u88ab\u5145\u5206\u63a2\u7d22\uff0c\u4e14\u5c06\u8fd9\u4e9b\u6a21\u578b\u76f4\u63a5\u96c6\u6210\u5230\u73b0\u6709\u7684\u8fed\u4ee3\u7f16\u8f91\u7b56\u7565\u4e2d\u5b58\u5728\u8ba1\u7b97\u6210\u672c\u9ad8\u6602\u548cAPI\u9650\u5236\u7b49\u95ee\u9898\u3002", "method": "EditCast3D\u901a\u8fc7\u4f7f\u7528\u89c6\u9891\u751f\u6210\u57fa\u7840\u6a21\u578b\u5c06\u7f16\u8f91\u4ece\u5355\u4e2a\u9996\u5e27\u4f20\u64ad\u5230\u6574\u4e2a\u6570\u636e\u96c6\uff0c\u5e76\u5728\u91cd\u5efa\u4e4b\u524d\u5b8c\u6210\u6b64\u6b65\u9aa4\u3002\u4e3a\u4e86\u89e3\u51b33D\u91cd\u5efa\u4e2d\u591a\u89c6\u89d2\u5bf9\u9f50\u7684\u4e00\u81f4\u6027\u95ee\u9898\uff0cEditCast3D\u5f15\u5165\u4e86\u89c6\u89d2\u9009\u62e9\u7b56\u7565\u6765\u8bc6\u522b\u4e00\u81f4\u4e14\u6709\u5229\u4e8e\u91cd\u5efa\u7684\u89c6\u89d2\uff0c\u5e76\u91c7\u7528\u524d\u9988\u91cd\u5efa\uff0c\u907f\u514d\u4e86\u6602\u8d35\u7684\u4f18\u5316\u8fc7\u7a0b\u3002", "result": "EditCast3D\u5728\u5e38\u7528\u76843D\u7f16\u8f91\u6570\u636e\u96c6\u4e0a\u8fdb\u884c\u4e86\u8bc4\u4f30\uff0c\u5e76\u4e0e\u6700\u5148\u8fdb\u76843D\u7f16\u8f91\u57fa\u7ebf\u8fdb\u884c\u4e86\u6bd4\u8f83\uff0c\u7ed3\u679c\u8868\u660e\u5176\u5728\u7f16\u8f91\u8d28\u91cf\u548c\u6548\u7387\u65b9\u9762\u90fd\u8868\u73b0\u51fa\u4e86\u5353\u8d8a\u7684\u6027\u80fd\u3002", "conclusion": "EditCast3D\u4e3a\u5c06\u57fa\u7840\u6a21\u578b\u96c6\u6210\u52303D\u7f16\u8f91\u6d41\u6c34\u7ebf\u63d0\u4f9b\u4e86\u4e00\u4e2a\u53ef\u6269\u5c55\u4e14\u901a\u7528\u7684\u8303\u5f0f\u3002"}}
{"id": "2510.13660", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2510.13660", "abs": "https://arxiv.org/abs/2510.13660", "authors": ["Hongyu Qu", "Jianan Wei", "Xiangbo Shu", "Yazhou Yao", "Wenguan Wang", "Jinhui Tang"], "title": "OmniGaze: Reward-inspired Generalizable Gaze Estimation In The Wild", "comment": "Accepted to NeurIPS 2025; Project page:\n  \\url{https://github.com/quhongyu/OmniGaze}", "summary": "Current 3D gaze estimation methods struggle to generalize across diverse data\ndomains, primarily due to i) the scarcity of annotated datasets, and ii) the\ninsufficient diversity of labeled data. In this work, we present OmniGaze, a\nsemi-supervised framework for 3D gaze estimation, which utilizes large-scale\nunlabeled data collected from diverse and unconstrained real-world environments\nto mitigate domain bias and generalize gaze estimation in the wild. First, we\nbuild a diverse collection of unlabeled facial images, varying in facial\nappearances, background environments, illumination conditions, head poses, and\neye occlusions. In order to leverage unlabeled data spanning a broader\ndistribution, OmniGaze adopts a standard pseudo-labeling strategy and devises a\nreward model to assess the reliability of pseudo labels. Beyond pseudo labels\nas 3D direction vectors, the reward model also incorporates visual embeddings\nextracted by an off-the-shelf visual encoder and semantic cues from gaze\nperspective generated by prompting a Multimodal Large Language Model to compute\nconfidence scores. Then, these scores are utilized to select high-quality\npseudo labels and weight them for loss computation. Extensive experiments\ndemonstrate that OmniGaze achieves state-of-the-art performance on five\ndatasets under both in-domain and cross-domain settings. Furthermore, we also\nevaluate the efficacy of OmniGaze as a scalable data engine for gaze\nestimation, which exhibits robust zero-shot generalization on four unseen\ndatasets.", "AI": {"tldr": "OmniGaze\u662f\u4e00\u4e2a\u534a\u76d1\u7763\u76843D\u6ce8\u89c6\u4f30\u8ba1\u6846\u67b6\uff0c\u5b83\u5229\u7528\u5927\u89c4\u6a21\u672a\u6807\u8bb0\u6570\u636e\u6765\u7f13\u89e3\u57df\u504f\u5dee\u5e76\u5728\u91ce\u5916\u6cdb\u5316\u6ce8\u89c6\u4f30\u8ba1\u3002", "motivation": "\u5f53\u524d3D\u6ce8\u89c6\u4f30\u8ba1\u65b9\u6cd5\u96be\u4ee5\u5728\u4e0d\u540c\u6570\u636e\u57df\u4e4b\u95f4\u8fdb\u884c\u6cdb\u5316\uff0c\u4e3b\u8981\u662f\u7531\u4e8ei)\u5e26\u6ce8\u91ca\u6570\u636e\u96c6\u7684\u7a00\u7f3a\u6027\uff0c\u4ee5\u53caii)\u6807\u8bb0\u6570\u636e\u591a\u6837\u6027\u4e0d\u8db3\u3002", "method": "\u6211\u4eec\u6784\u5efa\u4e86\u4e00\u4e2a\u591a\u6837\u5316\u7684\u672a\u6807\u6ce8\u4eba\u8138\u56fe\u50cf\u96c6\u3002OmniGaze\u91c7\u7528\u6807\u51c6\u7684\u4f2a\u6807\u8bb0\u7b56\u7565\uff0c\u5e76\u8bbe\u8ba1\u4e86\u4e00\u4e2a\u5956\u52b1\u6a21\u578b\u6765\u8bc4\u4f30\u4f2a\u6807\u8bb0\u7684\u53ef\u9760\u6027\u3002\u5956\u52b1\u6a21\u578b\u5c06\u89c6\u89c9\u5d4c\u5165\u548c\u6765\u81ea\u6ce8\u89c6\u89c6\u89d2\u7684\u8bed\u4e49\u7ebf\u7d22\u7ed3\u5408\u8d77\u6765\u8ba1\u7b97\u7f6e\u4fe1\u5ea6\u5206\u6570\uff0c\u7136\u540e\u5229\u7528\u8fd9\u4e9b\u5206\u6570\u9009\u62e9\u9ad8\u8d28\u91cf\u7684\u4f2a\u6807\u8bb0\u5e76\u5bf9\u5176\u8fdb\u884c\u52a0\u6743\u4ee5\u8fdb\u884c\u635f\u5931\u8ba1\u7b97\u3002", "result": "OmniGaze\u5728\u57df\u5185\u548c\u8de8\u57df\u8bbe\u7f6e\u4e0b\u7684\u4e94\u4e2a\u6570\u636e\u96c6\u4e0a\u5747\u8fbe\u5230\u4e86\u6700\u5148\u8fdb\u7684\u6027\u80fd\u3002OmniGaze\u4f5c\u4e3a\u6ce8\u89c6\u4f30\u8ba1\u7684\u53ef\u6269\u5c55\u6570\u636e\u5f15\u64ce\uff0c\u5728\u56db\u4e2a\u672a\u89c1\u8fc7\u7684\u6570\u636e\u96c6\u4e0a\u8868\u73b0\u51fa\u5f3a\u5927\u7684\u96f6\u6837\u672c\u6cdb\u5316\u80fd\u529b\u3002", "conclusion": "OmniGaze\u662f\u4e00\u79cd\u65b0\u9896\u7684\u534a\u76d1\u7763\u5b66\u4e60\u6846\u67b6\uff0c\u901a\u8fc7\u6709\u6548\u5229\u7528\u5927\u89c4\u6a21\u672a\u6807\u8bb0\u6570\u636e\uff0c\u663e\u8457\u63d0\u5347\u4e863D\u6ce8\u89c6\u4f30\u8ba1\u5728\u771f\u5b9e\u4e16\u754c\u573a\u666f\u4e2d\u7684\u6cdb\u5316\u80fd\u529b\u548c\u51c6\u786e\u6027\u3002"}}
{"id": "2510.13675", "categories": ["cs.CV", "cs.LG"], "pdf": "https://arxiv.org/pdf/2510.13675", "abs": "https://arxiv.org/abs/2510.13675", "authors": ["Hongkuan Zhou", "Lavdim Halilaj", "Sebastian Monka", "Stefan Schmid", "Yuqicheng Zhu", "Jingcheng Wu", "Nadeem Nazer", "Steffen Staab"], "title": "Seeing and Knowing in the Wild: Open-domain Visual Entity Recognition with Large-scale Knowledge Graphs via Contrastive Learning", "comment": null, "summary": "Open-domain visual entity recognition aims to identify and link entities\ndepicted in images to a vast and evolving set of real-world concepts, such as\nthose found in Wikidata. Unlike conventional classification tasks with fixed\nlabel sets, it operates under open-set conditions, where most target entities\nare unseen during training and exhibit long-tail distributions. This makes the\ntask inherently challenging due to limited supervision, high visual ambiguity,\nand the need for semantic disambiguation. In this work, we propose a\nKnowledge-guided Contrastive Learning (KnowCoL) framework that combines both\nimages and text descriptions into a shared semantic space grounded by\nstructured information from Wikidata. By abstracting visual and textual inputs\nto a conceptual level, the model leverages entity descriptions, type\nhierarchies, and relational context to support zero-shot entity recognition. We\nevaluate our approach on the OVEN benchmark, a large-scale open-domain visual\nrecognition dataset with Wikidata IDs as the label space. Our experiments show\nthat using visual, textual, and structured knowledge greatly improves accuracy,\nespecially for rare and unseen entities. Our smallest model improves the\naccuracy on unseen entities by 10.5% compared to the state-of-the-art, despite\nbeing 35 times smaller.", "AI": {"tldr": "\u8fd9\u7bc7\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3aKnowCoL\u7684\u77e5\u8bc6\u5f15\u5bfc\u5bf9\u6bd4\u5b66\u4e60\u6846\u67b6\uff0c\u7528\u4e8e\u5f00\u653e\u57df\u89c6\u89c9\u5b9e\u4f53\u8bc6\u522b\u3002\u8be5\u6846\u67b6\u5c06\u56fe\u50cf\u548c\u6587\u672c\u63cf\u8ff0\u7ed3\u5408\u5230\u5171\u4eab\u8bed\u4e49\u7a7a\u95f4\u4e2d\uff0c\u5e76\u5229\u7528\u7ef4\u57fa\u6570\u636e\u4e2d\u7684\u7ed3\u6784\u5316\u4fe1\u606f\uff0c\u663e\u8457\u63d0\u9ad8\u4e86\u5bf9\u7a00\u6709\u548c\u672a\u89c1\u5b9e\u4f53\u7684\u8bc6\u522b\u51c6\u786e\u6027\uff0c\u5c24\u5176\u662f\u5728\u5904\u7406\u672a\u89c1\u5b9e\u4f53\u65f6\uff0c\u76f8\u6bd4\u73b0\u6709\u6280\u672f\uff0c\u51c6\u786e\u7387\u63d0\u5347\u4e8610.5%\u3002", "motivation": "\u5f00\u653e\u57df\u89c6\u89c9\u5b9e\u4f53\u8bc6\u522b\u9762\u4e34\u6311\u6218\uff0c\u56e0\u4e3a\u5b9e\u4f53\u6570\u91cf\u5e9e\u5927\u4e14\u4e0d\u65ad\u6f14\u53d8\uff0c\u5927\u591a\u5728\u8bad\u7ec3\u4e2d\u672a\u88ab\u89c1\u5230\uff0c\u5e76\u4e14\u5448\u73b0\u957f\u5c3e\u5206\u5e03\uff0c\u5bfc\u81f4\u76d1\u7763\u4fe1\u606f\u6709\u9650\u3001\u89c6\u89c9\u6a21\u7cca\u6027\u9ad8\u4ee5\u53ca\u9700\u8981\u8bed\u4e49\u6d88\u6b67\u3002", "method": "\u672c\u7814\u7a76\u63d0\u51fa\u4e86\u77e5\u8bc6\u5f15\u5bfc\u5bf9\u6bd4\u5b66\u4e60\uff08KnowCoL\uff09\u6846\u67b6\uff0c\u8be5\u6846\u67b6\u5c06\u56fe\u50cf\u548c\u6587\u672c\u63cf\u8ff0\u6574\u5408\u5230\u7531\u7ef4\u57fa\u6570\u636e\u7ed3\u6784\u5316\u4fe1\u606f\u652f\u6301\u7684\u5171\u4eab\u8bed\u4e49\u7a7a\u95f4\u4e2d\u3002\u901a\u8fc7\u5c06\u89c6\u89c9\u548c\u6587\u672c\u8f93\u5165\u62bd\u8c61\u5230\u6982\u5ff5\u5c42\u9762\uff0c\u6a21\u578b\u5229\u7528\u5b9e\u4f53\u63cf\u8ff0\u3001\u7c7b\u578b\u5c42\u7ea7\u548c\u5173\u7cfb\u4e0a\u4e0b\u6587\u6765\u652f\u6301\u96f6\u6837\u672c\u5b9e\u4f53\u8bc6\u522b\u3002", "result": "\u5728OVEN\u57fa\u51c6\u6d4b\u8bd5\u96c6\u4e0a\u7684\u5b9e\u9a8c\u8868\u660e\uff0c\u7ed3\u5408\u89c6\u89c9\u3001\u6587\u672c\u548c\u7ed3\u6784\u5316\u77e5\u8bc6\u53ef\u4ee5\u663e\u8457\u63d0\u9ad8\u51c6\u786e\u6027\uff0c\u7279\u522b\u662f\u5bf9\u4e8e\u7a00\u6709\u548c\u672a\u89c1\u5b9e\u4f53\u3002\u6211\u4eec\u6700\u5c0f\u7684\u6a21\u578b\u5728\u672a\u89c1\u5b9e\u4f53\u4e0a\u7684\u51c6\u786e\u7387\u6bd4\u73b0\u6709\u6280\u672f\u63d0\u9ad8\u4e8610.5%\uff0c\u5c3d\u7ba1\u5176\u6a21\u578b\u4f53\u79ef\u5c0f\u4e8635\u500d\u3002", "conclusion": "KnowCoL\u6846\u67b6\u901a\u8fc7\u6574\u5408\u591a\u6a21\u6001\u548c\u7ed3\u6784\u5316\u77e5\u8bc6\uff0c\u6709\u6548\u89e3\u51b3\u4e86\u5f00\u653e\u57df\u89c6\u89c9\u5b9e\u4f53\u8bc6\u522b\u4e2d\u7684\u6311\u6218\uff0c\u5c24\u5176\u5728\u5904\u7406\u7a00\u6709\u548c\u672a\u89c1\u5b9e\u4f53\u65b9\u9762\u8868\u73b0\u51fa\u8272\uff0c\u4e3a\u672a\u6765\u7684\u7814\u7a76\u63d0\u4f9b\u4e86\u65b0\u7684\u65b9\u5411\u3002"}}
{"id": "2510.13678", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2510.13678", "abs": "https://arxiv.org/abs/2510.13678", "authors": ["Xinyang Li", "Tengfei Wang", "Zixiao Gu", "Shengchuan Zhang", "Chunchao Guo", "Liujuan Cao"], "title": "FlashWorld: High-quality 3D Scene Generation within Seconds", "comment": "Project Page: https://imlixinyang.github.io/FlashWorld-Project-Page/", "summary": "We propose FlashWorld, a generative model that produces 3D scenes from a\nsingle image or text prompt in seconds, 10~100$\\times$ faster than previous\nworks while possessing superior rendering quality. Our approach shifts from the\nconventional multi-view-oriented (MV-oriented) paradigm, which generates\nmulti-view images for subsequent 3D reconstruction, to a 3D-oriented approach\nwhere the model directly produces 3D Gaussian representations during multi-view\ngeneration. While ensuring 3D consistency, 3D-oriented method typically suffers\npoor visual quality. FlashWorld includes a dual-mode pre-training phase\nfollowed by a cross-mode post-training phase, effectively integrating the\nstrengths of both paradigms. Specifically, leveraging the prior from a video\ndiffusion model, we first pre-train a dual-mode multi-view diffusion model,\nwhich jointly supports MV-oriented and 3D-oriented generation modes. To bridge\nthe quality gap in 3D-oriented generation, we further propose a cross-mode\npost-training distillation by matching distribution from consistent 3D-oriented\nmode to high-quality MV-oriented mode. This not only enhances visual quality\nwhile maintaining 3D consistency, but also reduces the required denoising steps\nfor inference. Also, we propose a strategy to leverage massive single-view\nimages and text prompts during this process to enhance the model's\ngeneralization to out-of-distribution inputs. Extensive experiments demonstrate\nthe superiority and efficiency of our method.", "AI": {"tldr": "FlashWorld\u662f\u4e00\u79cd\u65b0\u9896\u7684\u751f\u6210\u6a21\u578b\uff0c\u80fd\u591f\u4ee5\u6bd4\u73b0\u6709\u65b9\u6cd5\u5feb10-100\u500d\u7684\u901f\u5ea6\uff0c\u4ece\u5355\u4e00\u56fe\u50cf\u6216\u6587\u672c\u63d0\u793a\u4e2d\u5728\u51e0\u79d2\u949f\u5185\u751f\u6210\u9ad8\u8d28\u91cf\u76843D\u573a\u666f\u3002", "motivation": "\u4f20\u7edf\u76843D\u573a\u666f\u751f\u6210\u65b9\u6cd5\u901a\u5e38\u91c7\u7528\u591a\u89c6\u89d2\u5bfc\u5411\uff08MV-oriented\uff09\u8303\u5f0f\uff0c\u5373\u5148\u751f\u6210\u591a\u89c6\u89d2\u56fe\u50cf\u518d\u8fdb\u884c3D\u91cd\u5efa\uff0c\u4f46\u8fd9\u79cd\u65b9\u6cd5\u901f\u5ea6\u8f83\u6162\u3002\u800c3D\u5bfc\u5411\u65b9\u6cd5\u867d\u7136\u80fd\u4fdd\u8bc13D\u4e00\u81f4\u6027\uff0c\u4f46\u89c6\u89c9\u8d28\u91cf\u8f83\u5dee\u3002\u56e0\u6b64\uff0c\u9700\u8981\u4e00\u79cd\u65b0\u7684\u65b9\u6cd5\u6765\u89e3\u51b3\u73b0\u6709\u65b9\u6cd5\u7684\u6548\u7387\u548c\u8d28\u91cf\u95ee\u9898\u3002", "method": "FlashWorld\u901a\u8fc7\u53cc\u6a21\u5f0f\u9884\u8bad\u7ec3\u548c\u8de8\u6a21\u5f0f\u540e\u8bad\u7ec3\u9636\u6bb5\uff0c\u6709\u6548\u5730\u7ed3\u5408\u4e86\u591a\u89c6\u89d2\u5bfc\u5411\u548c3D\u5bfc\u5411\u4e24\u79cd\u8303\u5f0f\u7684\u4f18\u70b9\u3002\u9996\u5148\uff0c\u5229\u7528\u89c6\u9891\u6269\u6563\u6a21\u578b\u7684\u5148\u9a8c\u77e5\u8bc6\uff0c\u9884\u8bad\u7ec3\u4e00\u4e2a\u53cc\u6a21\u5f0f\u591a\u89c6\u89d2\u6269\u6563\u6a21\u578b\uff0c\u652f\u6301\u591a\u89c6\u89d2\u5bfc\u5411\u548c3D\u5bfc\u5411\u4e24\u79cd\u751f\u6210\u6a21\u5f0f\u3002\u4e3a\u4e86\u5f25\u88653D\u5bfc\u5411\u751f\u6210\u5728\u89c6\u89c9\u8d28\u91cf\u4e0a\u7684\u4e0d\u8db3\uff0c\u8fdb\u4e00\u6b65\u63d0\u51fa\u4e86\u8de8\u6a21\u5f0f\u540e\u8bad\u7ec3\u84b8\u998f\uff0c\u901a\u8fc7\u5c06\u4e00\u81f4\u76843D\u5bfc\u5411\u6a21\u5f0f\u4e0e\u9ad8\u8d28\u91cf\u7684\u591a\u89c6\u89d2\u5bfc\u5411\u6a21\u5f0f\u7684\u5206\u5e03\u8fdb\u884c\u5339\u914d\u3002\u540c\u65f6\uff0c\u8be5\u65b9\u6cd5\u8fd8\u5229\u7528\u5927\u91cf\u7684\u5355\u89c6\u89d2\u56fe\u50cf\u548c\u6587\u672c\u63d0\u793a\u6765\u589e\u5f3a\u6a21\u578b\u5bf9\u5206\u5e03\u5916\u8f93\u5165\u7684\u6cdb\u5316\u80fd\u529b\u3002", "result": "FlashWorld\u5728\u4fdd\u63013D\u4e00\u81f4\u6027\u7684\u540c\u65f6\uff0c\u663e\u8457\u63d0\u5347\u4e86\u89c6\u89c9\u8d28\u91cf\uff0c\u5e76\u4e14\u51cf\u5c11\u4e86\u63a8\u7406\u6240\u9700\u7684\u53bb\u566a\u6b65\u9aa4\u3002\u5b9e\u9a8c\u8bc1\u660e\uff0c\u8be5\u65b9\u6cd5\u5728\u901f\u5ea6\u548c\u6e32\u67d3\u8d28\u91cf\u65b9\u9762\u5747\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\uff0c\u751f\u62103D\u573a\u666f\u7684\u901f\u5ea6\u6bd4\u4ee5\u5f80\u5de5\u4f5c\u5feb10-100\u500d\u3002", "conclusion": "FlashWorld\u901a\u8fc7\u521b\u65b0\u7684\u53cc\u6a21\u5f0f\u9884\u8bad\u7ec3\u548c\u8de8\u6a21\u5f0f\u540e\u8bad\u7ec3\u65b9\u6cd5\uff0c\u6210\u529f\u5730\u5c06\u591a\u89c6\u89d2\u5bfc\u5411\u548c3D\u5bfc\u5411\u7684\u4f18\u70b9\u7ed3\u5408\u8d77\u6765\uff0c\u6781\u5927\u5730\u63d0\u9ad8\u4e863D\u573a\u666f\u751f\u6210\u7684\u6548\u7387\u548c\u8d28\u91cf\uff0c\u4e3a\u4ece\u5355\u4e00\u56fe\u50cf\u6216\u6587\u672c\u751f\u6210\u9ad8\u8d28\u91cf3D\u573a\u666f\u63d0\u4f9b\u4e86\u4e00\u4e2a\u5f3a\u5927\u7684\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2510.13698", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2510.13698", "abs": "https://arxiv.org/abs/2510.13698", "authors": ["Jonghyun Park", "Minhyuk Seo", "Jonghyun Choi"], "title": "Risk-adaptive Activation Steering for Safe Multimodal Large Language Models", "comment": null, "summary": "One of the key challenges of modern AI models is ensuring that they provide\nhelpful responses to benign queries while refusing malicious ones. But often,\nthe models are vulnerable to multimodal queries with harmful intent embedded in\nimages. One approach for safety alignment is training with extensive safety\ndatasets at the significant costs in both dataset curation and training.\nInference-time alignment mitigates these costs, but introduces two drawbacks:\nexcessive refusals from misclassified benign queries and slower inference speed\ndue to iterative output adjustments. To overcome these limitations, we propose\nto reformulate queries to strengthen cross-modal attention to safety-critical\nimage regions, enabling accurate risk assessment at the query level. Using the\nassessed risk, it adaptively steers activations to generate responses that are\nsafe and helpful without overhead from iterative output adjustments. We call\nthis Risk-adaptive Activation Steering (RAS). Extensive experiments across\nmultiple benchmarks on multimodal safety and utility demonstrate that the RAS\nsignificantly reduces attack success rates, preserves general task performance,\nand improves inference speed over prior inference-time defenses.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3aRAS\uff08Risk-adaptive Activation Steering\uff09\u7684\u65b9\u6cd5\uff0c\u65e8\u5728\u89e3\u51b3\u591a\u6a21\u6001AI\u6a21\u578b\u5728\u5b89\u5168\u6027\u548c\u5b9e\u7528\u6027\u65b9\u9762\u7684\u6311\u6218\u3002RAS\u901a\u8fc7\u91cd\u65b0\u6784\u5efa\u67e5\u8be2\uff0c\u5f3a\u5316\u5bf9\u5b89\u5168\u5173\u952e\u56fe\u50cf\u533a\u57df\u7684\u8de8\u6a21\u6001\u5173\u6ce8\uff0c\u4ece\u800c\u5b9e\u73b0\u51c6\u786e\u7684\u67e5\u8be2\u7ea7\u522b\u98ce\u9669\u8bc4\u4f30\uff0c\u5e76\u6839\u636e\u8bc4\u4f30\u7ed3\u679c\u81ea\u9002\u5e94\u5730\u5f15\u5bfc\u6fc0\u6d3b\uff0c\u751f\u6210\u5b89\u5168\u4e14\u6709\u7528\u7684\u54cd\u5e94\uff0c\u4e14\u65e0\u9700\u8fed\u4ee3\u8f93\u51fa\u8c03\u6574\u7684\u5f00\u9500\u3002", "motivation": "\u73b0\u4ee3AI\u6a21\u578b\u9762\u4e34\u7684\u5173\u952e\u6311\u6218\u662f\u5982\u4f55\u786e\u4fdd\u5b83\u4eec\u5bf9\u826f\u6027\u67e5\u8be2\u63d0\u4f9b\u6709\u76ca\u7684\u56de\u590d\uff0c\u540c\u65f6\u62d2\u7edd\u6076\u610f\u67e5\u8be2\u3002\u7136\u800c\uff0c\u6a21\u578b\u901a\u5e38\u5bb9\u6613\u53d7\u5230\u56fe\u50cf\u4e2d\u5d4c\u5165\u6709\u5bb3\u610f\u56fe\u7684\u591a\u6a21\u6001\u67e5\u8be2\u7684\u653b\u51fb\u3002\u73b0\u6709\u7684\u5b89\u5168\u5bf9\u9f50\u65b9\u6cd5\u5b58\u5728\u6570\u636e\u96c6\u6784\u5efa\u548c\u8bad\u7ec3\u6210\u672c\u9ad8\u6602\u3001\u63a8\u7406\u65f6\u5bf9\u9f50\u53ef\u80fd\u5bfc\u81f4\u8fc7\u5ea6\u62d2\u7edd\u548c\u63a8\u7406\u901f\u5ea6\u53d8\u6162\u7b49\u95ee\u9898\u3002", "method": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u98ce\u9669\u81ea\u9002\u5e94\u6fc0\u6d3b\u5f15\u5bfc\uff08RAS\uff09\u65b9\u6cd5\u3002\u8be5\u65b9\u6cd5\u901a\u8fc7\u91cd\u65b0\u6784\u5efa\u67e5\u8be2\uff0c\u4ee5\u5f3a\u5316\u8de8\u6a21\u6001\u5bf9\u5b89\u5168\u5173\u952e\u56fe\u50cf\u533a\u57df\u7684\u5173\u6ce8\uff0c\u4ece\u800c\u5728\u67e5\u8be2\u7ea7\u522b\u5b9e\u73b0\u51c6\u786e\u7684\u98ce\u9669\u8bc4\u4f30\u3002\u7136\u540e\uff0c\u6839\u636e\u8bc4\u4f30\u7684\u98ce\u9669\uff0c\u5b83\u81ea\u9002\u5e94\u5730\u5f15\u5bfc\u6fc0\u6d3b\uff0c\u4ee5\u751f\u6210\u5b89\u5168\u4e14\u6709\u5e2e\u52a9\u7684\u54cd\u5e94\uff0c\u800c\u4e0d\u4f1a\u4ea7\u751f\u8fed\u4ee3\u8f93\u51fa\u8c03\u6574\u7684\u5f00\u9500\u3002", "result": "\u5728\u591a\u6a21\u6001\u5b89\u5168\u6027\u548c\u5b9e\u7528\u6027\u7684\u591a\u4e2a\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u8fdb\u884c\u7684\u5e7f\u6cdb\u5b9e\u9a8c\u8868\u660e\uff0cRAS\u663e\u8457\u964d\u4f4e\u4e86\u653b\u51fb\u6210\u529f\u7387\uff0c\u4fdd\u6301\u4e86\u901a\u7528\u4efb\u52a1\u6027\u80fd\uff0c\u5e76\u63d0\u9ad8\u4e86\u63a8\u7406\u901f\u5ea6\u3002", "conclusion": "RAS\u65b9\u6cd5\u6709\u6548\u5730\u89e3\u51b3\u4e86\u591a\u6a21\u6001AI\u6a21\u578b\u5728\u5b89\u5168\u5bf9\u9f50\u65b9\u9762\u7684\u6311\u6218\uff0c\u5728\u964d\u4f4e\u653b\u51fb\u6210\u529f\u7387\u3001\u4fdd\u6301\u6027\u80fd\u548c\u63d0\u9ad8\u63a8\u7406\u901f\u5ea6\u65b9\u9762\u53d6\u5f97\u4e86\u663e\u8457\u6548\u679c\u3002"}}
{"id": "2510.13729", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2510.13729", "abs": "https://arxiv.org/abs/2510.13729", "authors": ["Aymeric Fleith", "Julian Zirbel", "Daniel Cremers", "Niclas Zeller"], "title": "LiFMCR: Dataset and Benchmark for Light Field Multi-Camera Registration", "comment": "Accepted at the International Symposium on Visual Computing (ISVC)\n  2025", "summary": "We present LiFMCR, a novel dataset for the registration of multiple micro\nlens array (MLA)-based light field cameras. While existing light field datasets\nare limited to single-camera setups and typically lack external ground truth,\nLiFMCR provides synchronized image sequences from two high-resolution Raytrix\nR32 plenoptic cameras, together with high-precision 6-degrees of freedom (DoF)\nposes recorded by a Vicon motion capture system. This unique combination\nenables rigorous evaluation of multi-camera light field registration methods.\n  As a baseline, we provide two complementary registration approaches: a robust\n3D transformation estimation via a RANSAC-based method using cross-view point\nclouds, and a plenoptic PnP algorithm estimating extrinsic 6-DoF poses from\nsingle light field images. Both explicitly integrate the plenoptic camera\nmodel, enabling accurate and scalable multi-camera registration. Experiments\nshow strong alignment with the ground truth, supporting reliable multi-view\nlight field processing.\n  Project page: https://lifmcr.github.io/", "AI": {"tldr": "LiFMCR\u662f\u4e00\u4e2a\u7528\u4e8e\u591a\u5fae\u900f\u955c\u9635\u5217\uff08MLA\uff09\u5149\u573a\u76f8\u673a\u914d\u51c6\u7684\u65b0\u9896\u6570\u636e\u96c6\uff0c\u5b83\u63d0\u4f9b\u540c\u6b65\u56fe\u50cf\u5e8f\u5217\u548c\u9ad8\u7cbe\u5ea66\u81ea\u7531\u5ea6\u59ff\u6001\uff0c\u5e76\u96c6\u6210\u4e86\u5149\u573a\u76f8\u673a\u6a21\u578b\u4ee5\u5b9e\u73b0\u51c6\u786e\u9ad8\u6548\u7684\u591a\u76f8\u673a\u914d\u51c6\u3002", "motivation": "\u73b0\u6709\u7684\u5149\u573a\u6570\u636e\u96c6\u901a\u5e38\u4ec5\u9650\u4e8e\u5355\u76f8\u673a\u8bbe\u7f6e\u4e14\u7f3a\u4e4f\u5916\u90e8\u771f\u503c\uff0c\u8fd9\u9650\u5236\u4e86\u591a\u76f8\u673a\u5149\u573a\u914d\u51c6\u65b9\u6cd5\u7684\u7814\u7a76\u4e0e\u8bc4\u4f30\u3002", "method": "LiFMCR\u6570\u636e\u96c6\u63d0\u4f9b\u6765\u81ea\u4e24\u4e2a\u9ad8\u5206\u8fa8\u7387Raytrix R32\u5168\u5149\u76f8\u673a\u7684\u540c\u6b65\u56fe\u50cf\u5e8f\u5217\uff0c\u4ee5\u53ca\u7531Vicon\u8fd0\u52a8\u6355\u6349\u7cfb\u7edf\u8bb0\u5f55\u7684\u9ad8\u7cbe\u5ea66\u81ea\u7531\u5ea6\uff08DoF\uff09\u59ff\u6001\u3002\u8be5\u8bba\u6587\u63d0\u4f9b\u4e86\u4e24\u79cd\u4e92\u8865\u7684\u914d\u51c6\u65b9\u6cd5\u4f5c\u4e3a\u57fa\u7ebf\uff1a\u4e00\u79cd\u662f\u901a\u8fc7\u57fa\u4e8eRANSAC\u7684\u65b9\u6cd5\u4f7f\u7528\u4ea4\u53c9\u89c6\u56fe\u70b9\u4e91\u8fdb\u884c\u9c81\u68d2\u76843D\u53d8\u6362\u4f30\u8ba1\uff1b\u53e6\u4e00\u79cd\u662f\u5168\u5149PnP\u7b97\u6cd5\uff0c\u4ece\u5355\u4e2a\u5149\u573a\u56fe\u50cf\u4f30\u8ba1\u5916\u90e86\u81ea\u7531\u5ea6\u59ff\u6001\u3002\u4e24\u79cd\u65b9\u6cd5\u90fd\u660e\u786e\u96c6\u6210\u4e86\u5168\u5149\u76f8\u673a\u6a21\u578b\u3002", "result": "\u5b9e\u9a8c\u7ed3\u679c\u8868\u660e\uff0c\u8be5\u65b9\u6cd5\u4e0e\u771f\u503c\u5177\u6709\u5f88\u5f3a\u7684\u4e00\u81f4\u6027\uff0c\u652f\u6301\u53ef\u9760\u7684\u591a\u89c6\u56fe\u5149\u573a\u5904\u7406\u3002", "conclusion": "LiFMCR\u6570\u636e\u96c6\u53ca\u5176\u63d0\u4f9b\u7684\u57fa\u7ebf\u65b9\u6cd5\u80fd\u591f\u6709\u6548\u4fc3\u8fdb\u591a\u76f8\u673a\u5149\u573a\u914d\u51c6\u6280\u672f\u7684\u53d1\u5c55\u548c\u8bc4\u4f30\u3002"}}
{"id": "2510.13747", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2510.13747", "abs": "https://arxiv.org/abs/2510.13747", "authors": ["Wenwen Tong", "Hewei Guo", "Dongchuan Ran", "Jiangnan Chen", "Jiefan Lu", "Kaibin Wang", "Keqiang Li", "Xiaoxu Zhu", "Jiakui Li", "Kehan Li", "Xueheng Li", "Lumin Li", "Chenxu Guo", "Jiasheng Zhou", "Jiandong Chen", "Xianye Wu", "Jiahao Wang", "Silei Wu", "Lei Chen", "Hanming Deng", "Yuxuan Song", "Dinghao Zhou", "Guiping Zhong", "Ken Zheng", "Shiyin Kang", "Lewei Lu"], "title": "InteractiveOmni: A Unified Omni-modal Model for Audio-Visual Multi-turn Dialogue", "comment": null, "summary": "We introduce InteractiveOmni, a unified and open-source omni-modal large\nlanguage model for audio-visual multi-turn interaction, ranging from 4B to 8B\nparameters, designed to lead the field of lightweight models by offering\ncomprehensive omni-modal understanding and speech generation capabilities. To\nachieve this, we integrate the vision encoder, audio encoder, large language\nmodel, and speech decoder into a unified model for understanding and generation\ntasks. We design a multi-stage training strategy to ensure robust cross-modal\ncapabilities, including pre-training for omni-modal understanding, followed by\npost-training with speech conversation and audio-visual interaction. To enable\nhuman-like long-term conversational ability, we meticulously curate a\nmulti-turn training dataset that enhances the model's ability to handle complex\nand multi-turn interactions. To effectively evaluate the multi-turn memory and\nspeech interaction capabilities, we construct the multi-modal multi-turn memory\nbenchmark and the multi-turn speech interaction benchmark. Experiments\ndemonstrate that InteractiveOmni significantly outperforms leading open-source\nmodels and provides a more intelligent multi-turn audio-visual experience,\nparticularly in its long-term memory capabilities. Notably, InteractiveOmni-4B\nis comparable to the much larger model like Qwen2.5-Omni-7B on general\nbenchmarks, and it can retain 97% of the performance of the InteractiveOmni-8B\nwhile utilizing only 50% of the model size. Achieving state-of-the-art results\nagainst similarly sized models across image, audio, video understanding, and\nspeech generation tasks, InteractiveOmni is an accessible, open-source\nfoundation for next-generation intelligent interactive systems.", "AI": {"tldr": "InteractiveOmni\u662f\u4e00\u4e2a\u7edf\u4e00\u7684\u3001\u5f00\u6e90\u7684\u3001\u5168\u6a21\u6001\u5927\u8bed\u8a00\u6a21\u578b\uff0c\u652f\u6301\u97f3\u89c6\u9891\u591a\u8f6e\u4ea4\u4e92\uff0c\u53c2\u6570\u8303\u56f4\u4ece4B\u52308B\u3002\u5b83\u901a\u8fc7\u96c6\u6210\u89c6\u89c9\u7f16\u7801\u5668\u3001\u97f3\u9891\u7f16\u7801\u5668\u3001\u5927\u8bed\u8a00\u6a21\u578b\u548c\u8bed\u97f3\u89e3\u7801\u5668\uff0c\u5b9e\u73b0\u4e86\u5168\u9762\u7684\u5168\u6a21\u6001\u7406\u89e3\u548c\u8bed\u97f3\u751f\u6210\u80fd\u529b\u3002\u6a21\u578b\u91c7\u7528\u591a\u9636\u6bb5\u8bad\u7ec3\u7b56\u7565\uff0c\u5e76\u901a\u8fc7\u7cbe\u5fc3\u7b56\u5212\u7684\u591a\u8f6e\u8bad\u7ec3\u6570\u636e\u96c6\u63d0\u5347\u4e86\u957f\u7a0b\u5bf9\u8bdd\u80fd\u529b\u3002\u5728\u8bc4\u4f30\u65b9\u9762\uff0c\u6784\u5efa\u4e86\u591a\u6a21\u6001\u591a\u8f6e\u5185\u5b58\u57fa\u51c6\u548c\u591a\u8f6e\u8bed\u97f3\u4ea4\u4e92\u57fa\u51c6\u3002\u5b9e\u9a8c\u8bc1\u660e\uff0cInteractiveOmni\u5728\u6027\u80fd\u4e0a\u663e\u8457\u4f18\u4e8e\u9886\u5148\u7684\u5f00\u6e90\u6a21\u578b\uff0c\u5c24\u5176\u5728\u957f\u7a0b\u8bb0\u5fc6\u80fd\u529b\u65b9\u9762\u8868\u73b0\u51fa\u8272\uff0c\u5176\u4e2d4B\u7248\u672c\u5728\u901a\u7528\u57fa\u51c6\u6d4b\u8bd5\u4e0a\u53ef\u4e0e7B\u6a21\u578b\u5ab2\u7f8e\uff0c\u4e14\u5728\u540c\u7b49\u89c4\u6a21\u6a21\u578b\u4e2d\u8fbe\u5230\u4e86\u6700\u5148\u8fdb\u7684\u6c34\u5e73\u3002", "motivation": "\u76ee\u524d\u7f3a\u4e4f\u4e00\u4e2a\u7edf\u4e00\u7684\u3001\u5f00\u6e90\u7684\u3001\u8f7b\u91cf\u7ea7\u7684\u5168\u6a21\u6001\u5927\u8bed\u8a00\u6a21\u578b\uff0c\u80fd\u591f\u540c\u65f6\u63d0\u4f9b\u5168\u9762\u7684\u5168\u6a21\u6001\u7406\u89e3\u548c\u8bed\u97f3\u751f\u6210\u80fd\u529b\uff0c\u5e76\u652f\u6301\u957f\u7a0b\u3001\u591a\u8f6e\u7684\u97f3\u89c6\u9891\u4ea4\u4e92\u3002", "method": "InteractiveOmni\u901a\u8fc7\u6574\u5408\u89c6\u89c9\u7f16\u7801\u5668\u3001\u97f3\u9891\u7f16\u7801\u5668\u3001\u5927\u578b\u8bed\u8a00\u6a21\u578b\u548c\u8bed\u97f3\u89e3\u7801\u5668\uff0c\u6784\u5efa\u4e86\u4e00\u4e2a\u7edf\u4e00\u6a21\u578b\u3002\u6a21\u578b\u91c7\u7528\u591a\u9636\u6bb5\u8bad\u7ec3\u7b56\u7565\uff0c\u5305\u62ec\u5168\u6a21\u6001\u7406\u89e3\u7684\u9884\u8bad\u7ec3\u4ee5\u53ca\u8bed\u97f3\u5bf9\u8bdd\u548c\u97f3\u89c6\u9891\u4ea4\u4e92\u7684\u540e\u8bad\u7ec3\u3002\u4e3a\u4e86\u589e\u5f3a\u6a21\u578b\u7684\u957f\u7a0b\u5bf9\u8bdd\u80fd\u529b\uff0c\u9879\u76ee\u7ec4\u7cbe\u5fc3\u6574\u7406\u4e86\u591a\u8f6e\u8bad\u7ec3\u6570\u636e\u96c6\u3002\u4e3a\u4e86\u6709\u6548\u8bc4\u4f30\u591a\u8f6e\u8bb0\u5fc6\u548c\u8bed\u97f3\u4ea4\u4e92\u80fd\u529b\uff0c\u9879\u76ee\u7ec4\u8fd8\u6784\u5efa\u4e86\u591a\u6a21\u6001\u591a\u8f6e\u8bb0\u5fc6\u57fa\u51c6\u548c\u591a\u8f6e\u8bed\u97f3\u4ea4\u4e92\u57fa\u51c6\u3002", "result": "InteractiveOmni\u5728\u6027\u80fd\u4e0a\u663e\u8457\u4f18\u4e8e\u9886\u5148\u7684\u5f00\u6e90\u6a21\u578b\uff0c\u63d0\u4f9b\u4e86\u66f4\u667a\u80fd\u7684\u591a\u8f6e\u97f3\u89c6\u9891\u4ea4\u4e92\u4f53\u9a8c\uff0c\u5c24\u5176\u5728\u957f\u7a0b\u8bb0\u5fc6\u80fd\u529b\u65b9\u9762\u8868\u73b0\u7a81\u51fa\u3002InteractiveOmni-4B\u5728\u901a\u7528\u57fa\u51c6\u4e0a\u4e0e\u66f4\u5927\u7684\u6a21\u578b\uff08\u5982Qwen2.5-Omni-7B\uff09\u6027\u80fd\u76f8\u5f53\uff0c\u5e76\u4e14\u5728\u6a21\u578b\u5c3a\u5bf8\u51cf\u534a\u7684\u60c5\u51b5\u4e0b\uff0c\u4ecd\u80fd\u4fdd\u6301InteractiveOmni-8B 97%\u7684\u6027\u80fd\u3002\u5728\u56fe\u50cf\u3001\u97f3\u9891\u3001\u89c6\u9891\u7406\u89e3\u548c\u8bed\u97f3\u751f\u6210\u4efb\u52a1\u4e0a\uff0cInteractiveOmni\u5728\u540c\u7b49\u89c4\u6a21\u7684\u6a21\u578b\u4e2d\u53d6\u5f97\u4e86\u6700\u5148\u8fdb\u7684\u6210\u679c\u3002", "conclusion": "InteractiveOmni\u662f\u4e00\u4e2a\u7edf\u4e00\u7684\u3001\u5f00\u6e90\u7684\u3001\u8f7b\u91cf\u7ea7\u5168\u6a21\u6001\u5927\u8bed\u8a00\u6a21\u578b\uff0c\u901a\u8fc7\u521b\u65b0\u7684\u67b6\u6784\u548c\u8bad\u7ec3\u7b56\u7565\uff0c\u5728\u5168\u6a21\u6001\u7406\u89e3\u3001\u8bed\u97f3\u751f\u6210\u548c\u957f\u7a0b\u591a\u8f6e\u4ea4\u4e92\u65b9\u9762\u53d6\u5f97\u4e86\u663e\u8457\u8fdb\u5c55\u3002\u5176\u5353\u8d8a\u7684\u6027\u80fd\u548c\u9ad8\u6548\u6027\u4f7f\u5176\u6210\u4e3a\u4e0b\u4e00\u4ee3\u667a\u80fd\u4ea4\u4e92\u7cfb\u7edf\u7684\u53ef\u53ca\u4e14\u5f00\u6e90\u7684\u57fa\u7840\u3002"}}
{"id": "2510.13759", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2510.13759", "abs": "https://arxiv.org/abs/2510.13759", "authors": ["Kai Zou", "Ziqi Huang", "Yuhao Dong", "Shulin Tian", "Dian Zheng", "Hongbo Liu", "Jingwen He", "Bin Liu", "Yu Qiao", "Ziwei Liu"], "title": "Uni-MMMU: A Massive Multi-discipline Multimodal Unified Benchmark", "comment": "Equal contributions from frst three authors. Project page:\n  https://vchitect.github.io/Uni-MMMU-Project/ Code:\n  https://github.com/vchitect/Uni-MMMU", "summary": "Unified multimodal models aim to jointly enable visual understanding and\ngeneration, yet current benchmarks rarely examine their true integration.\nExisting evaluations either treat the two abilities in isolation or overlook\ntasks that inherently couple them. To address this gap, we present Uni-MMMU, a\ncomprehensive and discipline-aware benchmark that systematically unfolds the\nbidirectional synergy between generation and understanding across eight\nreasoning-centric domains, including science, coding, mathematics, and puzzles.\nEach task is bidirectionally coupled, demanding models to (i) leverage\nconceptual understanding to guide precise visual synthesis, or (ii) utilize\ngeneration as a cognitive scaffold for analytical reasoning. Uni-MMMU\nincorporates verifiable intermediate reasoning steps, unique ground truths, and\na reproducible scoring protocol for both textual and visual outputs. Through\nextensive evaluation of state-of-the-art unified, generation-only, and\nunderstanding-only models, we reveal substantial performance disparities and\ncross-modal dependencies, offering new insights into when and how these\nabilities reinforce one another, and establishing a reliable foundation for\nadvancing unified models.", "AI": {"tldr": "Uni-MMMU \u662f\u4e00\u4e2a\u5168\u9762\u7684\u3001\u8de8\u5b66\u79d1\u7684\u57fa\u51c6\u6d4b\u8bd5\u96c6\uff0c\u65e8\u5728\u8bc4\u4f30\u7edf\u4e00\u591a\u6a21\u6001\u6a21\u578b\u5728\u89c6\u89c9\u7406\u89e3\u548c\u751f\u6210\u53cc\u5411\u534f\u540c\u65b9\u9762\u7684\u80fd\u529b\u3002\u5b83\u5305\u542b\u516b\u4e2a\u4ee5\u63a8\u7406\u4e3a\u4e2d\u5fc3\u7684\u9886\u57df\uff0c\u6bcf\u4e2a\u4efb\u52a1\u90fd\u53cc\u5411\u8026\u5408\uff0c\u8981\u6c42\u6a21\u578b\u5229\u7528\u6982\u5ff5\u7406\u89e3\u6765\u6307\u5bfc\u7cbe\u786e\u7684\u89c6\u89c9\u5408\u6210\uff0c\u6216\u5229\u7528\u751f\u6210\u4f5c\u4e3a\u5206\u6790\u63a8\u7406\u7684\u8ba4\u77e5\u652f\u67b6\u3002", "motivation": "\u5f53\u524d\u57fa\u51c6\u6d4b\u8bd5\u5f88\u5c11\u80fd\u771f\u6b63\u68c0\u9a8c\u7edf\u4e00\u591a\u6a21\u6001\u6a21\u578b\u7684\u89c6\u89c9\u7406\u89e3\u548c\u751f\u6210\u80fd\u529b\u7684\u6574\u5408\uff0c\u73b0\u6709\u8bc4\u4f30\u8981\u4e48\u5b64\u7acb\u5730\u5904\u7406\u8fd9\u4e24\u79cd\u80fd\u529b\uff0c\u8981\u4e48\u5ffd\u7565\u4e86\u90a3\u4e9b\u672c\u8d28\u4e0a\u5c06\u5b83\u4eec\u8026\u5408\u5728\u4e00\u8d77\u7684\u4efb\u52a1\u3002", "method": "Uni-MMMU \u57fa\u51c6\u6d4b\u8bd5\u96c6\u901a\u8fc7\u516b\u4e2a\u4ee5\u63a8\u7406\u4e3a\u4e2d\u5fc3\u7684\u9886\u57df\uff08\u5305\u62ec\u79d1\u5b66\u3001\u7f16\u7801\u3001\u6570\u5b66\u548c\u8c1c\u9898\uff09\u7cfb\u7edf\u5730\u63ed\u793a\u4e86\u751f\u6210\u548c\u7406\u89e3\u4e4b\u95f4\u7684\u53cc\u5411\u534f\u540c\u4f5c\u7528\u3002\u6bcf\u4e2a\u4efb\u52a1\u90fd\u662f\u53cc\u5411\u8026\u5408\u7684\uff0c\u8981\u6c42\u6a21\u578b\u5229\u7528\u6982\u5ff5\u7406\u89e3\u6765\u6307\u5bfc\u7cbe\u786e\u7684\u89c6\u89c9\u5408\u6210\uff0c\u6216\u5229\u7528\u751f\u6210\u4f5c\u4e3a\u5206\u6790\u63a8\u7406\u7684\u8ba4\u77e5\u652f\u67b6\u3002Uni-MMMU \u5305\u542b\u53ef\u9a8c\u8bc1\u7684\u4e2d\u95f4\u63a8\u7406\u6b65\u9aa4\u3001\u72ec\u7279\u7684\u57fa\u51c6\u4e8b\u5b9e\u4ee5\u53ca\u9488\u5bf9\u6587\u672c\u548c\u89c6\u89c9\u8f93\u51fa\u7684\u53ef\u91cd\u73b0\u8bc4\u5206\u534f\u8bae\u3002", "result": "\u901a\u8fc7\u5bf9\u6700\u5148\u8fdb\u7684\u7edf\u4e00\u6a21\u578b\u3001\u4ec5\u751f\u6210\u6a21\u578b\u548c\u4ec5\u7406\u89e3\u6a21\u578b\u8fdb\u884c\u5e7f\u6cdb\u8bc4\u4f30\uff0c\u7814\u7a76\u63ed\u793a\u4e86\u663e\u8457\u7684\u6027\u80fd\u5dee\u5f02\u548c\u8de8\u6a21\u6001\u4f9d\u8d56\u6027\u3002", "conclusion": "Uni-MMMU \u4e3a\u63a8\u8fdb\u7edf\u4e00\u591a\u6a21\u6001\u6a21\u578b\u5960\u5b9a\u4e86\u53ef\u9760\u7684\u57fa\u7840\uff0c\u5e76\u4e3a\u8fd9\u4e9b\u80fd\u529b\u4f55\u65f6\u4ee5\u53ca\u5982\u4f55\u76f8\u4e92\u589e\u5f3a\u63d0\u4f9b\u4e86\u65b0\u7684\u89c1\u89e3\u3002"}}
{"id": "2510.13793", "categories": ["cs.CV", "cs.CR", "cs.LG"], "pdf": "https://arxiv.org/pdf/2510.13793", "abs": "https://arxiv.org/abs/2510.13793", "authors": ["Nir Goren", "Oren Katzir", "Abhinav Nakarmi", "Eyal Ronen", "Mahmood Sharif", "Or Patashnik"], "title": "NoisePrints: Distortion-Free Watermarks for Authorship in Private Diffusion Models", "comment": "code available at: https://github.com/nirgoren/NoisePrints", "summary": "With the rapid adoption of diffusion models for visual content generation,\nproving authorship and protecting copyright have become critical. This\nchallenge is particularly important when model owners keep their models private\nand may be unwilling or unable to handle authorship issues, making third-party\nverification essential. A natural solution is to embed watermarks for later\nverification. However, existing methods require access to model weights and\nrely on computationally heavy procedures, rendering them impractical and\nnon-scalable. To address these challenges, we propose , a lightweight\nwatermarking scheme that utilizes the random seed used to initialize the\ndiffusion process as a proof of authorship without modifying the generation\nprocess. Our key observation is that the initial noise derived from a seed is\nhighly correlated with the generated visual content. By incorporating a hash\nfunction into the noise sampling process, we further ensure that recovering a\nvalid seed from the content is infeasible. We also show that sampling an\nalternative seed that passes verification is infeasible, and demonstrate the\nrobustness of our method under various manipulations. Finally, we show how to\nuse cryptographic zero-knowledge proofs to prove ownership without revealing\nthe seed. By keeping the seed secret, we increase the difficulty of watermark\nremoval. In our experiments, we validate NoisePrints on multiple\nstate-of-the-art diffusion models for images and videos, demonstrating\nefficient verification using only the seed and output, without requiring access\nto model weights.", "AI": {"tldr": "\u8fd9\u7bc7\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u8f7b\u91cf\u7ea7\u6c34\u5370\u65b9\u6848NoisePrints\uff0c\u5b83\u5229\u7528\u6269\u6563\u6a21\u578b\u4e2d\u7528\u4e8e\u521d\u59cb\u5316\u6269\u6563\u8fc7\u7a0b\u7684\u968f\u673a\u79cd\u5b50\u4f5c\u4e3a\u4f5c\u8005\u8eab\u4efd\u8bc1\u660e\uff0c\u800c\u65e0\u9700\u4fee\u6539\u751f\u6210\u8fc7\u7a0b\uff0c\u4ece\u800c\u89e3\u51b3\u4e86\u73b0\u6709\u65b9\u6cd5\u9700\u8981\u8bbf\u95ee\u6a21\u578b\u6743\u91cd\u4e14\u8ba1\u7b97\u6210\u672c\u9ad8\u6602\u7684\u95ee\u9898\u3002", "motivation": "\u968f\u7740\u6269\u6563\u6a21\u578b\u5728\u89c6\u89c9\u5185\u5bb9\u751f\u6210\u4e2d\u7684\u5e7f\u6cdb\u5e94\u7528\uff0c\u8bc1\u660e\u4f5c\u54c1\u5f52\u5c5e\u6743\u548c\u4fdd\u62a4\u7248\u6743\u53d8\u5f97\u81f3\u5173\u91cd\u8981\u3002\u5f53\u6a21\u578b\u6240\u6709\u8005\u4e0d\u613f\u6216\u65e0\u6cd5\u5904\u7406\u4f5c\u8005\u8eab\u4efd\u95ee\u9898\u65f6\uff0c\u7b2c\u4e09\u65b9\u9a8c\u8bc1\u53d8\u5f97\u5fc5\u4e0d\u53ef\u5c11\u3002\u73b0\u6709\u6c34\u5370\u65b9\u6cd5\u9700\u8981\u8bbf\u95ee\u6a21\u578b\u6743\u91cd\u4e14\u8ba1\u7b97\u91cf\u5927\uff0c\u4e0d\u5207\u5b9e\u9645\u4e14\u4e0d\u53ef\u6269\u5c55\u3002", "method": "NoisePrints\u65b9\u6848\u7684\u6838\u5fc3\u601d\u60f3\u662f\u5229\u7528\u968f\u673a\u79cd\u5b50\u521d\u59cb\u5316\u6269\u6563\u8fc7\u7a0b\uff0c\u5e76\u901a\u8fc7\u5728\u566a\u58f0\u91c7\u6837\u8fc7\u7a0b\u4e2d\u52a0\u5165\u54c8\u5e0c\u51fd\u6570\uff0c\u786e\u4fdd\u4ece\u5185\u5bb9\u4e2d\u6062\u590d\u6709\u6548\u79cd\u5b50\u6216\u4f2a\u9020\u901a\u8fc7\u9a8c\u8bc1\u7684\u66ff\u4ee3\u79cd\u5b50\u662f\u4e0d\u53ef\u884c\u7684\u3002\u6b64\u5916\uff0c\u8be5\u65b9\u6cd5\u8fd8\u5229\u7528\u52a0\u5bc6\u96f6\u77e5\u8bc6\u8bc1\u660e\u5728\u4e0d\u6cc4\u9732\u79cd\u5b50\u7684\u60c5\u51b5\u4e0b\u8bc1\u660e\u6240\u6709\u6743\u3002", "result": "\u5b9e\u9a8c\u8bc1\u660eNoisePrints\u5728\u591a\u79cd\u5148\u8fdb\u7684\u56fe\u50cf\u548c\u89c6\u9891\u6269\u6563\u6a21\u578b\u4e0a\u5b9e\u73b0\u4e86\u9ad8\u6548\u9a8c\u8bc1\uff0c\u4ec5\u9700\u79cd\u5b50\u548c\u8f93\u51fa\uff0c\u65e0\u9700\u8bbf\u95ee\u6a21\u578b\u6743\u91cd\u3002", "conclusion": "NoisePrints\u901a\u8fc7\u5229\u7528\u6269\u6563\u6a21\u578b\u7684\u968f\u673a\u79cd\u5b50\u4f5c\u4e3a\u4f5c\u8005\u8eab\u4efd\u8bc1\u660e\uff0c\u63d0\u4f9b\u4e86\u4e00\u79cd\u8f7b\u91cf\u7ea7\u4e14\u9c81\u68d2\u7684\u6c34\u5370\u65b9\u6848\uff0c\u89e3\u51b3\u4e86\u5f53\u524d\u5185\u5bb9\u751f\u6210\u9886\u57df\u4f5c\u8005\u8eab\u4efd\u9a8c\u8bc1\u7684\u6311\u6218\uff0c\u5e76\u5728\u4fdd\u62a4\u7248\u6743\u65b9\u9762\u5177\u6709\u91cd\u8981\u610f\u4e49\u3002"}}
{"id": "2510.13800", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2510.13800", "abs": "https://arxiv.org/abs/2510.13800", "authors": ["Yiming Chen", "Zekun Qi", "Wenyao Zhang", "Xin Jin", "Li Zhang", "Peidong Liu"], "title": "Reasoning in Space via Grounding in the World", "comment": "20 pages, 7 figures", "summary": "In this paper, we claim that 3D visual grounding is the cornerstone of\nspatial reasoning and introduce the Grounded-Spatial Reasoner (GS-Reasoner) to\nexplore the effective spatial representations that bridge the gap between them.\nExisting 3D LLMs suffer from the absence of a unified 3D representation capable\nof jointly capturing semantic and geometric information. This deficiency is\nmanifested either in poor performance on grounding or in an excessive reliance\non external modules, ultimately hindering the seamless integration of grounding\nand spatial reasoning. To address this, we propose a simple yet effective\ndual-path pooling mechanism that tightly aligns geometric features with both\nsemantic and positional cues, constructing a unified image patch-based 3D\nrepresentation that encapsulates all essential information without increasing\nthe number of input tokens. Leveraging this holistic representation,\nGS-Reasoner is the first 3D LLM that achieves autoregressive grounding entirely\nwithout external modules while delivering performance comparable to\nstate-of-the-art models, establishing a unified and self-contained framework\nfor 3D spatial reasoning. To further bridge grounding and spatial reasoning, we\nintroduce the Grounded Chain-of-Thought (GCoT) dataset. This dataset is\nmeticulously curated to include both 3D bounding box annotations for objects\nreferenced in reasoning questions and step-by-step reasoning paths that\nintegrate grounding as a core component of the problem-solving process.\nExtensive experiments demonstrate that GS-Reasoner achieves impressive results\non 3D visual grounding, which in turn significantly enhances its spatial\nreasoning capabilities, leading to state-of-the-art performance.", "AI": {"tldr": "\u672c\u6587\u4ecb\u7ecd\u4e86\u4e00\u79cd\u540d\u4e3aGS-Reasoner\u76843D\u5927\u8bed\u8a00\u6a21\u578b\uff0c\u5b83\u901a\u8fc7\u53cc\u8def\u5f84\u6c60\u5316\u673a\u5236\u7edf\u4e00\u4e863D\u8868\u793a\uff0c\u5b9e\u73b0\u4e86\u65e0\u5916\u90e8\u6a21\u5757\u7684\u81ea\u56de\u5f52\u89c6\u89c9\u63a5\u5730\uff0c\u5e76\u663e\u8457\u63d0\u5347\u4e863D\u7a7a\u95f4\u63a8\u7406\u80fd\u529b\u3002", "motivation": "\u73b0\u6709\u76843D\u5927\u8bed\u8a00\u6a21\u578b\u5728\u7edf\u4e00\u6355\u83b7\u8bed\u4e49\u548c\u51e0\u4f55\u4fe1\u606f\u76843D\u8868\u793a\u65b9\u9762\u5b58\u5728\u4e0d\u8db3\uff0c\u5bfc\u81f4\u63a5\u5730\u6027\u80fd\u4e0d\u4f73\u6216\u8fc7\u5ea6\u4f9d\u8d56\u5916\u90e8\u6a21\u5757\uff0c\u963b\u788d\u4e86\u63a5\u5730\u4e0e\u7a7a\u95f4\u63a8\u7406\u7684\u65e0\u7f1d\u96c6\u6210\u3002", "method": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u7b80\u5355\u800c\u6709\u6548\u7684\u53cc\u8def\u5f84\u6c60\u5316\u673a\u5236\uff0c\u5c06\u51e0\u4f55\u7279\u5f81\u4e0e\u8bed\u4e49\u548c\u4f4d\u7f6e\u4fe1\u606f\u7d27\u5bc6\u5bf9\u9f50\uff0c\u6784\u5efa\u4e86\u7edf\u4e00\u7684\u57fa\u4e8e\u56fe\u50cf\u5757\u76843D\u8868\u793a\uff0c\u5728\u4e0d\u589e\u52a0\u8f93\u5165token\u6570\u91cf\u7684\u60c5\u51b5\u4e0b\u5305\u542b\u4e86\u6240\u6709\u5fc5\u8981\u4fe1\u606f\u3002\u5728\u6b64\u57fa\u7840\u4e0a\uff0cGS-Reasoner\u662f\u7b2c\u4e00\u4e2a\u5b9e\u73b0\u5b8c\u5168\u81ea\u56de\u5f52\u63a5\u5730\u76843D\u5927\u8bed\u8a00\u6a21\u578b\u3002\u6b64\u5916\uff0c\u672c\u6587\u8fd8\u5f15\u5165\u4e86Grounded Chain-of-Thought (GCoT) \u6570\u636e\u96c6\uff0c\u5305\u542b\u4e863D\u8fb9\u754c\u6846\u6807\u6ce8\u548c\u5206\u6b65\u63a8\u7406\u8def\u5f84\uff0c\u5c06\u63a5\u5730\u4f5c\u4e3a\u89e3\u51b3\u95ee\u9898\u7684\u6838\u5fc3\u7ec4\u6210\u90e8\u5206\u3002", "result": "GS-Reasoner\u57283D\u89c6\u89c9\u63a5\u5730\u65b9\u9762\u53d6\u5f97\u4e86\u663e\u8457\u6210\u679c\uff0c\u53cd\u8fc7\u6765\u53c8\u663e\u8457\u589e\u5f3a\u4e86\u5176\u7a7a\u95f4\u63a8\u7406\u80fd\u529b\uff0c\u8fbe\u5230\u4e86\u6700\u5148\u8fdb\u7684\u6027\u80fd\u3002", "conclusion": "GS-Reasoner\u901a\u8fc7\u5176\u7edf\u4e00\u76843D\u8868\u793a\u548c\u81ea\u56de\u5f52\u63a5\u5730\u80fd\u529b\uff0c\u4e3a3D\u7a7a\u95f4\u63a8\u7406\u63d0\u4f9b\u4e86\u4e00\u4e2a\u7edf\u4e00\u7684\u3001\u81ea\u5305\u542b\u7684\u6846\u67b6\uff0c\u5e76\u4e3a\u672a\u6765\u7684\u7814\u7a76\u5960\u5b9a\u4e86\u57fa\u7840\u3002"}}
{"id": "2510.13808", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2510.13808", "abs": "https://arxiv.org/abs/2510.13808", "authors": ["Dominick Reilly", "Manish Kumar Govind", "Le Xue", "Srijan Das"], "title": "VisCoP: Visual Probing for Video Domain Adaptation of Vision Language Models", "comment": null, "summary": "Large Vision-Language Models (VLMs) excel at general visual reasoning tasks\nbut exhibit sharp performance degradation when applied to novel domains with\nsubstantial distribution shifts from pretraining data. Existing domain\nadaptation approaches finetune different VLM components, but this often results\nin limited domain-specific feature learning or catastrophic forgetting of prior\ncapabilities. To address these issues, we introduce Vision Contextualized\nProbing (VisCoP), which augments the VLM's vision encoder with a compact set of\nlearnable visual probes. These probes enable efficient domain-specific\nadaptation with minimal modification to pretrained parameters. We evaluate\nVisCoP across three challenging domain adaptation settings-cross-view\n(exocentric to egocentric), cross-modal (RGB to depth), and cross-task (human\nunderstanding to robot control). Experiments show that VisCoP consistently\noutperforms existing adaptation strategies, achieving superior performance on\ntarget domains while effectively retaining source-domain knowledge.", "AI": {"tldr": "VisCoP\u901a\u8fc7\u5f15\u5165\u4e00\u7ec4\u53ef\u5b66\u4e60\u7684\u89c6\u89c9\u63a2\u6d4b\u5668\u6765\u589e\u5f3aVLM\u7684\u89c6\u89c9\u7f16\u7801\u5668\uff0c\u5b9e\u73b0\u9ad8\u6548\u7684\u7279\u5b9a\u9886\u57df\u9002\u5e94\uff0c\u540c\u65f6\u4fdd\u6301\u9884\u8bad\u7ec3\u6a21\u578b\u7684\u6027\u80fd\u3002", "motivation": "\u89e3\u51b3\u5927\u578b\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\uff08VLMs\uff09\u5728\u9762\u5bf9\u4e0e\u9884\u8bad\u7ec3\u6570\u636e\u5b58\u5728\u663e\u8457\u5206\u5e03\u504f\u79fb\u7684\u65b0\u9886\u57df\u65f6\u6027\u80fd\u6025\u5267\u4e0b\u964d\u7684\u95ee\u9898\u3002\u73b0\u6709\u7684\u57df\u9002\u5e94\u65b9\u6cd5\u5728\u7279\u5b9a\u9886\u57df\u7279\u5f81\u5b66\u4e60\u65b9\u9762\u8868\u73b0\u6709\u9650\uff0c\u6216\u5bfc\u81f4\u707e\u96be\u6027\u9057\u5fd8\u3002", "method": "\u672c\u6587\u5f15\u5165\u4e86Vision Contextualized Probing (VisCoP)\u65b9\u6cd5\uff0c\u901a\u8fc7\u5728VLM\u7684\u89c6\u89c9\u7f16\u7801\u5668\u4e2d\u6dfb\u52a0\u4e00\u7ec4\u7d27\u51d1\u7684\u53ef\u5b66\u4e60\u7684\u89c6\u89c9\u63a2\u6d4b\u5668\uff0c\u4ee5\u6700\u5c0f\u7684\u9884\u8bad\u7ec3\u53c2\u6570\u4fee\u6539\u5b9e\u73b0\u9ad8\u6548\u7684\u9886\u57df\u7279\u5b9a\u9002\u5e94\u3002", "result": "VisCoP\u5728\u4ea4\u53c9\u89c6\u89d2\uff08\u4ece\u4ee5\u81ea\u6211\u4e3a\u4e2d\u5fc3\u5230\u4ee5\u4ed6\u4eba\u4e3a\u4e2d\u5fc3\uff09\u3001\u4ea4\u53c9\u6a21\u6001\uff08RGB\u5230\u6df1\u5ea6\uff09\u548c\u4ea4\u53c9\u4efb\u52a1\uff08\u4eba\u7c7b\u7406\u89e3\u5230\u673a\u5668\u4eba\u63a7\u5236\uff09\u4e09\u79cd\u57df\u9002\u5e94\u8bbe\u7f6e\u4e2d\uff0c\u5747\u4f18\u4e8e\u73b0\u6709\u9002\u5e94\u7b56\u7565\u3002", "conclusion": "VisCoP\u5728\u76ee\u6807\u9886\u57df\u5b9e\u73b0\u4e86\u5353\u8d8a\u7684\u6027\u80fd\uff0c\u540c\u65f6\u6709\u6548\u4fdd\u7559\u4e86\u6e90\u9886\u57df\u77e5\u8bc6\u3002"}}
