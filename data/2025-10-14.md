<div id=toc></div>

# Table of Contents

- [cs.CV](#cs.CV) [Total: 164]
- [cs.AI](#cs.AI) [Total: 53]
- [cs.SI](#cs.SI) [Total: 3]
- [cs.IT](#cs.IT) [Total: 5]
- [cs.GT](#cs.GT) [Total: 2]
- [cs.MA](#cs.MA) [Total: 2]


<div id='cs.CV'></div>

# cs.CV [[Back]](#toc)

### [1] [TinyViT-Batten: Few-Shot Vision Transformer with Explainable Attention for Early Batten-Disease Detection on Pediatric MRI](https://arxiv.org/abs/2510.09649)
*Khartik Uppalapati,Bora Yimenicioglu,Shakeel Abdulkareem,Adan Eftekhari,Bhavya Uppalapati,Viraj Kamath*

Main category: cs.CV

TL;DR: TinyViT-Batten是一个基于Vision Transformer的少量样本学习框架，用于在有限的训练案例下从儿科脑部MRI中检测早期Batten病。


<details>
  <summary>Details</summary>
Motivation: Batten病（神经元类脂褐质沉积症）是一种罕见的儿科神经退行性疾病，其早期MRI迹象不明显且经常被忽视。

Method: 我们从一个大型的教师ViT中提取出一个5M参数的TinyViT，并使用基于度量的少量样本学习（带有5-shot episodic的Prototypical Loss）对其进行微调。

Result: 我们的模型在79例基因确诊的Batten病MRI（多中心数据集）和90例年龄匹配的对照组上实现了高精度（约91%）和至少0.95的ROC曲线下面积，优于3D-ResNet和Swin-Tiny基线。我们还将Grad-CAM集成进来，以突出与疾病相关的大脑区域，从而实现可解释的预测。

Conclusion: 该模型体积小，性能强（灵敏度大于90%，特异性约90%），为早期Batten病检测提供了一个实用的AI解决方案。

Abstract: Batten disease (neuronal ceroid lipofuscinosis) is a rare pediatric
neurodegenerative disorder whose early MRI signs are subtle and often missed.
We propose TinyViT-Batten, a few-shot Vision Transformer (ViT) framework to
detect early Batten disease from pediatric brain MRI with limited training
cases. We distill a large teacher ViT into a 5 M-parameter TinyViT and
fine-tune it using metric-based few-shot learning (prototypical loss with
5-shot episodes). Our model achieves high accuracy (approximately 91%) and area
under ROC of at least 0.95 on a multi-site dataset of 79 genetically confirmed
Batten-disease MRIs (27 CLN3 from the Hochstein natural-history study, 32 CLN2
from an international longitudinal cohort, 12 early-manifestation CLN2 cases
reported by Cokal et al., and 8 public Radiopaedia scans) together with 90
age-matched controls, outperforming a 3D-ResNet and Swin-Tiny baseline. We
further integrate Gradient-weighted Class Activation Mapping (Grad-CAM) to
highlight disease-relevant brain regions, enabling explainable predictions. The
model's small size and strong performance (sensitivity greater than 90%,
specificity approximately 90%) demonstrates a practical AI solution for early
Batten disease detection.

</details>


### [2] [Ultralytics YOLO Evolution: An Overview of YOLO26, YOLO11, YOLOv8 and YOLOv5 Object Detectors for Computer Vision and Pattern Recognition](https://arxiv.org/abs/2510.09653)
*Ranjan Sapkota,Manoj Karkee*

Main category: cs.CV

TL;DR: 本文概述了Ultralytics YOLO系列目标检测器，涵盖了其架构演变、基准测试、部署和未来挑战。


<details>
  <summary>Details</summary>
Motivation: 全面概述Ultralytics YOLO系列目标检测器，包括其架构演变、基准测试、部署考量和未来挑战。

Method: 本文首先介绍了YOLO26（YOLOv26）的关键创新，然后追溯了YOLO11、YOLOv8和YOLOv5的进展。通过在MS COCO数据集上对YOLOv5、YOLOv8、YOLO11和YOLO26进行基准测试，并与其他检测器进行比较，分析了精确度、召回率、F1分数、平均精度和推理速度等指标。

Result: YOLO26引入了DFL移除、原生NMS-free推理、ProgLoss、STAL和MuSGD优化器等创新。YOLO系列在准确性和效率之间取得了平衡，并在机器人、农业、监控和制造业等领域有实际应用。

Conclusion: Ultralytics YOLO系列在目标检测领域取得了显著进展，但仍面临密集场景限制、混合CNN-Transformer集成、开放词汇检测和边缘感知训练等挑战，需要进一步研究。

Abstract: This paper presents a comprehensive overview of the Ultralytics YOLO(You Only
Look Once) family of object detectors, focusing the architectural evolution,
benchmarking, deployment perspectives, and future challenges. The review begins
with the most recent release, YOLO26 (YOLOv26), which introduces key
innovations including Distribution Focal Loss (DFL) removal, native NMS-free
inference, Progressive Loss Balancing (ProgLoss), Small-Target-Aware Label
Assignment (STAL), and the MuSGD optimizer for stable training. The progression
is then traced through YOLO11, with its hybrid task assignment and
efficiency-focused modules; YOLOv8, which advanced with a decoupled detection
head and anchor-free predictions; and YOLOv5, which established the modular
PyTorch foundation that enabled modern YOLO development. Benchmarking on the MS
COCO dataset provides a detailed quantitative comparison of YOLOv5, YOLOv8,
YOLO11, and YOLO26, alongside cross-comparisons with YOLOv12, YOLOv13, RT-DETR,
and DEIM. Metrics including precision, recall, F1 score, mean Average
Precision, and inference speed are analyzed to highlight trade-offs between
accuracy and efficiency. Deployment and application perspectives are further
discussed, covering export formats, quantization strategies, and real-world use
in robotics, agriculture, surveillance, and manufacturing. Finally, the paper
identifies challenges and future directions, including dense-scene limitations,
hybrid CNN-Transformer integration, open-vocabulary detection, and edge-aware
training approaches.

</details>


### [3] [OmniSAT: Compact Action Token, Faster Auto Regression](https://arxiv.org/abs/2510.09667)
*Huaihai Lyu,Chaofan Chen,Senwei Xie,Pengwei Wang,Xiansheng Chen,Shanghang Zhang,Changsheng Xu*

Main category: cs.CV

TL;DR: 本文介绍了一种名为OmniSAT的通用快速动作分词器，它能学习紧凑、可迁移的动作表示，有效缩短训练序列并降低目标熵，从而加速AR训练收敛并提升模型性能。


<details>
  <summary>Details</summary>
Motivation: 现有的VLA模型在处理扩展和高维动作序列时，面临着序列长度过长的问题，而先前的压缩方法又存在重建质量差或压缩效率低的问题。

Method: OmniSAT首先通过B-Spline编码统一动作表示，然后对位置、旋转和夹持器子空间应用多阶段残差量化，以实现粗到细粒度的离散令牌压缩。接着，将OmniSAT应用于大规模数据集Droid进行预训练，并提出一种跨本体学习策略，利用统一的动作模式空间，结合机器人和人类的演示数据进行联合学习。

Result: 经过预训练，OmniSAT将训练序列缩短了6.8倍，并降低了目标熵。此外，它在实现更高压缩率的同时，保持了重建质量，加速了AR训练收敛并提升了模型性能。

Conclusion: OmniSAT通过学习紧凑、可迁移的动作表示，有效解决了VLA模型在处理长序列时的效率问题，为大规模预训练和跨本体学习提供了新的可能。

Abstract: Existing Vision-Language-Action (VLA) models can be broadly categorized into
diffusion-based and auto-regressive (AR) approaches: diffusion models capture
continuous action distributions but rely on computationally heavy iterative
denoising. In contrast, AR models enable efficient optimization and flexible
sequence construction, making them better suited for large-scale pretraining.
To further improve AR efficiency, particularly when action chunks induce
extended and high-dimensional sequences, prior work applies entropy-guided and
token-frequency techniques to shorten the sequence length. However, such
compression struggled with \textit{poor reconstruction or inefficient
compression}. Motivated by this, we introduce an Omni Swift Action Tokenizer,
which learns a compact, transferable action representation. Specifically, we
first normalize value ranges and temporal horizons to obtain a consistent
representation with B-Spline encoding. Then, we apply multi-stage residual
quantization to the position, rotation, and gripper subspaces, producing
compressed discrete tokens with coarse-to-fine granularity for each part. After
pre-training on the large-scale dataset Droid, the resulting discrete
tokenization shortens the training sequence by 6.8$\times$, and lowers the
target entropy. To further explore the potential of OmniSAT, we develop a
cross-embodiment learning strategy that builds on the unified action-pattern
space and jointly leverages robot and human demonstrations. It enables scalable
auxiliary supervision from heterogeneous egocentric videos. Across diverse
real-robot and simulation experiments, OmniSAT encompasses higher compression
while preserving reconstruction quality, enabling faster AR training
convergence and model performance.

</details>


### [4] [Multi Camera Connected Vision System with Multi View Analytics: A Comprehensive Survey](https://arxiv.org/abs/2510.09731)
*Muhammad Munsif,Waqas Ahmad,Amjid Ali,Mohib Ullah,Adnan Hussain,Sung Wook Baik*

Main category: cs.CV

TL;DR: 本文对多视角多摄像头（MVMC）连接视觉系统（CVS）进行了全面而综合的综述，统一了MVMC跟踪、Re-ID和动作理解，并提出了独特的分类方法、总结了现有技术水平，并指出了未来的研究方向。


<details>
  <summary>Details</summary>
Motivation: 现有的调查主要关注跟踪、Re-ID和动作理解等孤立任务，忽略了它们在多视角多摄像头（MVMC）连接视觉系统（CVS）中的整合，也忽视了多摄像头协作和多视角数据分析带来的复杂性和机遇。

Method: 本文提出了一种独特的分类方法，将连接视觉系统分为MVMC跟踪、Re-ID、动作理解和组合方法四个关键部分。系统地整理和总结了最先进的数据集、方法、结果和评估指标。

Result: 本文首次提供了对多视角多摄像头（MVMC）连接视觉系统（CVS）的全面而综合的综述，统一了MVMC跟踪、Re-ID和动作理解，并提出了独特的分类方法、总结了现有技术水平。

Conclusion: 本文指出了开放研究问题和挑战，以及新兴技术，并概述了加强CVS在复杂现实应用中的鲁棒性、效率和适应性的关键研究方向，以期激励创新解决方案并指导未来的研究。

Abstract: Connected Vision Systems (CVS) are transforming a variety of applications,
including autonomous vehicles, smart cities, surveillance, and human-robot
interaction. These systems harness multi-view multi-camera (MVMC) data to
provide enhanced situational awareness through the integration of MVMC
tracking, re-identification (Re-ID), and action understanding (AU). However,
deploying CVS in real-world, dynamic environments presents a number of
challenges, particularly in addressing occlusions, diverse viewpoints, and
environmental variability. Existing surveys have focused primarily on isolated
tasks such as tracking, Re-ID, and AU, often neglecting their integration into
a cohesive system. These reviews typically emphasize single-view setups,
overlooking the complexities and opportunities provided by multi-camera
collaboration and multi-view data analysis. To the best of our knowledge, this
survey is the first to offer a comprehensive and integrated review of MVMC that
unifies MVMC tracking, Re-ID, and AU into a single framework. We propose a
unique taxonomy to better understand the critical components of CVS, dividing
it into four key parts: MVMC tracking, Re-ID, AU, and combined methods. We
systematically arrange and summarize the state-of-the-art datasets,
methodologies, results, and evaluation metrics, providing a structured view of
the field's progression. Furthermore, we identify and discuss the open research
questions and challenges, along with emerging technologies such as lifelong
learning, privacy, and federated learning, that need to be addressed for future
advancements. The paper concludes by outlining key research directions for
enhancing the robustness, efficiency, and adaptability of CVS in complex,
real-world applications. We hope this survey will inspire innovative solutions
and guide future research toward the next generation of intelligent and
adaptive CVS.

</details>


### [5] [Constructive Distortion: Improving MLLMs with Attention-Guided Image Warping](https://arxiv.org/abs/2510.09741)
*Dwip Dalal,Gautam Vashishtha,Utkarsh Mishra,Jeonghwan Kim,Madhav Kanda,Hyeonjeong Ha,Svetlana Lazebnik,Heng Ji,Unnat Jain*

Main category: cs.CV

TL;DR: AttWarp通过对图像进行注意力引导的扭曲，将更多分辨率分配给与查询相关的内容，同时压缩信息较少的区域，从而提高多模态大型语言模型（MLLMs）在细粒度感知方面的性能，减少幻觉并改善组合推理。


<details>
  <summary>Details</summary>
Motivation: 多模态大型语言模型（MLLMs）在处理复杂场景时，往往会忽略小的细节和空间关系，导致在细粒度感知接地方面出现错误。

Method: AttWarp是一种轻量级方法，它在测试时利用MLLM的跨模态注意力对输入图像进行直线扭曲，将空间分辨率重新分配给模型认为重要的区域，同时保留全局上下文，而不改变模型权重或架构。

Result: AttWarp在五个基准测试（TextVQA、GQA、DocVQA、POPE、MMMU）和四个MLLM（LLaVA、Qwen-VL、InternVL和InstructBLIP）上一致地提高了准确性，增强了组合推理，并减少了幻觉，优于四种竞争基线方法。

Conclusion: 注意力引导的扭曲能够优先处理与查询相关的信息，同时保留上下文，并且相同的MLLM在给定此类扭曲输入时表现更好。

Abstract: Multimodal large language models (MLLMs) often miss small details and spatial
relations in cluttered scenes, leading to errors in fine-grained perceptual
grounding. We introduce AttWarp, a lightweight method that allocates more
resolution to query-relevant content while compressing less informative areas,
all while preserving global context. At test time, the approach uses an MLLM's
cross-modal attention to perform rectilinear warping of the input image,
reallocating spatial resolution toward regions the model deems important,
without changing model weights or architecture. This attention-guided warping
preserves all original image information but redistributes it non-uniformly, so
small objects and subtle relationships become easier for the same model to read
while the global layout remains intact. Across five benchmarks (TextVQA, GQA,
DocVQA, POPE, MMMU) and four MLLMs (LLaVA, Qwen-VL, InternVL, and
InstructBLIP), AttWarp consistently improves accuracy, strengthens
compositional reasoning, and reduces hallucinations, outperforming four
competitive baselines that manipulate raw images at test time. Together, these
results show that attention-guided warping prioritizes information relevant to
the query while preserving context, and that the same MLLMs perform better when
given such warped inputs.

</details>


### [6] [Towards Understanding Ambiguity Resolution in Multimodal Inference of Meaning](https://arxiv.org/abs/2510.09815)
*Yufei Wang,Adriana Kovashka,Loretta Fernández,Marc N. Coutanche,Seth Wiener*

Main category: cs.CV

TL;DR: 这篇文章探讨了一种新的外语学习环境，即学习者在多模态语境的句子中推断生词的意义，并配对图片。文章研究了数据特征和参与者的语言背景如何影响学习效果，并分析了人工智能系统在此任务中的表现。


<details>
  <summary>Details</summary>
Motivation: 研究在一个新的多模态外语学习环境中，学习者如何通过图像和句子来推断生词的意义，并找出影响推断成功的因素。

Method: 通过与真人参与者进行实验，使用不同的图像-文本对，分析数据（图像和文本）的特征以及参与者的语言背景与推断成功率的相关性。同时，也分析了人工智能系统在预测参与者表现方面的能力。

Result: 发现只有少数直观特征与参与者的表现有强相关性。人工智能系统在推断参与者表现方面展现出一定的潜力。

Conclusion: 需要进一步研究预测学习成功率的特征。人工智能系统在理解和预测人类学习表现方面有待进一步提高。

Abstract: We investigate a new setting for foreign language learning, where learners
infer the meaning of unfamiliar words in a multimodal context of a sentence
describing a paired image. We conduct studies with human participants using
different image-text pairs. We analyze the features of the data (i.e., images
and texts) that make it easier for participants to infer the meaning of a
masked or unfamiliar word, and what language backgrounds of the participants
correlate with success. We find only some intuitive features have strong
correlations with participant performance, prompting the need for further
investigating of predictive features for success in these tasks. We also
analyze the ability of AI systems to reason about participant performance, and
discover promising future directions for improving this reasoning ability.

</details>


### [7] [Task-Aware Resolution Optimization for Visual Large Language Models](https://arxiv.org/abs/2510.09822)
*Weiqing Luo,Zhen Tan,Yifan Li,Xinyu Zhao,Kwonjoon Lee,Behzad Dariush,Tianlong Chen*

Main category: cs.CV

TL;DR: 这篇论文介绍了一种根据图像复杂度和VLLM不确定性方差，为视觉语言任务确定最佳分辨率的方法。


<details>
  <summary>Details</summary>
Motivation: 现有的视觉大型语言模型（VLLM）在处理不同感知粒度的视觉语言任务时，预设了固定的分辨率，导致性能不佳。

Method: 1. 调查了不同视觉语言任务的分辨率偏好，发现分辨率偏好与图像复杂度和VLLM在不同图像输入分辨率下的不确定性方差相关。 2. 提出了一个结合图像复杂度和不确定性方差的经验公式，用于确定给定视觉语言任务的最优分辨率。 3. 提出了一种新颖的参数高效微调技术，将预训练VLLM的视觉输入分辨率扩展到所确定的最优分辨率。

Result: 在各种视觉语言任务上的大量实验验证了该方法的有效性。

Conclusion: 通过根据图像复杂度和VLLM的不确定性方差，为视觉语言任务确定最优分辨率，并采用参数高效微调技术，可以有效提升VLLM的性能。

Abstract: Real-world vision-language applications demand varying levels of perceptual
granularity. However, most existing visual large language models (VLLMs), such
as LLaVA, pre-assume a fixed resolution for downstream tasks, which leads to
subpar performance. To address this problem, we first conduct a comprehensive
and pioneering investigation into the resolution preferences of different
vision-language tasks, revealing a correlation between resolution preferences
with image complexity, and uncertainty variance of the VLLM at different image
input resolutions. Building on this insight, we propose an empirical formula to
determine the optimal resolution for a given vision-language task, combining
these two factors. Second, based on rigorous experiments, we propose a novel
parameter-efficient fine-tuning technique to extend the visual input resolution
of pre-trained VLLMs to the identified optimal resolution. Extensive
experiments on various vision-language tasks validate the effectiveness of our
method.

</details>


### [8] [Post Processing of image segmentation using Conditional Random Fields](https://arxiv.org/abs/2510.09833)
*Aashish Dhawan,Pankaj Bodani,Vishal Garg*

Main category: cs.CV

TL;DR: 这篇论文旨在通过寻找合适的条件随机场（CRF）来改善卫星图像分割结果的清晰度。


<details>
  <summary>Details</summary>
Motivation: 由于卫星图像特征质量较低，图像分割的输出通常不够清晰。本研究的目的是寻找合适的条件随机场（CRF）以获得更好的分割图像清晰度。

Method: 研究者首先研究了不同类型的CRF，分析其是否适合此目的。然后，他们在两个不同的数据集（低质量特征的卫星图像和高质量的航拍照片）上评估了他们的方法。在研究过程中，他们尝试了各种CRF，以找到在图像上表现最佳的CRF。

Result: 通过在两种数据集上比较不同方法的结果，研究者展示了不同方法的缺陷和潜力。

Conclusion: 本研究通过实验找到了在卫星图像分割中表现最佳的CRF，并揭示了不同CRF方法的优缺点。

Abstract: The output of image the segmentation process is usually not very clear due to
low quality features of Satellite images. The purpose of this study is to find
a suitable Conditional Random Field (CRF) to achieve better clarity in a
segmented image. We started with different types of CRFs and studied them as to
why they are or are not suitable for our purpose. We evaluated our approach on
two different datasets - Satellite imagery having low quality features and high
quality Aerial photographs. During the study we experimented with various CRFs
to find which CRF gives the best results on images and compared our results on
these datasets to show the pitfalls and potentials of different approaches.

</details>


### [9] [Exploration of Incremental Synthetic Non-Morphed Images for Single Morphing Attack Detection](https://arxiv.org/abs/2510.09836)
*David Benavente-Rios,Juan Ruiz Rodriguez,Gustavo Gatica*

Main category: cs.CV

TL;DR: 该文探讨了如何利用合成人脸数据增强单次形变攻击检测（S-MAD），以应对因隐私问题导致的大规模真实图像数据集获取受限的挑战。


<details>
  <summary>Details</summary>
Motivation: 解决真实人脸数据在S-MAD领域中因隐私问题导致的获取困难，从而提高模型的泛化能力。

Method: 作者使用了多种形变工具和跨数据集评估方案。通过增量测试协议，评估了随着合成图像数量增加，模型的泛化能力变化。

Result: 实验结果表明，在现有数据集中谨慎地加入受控数量的合成图像，或者在训练过程中逐步增加真实图像，都能有效提升模型的泛化能力。然而，不加区分地使用合成数据会导致性能下降。仅使用合成数据（包括形变和非形变图像）会导致最高的等错误率（EER），这表明在实际操作中，S-MAD不应完全依赖合成数据。

Conclusion: 通过策略性地整合合成数据或逐步添加真实数据，可以有效提升S-MAD的性能和泛化能力，但需要避免过度或不加区分地使用合成数据，因为这可能导致性能下降。在实际应用中，不能单独依赖合成数据。

Abstract: This paper investigates the use of synthetic face data to enhance
Single-Morphing Attack Detection (S-MAD), addressing the limitations of
availability of large-scale datasets of bona fide images due to privacy
concerns. Various morphing tools and cross-dataset evaluation schemes were
utilized to conduct this study. An incremental testing protocol was implemented
to assess the generalization capabilities as more and more synthetic images
were added. The results of the experiments show that generalization can be
improved by carefully incorporating a controlled number of synthetic images
into existing datasets or by gradually adding bona fide images during training.
However, indiscriminate use of synthetic data can lead to sub-optimal
performance. Evenmore, the use of only synthetic data (morphed and non-morphed
images) achieves the highest Equal Error Rate (EER), which means in operational
scenarios the best option is not relying only on synthetic data for S-MAD.

</details>


### [10] [Cell Instance Segmentation: The Devil Is in the Boundaries](https://arxiv.org/abs/2510.09848)
*Peixian Liang,Yifan Ding,Yizhe Zhang,Jianxu Chen,Hao Zheng,Hongxiao Wang,Yejia Zhang,Guangyu Meng,Tim Weninger,Michael Niemier,X. Sharon Hu,Danny Z Chen*

Main category: cs.CV

TL;DR: Ceb是一种新颖的像素聚类方法，它利用细胞边界特征和标签将前景像素划分为细胞实例。


<details>
  <summary>Details</summary>
Motivation: 目前最先进的细胞实例分割方法都是基于深度学习语义分割方法。为了从前景像素中识别细胞实例，大多数方法将实例信息分解为像素级目标，如到前景-背景边界的距离，以中心点为热源的热扩散图，以及从中心点到具有固定角度的前景-背景边界的距离。然而，像素级目标可能会丢失细胞实例的重要几何属性，如形状、曲率和凸度，这些属性需要像素集合来表示。

Method: Ceb首先从语义分割生成的概率图中提取潜在的前景-前景边界。对于每个边界候选，通过从当前前景-前景边界以及相邻的背景-前景边界采样像素来构建边界特征表示（称为边界签名）。接下来，使用边界分类器根据相应的边界签名预测其二元边界标签。最后，通过根据预测的边界标签划分或合并相邻区域来获得细胞实例。

Result: 在六个数据集上进行了广泛的实验，结果表明Ceb在语义分割概率图上优于现有的像素聚类方法。

Conclusion: Ceb实现了与SOTA细胞实例分割方法相比极具竞争力的性能。

Abstract: State-of-the-art (SOTA) methods for cell instance segmentation are based on
deep learning (DL) semantic segmentation approaches, focusing on distinguishing
foreground pixels from background pixels. In order to identify cell instances
from foreground pixels (e.g., pixel clustering), most methods decompose
instance information into pixel-wise objectives, such as distances to
foreground-background boundaries (distance maps), heat gradients with the
center point as heat source (heat diffusion maps), and distances from the
center point to foreground-background boundaries with fixed angles (star-shaped
polygons). However, pixel-wise objectives may lose significant geometric
properties of the cell instances, such as shape, curvature, and convexity,
which require a collection of pixels to represent. To address this challenge,
we present a novel pixel clustering method, called Ceb (for Cell boundaries),
to leverage cell boundary features and labels to divide foreground pixels into
cell instances. Starting with probability maps generated from semantic
segmentation, Ceb first extracts potential foreground-foreground boundaries
with a revised Watershed algorithm. For each boundary candidate, a boundary
feature representation (called boundary signature) is constructed by sampling
pixels from the current foreground-foreground boundary as well as the
neighboring background-foreground boundaries. Next, a boundary classifier is
used to predict its binary boundary label based on the corresponding boundary
signature. Finally, cell instances are obtained by dividing or merging
neighboring regions based on the predicted boundary labels. Extensive
experiments on six datasets demonstrate that Ceb outperforms existing pixel
clustering methods on semantic segmentation probability maps. Moreover, Ceb
achieves highly competitive performance compared to SOTA cell instance
segmentation methods.

</details>


### [11] [Cluster-Aware Prompt Ensemble Learning for Few-Shot Vision-Language Model Adaptation](https://arxiv.org/abs/2510.09867)
*Zhi Chen,Xin Yu,Xiaohui Tao,Yan Li,Zi Huang*

Main category: cs.CV

TL;DR: 本文提出了一种名为CAPEL的视觉语言模型，通过在分类逻辑空间中进行提示集成，并引入簇保持正则化项和自适应提示加权技术，解决了传统提示集成方法的次优性问题，从而提高了模型在零样本设置下的性能。


<details>
  <summary>Details</summary>
Motivation: 传统的视觉语言模型（如CLIP）在零样本迁移中表现出色，但其通过平均上下文提示的文本特征进行提示集成的方式可能导致类中心偏离真实的类分布，从而产生次优结果。

Method: 我们提出了CAPEL框架，它通过将图像分类到多个类簇中的一个，每个类簇由一个独特的提示表示。CAPEL在分类逻辑空间而不是特征空间中进行提示集成，以更好地与视觉特征分布对齐。为了优化提示微调并保持簇的判别能力，我们引入了一个簇保持正则化项，确保提示保持独立和专业化，防止其坍塌为统一方向。此外，我们还集成了一种自适应提示加权技术，以动态调整对有缺陷或模糊提示的注意力权重。

Result: CAPEL框架解决了传统提示集成方法中类中心偏离真实类分布的问题，通过在分类逻辑空间集成提示、引入簇保持正则化项和自适应提示加权技术，提高了视觉语言模型在零样本设置下的性能。

Conclusion: CAPEL通过创新的提示集成策略和正则化技术，显著提升了视觉语言模型在零样本学习场景下的鲁棒性和准确性，为未来的视觉语言模型发展提供了新的方向。

Abstract: Vision-language models (VLMs) such as CLIP achieve zero-shot transfer across
various tasks by pre-training on numerous image-text pairs. These models often
benefit from using an ensemble of context prompts to represent a class. Despite
being effective, conventional prompt ensembling that averages textual features
of context prompts often yields suboptimal results. This is because feature
averaging shifts the class centroids away from the true class distribution. To
address this issue, we propose the Cluster-Aware Prompt Ensemble Learning
(CAPEL) framework, which preserves the cluster nature of context prompts. CAPEL
classifies images into one of several class clusters, each represented by a
distinct prompt. Instead of ensembling prompts in the feature space, we perform
ensembling in the classification logits space, aligning better with the visual
feature distribution. To further optimize prompt fine-tuning while maintaining
cluster-specific discriminative power, we introduce a cluster-preserving
regularization term. This ensures that prompts remain distinct and specialized
for different clusters, preventing collapse into a uniform direction.
Additionally, we integrate an adaptive prompt weighting technique to
dynamically adjust the attention weights for flawed or ambiguous prompts,
ensuring robust performance across diverse datasets and tasks.

</details>


### [12] [Fast Self-Supervised depth and mask aware Association for Multi-Object Tracking](https://arxiv.org/abs/2510.09878)
*Milad Khanchi,Maria Amer,Charalambos Poullis*

Main category: cs.CV

TL;DR: 该论文提出了一种新的多目标跟踪（MOT）方法，通过融合深度和掩码特征并利用自监督编码器生成稳定的对象表示，从而在不计算分割IoU的情况下，结合边界框IoU和Re-ID特征进行匹配。


<details>
  <summary>Details</summary>
Motivation: 现有的多目标跟踪（MOT）方法常依赖于IoU进行关联，但在对象相似或遮挡时变得不可靠。此外，计算分割掩码的IoU计算成本高昂。

Method: 本研究不计算分割IoU，而是融合深度和掩码特征，并将其通过一个自监督训练的紧凑编码器。该编码器生成稳定的对象表示，作为除边界框IoU和Re-ID特征之外的额外相似性线索进行匹配。深度图来自零样本深度估计器，对象掩码来自可提示的视觉分割模型，以获取细粒度空间线索。

Result: 在具有非线性运动、遮挡和拥挤场景（如SportsMOT和DanceTrack）的挑战性基准测试中，我们的TBD方法在大多数指标上优于TBD领域最先进的技术。在具有线性运动的简单基准测试（如MOT17）上，也取得了有竞争力的性能。

Conclusion: 该方法是首个使用自监督编码器来细化分割掩码而无需计算掩码IoU的MOT方法。它在具有挑战性的场景中表现出色，并在更简单的场景中保持竞争力，证明了其在多目标跟踪领域的有效性和效率。

Abstract: Multi-object tracking (MOT) methods often rely on Intersection-over-Union
(IoU) for association. However, this becomes unreliable when objects are
similar or occluded. Also, computing IoU for segmentation masks is
computationally expensive. In this work, we use segmentation masks to capture
object shapes, but we do not compute segmentation IoU. Instead, we fuse depth
and mask features and pass them through a compact encoder trained
self-supervised. This encoder produces stable object representations, which we
use as an additional similarity cue alongside bounding box IoU and
re-identification features for matching. We obtain depth maps from a zero-shot
depth estimator and object masks from a promptable visual segmentation model to
obtain fine-grained spatial cues. Our MOT method is the first to use the
self-supervised encoder to refine segmentation masks without computing masks
IoU. MOT can be divided into joint detection-ReID (JDR) and
tracking-by-detection (TBD) models. The latter are computationally more
efficient. Experiments of our TBD method on challenging benchmarks with
non-linear motion, occlusion, and crowded scenes, such as SportsMOT and
DanceTrack, show that our method outperforms the TBD state-of-the-art on most
metrics, while achieving competitive performance on simpler benchmarks with
linear motion, such as MOT17.

</details>


### [13] [CHUG: Crowdsourced User-Generated HDR Video Quality Dataset](https://arxiv.org/abs/2510.09879)
*Shreshth Saini,Alan C. Bovik,Neil Birkbeck,Yilin Wang,Balu Adsumilli*

Main category: cs.CV

TL;DR: 该文章介绍了CHUG数据集，一个用于评估用户生成HDR视频质量的大型数据集，以解决现有数据集中PGC内容为主，缺乏UGC内容的问题。


<details>
  <summary>Details</summary>
Motivation: 现有HDR视频质量评估数据集主要关注专业生成内容（PGC），忽视了用户生成内容（UGC）中由于多样拍摄条件、编辑伪影和压缩失真带来的独特挑战，导致无法有效理解真实世界UGC-HDR的质量下降问题。

Method: 本文推出了CHUG数据集，这是首个针对UGC-HDR视频质量进行大规模主观研究的数据集。CHUG包含了856个UGC-HDR源视频，通过多分辨率和比特率转码模拟真实世界场景，最终得到5,992个视频。研究通过Amazon Mechanical Turk收集了211,848个人类感知评分。

Result: CHUG数据集为分析HDR视频中UGC特有的失真提供了一个基准。

Conclusion: CHUG数据集的推出预计将通过提供一个大规模、多样化且真实的UGC数据集，推动无参考（NR）HDR-VQA研究的进展。

Abstract: High Dynamic Range (HDR) videos enhance visual experiences with superior
brightness, contrast, and color depth. The surge of User-Generated Content
(UGC) on platforms like YouTube and TikTok introduces unique challenges for HDR
video quality assessment (VQA) due to diverse capture conditions, editing
artifacts, and compression distortions. Existing HDR-VQA datasets primarily
focus on professionally generated content (PGC), leaving a gap in understanding
real-world UGC-HDR degradations. To address this, we introduce CHUG:
Crowdsourced User-Generated HDR Video Quality Dataset, the first large-scale
subjective study on UGC-HDR quality. CHUG comprises 856 UGC-HDR source videos,
transcoded across multiple resolutions and bitrates to simulate real-world
scenarios, totaling 5,992 videos. A large-scale study via Amazon Mechanical
Turk collected 211,848 perceptual ratings. CHUG provides a benchmark for
analyzing UGC-specific distortions in HDR videos. We anticipate CHUG will
advance No-Reference (NR) HDR-VQA research by offering a large-scale, diverse,
and real-world UGC dataset. The dataset is publicly available at:
https://shreshthsaini.github.io/CHUG/.

</details>


### [14] [Geometry-Aware Scene Configurations for Novel View Synthesis](https://arxiv.org/abs/2510.09880)
*Minkwan Kim,Changwoon Choi,Young Min Kim*

Main category: cs.CV

TL;DR: 本文提出了一种场景自适应策略，用于高效分配表示能力，从而在不完整观测中生成室内环境的沉浸式体验。


<details>
  <summary>Details</summary>
Motivation: 室内场景通常布局不规则，包含杂乱、遮挡和平面墙壁，现有方法在渲染质量和内存消耗上存在不足。

Method: 通过利用几何先验，引导基的优化放置，并提出场景自适应虚拟视点来弥补几何缺陷和施加必要的正则化。

Result: 与采用常规放置的基线方法相比，本文方法在渲染质量和内存需求方面均有显著提升。

Conclusion: 本文提出的场景自适应策略能在有限资源下生成高质量的室内环境沉浸式体验。

Abstract: We propose scene-adaptive strategies to efficiently allocate representation
capacity for generating immersive experiences of indoor environments from
incomplete observations. Indoor scenes with multiple rooms often exhibit
irregular layouts with varying complexity, containing clutter, occlusion, and
flat walls. We maximize the utilization of limited resources with guidance from
geometric priors, which are often readily available after pre-processing
stages. We record observation statistics on the estimated geometric scaffold
and guide the optimal placement of bases, which greatly improves upon the
uniform basis arrangements adopted by previous scalable Neural Radiance Field
(NeRF) representations. We also suggest scene-adaptive virtual viewpoints to
compensate for geometric deficiencies inherent in view configurations in the
input trajectory and impose the necessary regularization. We present a
comprehensive analysis and discussion regarding rendering quality and memory
requirements in several large-scale indoor scenes, demonstrating significant
enhancements compared to baselines that employ regular placements.

</details>


### [15] [LTGS: Long-Term Gaussian Scene Chronology From Sparse View Updates](https://arxiv.org/abs/2510.09881)
*Minkwan Kim,Seungmin Lee,Junho Kim,Young Min Kim*

Main category: cs.CV

TL;DR: LTGS是一种高效的场景表示方法，可以从高度不受约束的偶然捕获中实现日常变化。


<details>
  <summary>Details</summary>
Motivation: 传统的相机捕获在获取日常环境时面临挑战，因为场景频繁变化，需要空间和时间上密集的观察。

Method: 我们构建了对象作为模板高斯，通过少量观察，模板高斯通过进一步的细化管道来适应时间变化的环境。

Result: 与现有基线相比，我们的框架实现了卓越的重建质量，同时支持快速和轻量级更新。

Conclusion: 实验表明，我们的框架能够通过简单的转换在多个时间步长上进行泛化，显著增强了3D环境时间演化的可扩展性。

Abstract: Recent advances in novel-view synthesis can create the photo-realistic
visualization of real-world environments from conventional camera captures.
However, acquiring everyday environments from casual captures faces challenges
due to frequent scene changes, which require dense observations both spatially
and temporally. We propose long-term Gaussian scene chronology from sparse-view
updates, coined LTGS, an efficient scene representation that can embrace
everyday changes from highly under-constrained casual captures. Given an
incomplete and unstructured Gaussian splatting representation obtained from an
initial set of input images, we robustly model the long-term chronology of the
scene despite abrupt movements and subtle environmental variations. We
construct objects as template Gaussians, which serve as structural, reusable
priors for shared object tracks. Then, the object templates undergo a further
refinement pipeline that modulates the priors to adapt to temporally varying
environments based on few-shot observations. Once trained, our framework is
generalizable across multiple time steps through simple transformations,
significantly enhancing the scalability for a temporal evolution of 3D
environments. As existing datasets do not explicitly represent the long-term
real-world changes with a sparse capture setup, we collect real-world datasets
to evaluate the practicality of our pipeline. Experiments demonstrate that our
framework achieves superior reconstruction quality compared to other baselines
while enabling fast and light-weight updates.

</details>


### [16] [SpectralCA: Bi-Directional Cross-Attention for Next-Generation UAV Hyperspectral Vision](https://arxiv.org/abs/2510.09912)
*D. V. Brovko*

Main category: cs.CV

TL;DR: 这篇论文提出了一种结合高光谱成像（HSI）与无人机（UAV）感知深度学习架构，用于在复杂环境下实现导航、目标检测及地形分类等任务，并通过在MDvT中引入SpectralCA模块，利用双向交叉注意力融合光谱与空间特征，在WHU-Hi-HongHu数据集上验证了其有效性。


<details>
  <summary>Details</summary>
Motivation: 在复杂环境下，由于干扰、能见度差或伪装，传统导航变得不可靠，日益增长的无人机（UAV）需求促使高光谱成像（HSI）在UAV计算机视觉中展现出独特优势，可以实现细粒度的材料识别和物体区分，这对于导航、监视、农业和环境监测至关重要。

Method: 论文通过修改Mobile 3D Vision Transformer (MDvT)，引入了SpectralCA模块。SpectralCA模块利用双向交叉注意力融合光谱和空间特征，以提高准确性，同时减少参数和推理时间。

Result: 在WHU-Hi-HongHu数据集上的实验评估结果表明，所提出的架构提高了无人机感知效率，实现了导航、目标识别和环境监测任务的实时操作。评估指标包括总体准确度（Overall Accuracy）、平均准确度（Average Accuracy）和Kappa系数。

Conclusion: 本研究通过开发一种集成了高光谱成像的深度学习架构，显著提升了无人机在复杂环境下的感知能力。所提出的SpectralCA模块在融合光谱与空间特征方面表现出色，有效提升了导航、目标检测和地形分类的准确性及效率，为实时无人机应用提供了有力支持。

Abstract: The relevance of this research lies in the growing demand for unmanned aerial
vehicles (UAVs) capable of operating reliably in complex environments where
conventional navigation becomes unreliable due to interference, poor
visibility, or camouflage. Hyperspectral imaging (HSI) provides unique
opportunities for UAV-based computer vision by enabling fine-grained material
recognition and object differentiation, which are critical for navigation,
surveillance, agriculture, and environmental monitoring. The aim of this work
is to develop a deep learning architecture integrating HSI into UAV perception
for navigation, object detection, and terrain classification. Objectives
include: reviewing existing HSI methods, designing a hybrid 2D/3D convolutional
architecture with spectral-spatial cross-attention, training, and benchmarking.
The methodology is based on the modification of the Mobile 3D Vision
Transformer (MDvT) by introducing the proposed SpectralCA block. This block
employs bi-directional cross-attention to fuse spectral and spatial features,
enhancing accuracy while reducing parameters and inference time. Experimental
evaluation was conducted on the WHU-Hi-HongHu dataset, with results assessed
using Overall Accuracy, Average Accuracy, and the Kappa coefficient. The
findings confirm that the proposed architecture improves UAV perception
efficiency, enabling real-time operation for navigation, object recognition,
and environmental monitoring tasks.
  Keywords: SpectralCA, deep learning, computer vision, hyperspectral imaging,
unmanned aerial vehicle, object detection, semi-supervised learning.

</details>


### [17] [HeadsUp! High-Fidelity Portrait Image Super-Resolution](https://arxiv.org/abs/2510.09924)
*Renjie Li,Zihao Zhu,Xiaoyu Wang,Zhengzhong Tu*

Main category: cs.CV

TL;DR: HeadsUp是一个单步扩散模型，能够无缝地恢复和放大肖像图像，解决了现有方法在处理人脸和背景时引入的融合或边界伪影问题。


<details>
  <summary>Details</summary>
Motivation: 现有的图像超分辨率技术在处理肖像照片时，通常需要将人脸专家模型和通用模型进行混合，但这会导致人脸区域出现融合或边界伪影。

Method: HeadsUp模型建立在单步扩散模型的基础上，开发了人脸监督机制来引导模型关注人脸区域，并集成了基于参考的机制来帮助恢复身份。此外，还构建了一个高质量的4K肖像图像ISR数据集PortraitSR-4K用于模型训练和基准测试。

Result: HeadsUp在PortraitISR任务上取得了最先进的性能，同时在通用图像和对齐人脸数据集上保持了相当或更高的性能。

Conclusion: HeadsUp作为一个端到端的单步扩散模型，通过人脸监督和参考机制，有效解决了肖像图像超分辨率中的伪影问题，并发布了高质量数据集，显著提升了该领域的性能。

Abstract: Portrait pictures, which typically feature both human subjects and natural
backgrounds, are one of the most prevalent forms of photography on social
media. Existing image super-resolution (ISR) techniques generally focus either
on generic real-world images or strictly aligned facial images (i.e., face
super-resolution). In practice, separate models are blended to handle portrait
photos: the face specialist model handles the face region, and the general
model processes the rest. However, these blending approaches inevitably
introduce blending or boundary artifacts around the facial regions due to
different model training recipes, while human perception is particularly
sensitive to facial fidelity. To overcome these limitations, we study the
portrait image supersolution (PortraitISR) problem, and propose HeadsUp, a
single-step diffusion model that is capable of seamlessly restoring and
upscaling portrait images in an end-to-end manner. Specifically, we build our
model on top of a single-step diffusion model and develop a face supervision
mechanism to guide the model in focusing on the facial region. We then
integrate a reference-based mechanism to help with identity restoration,
reducing face ambiguity in low-quality face restoration. Additionally, we have
built a high-quality 4K portrait image ISR dataset dubbed PortraitSR-4K, to
support model training and benchmarking for portrait images. Extensive
experiments show that HeadsUp achieves state-of-the-art performance on the
PortraitISR task while maintaining comparable or higher performance on both
general image and aligned face datasets.

</details>


### [18] [Denoising Diffusion as a New Framework for Underwater Images](https://arxiv.org/abs/2510.09934)
*Nilesh Jain,Elie Alhajjar*

Main category: cs.CV

TL;DR: 为了解决水下图像质量差、数据集多样性不足的问题，本文提出利用去噪扩散模型扩展数据集，并使用Controlnet提升图像质量。


<details>
  <summary>Details</summary>
Motivation: 水下图像对于海洋研究至关重要，但复杂的水下环境导致图像质量差，现有增强方法也存在泛化性差和依赖高质量数据集的问题，且现有数据集缺乏多样性和质量。

Method: 1. 利用去噪扩散模型 (Denoising Diffusion Model) 扩展数据集，以包含立体、广角、微距和特写等多种图像类型。
2. 推荐使用 Controlnet 增强图像质量，以评估和提升相应数据集的质量。

Result: 本文未直接给出实验结果，但提出的方法旨在解决水下图像的低质量和数据集多样性不足的问题，并通过扩充和增强数据集来改善海洋生态系统研究。

Conclusion: 通过利用去噪扩散模型扩展数据集的多样性，并结合 Controlnet 提升图像质量，可以有效克服现有水下图像处理的局限性，从而更好地服务于海洋生态系统研究。

Abstract: Underwater images play a crucial role in ocean research and marine
environmental monitoring since they provide quality information about the
ecosystem. However, the complex and remote nature of the environment results in
poor image quality with issues such as low visibility, blurry textures, color
distortion, and noise. In recent years, research in image enhancement has
proven to be effective but also presents its own limitations, like poor
generalization and heavy reliance on clean datasets. One of the challenges
herein is the lack of diversity and the low quality of images included in these
datasets. Also, most existing datasets consist only of monocular images, a fact
that limits the representation of different lighting conditions and angles. In
this paper, we propose a new plan of action to overcome these limitations. On
one hand, we call for expanding the datasets using a denoising diffusion model
to include a variety of image types such as stereo, wide-angled, macro, and
close-up images. On the other hand, we recommend enhancing the images using
Controlnet to evaluate and increase the quality of the corresponding datasets,
and hence improve the study of the marine ecosystem.
  Tags - Underwater Images, Denoising Diffusion, Marine ecosystem, Controlnet

</details>


### [19] [Semi-disentangled spatiotemporal implicit neural representations of longitudinal neuroimaging data for trajectory classification](https://arxiv.org/abs/2510.09936)
*Agampreet Aulakh,Nils D. Forkert,Matthias Wilms*

Main category: cs.CV

TL;DR: 该研究提出了一种利用隐式神经表示（INRs）对人类大脑的结构变化进行建模和分类的新方法，该方法在处理纵向MRI数据（特别是时间采样不规则的数据）时，比传统深度学习方法具有更高的准确性。


<details>
  <summary>Details</summary>
Motivation: 人脑在整个生命周期中会经历动态的、可能由病理驱动的结构变化。纵向磁共振成像（MRI）及其他神经影像数据对于揭示与典型和非典型衰老相关的变化轨迹非常有价值。然而，由于这些数据在个体内部和跨人群中具有不同的空间和时间图像采样模式，使得其分析极具挑战性，传统深度学习方法难以表征潜在的连续生物过程。

Method: 本文提出了一种全新的、完全数据驱动的方法，通过使用隐式神经表示（INRs）将受试者特定的纵向T1加权MRI数据建模为连续函数，从而表征整个大脑的衰老轨迹。研究引入了一种新颖的INR架构，能够部分解耦空间和时间轨迹参数，并设计了一个高效的框架，直接在INRs的参数空间上进行操作，以对大脑衰老轨迹进行分类。为了在受控数据环境中评估该方法，研究开发了一个生物学上合理的轨迹模拟，并生成了450名健康和痴呆症受试者在规则和不规则采样时间点的T1加权3D MRI数据。

Result: 在更真实的非常规采样实验中，本文基于INR的方法在脑老化轨迹分类任务中达到了81.3%的准确率，优于标准深度学习基线模型（73.7%）。

Conclusion: 该研究提出的基于INR的方法能够有效地处理纵向MRI数据不规则采样的问题，并在脑老化轨迹分类任务中实现了比传统深度学习方法更高的准确率，为理解和分类大脑衰老过程提供了新的工具。

Abstract: The human brain undergoes dynamic, potentially pathology-driven, structural
changes throughout a lifespan. Longitudinal Magnetic Resonance Imaging (MRI)
and other neuroimaging data are valuable for characterizing trajectories of
change associated with typical and atypical aging. However, the analysis of
such data is highly challenging given their discrete nature with different
spatial and temporal image sampling patterns within individuals and across
populations. This leads to computational problems for most traditional deep
learning methods that cannot represent the underlying continuous biological
process. To address these limitations, we present a new, fully data-driven
method for representing aging trajectories across the entire brain by modelling
subject-specific longitudinal T1-weighted MRI data as continuous functions
using Implicit Neural Representations (INRs). Therefore, we introduce a novel
INR architecture capable of partially disentangling spatial and temporal
trajectory parameters and design an efficient framework that directly operates
on the INRs' parameter space to classify brain aging trajectories. To evaluate
our method in a controlled data environment, we develop a biologically grounded
trajectory simulation and generate T1-weighted 3D MRI data for 450 healthy and
dementia-like subjects at regularly and irregularly sampled timepoints. In the
more realistic irregular sampling experiment, our INR-based method achieves
81.3% accuracy for the brain aging trajectory classification task,
outperforming a standard deep learning baseline model (73.7%).

</details>


### [20] [Explainable Human-in-the-Loop Segmentation via Critic Feedback Signals](https://arxiv.org/abs/2510.09945)
*Pouya Shaeri,Ryan T. Woo,Yasaman Mohammadpour,Ariane Middel*

Main category: cs.CV

TL;DR: 该论文提出了一种人机交互框架，通过人工错误修正来训练分割模型，使其学习鲁棒的语义特征，从而提高模型在真实世界数据上的泛化能力和准确性，并显著减少标注工作量。


<details>
  <summary>Details</summary>
Motivation: 传统的分割模型在基准测试上表现良好，但在实际应用中，由于依赖虚假关联而非真实的物体边界，导致性能下降。

Method: 该方法将人工修正视为干预信号，用于指出模型何时不应依赖表面特征（如颜色或纹理）。系统通过将修正信息传播到视觉相似的图像中来学习这些干预，有效地引导模型学习鲁棒的、有语义意义的特征，而不是数据集中特定的伪影。

Result: 该框架将具有挑战性的cubemap数据上的分割精度提高了9mIoU点（相对提高12-15%），与标准再训练相比，标注工作量减少了3-4倍，同时在基准数据集上保持了有竞争力的性能。

Conclusion: 本研究提供了一个实用的框架，使研究人员和实践者能够构建准确、对数据集偏差具有鲁棒性、数据高效且适用于城市气候监测和自动驾驶等实际领域的分割系统。

Abstract: Segmentation models achieve high accuracy on benchmarks but often fail in
real-world domains by relying on spurious correlations instead of true object
boundaries. We propose a human-in-the-loop interactive framework that enables
interventional learning through targeted human corrections of segmentation
outputs. Our approach treats human corrections as interventional signals that
show when reliance on superficial features (e.g., color or texture) is
inappropriate. The system learns from these interventions by propagating
correction-informed edits across visually similar images, effectively steering
the model toward robust, semantically meaningful features rather than
dataset-specific artifacts. Unlike traditional annotation approaches that
simply provide more training data, our method explicitly identifies when and
why the model fails and then systematically corrects these failure modes across
the entire dataset. Through iterative human feedback, the system develops
increasingly robust representations that generalize better to novel domains and
resist artifactual correlations. We demonstrate that our framework improves
segmentation accuracy by up to 9 mIoU points (12-15\% relative improvement) on
challenging cubemap data and yields 3-4$\times$ reductions in annotation effort
compared to standard retraining, while maintaining competitive performance on
benchmark datasets. This work provides a practical framework for researchers
and practitioners seeking to build segmentation systems that are accurate,
robust to dataset biases, data-efficient, and adaptable to real-world domains
such as urban climate monitoring and autonomous driving.

</details>


### [21] [Scaling Traffic Insights with AI and Language Model-Powered Camera Systems for Data-Driven Transportation Decision Making](https://arxiv.org/abs/2510.09981)
*Fan Zuo,Donglin Zhou,Jingqin Gao,Kaan Ozbay*

Main category: cs.CV

TL;DR: 该研究提出了一个端到端的人工智能框架，利用现有交通摄像头基础设施进行大规模、高分辨率、纵向交通分析。


<details>
  <summary>Details</summary>
Motivation: 现有的交通监控方式存在传感器部署成本高、视频分析难以处理动态视角和海量数据等问题。

Method: 本研究提出了一个端到端的人工智能框架，利用现有交通摄像头基础设施进行大规模、高分辨率、纵向交通分析。具体方法包括：1. 针对本地城市场景微调的YOLOv11模型，实时提取多模式交通密度和分类指标。 2. 引入了一种新颖的基于图的视角归一化方法，以解决非固定式平移-倾斜-缩放摄像机带来的不一致性。 3. 集成了领域特定的大型语言模型，处理来自24/7视频流的海量数据，自动生成不断变化的交通模式摘要。

Result: 1. 拥堵缓解区工作日乘用车密度下降9%。 2. 早期卡车流量减少，并有反弹迹象。 3. 走廊和区域尺度的行人和骑自行车活动持续增加。 4. 实验表明，基于示例的提示改善了LLM的数值准确性并减少了幻觉。

Conclusion: 该框架作为一种实用、基础设施就绪的解决方案，可用于大规模、政策相关的交通监测，且 P干预最少。

Abstract: Accurate, scalable traffic monitoring is critical for real-time and long-term
transportation management, particularly during disruptions such as natural
disasters, large construction projects, or major policy changes like New York
City's first-in-the-nation congestion pricing program. However, widespread
sensor deployment remains limited due to high installation, maintenance, and
data management costs. While traffic cameras offer a cost-effective
alternative, existing video analytics struggle with dynamic camera viewpoints
and massive data volumes from large camera networks. This study presents an
end-to-end AI-based framework leveraging existing traffic camera infrastructure
for high-resolution, longitudinal analysis at scale. A fine-tuned YOLOv11
model, trained on localized urban scenes, extracts multimodal traffic density
and classification metrics in real time. To address inconsistencies from
non-stationary pan-tilt-zoom cameras, we introduce a novel graph-based
viewpoint normalization method. A domain-specific large language model was also
integrated to process massive data from a 24/7 video stream to generate
frequent, automated summaries of evolving traffic patterns, a task far
exceeding manual capabilities. We validated the system using over 9 million
images from roughly 1,000 traffic cameras during the early rollout of NYC
congestion pricing in 2025. Results show a 9% decline in weekday passenger
vehicle density within the Congestion Relief Zone, early truck volume
reductions with signs of rebound, and consistent increases in pedestrian and
cyclist activity at corridor and zonal scales. Experiments showed that
example-based prompts improved LLM's numerical accuracy and reduced
hallucinations. These findings demonstrate the framework's potential as a
practical, infrastructure-ready solution for large-scale, policy-relevant
traffic monitoring with minimal human intervention.

</details>


### [22] [FlareX: A Physics-Informed Dataset for Lens Flare Removal via 2D Synthesis and 3D Rendering](https://arxiv.org/abs/2510.09995)
*Lishen Qu,Zhihao Liu,Jinshan Pan,Shihao Zhou,Jinglei Shi,Duosheng Chen,Jufeng Yang*

Main category: cs.CV

TL;DR: 这篇论文提出了一种新的、物理信息更丰富的镜头眩光数据生成方法，名为FlareX，结合了2D合成和3D渲染，以解决现有数据集中眩光多样性不足和物理原理被忽略的问题。该方法生成的混合数据集在真实世界场景中具有更好的泛化性能。


<details>
  <summary>Details</summary>
Motivation: 现有镜头眩光数据集通常通过将人工眩光模板叠加到背景图像上进行2D合成，导致眩光多样性不足且合成过程忽略了物理原理。这使得在此类数据集上训练的模型难以泛化到真实世界场景。

Method: 提出了一种新的物理信息眩光数据生成方法，该方法包含三个阶段：1. 参数化模板创建；2. 考虑照明规律的2D合成；3. 基于物理引擎的3D渲染。最终生成了一个结合2D和3D视角的混合眩光数据集FlareX，其中包含9500个2D模板（来自95种眩光模式）和3000个渲染自60个3D场景的眩光图像对。此外，还设计了一种掩码方法，从受损图像中获取真实世界的无眩光图像，以评估模型在真实世界图像上的性能。

Result: 通过所提出的方法，成功构建了一个包含2D和3D眩光数据的综合数据集FlareX。大规模实验证明了该方法和数据集的有效性，表明训练出的模型在真实世界场景中具有更好的泛化能力。

Conclusion: 本研究通过提出一种物理信息更丰富的镜头眩光数据生成方法和相应的FlareX数据集，有效解决了现有镜头眩光数据集中眩光多样性不足和物理原理缺失的问题。新方法结合了参数化模板创建、照明感知2D合成和物理引擎3D渲染，为模型训练提供了更真实、更具泛化性的数据。

Abstract: Lens flare occurs when shooting towards strong light sources, significantly
degrading the visual quality of images. Due to the difficulty in capturing
flare-corrupted and flare-free image pairs in the real world, existing datasets
are typically synthesized in 2D by overlaying artificial flare templates onto
background images. However, the lack of flare diversity in templates and the
neglect of physical principles in the synthesis process hinder models trained
on these datasets from generalizing well to real-world scenarios. To address
these challenges, we propose a new physics-informed method for flare data
generation, which consists of three stages: parameterized template creation,
the laws of illumination-aware 2D synthesis, and physical engine-based 3D
rendering, which finally gives us a mixed flare dataset that incorporates both
2D and 3D perspectives, namely FlareX. This dataset offers 9,500 2D templates
derived from 95 flare patterns and 3,000 flare image pairs rendered from 60 3D
scenes. Furthermore, we design a masking approach to obtain real-world
flare-free images from their corrupted counterparts to measure the performance
of the model on real-world images. Extensive experiments demonstrate the
effectiveness of our method and dataset.

</details>


### [23] [BurstDeflicker: A Benchmark Dataset for Flicker Removal in Dynamic Scenes](https://arxiv.org/abs/2510.09996)
*Lishen Qu,Zhihao Liu,Shihao Zhou,Yaqi Luo,Jie Liang,Hui Zeng,Lei Zhang,Jufeng Yang*

Main category: cs.CV

TL;DR: 这篇论文介绍了一个名为BurstDeflicker的大型图像数据集，旨在解决短曝光图像中的闪烁伪影问题。该数据集通过三种互补的数据采集策略构建，包括基于Retinex的合成管线、4000张真实世界闪烁图像的捕获以及一种结合运动的绿幕方法，以促进闪烁去除技术的研究进展。


<details>
  <summary>Details</summary>
Motivation: 短曝光图像中的闪烁伪影是由卷帘快门的行曝光机制与交流电源照明的时间强度变化相互作用引起的。这些伪影表现为图像亮度分布不均和暗带，不仅损害图像质量，还会影响目标检测和跟踪等高级视觉任务。当前研究面临的主要障碍是缺乏大规模、真实的闪烁数据集。

Method: 1. 开发了一个基于Retinex的合成管线，该管线重新定义了闪烁去除的目标，并能够控制闪烁相关的关键属性（如强度、区域和频率），从而生成多样化的闪烁模式。2. 捕获了来自不同场景的4000张真实世界闪烁图像，帮助模型更好地理解真实闪烁伪影的空间和时间特征，并更有效地推广到野外场景。3. 提出了一种绿幕方法，在保持真实闪烁退化的同时，将运动整合到图像对中，以应对动态场景不可重复的问题。

Result: 通过综合实验，证明了所构建数据集的有效性及其在推动闪烁去除研究方面的潜力。

Conclusion: BurstDeflicker数据集的构建为闪烁去除研究提供了大规模、高质量的测试平台，有望加速该领域的技术突破。

Abstract: Flicker artifacts in short-exposure images are caused by the interplay
between the row-wise exposure mechanism of rolling shutter cameras and the
temporal intensity variations of alternating current (AC)-powered lighting.
These artifacts typically appear as uneven brightness distribution across the
image, forming noticeable dark bands. Beyond compromising image quality, this
structured noise also affects high-level tasks, such as object detection and
tracking, where reliable lighting is crucial. Despite the prevalence of
flicker, the lack of a large-scale, realistic dataset has been a significant
barrier to advancing research in flicker removal. To address this issue, we
present BurstDeflicker, a scalable benchmark constructed using three
complementary data acquisition strategies. First, we develop a Retinex-based
synthesis pipeline that redefines the goal of flicker removal and enables
controllable manipulation of key flicker-related attributes (e.g., intensity,
area, and frequency), thereby facilitating the generation of diverse flicker
patterns. Second, we capture 4,000 real-world flicker images from different
scenes, which help the model better understand the spatial and temporal
characteristics of real flicker artifacts and generalize more effectively to
wild scenarios. Finally, due to the non-repeatable nature of dynamic scenes, we
propose a green-screen method to incorporate motion into image pairs while
preserving real flicker degradation. Comprehensive experiments demonstrate the
effectiveness of our dataset and its potential to advance research in flicker
removal.

</details>


### [24] [MIMO: A medical vision language model with visual referring multimodal input and pixel grounding multimodal output](https://arxiv.org/abs/2510.10011)
*Yanyuan Chen,Dexuan Xu,Yu Huang,Songkun Zhan,Hanpin Wang,Dongxue Chen,Xueping Wang,Meikang Qiu,Hang Li*

Main category: cs.CV

TL;DR: 该文章介绍了一个名为MIMO的统一医学视觉语言模型，它解决了现有模型在医学视觉问答任务中输入缺乏视觉线索理解和输出缺乏与图像关键区域连接的问题。


<details>
  <summary>Details</summary>
Motivation: 现有的医学视觉语言模型在输入方面仅依赖文本指令，缺乏对图像中视觉线索的直接理解；在输出方面，模型只给出文本答案，缺乏与图像中关键区域的连接。

Method: 本文提出了一个统一的医学视觉语言模型MIMO，具有视觉指代多模态输入和像素接地多模态输出。MIMO能够结合视觉线索和文本指令来理解复杂的医学图像和语义，并且能够将文本输出中的医学术语与图像中的区域进行关联。为了解决医学领域相关数据稀缺的问题，本文提出了一个名为MIMOSeg的综合医学多模态数据集，包含89.5万个样本，涵盖了多模态输入和多模态输出的指令遵循和复杂问答。

Result: MIMO能够独特地结合视觉指代和像素接地能力，这是以前的模型所不具备的。

Conclusion: MIMO模型通过其多模态输入和输出能力，以及MIMOSeg数据集的支持，有效解决了现有医学视觉语言模型在理解和输出方面的局限性，在多个下游医学多模态任务中表现出色。

Abstract: Currently, medical vision language models are widely used in medical vision
question answering tasks. However, existing models are confronted with two
issues: for input, the model only relies on text instructions and lacks direct
understanding of visual clues in the image; for output, the model only gives
text answers and lacks connection with key areas in the image. To address these
issues, we propose a unified medical vision language model MIMO, with visual
referring Multimodal Input and pixel grounding Multimodal Output. MIMO can not
only combine visual clues and textual instructions to understand complex
medical images and semantics, but can also ground medical terminologies in
textual output within the image. To overcome the scarcity of relevant data in
the medical field, we propose MIMOSeg, a comprehensive medical multimodal
dataset including 895K samples. MIMOSeg is constructed from four different
perspectives, covering basic instruction following and complex question
answering with multimodal input and multimodal output. We conduct experiments
on several downstream medical multimodal tasks. Extensive experimental results
verify that MIMO can uniquely combine visual referring and pixel grounding
capabilities, which are not available in previous models.

</details>


### [25] [Q-Adapter: Visual Query Adapter for Extracting Textually-related Features in Video Captioning](https://arxiv.org/abs/2510.10022)
*Junan Chen,Trung Thanh Nguyen,Takahiro Komamizu,Ichiro Ide*

Main category: cs.CV

TL;DR: 本文提出Q-Adapter，这是一种参数高效的视觉适配器模块，专门用于多模态大模型在视频字幕任务中的微调。它在保持高性能的同时，显著减少了所需的参数量。


<details>
  <summary>Details</summary>
Motivation: 大型预训练模型在视频字幕领域取得了显著进展，但“预训练后微调”的范式随着模型规模增大，计算成本急剧上升。现有的参数高效微调（PEFT）方法主要集中在多模态大模型的语言部分，在多模态任务中探索不足，且对微调过程中视觉信息的理解不够深入。

Method: 本文提出了Query-Adapter（Q-Adapter），这是一个轻量级的视觉适配器模块，旨在通过引入可学习的查询tokens和门控层到视觉编码器中，以增强多模态大模型，从而实现高效的视频字幕任务微调。Q-Adapter能够有效地提取与字幕相关的稀疏特征，而无需依赖外部文本监督。

Result: Q-Adapter在MSR-VTT和MSVD两个视频字幕数据集上进行了评估，在采用PEFT方法的模型中，于BLEU@4、METEOR、ROUGE-L和CIDEr等指标上达到了最先进的性能。与采用完全微调方法相比，Q-Adapter在仅使用1.4%参数的情况下也取得了具有竞争力的性能。

Conclusion: Q-Adapter在视频字幕任务中实现了字幕质量和参数效率之间的良好平衡，展示了其在视频语言建模方面的可扩展性。

Abstract: Recent advances in video captioning are driven by large-scale pretrained
models, which follow the standard "pre-training followed by fine-tuning"
paradigm, where the full model is fine-tuned for downstream tasks. Although
effective, this approach becomes computationally prohibitive as the model size
increases. The Parameter-Efficient Fine-Tuning (PEFT) approach offers a
promising alternative, but primarily focuses on the language components of
Multimodal Large Language Models (MLLMs). Despite recent progress, PEFT remains
underexplored in multimodal tasks and lacks sufficient understanding of visual
information during fine-tuning the model. To bridge this gap, we propose
Query-Adapter (Q-Adapter), a lightweight visual adapter module designed to
enhance MLLMs by enabling efficient fine-tuning for the video captioning task.
Q-Adapter introduces learnable query tokens and a gating layer into Vision
Encoder, enabling effective extraction of sparse, caption-relevant features
without relying on external textual supervision. We evaluate Q-Adapter on two
well-known video captioning datasets, MSR-VTT and MSVD, where it achieves
state-of-the-art performance among the methods that take the PEFT approach
across BLEU@4, METEOR, ROUGE-L, and CIDEr metrics. Q-Adapter also achieves
competitive performance compared to methods that take the full fine-tuning
approach while requiring only 1.4% of the parameters. We further analyze the
impact of key hyperparameters and design choices on fine-tuning effectiveness,
providing insights into optimization strategies for adapter-based learning.
These results highlight the strong potential of Q-Adapter in balancing caption
quality and parameter efficiency, demonstrating its scalability for
video-language modeling.

</details>


### [26] [P-4DGS: Predictive 4D Gaussian Splatting with 90$\times$ Compression](https://arxiv.org/abs/2510.10030)
*Henan Wang,Hanxin Zhu,Xinliang Gong,Tianyu He,Xin Li,Zhibo Chen*

Main category: cs.CV

TL;DR: P-4DGS是一种新颖的动态3DGS表示，用于紧凑的4D场景建模，它通过空间时间预测模块和自适应量化策略显著减少了内存消耗，实现了最先进的重建质量、最快的渲染速度和极低的存储占用。


<details>
  <summary>Details</summary>
Motivation: 现有的动态3DGS算法忽略了动态场景中固有的时间和空间冗余，导致内存消耗过大。

Method: P-4DGS通过以下方法解决问题：1. 设计了一个基于3D锚点的时空预测模块，以充分利用不同3D高斯基元之间的时空相关性。2. 采用自适应量化策略结合基于上下文的熵编码，进一步减小3D锚点的大小，从而提高压缩效率。

Result: P-4DGS在合成和真实世界数据集上都取得了最先进的重建质量和最快的渲染速度。平均存储占用约为1MB，在合成场景和真实世界场景中分别实现了高达40倍和90倍的压缩。

Conclusion: P-4DGS通过其创新的时空预测和自适应量化策略，显著提升了动态3DGS的压缩效率和性能，使其在4D重建中具有极大的潜力。

Abstract: 3D Gaussian Splatting (3DGS) has garnered significant attention due to its
superior scene representation fidelity and real-time rendering performance,
especially for dynamic 3D scene reconstruction (\textit{i.e.}, 4D
reconstruction). However, despite achieving promising results, most existing
algorithms overlook the substantial temporal and spatial redundancies inherent
in dynamic scenes, leading to prohibitive memory consumption. To address this,
we propose P-4DGS, a novel dynamic 3DGS representation for compact 4D scene
modeling. Inspired by intra- and inter-frame prediction techniques commonly
used in video compression, we first design a 3D anchor point-based
spatial-temporal prediction module to fully exploit the spatial-temporal
correlations across different 3D Gaussian primitives. Subsequently, we employ
an adaptive quantization strategy combined with context-based entropy coding to
further reduce the size of the 3D anchor points, thereby achieving enhanced
compression efficiency. To evaluate the rate-distortion performance of our
proposed P-4DGS in comparison with other dynamic 3DGS representations, we
conduct extensive experiments on both synthetic and real-world datasets.
Experimental results demonstrate that our approach achieves state-of-the-art
reconstruction quality and the fastest rendering speed, with a remarkably low
storage footprint (around \textbf{1MB} on average), achieving up to
\textbf{40$\times$} and \textbf{90$\times$} compression on synthetic and
real-world scenes, respectively.

</details>


### [27] [Complementary and Contrastive Learning for Audio-Visual Segmentation](https://arxiv.org/abs/2510.10051)
*Sitong Gong,Yunzhi Zhuge,Lu Zhang,Pingping Zhang,Huchuan Lu*

Main category: cs.CV

TL;DR: 这篇论文介绍了一种名为CCFormer的音频视觉分割（AVS）框架，旨在通过创新的模块和对比学习，解决现有方法在处理局部/全局信息、多模态系数和时序动态方面的局限性，并在多个数据集上取得了最先进的性能。


<details>
  <summary>Details</summary>
Motivation: 现有的音频视觉分割方法在处理局部和全局信息、多模态特征提取以及捕捉时序动态方面存在不足。CNN方法受限于局部感受野，而Transformer方法在提取多模态系数和时序动态上表现不佳。

Method: 本文提出了互补对比Transformer（CCFormer）框架。该框架包含：1. 早期集成模块（EIM）：采用并行双边架构，将多尺度视觉特征与音频数据融合，增强跨模态互补性。2. 多查询Transformer模块（MTM）：动态赋予音频查询学习能力，同时建模帧和视频级别的关系，以提取帧内空间特征并促进时间连贯性感知。3. 双模态对比学习（BCL）：在统一的特征空间中促进两种模态的对齐。

Result: CCFormer方法在S4、MS3和AVSS数据集上取得了最先进的性能。

Conclusion: CCFormer通过创新的EIM、MTM和BCL模块，有效地结合了局部与全局信息处理能力，并全面捕捉了时空上下文，从而显著提升了音频视觉分割的性能，并解决了现有方法的局限性。

Abstract: Audio-Visual Segmentation (AVS) aims to generate pixel-wise segmentation maps
that correlate with the auditory signals of objects. This field has seen
significant progress with numerous CNN and Transformer-based methods enhancing
the segmentation accuracy and robustness. Traditional CNN approaches manage
audio-visual interactions through basic operations like padding and
multiplications but are restricted by CNNs' limited local receptive field. More
recently, Transformer-based methods treat auditory cues as queries, utilizing
attention mechanisms to enhance audio-visual cooperation within frames.
Nevertheless, they typically struggle to extract multimodal coefficients and
temporal dynamics adequately. To overcome these limitations, we present the
Complementary and Contrastive Transformer (CCFormer), a novel framework adept
at processing both local and global information and capturing spatial-temporal
context comprehensively. Our CCFormer initiates with the Early Integration
Module (EIM) that employs a parallel bilateral architecture, merging
multi-scale visual features with audio data to boost cross-modal
complementarity. To extract the intra-frame spatial features and facilitate the
perception of temporal coherence, we introduce the Multi-query Transformer
Module (MTM), which dynamically endows audio queries with learning capabilities
and models the frame and video-level relations simultaneously. Furthermore, we
propose the Bi-modal Contrastive Learning (BCL) to promote the alignment across
both modalities in the unified feature space. Through the effective combination
of those designs, our method sets new state-of-the-art benchmarks across the
S4, MS3 and AVSS datasets. Our source code and model weights will be made
publicly available at https://github.com/SitongGong/CCFormer

</details>


### [28] [Think Twice to See More: Iterative Visual Reasoning in Medical VLMs](https://arxiv.org/abs/2510.10052)
*Kaitao Chen,Shaohao Rui,Yankai Jiang,Jiamin Wu,Qihao Zheng,Chunfeng Song,Xiaosong Wang,Mu Zhou,Mianxin Liu*

Main category: cs.CV

TL;DR: 本文介绍了ViTAR，这是一个新的医学视觉语言模型框架，它通过模拟人类专家的迭代推理过程来弥合机器与人类感知之间的差距，并在医学图像理解任务中取得了最先进的性能。


<details>
  <summary>Details</summary>
Motivation: 现有的医学视觉语言模型在图像-文本理解方面表现出色，但通常依赖于单次推理，忽略了局部视觉线索。 然而，在临床实践中，人类专家会迭代地扫描、聚焦和提炼感兴趣区域，然后才得出最终诊断。

Method: 1. 引入ViTAR框架：该框架通过“思考-行动-再思考-回答”的认知链来模拟人类专家的迭代推理过程，将医学图像视为交互对象，使模型能够进行多步骤的视觉推理。
2. 数据集构建：
   - 整理了一个包含1K个交互式示例的高质量指令数据集，该数据集编码了专家级的诊断行为。
   - 整理了16K个视觉问答训练数据，用于细粒度的视觉诊断。
3. 训练策略：引入了两阶段训练策略：
   - 第一阶段：监督微调，以指导认知轨迹。
   - 第二阶段：强化学习，以优化决策制定。

Result: 1. 性能超越：ViTAR在医学图像理解任务中超越了目前最先进的模型。
2. 视觉注意分析：视觉注意分析表明，从“思考”到“再思考”的阶段，ViTAR越来越多地将视觉基础锚定在临床关键区域，并在推理过程中保持对视觉tokens的高度注意分配。
3. 机制洞察：这提供了对其性能改进的机制洞察。

Conclusion: 将专家风格的迭代思维链嵌入到视觉语言模型中，可以提高医学人工智能的性能和可信度。

Abstract: Medical vision-language models (VLMs) excel at image-text understanding but
typically rely on a single-pass reasoning that neglects localized visual cues.
In clinical practice, however, human experts iteratively scan, focus, and
refine the regions of interest before reaching a final diagnosis. To narrow
this machine-human perception gap, we introduce ViTAR, a novel VLM framework
that emulates the iterative reasoning process of human experts through a
cognitive chain of "think-act-rethink-answer". ViTAR treats medical images as
interactive objects, enabling models to engage multi-step visual reasoning. To
support this approach, we curate a high-quality instruction dataset comprising
1K interactive examples that encode expert-like diagnostic behaviors. In
addition, a 16K visual question answering training data has been curated
towards fine-grained visual diagnosis. We introduce a two-stage training
strategy that begins with supervised fine-tuning to guide cognitive
trajectories, followed by the reinforcement learning to optimize
decision-making. Extensive evaluations demonstrate that ViTAR outperforms
strong state-of-the-art models. Visual attention analysis reveals that from the
"think" to "rethink" rounds, ViTAR increasingly anchors visual grounding to
clinically critical regions and maintains high attention allocation to visual
tokens during reasoning, providing mechanistic insight into its improved
performance. These findings demonstrate that embedding expert-style iterative
thinking chains into VLMs enhances both performance and trustworthiness of
medical AI.

</details>


### [29] [DREAM: A Benchmark Study for Deepfake REalism AssessMent](https://arxiv.org/abs/2510.10053)
*Bo Peng,Zichuan Wang,Sheng Yu,Xiaochuan Jin,Wei Wang,Jing Dong*

Main category: cs.CV

TL;DR: 这篇论文介绍了一个名为DREAM的综合基准，用于评估Deepfake视频的视觉真实感，旨在弥补该领域研究的不足，并为未来研究奠定基础。


<details>
  <summary>Details</summary>
Motivation: 目前Deepfake检测主要集中在客观区分真伪，但对Deepfake的主观感知（即视觉真实感评估）缺乏充分研究。这种评估对于衡量Deepfake的质量和欺骗性，预测其在互联网上的影响力，以及改进Deepfake生成过程都至关重要。

Method: 本研究提出了一个名为DREAM（Deepfake REalism AssessMent）的综合基准，包括：1. 一个包含多种质量的Deepfake视频数据集。2. 一个大规模的人工标注，涵盖3500名标注者提供的140,000个真实感评分和文本描述。3. 对16种代表性真实感评估方法（包括最新的大型视觉语言模型和新提出的描述对齐CLIP方法）进行了全面评估和分析。

Result: DREAM基准为Deepfake的视觉真实感评估提供了一个全面的平台，并通过对现有方法的评估和分析，为该领域未来的研究奠定了基础。

Conclusion: 本论文通过引入DREAM基准，开创了Deepfake视觉真实感评估的新方向。该基准及其提供的见解将有助于未来在Deepfake真实感评估及相关领域的研究。

Abstract: Deep learning based face-swap videos, widely known as deepfakes, have drawn
wide attention due to their threat to information credibility. Recent works
mainly focus on the problem of deepfake detection that aims to reliably tell
deepfakes apart from real ones, in an objective way. On the other hand, the
subjective perception of deepfakes, especially its computational modeling and
imitation, is also a significant problem but lacks adequate study. In this
paper, we focus on the visual realism assessment of deepfakes, which is defined
as the automatic assessment of deepfake visual realism that approximates human
perception of deepfakes. It is important for evaluating the quality and
deceptiveness of deepfakes which can be used for predicting the influence of
deepfakes on Internet, and it also has potentials in improving the deepfake
generation process by serving as a critic. This paper prompts this new
direction by presenting a comprehensive benchmark called DREAM, which stands
for Deepfake REalism AssessMent. It is comprised of a deepfake video dataset of
diverse quality, a large scale annotation that includes 140,000 realism scores
and textual descriptions obtained from 3,500 human annotators, and a
comprehensive evaluation and analysis of 16 representative realism assessment
methods, including recent large vision language model based methods and a newly
proposed description-aligned CLIP method. The benchmark and insights included
in this study can lay the foundation for future research in this direction and
other related areas.

</details>


### [30] [Collaborative Learning of Semantic-Aware Feature Learning and Label Recovery for Multi-Label Image Recognition with Incomplete Labels](https://arxiv.org/abs/2510.10055)
*Zhi-Fen He,Ren-Dong Xie,Bo Li,Bin Liu,Jin-Yan Hu*

Main category: cs.CV

TL;DR: 这篇论文提出了一个名为CLSL的新方法，用于解决不完整标签多标签图像识别中的语义感知特征学习和缺失标签恢复问题。


<details>
  <summary>Details</summary>
Motivation: 不完整标签多标签图像识别面临两大挑战：语义感知特征学习和缺失标签恢复。

Method: 本文提出了一种协同学习的语义感知特征学习和标签恢复（CLSL）方法。它设计了一个语义相关特征学习模块来发现语义信息和标签相关性，从而学习鲁棒的语义相关特征。然后，提出了一个语义引导的特征增强模块，通过有效对齐视觉和语义特征空间来生成高质量的判别性语义感知特征。最后，引入了一个协同学习框架，该框架整合了语义感知特征学习和标签恢复，这不仅可以动态增强语义感知特征的可辨别性，还可以自适应地推断和恢复缺失标签，在两个过程之间形成相互强化的循环。

Result: 在三个广泛使用的公共数据集（MS-COCO、VOC2007和NUS-WIDE）上的大量实验表明，CLSL优于目前最先进的带有不完整标签的多标签图像识别方法。

Conclusion: CLSL方法通过协同学习框架，有效地解决了不完整标签多标签图像识别中的语义感知特征学习和缺失标签恢复问题，取得了 SOTA 性能。

Abstract: Multi-label image recognition with incomplete labels is a critical learning
task and has emerged as a focal topic in computer vision. However, this task is
confronted with two core challenges: semantic-aware feature learning and
missing label recovery. In this paper, we propose a novel Collaborative
Learning of Semantic-aware feature learning and Label recovery (CLSL) method
for multi-label image recognition with incomplete labels, which unifies the two
aforementioned challenges into a unified learning framework. More specifically,
we design a semantic-related feature learning module to learn robust
semantic-related features by discovering semantic information and label
correlations. Then, a semantic-guided feature enhancement module is proposed to
generate high-quality discriminative semantic-aware features by effectively
aligning visual and semantic feature spaces. Finally, we introduce a
collaborative learning framework that integrates semantic-aware feature
learning and label recovery, which can not only dynamically enhance the
discriminability of semantic-aware features but also adaptively infer and
recover missing labels, forming a mutually reinforced loop between the two
processes. Extensive experiments on three widely used public datasets (MS-COCO,
VOC2007, and NUS-WIDE) demonstrate that CLSL outperforms the state-of-the-art
multi-label image recognition methods with incomplete labels.

</details>


### [31] [Probabilistic Hyper-Graphs using Multiple Randomly Masked Autoencoders for Semi-supervised Multi-modal Multi-task Learning](https://arxiv.org/abs/2510.10068)
*Pîrvu Mihai-Cristian,Leordeanu Marius*

Main category: cs.CV

TL;DR: 介绍了PHG-MAE，这是一种结合了神经图和掩码自动编码器的新模型，用于多模态计算机视觉任务。


<details>
  <summary>Details</summary>
Motivation: 计算机视觉领域受益于大量数据，尤其是在自监督预训练方面。MAE方法在下游任务优化前常用作第一步，但仅限于对图像块进行掩码。本文旨在通过引入对整个模态进行随机掩码以及统一预训练和微调过程来改进MAE方法。

Method: PHG-MAE模型将神经图与掩码自动编码器相结合，形成一个统一的理论框架。 该模型通过随机掩码整个模态（而不仅仅是图像块）来从超边分布中采样。 它还将预训练和微调结合到单个训练循环中。 此外，该方法可以创建推理时集成，并通过聚合提高预测性能和一致性。最后，它还在集成之上应用知识蒸馏，即使参数少于1M的模型也能保持性能。同时，PHG-MAE开发了一个数据管道软件，以简化外部预训练专家在计算机视觉多模态多任务学习场景中的集成。

Result: PHG-MAE模型通过掩码整个模态并在单个训练循环中结合预训练和微调，提高了预测性能和一致性。它还能在集成模型上应用知识蒸馏，即使是参数量较小的模型也能保持性能。通过数据管道软件，Dronescapes数据集的扩展已实现全自动化。

Conclusion: PHG-MAE是一种有效且通用的多模态多任务学习模型，它通过统一神经图和掩码自动编码器，并引入模态级掩码和单循环训练，显著提高了计算机视觉任务的性能和效率。该方法可应用于其他类似领域，如自动驾驶和室内机器人。随之发布的工具和数据集扩展将进一步推动该领域的研究。

Abstract: The computer vision domain has greatly benefited from an abundance of data
across many modalities to improve on various visual tasks. Recently, there has
been a lot of focus on self-supervised pre-training methods through Masked
Autoencoders (MAE) \cite{he2022masked,bachmann2022multimae}, usually used as a
first step before optimizing for a downstream task, such as classification or
regression. This is very useful as it doesn't require any manually labeled
data. In this work, we introduce Probabilistic Hyper-Graphs using Masked
Autoencoders (PHG-MAE): a novel model that unifies the classical work on neural
graphs \cite{leordeanu2021semi} with the modern approach of masked autoencoders
under a common theoretical framework. Through random masking of entire
modalities, not just patches, the model samples from the distribution of
hyper-edges on each forward pass. Additionally, the model adapts the standard
MAE algorithm by combining pre-training and fine-tuning into a single training
loop. Moreover, our approach enables the creation of inference-time ensembles
which, through aggregation, boost the final prediction performance and
consistency. Lastly, we show that we can apply knowledge distillation on top of
the ensembles with little loss in performance, even with models that have fewer
than 1M parameters. While our work mostly focuses on outdoor UAV scenes that
contain multiple world interpretations and modalities, the same steps can be
followed in other similar domains, such as autonomous driving or indoor
robotics. In order to streamline the process of integrating external
pre-trained experts for computer vision multi-modal multi-task learning (MTL)
scenarios, we developed a data-pipeline software. Using this tool, we have
created and released a fully-automated extension of the Dronescapes dataset.
All the technical details, code and reproduction steps are publicly released.

</details>


### [32] [Tracking the Spatiotemporal Evolution of Landslide Scars Using a Vision Foundation Model: A Novel and Universal Framework](https://arxiv.org/abs/2510.10084)
*Meijun Zhou,Gang Mei,Zhengjing Ma,Nengxiong Xu,Jianbing Peng*

Main category: cs.CV

TL;DR: 该研究提出了一个使用视觉基础模型追踪大型滑坡疤痕时空演变的新框架，通过将离散遥感图像重建为连续视频序列，实现了滑坡疤痕的连续准确识别，可用于早期预警和灾后评估。


<details>
  <summary>Details</summary>
Motivation: 目前的研究大多集中于单期或双期滑坡识别，难以追踪滑坡疤痕的时空演变。

Method: 提出了一种新颖的通用框架，利用视觉基础模型，将离散光学遥感图像重建为连续视频序列，然后使用为视频分割开发的视觉基础模型来追踪滑坡疤痕的演变。该框架在知识引导、自动传播和交互式精炼范式下运行。

Result: 所提出的框架能够连续追踪滑坡疤痕，捕获对早期预警至关重要的破坏前兆，以及对评估次生灾害和长期稳定性至关重要的破坏后演变。

Conclusion: 该框架通过将离散图像转换为连续视频序列，并利用视觉基础模型进行分割和追踪，有效解决了当前滑坡研究中追踪滑坡疤痕时空演变难题，为滑坡早期预警和灾后评估提供了新途径。

Abstract: Tracking the spatiotemporal evolution of large-scale landslide scars is
critical for understanding the evolution mechanisms and failure precursors,
enabling effective early-warning. However, most existing studies have focused
on single-phase or pre- and post-failure dual-phase landslide identification.
Although these approaches delineate post-failure landslide boundaries, it is
challenging to track the spatiotemporal evolution of landslide scars. To
address this problem, this study proposes a novel and universal framework for
tracking the spatiotemporal evolution of large-scale landslide scars using a
vision foundation model. The key idea behind the proposed framework is to
reconstruct discrete optical remote sensing images into a continuous video
sequence. This transformation enables a vision foundation model, which is
developed for video segmentation, to be used for tracking the evolution of
landslide scars. The proposed framework operates within a knowledge-guided,
auto-propagation, and interactive refinement paradigm to ensure the continuous
and accurate identification of landslide scars. The proposed framework was
validated through application to two representative cases: the post-failure
Baige landslide and the active Sela landslide (2017-2025). Results indicate
that the proposed framework enables continuous tracking of landslide scars,
capturing both failure precursors critical for early warning and post-failure
evolution essential for assessing secondary hazards and long-term stability.

</details>


### [33] [Gesplat: Robust Pose-Free 3D Reconstruction via Geometry-Guided Gaussian Splatting](https://arxiv.org/abs/2510.10097)
*Jiahui Lu,Haihong Xiao,Xueyan Zhao,Wenxiong Kang*

Main category: cs.CV

TL;DR: Gesplat是一个基于3DGS的框架，可以在未给定姿态的稀疏图像中实现鲁棒的新颖视图合成和几何一致性重建。


<details>
  <summary>Details</summary>
Motivation: NeRF和3DGS在3D重建和新颖视图合成方面取得了进展，但严重依赖精确的相机姿态和密集的视角覆盖，这限制了它们在稀疏视图设置中的适用性。

Method: Gesplat利用VGGT基础模型获取更可靠的初始姿态和密集点云，而不是COLMAP。它集成了混合高斯表示、图引导属性优化模块和基于流的深度正则化。

Result: Gesplat在正向和大规模复杂数据集上都取得了比其他无姿态方法更强大的性能。

Conclusion: Gesplat通过利用VGGT基础模型和结合多项创新技术，在稀疏视图设置下实现了鲁棒的新颖视图合成和几何一致性重建。

Abstract: Neural Radiance Fields (NeRF) and 3D Gaussian Splatting (3DGS) have advanced
3D reconstruction and novel view synthesis, but remain heavily dependent on
accurate camera poses and dense viewpoint coverage. These requirements limit
their applicability in sparse-view settings, where pose estimation becomes
unreliable and supervision is insufficient. To overcome these challenges, we
introduce Gesplat, a 3DGS-based framework that enables robust novel view
synthesis and geometrically consistent reconstruction from unposed sparse
images. Unlike prior works that rely on COLMAP for sparse point cloud
initialization, we leverage the VGGT foundation model to obtain more reliable
initial poses and dense point clouds. Our approach integrates several key
innovations: 1) a hybrid Gaussian representation with dual position-shape
optimization enhanced by inter-view matching consistency; 2) a graph-guided
attribute refinement module to enhance scene details; and 3) flow-based depth
regularization that improves depth estimation accuracy for more effective
supervision. Comprehensive quantitative and qualitative experiments demonstrate
that our approach achieves more robust performance on both forward-facing and
large-scale complex datasets compared to other pose-free methods.

</details>


### [34] [Cooperative Pseudo Labeling for Unsupervised Federated Classification](https://arxiv.org/abs/2510.10100)
*Kuangpu Guo,Lijun Sheng,Yongcan Yu,Jian Liang,Zilei Wang,Ran He*

Main category: cs.CV

TL;DR: 本文提出了一种名为FedCoPL的联邦无监督学习方法，首次将UFL扩展到分类问题中，并利用CLIP进行零样本预测，通过客户端伪标签分布估计和服务器调整，以及部分提示词聚合协议，实现了优于基线方法的性能。


<details>
  <summary>Details</summary>
Motivation: 无监督联邦学习（UFL）以前主要集中在表示学习和聚类任务。最近，视觉语言模型（如CLIP）的零样本预测能力为UFL范式下的分类问题提供了新的机会，但尚未被充分探索。

Method: FedCoPL方法通过客户端估计并上传伪标签分布，服务器调整和重新分配以避免全局类别不平衡。此外，引入了部分提示词聚合协议，其中视觉提示词在服务器聚合，文本提示词在本地保留，以实现有效的协作和个性化。

Result: FedCoPL在实验中表现出优于基线方法的性能。

Conclusion: FedCoPL首次将无监督联邦学习扩展到分类问题中，利用CLIP的零样本预测能力，并通过伪标签分布调整和部分提示词聚合协议，解决了UFL在分类任务中的挑战，并取得了优异的实验结果。

Abstract: Unsupervised Federated Learning (UFL) aims to collaboratively train a global
model across distributed clients without sharing data or accessing label
information. Previous UFL works have predominantly focused on representation
learning and clustering tasks. Recently, vision language models (e.g., CLIP)
have gained significant attention for their powerful zero-shot prediction
capabilities. Leveraging this advancement, classification problems that were
previously infeasible under the UFL paradigm now present promising new
opportunities, yet remain largely unexplored. In this paper, we extend UFL to
the classification problem with CLIP for the first time and propose a novel
method, \underline{\textbf{Fed}}erated \underline{\textbf{Co}}operative
\underline{\textbf{P}}seudo \underline{\textbf{L}}abeling (\textbf{FedCoPL}).
Specifically, clients estimate and upload their pseudo label distribution, and
the server adjusts and redistributes them to avoid global imbalance among
classes. Moreover, we introduce a partial prompt aggregation protocol for
effective collaboration and personalization. In particular, visual prompts
containing general image features are aggregated at the server, while text
prompts encoding personalized knowledge are retained locally. Extensive
experiments demonstrate the superior performance of our FedCoPL compared to
baseline methods. Our code is available at
\href{https://github.com/krumpguo/FedCoPL}{https://github.com/krumpguo/FedCoPL}.

</details>


### [35] [Answer-Consistent Chain-of-thought Reinforcement Learning For Multi-modal Large Langauge Models](https://arxiv.org/abs/2510.10104)
*Minbin Huang,Runhui Huang,Chuanyang Zheng,Jingyao Li,Guoxuan Chen,Han Shi,Hong Cheng*

Main category: cs.CV

TL;DR: 本文提出了一种名为ACRE的新方法，通过引入一致性验证奖励来解决大型多模态语言模型中推理链与最终答案不一致的问题，从而提高了模型在视觉和数学推理任务上的性能。


<details>
  <summary>Details</summary>
Motivation: 现有的多模态大型语言模型（LLMs）虽然通过可验证奖励强化学习（RLVR）提高了答案的准确性，但却可能导致推理链与最终答案脱节，从而产生不一致。

Method: 本文提出了一种名为Answer-Consistent Reinforcement Learning (ACRE) 的方法，它在GRPO算法的基础上增加了一个辅助一致性检查。具体来说，模型在生成思维链和初始答案后，会打乱答案选项并再次提示模型，使其利用相同的推理轨迹预测第二个答案。然后，设计一个一致性验证奖励机制，只有当原始答案和打乱后的答案都正确且一致时，才会给予高奖励，否则给予较低奖励。

Result: ACRE在具有挑战性的视频推理基准和多模态数学推理基准上进行了评估，与GRPO基线相比，视频推理任务平均提高了2.2%，数学推理任务平均提高了1.5%。

Conclusion: ACRE通过惩罚推理与答案的不一致性，并阻止模型依赖虚假模式（例如选项排序偏差），有效地提高了多模态LLMs在复杂推理任务中的性能和一致性。

Abstract: Recent advances in large language models (LLMs) have demonstrated that
reinforcement learning with verifiable rewards (RLVR) can significantly enhance
reasoning abilities by directly optimizing correctness, rather than relying
solely on supervised imitation. This paradigm has been extended to multimodal
LLMs for complex video and image understanding tasks. However, while
outcome-driven RL improves answer accuracy, it can inadvertently decouple the
reasoning chain from the final answer, leading to situations where models
produce inconsistency between the reasoning trace and final answer. In our
experiments on multiple-choice visual question-answering tasks, the standard
GRPO method yields only 79.7\% consistency on MMVU between the reasoning steps
and the chosen answers, indicating frequent mismatches between answers and
reasoning. To this end, we propose Answer-Consistent Reinforcement Learning
(ACRE) that modifies the GRPO algorithm with an auxiliary consistency check.
After the model generates a chain of thought and an initial answer for a given
question, we shuffle the answer options and prompt the model again with the
same reasoning trace to predict a second answer. We design a
consistency-verification reward that grants a high reward only if both the
original and the post-shuffle answers agree and are correct; otherwise, a lower
reward is assigned accordingly. This mechanism penalizes reasoning-answer
misalignment and discourages the model from relying on spurious patterns, such
as option ordering biases. We evaluate ACRE on challenging Video Reasoning
benchmarks and multimodal math reasoning benchmarks, achieving an average 2.2\%
and 1.5\% improvement for Video Reasoning and Math Reasoning tasks over the
GRPO baseline.

</details>


### [36] [Training-Free In-Context Forensic Chain for Image Manipulation Detection and Localization](https://arxiv.org/abs/2510.10111)
*Rui Chen,Bin Liu,Changtao Miao,Xinghao Wang,Yi Li,Tao Gong,Qi Chu,Nenghai Yu*

Main category: cs.CV

TL;DR: 本文提出了一种无需训练的图像篡改定位框架ICFC，它利用多模态大型语言模型，在不依赖像素级标注的情况下，实现了业界领先的篡改检测、定位和可解释性。


<details>
  <summary>Details</summary>
Motivation: 现有的图像篡改定位方法要么依赖昂贵的像素级标注，要么性能不佳且缺乏可解释性，因此需要一种无需训练、可解释且高性能的图像篡改定位方法。

Method: ICFC框架整合了对象化规则构建、自适应过滤以建立可靠的知识库，并采用多步骤渐进推理流程，模拟专家取证工作流程，从而实现从粗略提议到细粒度取证结果的图像级分类、像素级定位和文本级可解释性。

Result: ICFC在多个基准测试中，不仅超越了最先进的无训练方法，而且与弱监督和全监督方法相比，也取得了有竞争力的甚至更优的性能。

Conclusion: ICFC是一个无需训练的框架，它利用多模态大型语言模型，通过模拟专家取证流程，在图像篡改定位任务中取得了卓越的性能和可解释性，有效解决了传统方法的局限性。

Abstract: Advances in image tampering pose serious security threats, underscoring the
need for effective image manipulation localization (IML). While supervised IML
achieves strong performance, it depends on costly pixel-level annotations.
Existing weakly supervised or training-free alternatives often underperform and
lack interpretability. We propose the In-Context Forensic Chain (ICFC), a
training-free framework that leverages multi-modal large language models
(MLLMs) for interpretable IML tasks. ICFC integrates an objectified rule
construction with adaptive filtering to build a reliable knowledge base and a
multi-step progressive reasoning pipeline that mirrors expert forensic
workflows from coarse proposals to fine-grained forensics results. This design
enables systematic exploitation of MLLM reasoning for image-level
classification, pixel-level localization, and text-level interpretability.
Across multiple benchmarks, ICFC not only surpasses state-of-the-art
training-free methods but also achieves competitive or superior performance
compared to weakly and fully supervised approaches.

</details>


### [37] [ImmerIris: A Large-Scale Dataset and Benchmark for Immersive Iris Recognition in Open Scenes](https://arxiv.org/abs/2510.10113)
*Yuxi Mi,Qiuyang Yuan,Zhizhou Zhong,Xuan Zhao,Jiaogen Zhou,Fubao Zhu,Jihong Guan,Shuigeng Zhou*

Main category: cs.CV

TL;DR: 这篇论文介绍了ImmerIris，一个用于沉浸式虹膜识别的大型数据集，它通过VR头显收集了近50万张偏轴眼部图像。研究发现，传统的识别方法在这种设置下表现不佳，因此提出了一种无需归一化的新范式，该方法有效提升了识别性能。


<details>
  <summary>Details</summary>
Motivation: 在增强现实和虚拟现实等以自我为中心的沉浸式应用中，虹膜识别作为一种精确且无缝的身份验证方式正在兴起。然而，与传统的轴上采集系统不同，沉浸式设置主要通过倾斜放置的头显摄像头捕获偏轴虹膜图像，这导致了透视畸变、质量下降和虹膜纹理类内变化等挑战。目前缺乏能够反映这些挑战的数据集，因此本研究旨在通过构建大规模数据集并提出新的识别范式来解决这些问题。

Method: 本文提出了一种名为ImmerIris的大型数据集，该数据集通过VR头显收集了来自564名受试者的499,791张眼部图像。基于此数据集，论文构建了评估协议，用于在不同挑战因素下对识别方法进行基准测试。针对传统方法在沉浸式设置中表现不佳的问题，论文提出了一种无需归一化的范式，该范式直接从眼部图像中学习，并进行最小的调整。

Result: ImmerIris数据集是目前最大的公共数据集，也是首批专门用于偏轴采集的数据集之一。基于ImmerIris的评估结果表明，主要为传统轴上图像设计的现有识别方法，在沉浸式设置中表现不佳，这主要是由于它们依赖于容易出错的归一化。本文提出的无需归一化范式，尽管简单，但其性能始终优于基于归一化的对应方法。

Conclusion: 为了应对沉浸式虹膜识别中偏轴图像采集带来的挑战，本文构建了迄今为止最大的公共数据集ImmerIris。通过对现有方法的评估，发现传统的归一化方法效果不佳。因此，本文提出了一种简单而有效的无需归一化范式，该方法在沉浸式识别中表现出更好的鲁棒性，为未来的研究指明了 promising 的方向。

Abstract: In egocentric applications such as augmented and virtual reality, immersive
iris recognition is emerging as an accurate and seamless way to identify
persons. While classic systems acquire iris images on-axis, i.e., via dedicated
frontal sensors in controlled settings, the immersive setup primarily captures
off-axis irises through tilt-placed headset cameras, with only mild control in
open scenes. This yields unique challenges, including perspective distortion,
intensified quality degradations, and intra-class variations in iris texture.
Datasets capturing these challenges remain scarce. To fill this gap, this paper
introduces ImmerIris, a large-scale dataset collected via VR headsets,
containing 499,791 ocular images from 564 subjects. It is, to the best of
current knowledge, the largest public dataset and among the first dedicated to
off-axis acquisition. Based on ImmerIris, evaluation protocols are constructed
to benchmark recognition methods under different challenging factors. Current
methods, primarily designed for classic on-axis imagery, perform
unsatisfactorily on the immersive setup, mainly due to reliance on fallible
normalization. To this end, this paper further proposes a normalization-free
paradigm that directly learns from ocular images with minimal adjustment.
Despite its simplicity, this approach consistently outperforms
normalization-based counterparts, pointing to a promising direction for robust
immersive recognition.

</details>


### [38] [Multi Class Parkinsons Disease Detection Based on Finger Tapping Using Attention-Enhanced CNN BiLSTM](https://arxiv.org/abs/2510.10121)
*Abu Saleh Musa Miah,Najmul Hassan,Md Maruf Al Hossain,Yuichi Okuyama,Jungpil Shin*

Main category: cs.CV

TL;DR: 本文提出了一种基于指尖敲击视频的多类别帕金森病（PD）严重程度检测系统，该系统采用注意力增强型CNN-BiLSTM模型，并取得了良好的性能。


<details>
  <summary>Details</summary>
Motivation: 目前基于手势的帕金森病识别系统性能不尽如人意，临床管理和干预开发需要更准确的PD严重程度评估。

Method: 收集指尖敲击视频，并从中提取腕部和手部运动的时间、频率和幅度特征。然后，提出一种混合深度学习框架，该框架集成了CNN、BiLSTM和注意力机制，用于从视频派生的运动特征中进行多类别PD严重程度分类。输入序列通过Conv1D-MaxPooling块捕获局部空间依赖性，然后输入到BiLSTM层以建模时间动态。注意力机制关注信息最丰富的时间特征，生成上下文向量，该向量由第二个BiLSTM层进一步处理。CNN派生的特征和注意力增强的BiLSTM输出被串联起来，然后是密集层和dropout层，最后由softmax分类器输出预测的PD严重程度级别。

Result: 该模型在区分五种严重程度类别方面表现出强大的性能。

Conclusion: 将时空表示与注意力机制集成可以改善自动PD严重程度检测，使其成为支持临床医生进行PD监测和进展跟踪的有前景的非侵入性工具。

Abstract: Effective clinical management and intervention development depend on accurate
evaluation of Parkinsons disease (PD) severity. Many researchers have worked on
developing gesture-based PD recognition systems; however, their performance
accuracy is not satisfactory. In this study, we propose a multi-class Parkinson
Disease detection system based on finger tapping using an attention-enhanced
CNN BiLSTM. We collected finger tapping videos and derived temporal, frequency,
and amplitude based features from wrist and hand movements. Then, we proposed a
hybrid deep learning framework integrating CNN, BiLSTM, and attention
mechanisms for multi-class PD severity classification from video-derived motion
features. First, the input sequence is reshaped and passed through a Conv1D
MaxPooling block to capture local spatial dependencies. The resulting feature
maps are fed into a BiLSTM layer to model temporal dynamics. An attention
mechanism focuses on the most informative temporal features, producing a
context vector that is further processed by a second BiLSTM layer. CNN-derived
features and attention-enhanced BiLSTM outputs are concatenated, followed by
dense and dropout layers, before the final softmax classifier outputs the
predicted PD severity level. The model demonstrated strong performance in
distinguishing between the five severity classes, suggesting that integrating
spatial temporal representations with attention mechanisms can improve
automated PD severity detection, making it a promising non-invasive tool to
support clinicians in PD monitoring and progression tracking.

</details>


### [39] [DeepFusionNet: Autoencoder-Based Low-Light Image Enhancement and Super-Resolution](https://arxiv.org/abs/2510.10122)
*Halil Hüseyin Çalışkan,Talha Koruk*

Main category: cs.CV

TL;DR: DeepFusionNet是一种用于提高低光照图像亮度和色彩以及图像超分辨率的架构。它在LOL-v1数据集上取得了高SSIM和PSNR分数，同时具有较少的参数。


<details>
  <summary>Details</summary>
Motivation: 计算机视觉和图像处理应用受到黑暗和低光照图像的困扰，尤其是在实时图像传输过程中。当前的自动编码器方法在处理低光照图像时PSNR和SSIM分数较低，并需要大量的计算能力。

Method: DeepFusionNet架构，以及基于DeepFusionNet的超分辨率模型。

Result: DeepFusionNet在LOL-v1数据集上实现了92.8%的SSIM和26.30的PSNR分数，参数量约为250万。基于DeepFusionNet的超分辨率方法在验证集上实现了25.30的PSNR和80.7%的SSIM分数，参数量约为10万。

Conclusion: DeepFusionNet架构在处理低光照图像增强和图像超分辨率方面表现出色，具有较高的性能和较低的计算成本。

Abstract: Computer vision and image processing applications suffer from dark and
low-light images, particularly during real-time image transmission. Currently,
low light and dark images are converted to bright and colored forms using
autoencoders; however, these methods often achieve low SSIM and PSNR scores and
require high computational power due to their large number of parameters. To
address these challenges, the DeepFusionNet architecture has been developed.
According to the results obtained with the LOL-v1 dataset, DeepFusionNet
achieved an SSIM of 92.8% and a PSNR score of 26.30, while containing only
approximately 2.5 million parameters. On the other hand, conversion of blurry
and low-resolution images into high-resolution and blur-free images has gained
importance in image processing applications. Unlike GAN-based super-resolution
methods, an autoencoder-based super resolution model has been developed that
contains approximately 100 thousand parameters and uses the DeepFusionNet
architecture. According to the results of the tests, the DeepFusionNet based
super-resolution method achieved a PSNR of 25.30 and a SSIM score of 80.7
percent according to the validation set.

</details>


### [40] [YOLOv11-Litchi: Efficient Litchi Fruit Detection based on UAV-Captured Agricultural Imagery in Complex Orchard Environments](https://arxiv.org/abs/2510.10141)
*Hongxing Peng,Haopei Xie,Weijia Lia,Huanai Liuc,Ximing Li*

Main category: cs.CV

TL;DR: YOLOv11-Litchi是一个轻量级、鲁棒的检测模型，专为无人机荔枝检测而设计，解决了小目标、模型部署和目标遮挡等挑战。


<details>
  <summary>Details</summary>
Motivation: 传统的荔枝人工筛选方法已无法满足现代生产需求，将无人机航空图像与深度学习相结合为提高效率和降低成本提供了有前景的解决方案。

Method: 该模型在YOLOv11框架基础上，引入了三项主要创新：多尺度残差模块以改善跨尺度上下文特征提取，轻量级特征融合方法以减小模型尺寸和计算成本，以及荔枝遮挡检测头部以减轻遮挡效应。

Result: YOLOv11-Litchi的模型参数大小为6.35 MB，比YOLOv11基线小32.5%，同时mAP提高了2.5%达到90.1%，F1-Score提高了1.4%达到85.5%。此外，模型实现了57.2 FPS的帧率。

Conclusion: YOLOv11-Litchi适用于复杂果园环境中的无人机荔枝检测，并展示了其在精准农业中更广泛应用的潜力。

Abstract: Litchi is a high-value fruit, yet traditional manual selection methods are
increasingly inadequate for modern production demands. Integrating UAV-based
aerial imagery with deep learning offers a promising solution to enhance
efficiency and reduce costs. This paper introduces YOLOv11-Litchi, a
lightweight and robust detection model specifically designed for UAV-based
litchi detection. Built upon the YOLOv11 framework, the proposed model
addresses key challenges such as small target size, large model parameters
hindering deployment, and frequent target occlusion. To tackle these issues,
three major innovations are incorporated: a multi-scale residual module to
improve contextual feature extraction across scales, a lightweight feature
fusion method to reduce model size and computational costs while maintaining
high accuracy, and a litchi occlusion detection head to mitigate occlusion
effects by emphasizing target regions and suppressing background interference.
Experimental results validate the model's effectiveness. YOLOv11-Litchi
achieves a parameter size of 6.35 MB - 32.5% smaller than the YOLOv11 baseline
- while improving mAP by 2.5% to 90.1% and F1-Score by 1.4% to 85.5%.
Additionally, the model achieves a frame rate of 57.2 FPS, meeting real-time
detection requirements. These findings demonstrate the suitability of
YOLOv11-Litchi for UAV-based litchi detection in complex orchard environments,
showcasing its potential for broader applications in precision agriculture.

</details>


### [41] [Color3D: Controllable and Consistent 3D Colorization with Personalized Colorizer](https://arxiv.org/abs/2510.10152)
*Yecong Wan,Mingwen Shao,Renlong Wu,Wangmeng Zuo*

Main category: cs.CV

TL;DR: Color3D 是一个高度适应性的框架，能够对静态和动态 3D 场景进行着色，通过个性化着色器和 Lab 颜色空间高斯溅射表示，实现颜色多样性、可控性以及跨视图和跨时间一致性。


<details>
  <summary>Details</summary>
Motivation: 现有的 3D 着色方法主要关注静态场景，并通过平均颜色变化来强制实现多视图一致性，这不可避免地牺牲了色彩丰富度和可控性。

Method: 本方法的核心思想是只对单个关键视图进行着色，然后微调一个个性化的着色器，将其颜色传播到新的视图和时间步。通过个性化，着色器学习参考视图下场景特定的确定性颜色映射，从而能够通过其固有的归纳偏差将相应的颜色一致地投射到新视图和视频帧中的内容。训练完成后，个性化的着色器可用于推断所有其他图像的一致色度，从而通过专用的 Lab 颜色空间高斯溅射表示直接重建彩色 3D 场景。

Result: Color3D 框架成功地将复杂的 3D 着色 recast 为更易处理的单图像范式，实现了任意图像着色模型的无缝集成，并增强了灵活性和可控性。

Conclusion: 广泛的实验证明，Color3D 方法可以在多样化的静态和动态 3D 着色基准上提供更一致、色彩更丰富的渲染，并具有精确的用户控制。

Abstract: In this work, we present Color3D, a highly adaptable framework for colorizing
both static and dynamic 3D scenes from monochromatic inputs, delivering
visually diverse and chromatically vibrant reconstructions with flexible
user-guided control. In contrast to existing methods that focus solely on
static scenarios and enforce multi-view consistency by averaging color
variations which inevitably sacrifice both chromatic richness and
controllability, our approach is able to preserve color diversity and
steerability while ensuring cross-view and cross-time consistency. In
particular, the core insight of our method is to colorize only a single key
view and then fine-tune a personalized colorizer to propagate its color to
novel views and time steps. Through personalization, the colorizer learns a
scene-specific deterministic color mapping underlying the reference view,
enabling it to consistently project corresponding colors to the content in
novel views and video frames via its inherent inductive bias. Once trained, the
personalized colorizer can be applied to infer consistent chrominance for all
other images, enabling direct reconstruction of colorful 3D scenes with a
dedicated Lab color space Gaussian splatting representation. The proposed
framework ingeniously recasts complicated 3D colorization as a more tractable
single image paradigm, allowing seamless integration of arbitrary image
colorization models with enhanced flexibility and controllability. Extensive
experiments across diverse static and dynamic 3D colorization benchmarks
substantiate that our method can deliver more consistent and chromatically rich
renderings with precise user control. Project Page
https://yecongwan.github.io/Color3D/.

</details>


### [42] [Stroke Locus Net: Occluded Vessel Localization from MRI Modalities](https://arxiv.org/abs/2510.10155)
*Mohamed Hamad,Muhammad Khan,Tamer Khattab,Mohamed Mabrok*

Main category: cs.CV

TL;DR: 该研究介绍了Stroke Locus Net，一个用于缺血性卒中诊断的端到端深度学习流程，它通过结合病灶分割、动脉图谱和MRA图像合成，实现了闭塞血管的定位。


<details>
  <summary>Details</summary>
Motivation: 目前机器学习在缺血性卒中诊断中主要关注病变分割，而闭塞血管定位的工作有限，这是一个关键挑战。

Method: Stroke Locus Net是一个端到端的深度学习流程。它结合了：1. 使用nnUNet的分割分支进行病灶检测。2. 利用动脉图谱进行血管映射和识别。3. 使用pGAN的生成分支从MRI图像合成MRA图像。

Result: 该实现展示了在受卒中影响的T1 MRI扫描上定位闭塞血管的良好结果。

Conclusion: Stroke Locus Net系统在缺血性卒中诊断中，尤其是在闭塞血管定位方面，具有更快速、更明智的潜力。

Abstract: A key challenge in ischemic stroke diagnosis using medical imaging is the
accurate localization of the occluded vessel. Current machine learning methods
in focus primarily on lesion segmentation, with limited work on vessel
localization. In this study, we introduce Stroke Locus Net, an end-to-end deep
learning pipeline for detection, segmentation, and occluded vessel localization
using only MRI scans. The proposed system combines a segmentation branch using
nnUNet for lesion detection with an arterial atlas for vessel mapping and
identification, and a generation branch using pGAN to synthesize MRA images
from MRI. Our implementation demonstrates promising results in localizing
occluded vessels on stroke-affected T1 MRI scans, with potential for faster and
more informed stroke diagnosis.

</details>


### [43] [SaFiRe: Saccade-Fixation Reiteration with Mamba for Referring Image Segmentation](https://arxiv.org/abs/2510.10160)
*Zhenjie Mao,Yuhuan Yang,Chaofan Ma,Dongsheng Jiang,Jiangchao Yao,Ya Zhang,Yanfeng Wang*

Main category: cs.CV

TL;DR: 该文章介绍了一种名为 SaFiRe 的新型参照图像分割（RIS）框架，其灵感来源于人类的认知过程，旨在解决现有 RIS 模型在处理复杂参照表达（例如物体干扰表达和类别隐含表达）时遇到的挑战。文章还引入了 aRefCOCO，一个用于评估 RIS 模型在模糊参照表达下性能的新基准。


<details>
  <summary>Details</summary>
Motivation: 现有的参照图像分割（RIS）方法主要关注简单的表达，如“红色汽车”或“左边的女孩”，这使得 RIS 降级为关键词/概念匹配问题，限制了模型处理表达中参照模糊性的能力。这项工作的动机是为了解决在处理复杂参照表达时，现有 RIS 模型所面临的挑战，特别是在物体干扰表达和类别隐含表达这两种真实世界场景中的不足。

Method: 本文提出了一种名为 SaFiRe 的新型框架，它模仿人类的两阶段认知过程：首先形成全局理解，然后通过细节导向的检查进行细化。Mamba 的扫描-更新特性自然支持了这种方法，它与本文的阶段性设计相吻合，并能以线性复杂度实现高效的多周期细化。此外，本文还引入了 aRefCOCO，一个新基准，用于评估 RIS 模型在模糊参照表达下的性能。

Result: 在标准数据集和本文提出的数据集上进行的广泛实验表明，SaFiRe 优于最先进的基线方法。

Conclusion: SaFiRe 框架通过模仿人类的两阶段认知过程，并利用 Mamba 的特性，有效解决了参照图像分割中复杂和模糊表达的挑战。aRefCOCO 基准的引入，为未来研究提供了一个衡量模型在真实世界复杂情境下性能的工具。

Abstract: Referring Image Segmentation (RIS) aims to segment the target object in an
image given a natural language expression. While recent methods leverage
pre-trained vision backbones and more training corpus to achieve impressive
results, they predominantly focus on simple expressions--short, clear noun
phrases like "red car" or "left girl". This simplification often reduces RIS to
a key word/concept matching problem, limiting the model's ability to handle
referential ambiguity in expressions. In this work, we identify two challenging
real-world scenarios: object-distracting expressions, which involve multiple
entities with contextual cues, and category-implicit expressions, where the
object class is not explicitly stated. To address the challenges, we propose a
novel framework, SaFiRe, which mimics the human two-phase cognitive
process--first forming a global understanding, then refining it through
detail-oriented inspection. This is naturally supported by Mamba's
scan-then-update property, which aligns with our phased design and enables
efficient multi-cycle refinement with linear complexity. We further introduce
aRefCOCO, a new benchmark designed to evaluate RIS models under ambiguous
referring expressions. Extensive experiments on both standard and proposed
datasets demonstrate the superiority of SaFiRe over state-of-the-art baselines.

</details>


### [44] [SparseUWSeg: Active Sparse Point-Label Augmentation for Underwater Semantic Segmentation](https://arxiv.org/abs/2510.10163)
*César Borja,Carlos Plou,Rubén Martinez-Cantín,Ana C. Murillo*

Main category: cs.CV

TL;DR: SparseUWSeg是一个新颖的框架，它采用主动采样策略指导注释者，最大化点标签的价值。然后，它通过混合方法传播这些稀疏标签，该方法利用SAM2和基于超像素的方法的优点。


<details>
  <summary>Details</summary>
Motivation: 语义分割对于自动化水下图像分析以进行生态监测至关重要。D+NN在水下场景分析方面仍是一个未解决的问题，甚至对于顶级分割模型也是如此。获得密集的、专家注释的分割标签的成本很高，这阻碍了该领域模型的监督。

Method: SparseUWSeg采用了主动采样策略来指导注释者，最大限度地提高点标签的价值。然后，它通过结合SAM2和基于超像素的方法的混合方法传播这些稀疏标签。

Result: 在两个不同的水下数据集上进行的实验表明，SparseUWSeg优于最先进的方法，与D+NN相比，mIoU提高了5%。

Conclusion: SparseUWSeg是一个简单但有效的交互式注释工具，它集成了我们的算法，并使生态研究人员能够利用基础模型和计算机视觉来高效生成高质量的分割掩模以处理其数据。

Abstract: Semantic segmentation is essential to automate underwater imagery analysis
with ecology monitoring purposes. Unfortunately, fine grained underwater scene
analysis is still an open problem even for top performing segmentation models.
The high cost of obtaining dense, expert-annotated, segmentation labels hinders
the supervision of models in this domain. While sparse point-labels are easier
to obtain, they introduce challenges regarding which points to annotate and how
to propagate the sparse information. We present SparseUWSeg, a novel framework
that addresses both issues. SparseUWSeg employs an active sampling strategy to
guide annotators, maximizing the value of their point labels. Then, it
propagates these sparse labels with a hybrid approach leverages both the best
of SAM2 and superpixel-based methods. Experiments on two diverse underwater
datasets demonstrate the benefits of SparseUWSeg over state-of-the-art
approaches, achieving up to +5\% mIoU over D+NN. Our main contribution is the
design and release of a simple but effective interactive annotation tool,
integrating our algorithms. It enables ecology researchers to leverage
foundation models and computer vision to efficiently generate high-quality
segmentation masks to process their data.

</details>


### [45] [ViConEx-Med: Visual Concept Explainability via Multi-Concept Token Transformer for Medical Image Analysis](https://arxiv.org/abs/2510.10174)
*Cristiano Patrício,Luís F. Teixeira,João C. Neves*

Main category: cs.CV

TL;DR: 本文提出了一种名为ViConEx-Med的Transformer框架，用于医学领域的视觉概念可解释性，它通过引入多概念可学习token来共同预测和定位视觉概念，从而提供概念层面的定位图。


<details>
  <summary>Details</summary>
Motivation: 现有的概念模型通常将概念视为数值属性，缺乏互补的视觉解释，限制了它们在现实世界，特别是医疗等高风险场景中的应用。

Method: ViConEx-Med是一种新颖的基于Transformer的视觉概念可解释性框架，它引入了多概念可学习token来共同预测和定位视觉概念。该方法利用专门的注意力层处理视觉和基于文本的概念token，从而生成概念层面的定位图。

Result: 在合成和真实世界的医疗数据集上的实验表明，ViConEx-Med在概念检测和定位精度方面优于先前的概念模型，并与黑盒模型取得了有竞争力的性能。

Conclusion: ViConEx-Med为构建基于视觉概念的固有可解释模型提供了一个有前景的方向。

Abstract: Concept-based models aim to explain model decisions with human-understandable
concepts. However, most existing approaches treat concepts as numerical
attributes, without providing complementary visual explanations that could
localize the predicted concepts. This limits their utility in real-world
applications and particularly in high-stakes scenarios, such as medical
use-cases. This paper proposes ViConEx-Med, a novel transformer-based framework
for visual concept explainability, which introduces multi-concept learnable
tokens to jointly predict and localize visual concepts. By leveraging
specialized attention layers for processing visual and text-based concept
tokens, our method produces concept-level localization maps while maintaining
high predictive accuracy. Experiments on both synthetic and real-world medical
datasets demonstrate that ViConEx-Med outperforms prior concept-based models
and achieves competitive performance with black-box models in terms of both
concept detection and localization precision. Our results suggest a promising
direction for building inherently interpretable models grounded in visual
concepts. Code is publicly available at
https://github.com/CristianoPatricio/viconex-med.

</details>


### [46] [HccePose(BF): Predicting Front \& Back Surfaces to Construct Ultra-Dense 2D-3D Correspondences for Pose Estimation](https://arxiv.org/abs/2510.10177)
*Yulin Wang,Mengting Hu,Hongli Li,Chen Luo*

Main category: cs.CV

TL;DR: 本文提出了一种新的姿态估计算法，通过预测物体前后表面的3D坐标并在它们之间进行密集采样来创建超密集2D-3D对应关系，从而提高姿态估计的准确性。此外，还提出了分层连续坐标编码（HCCE）以实现更准确和高效的坐标表示。


<details>
  <summary>Details</summary>
Motivation: 目前的物体姿态估计算法主要关注物体前表面的3D坐标预测，忽略了结合物体后表面和内部的潜力。

Method: 本文预测了物体前后表面的3D坐标，并在它们之间密集采样3D坐标，以创建超密集的2D-3D对应关系。此外，还提出了分层连续坐标编码（HCCE）来更准确和高效地表示前后表面坐标。

Result: 实验结果表明，在BOP网站的七个经典BOP核心数据集上，本文提出的方法优于现有的最先进方法。

Conclusion: 本文提出的方法通过利用物体前后表面和内部的3D坐标，并结合HCCE编码，显著提高了物体姿态估计的准确性。

Abstract: In pose estimation for seen objects, a prevalent pipeline involves using
neural networks to predict dense 3D coordinates of the object surface on 2D
images, which are then used to establish dense 2D-3D correspondences. However,
current methods primarily focus on more efficient encoding techniques to
improve the precision of predicted 3D coordinates on the object's front
surface, overlooking the potential benefits of incorporating the back surface
and interior of the object. To better utilize the full surface and interior of
the object, this study predicts 3D coordinates of both the object's front and
back surfaces and densely samples 3D coordinates between them. This process
creates ultra-dense 2D-3D correspondences, effectively enhancing pose
estimation accuracy based on the Perspective-n-Point (PnP) algorithm.
Additionally, we propose Hierarchical Continuous Coordinate Encoding (HCCE) to
provide a more accurate and efficient representation of front and back surface
coordinates. Experimental results show that, compared to existing
state-of-the-art (SOTA) methods on the BOP website, the proposed approach
outperforms across seven classic BOP core datasets. Code is available at
https://github.com/WangYuLin-SEU/HCCEPose.

</details>


### [47] [TCMA: Text-Conditioned Multi-granularity Alignment for Drone Cross-Modal Text-Video Retrieval](https://arxiv.org/abs/2510.10180)
*Zixu Zhao,Yang Zhan*

Main category: cs.CV

TL;DR: 该文提出了一个名为DVTMD的无人机文本-视频匹配数据集，并提出了TCMA框架以解决无人机领域中文本-视频检索的挑战。


<details>
  <summary>Details</summary>
Motivation: 无人机（UAV）在实时、高分辨率数据采集中发挥着强大作用，产生了大量的航空视频。从这些视频中高效检索相关内容对于城市管理、应急响应、安全和灾害救援等应用至关重要。然而，现有数据集中粗糙和冗余的字幕限制了无人机领域文本-视频检索的发展。

Method: 1. 构建DVTMD数据集：包含2,864个视频和14,320个细粒度、语义多样化的字幕。
2. 提出TCMA（Text-Conditioned Multi-granularity Alignment）框架：该框架整合了全局视频-句子对齐、句子引导的帧聚合以及单词引导的补丁对齐。
3. 设计词语和补丁选择模块：用于过滤不相关内容。
4. 设计文本自适应动态温度机制：使注意力锐度适应文本类型。

Result: DVTMD和CapERA上的实验建立了无人机文本-视频检索的第一个完整基准。TCMA框架在文本到视频检索中实现了45.5%的R@1，在视频到文本检索中实现了42.8%的R@1，达到了最先进的性能。

Conclusion: 该工作通过构建DVTMD数据集和提出TCMA框架，为无人机领域的文本-视频检索提供了有效的解决方案，并取得了最先进的性能。

Abstract: Unmanned aerial vehicles (UAVs) have become powerful platforms for real-time,
high-resolution data collection, producing massive volumes of aerial videos.
Efficient retrieval of relevant content from these videos is crucial for
applications in urban management, emergency response, security, and disaster
relief. While text-video retrieval has advanced in natural video domains, the
UAV domain remains underexplored due to limitations in existing datasets, such
as coarse and redundant captions. Thus, in this work, we construct the Drone
Video-Text Match Dataset (DVTMD), which contains 2,864 videos and 14,320
fine-grained, semantically diverse captions. The annotations capture multiple
complementary aspects, including human actions, objects, background settings,
environmental conditions, and visual style, thereby enhancing text-video
correspondence and reducing redundancy. Building on this dataset, we propose
the Text-Conditioned Multi-granularity Alignment (TCMA) framework, which
integrates global video-sentence alignment, sentence-guided frame aggregation,
and word-guided patch alignment. To further refine local alignment, we design a
Word and Patch Selection module that filters irrelevant content, as well as a
Text-Adaptive Dynamic Temperature Mechanism that adapts attention sharpness to
text type. Extensive experiments on DVTMD and CapERA establish the first
complete benchmark for drone text-video retrieval. Our TCMA achieves
state-of-the-art performance, including 45.5% R@1 in text-to-video and 42.8%
R@1 in video-to-text retrieval, demonstrating the effectiveness of our dataset
and method. The code and dataset will be released.

</details>


### [48] [Fairness Without Labels: Pseudo-Balancing for Bias Mitigation in Face Gender Classification](https://arxiv.org/abs/2510.10191)
*Haohua Dong,Ana Manzano Rodríguez,Camille Guinaudeau,Shin'ichi Satoh*

Main category: cs.CV

TL;DR: 这篇论文介绍了一种名为“伪平衡”的策略，用于减轻半监督学习中人脸性别分类模型的偏见。


<details>
  <summary>Details</summary>
Motivation: 人脸性别分类模型通常会反映和放大训练数据中存在的人口统计学偏差，导致在不同性别和种族亚群中表现不均。

Method: 该方法在伪标签选择过程中强制执行人口统计学平衡，仅使用来自种族平衡数据集的未标记图像，无需访问真实标注。

Result: 伪平衡策略在两种情况下进行了评估：1) 使用FairFace数据集中的未标记图像微调有偏见的性别分类器，2) 使用故意不平衡的训练数据进行方法压力测试，模拟受控的偏见场景。在两种情况下，模型都在All-Age-Faces (AAF) 基准上进行评估。结果显示，伪平衡始终能提高公平性，同时保持或提高准确性。该方法达到了79.81%的总体准确率，比基线提高了6.53%，并将性别准确率差距缩小了44.17%。在东亚亚群中，基线差异超过49%的情况下，差距缩小到仅为5.01%。

Conclusion: 即使在没有标签监督的情况下，访问人口统计学平衡或适度倾斜的未标记数据集也可以作为消除现有计算机视觉模型偏见的强大资源。

Abstract: Face gender classification models often reflect and amplify demographic
biases present in their training data, leading to uneven performance across
gender and racial subgroups. We introduce pseudo-balancing, a simple and
effective strategy for mitigating such biases in semi-supervised learning. Our
method enforces demographic balance during pseudo-label selection, using only
unlabeled images from a race-balanced dataset without requiring access to
ground-truth annotations.
  We evaluate pseudo-balancing under two conditions: (1) fine-tuning a biased
gender classifier using unlabeled images from the FairFace dataset, and (2)
stress-testing the method with intentionally imbalanced training data to
simulate controlled bias scenarios. In both cases, models are evaluated on the
All-Age-Faces (AAF) benchmark, which contains a predominantly East Asian
population. Our results show that pseudo-balancing consistently improves
fairness while preserving or enhancing accuracy. The method achieves 79.81%
overall accuracy - a 6.53% improvement over the baseline - and reduces the
gender accuracy gap by 44.17%. In the East Asian subgroup, where baseline
disparities exceeded 49%, the gap is narrowed to just 5.01%. These findings
suggest that even in the absence of label supervision, access to a
demographically balanced or moderately skewed unlabeled dataset can serve as a
powerful resource for debiasing existing computer vision models.

</details>


### [49] [From Generic to Specialized: A Subspecialty Diagnostic System Powered by Self-Supervised Learning for Cervical Histopathology](https://arxiv.org/abs/2510.10196)
*Yizhi Wang,Li Chen,Qiang Huang,Tian Guan,Xi Deng,Zhiyuan Shen,Jiawen Li,Xinrui Chen,Bin Hu,Xitong Ling,Taojie Zhu,Zirui Huang,Deshui Yu,Yan Liu,Jiurun Chen,Lianghui Zhu,Qiming He,Yiqing Liu,Diwei Shi,Hanzhong Liu,Junbo Hu,Hongyi Gao,Zhen Song,Xilong Zhao,Chao He,Ming Zhao,Yonghong He*

Main category: cs.CV

TL;DR: CerS-Path是一种专为宫颈癌诊断设计的AI系统，通过两阶段预训练和多模态增强，在宫颈病理诊断中展现出卓越的准确性和泛化能力。


<details>
  <summary>Details</summary>
Motivation: 目前深度学习模型在宫颈癌诊断中存在准确性和泛化能力不足的问题，而通用基础模型又难以捕捉亚专科特异性特征和适应任务。

Method: CerS-Path通过两个协同的预训练阶段开发：首先，在一个包含约1.9亿个组织切片（来自14万张玻片）的数据集上进行自监督学习，构建一个宫颈特异性特征提取器；其次，通过250万个图像-文本对进行多模态增强，然后与多个下游诊断功能集成。

Result: CerS-Path支持八种诊断功能，包括罕见癌症分类和多模态问答。它在范围和临床适用性方面超越了以前的基础模型。在前瞻性测试中，对来自五个中心的3173个病例进行了评估，保持了99.38%的筛查敏感性，并表现出出色的泛化能力。

Conclusion: CerS-Path在宫颈病理学领域取得了显著进展，有望实现亚专科诊断转化和宫颈癌筛查，解决了现有模型在准确性和泛化能力方面的不足。

Abstract: Cervical cancer remains a major malignancy, necessitating extensive and
complex histopathological assessments and comprehensive support tools. Although
deep learning shows promise, these models still lack accuracy and
generalizability. General foundation models offer a broader reach but remain
limited in capturing subspecialty-specific features and task adaptability. We
introduce the Cervical Subspecialty Pathology (CerS-Path) diagnostic system,
developed through two synergistic pretraining stages: self-supervised learning
on approximately 190 million tissue patches from 140,000 slides to build a
cervical-specific feature extractor, and multimodal enhancement with 2.5
million image-text pairs, followed by integration with multiple downstream
diagnostic functions. Supporting eight diagnostic functions, including rare
cancer classification and multimodal Q&A, CerS-Path surpasses prior foundation
models in scope and clinical applicability. Comprehensive evaluations
demonstrate a significant advance in cervical pathology, with prospective
testing on 3,173 cases across five centers maintaining 99.38% screening
sensitivity and excellent generalizability, highlighting its potential for
subspecialty diagnostic translation and cervical cancer screening.

</details>


### [50] [A Style-Based Metric for Quantifying the Synthetic-to-Real Gap in Autonomous Driving Image Datasets](https://arxiv.org/abs/2510.10203)
*Dingyi Yao,Xinyao Han,Ruibo Ming,Zhihang Song,Lihui Peng,Jianming Hu,Danya Yao,Yi Zhang*

Main category: cs.CV

TL;DR: 这篇论文提出了一种量化自动驾驶感知系统中合成数据与真实世界数据之间差距的系统框架和新评估指标，以提升合成数据集的实用性及自动驾驶系统的发展。


<details>
  <summary>Details</summary>
Motivation: 自动驾驶感知系统需要大量的环境测试，但真实世界的执行不切实际。合成数据集虽然有成本效益、无偏标记和场景可控等优点，但合成数据与真实世界数据之间的领域差异是AI自动驾驶模型泛化的关键瓶颈。因此，量化这种合成到真实的差距对于评估数据集效用和指导更有效的训练流程至关重要。

Method: 本研究建立了一个系统框架来量化自动驾驶系统中的合成到真实差距，并提出了Style Embedding Distribution Discrepancy (SEDD) 作为一种新的评估指标。该框架结合了基于Gram矩阵的风格提取和优化了类内紧凑性及类间分离的度量学习，以提取风格嵌入。此外，研究还使用公开数据集建立了一个基准。

Result: 在各种数据集和sim-to-real方法上进行的实验表明，所提出的方法能够有效地量化合成到真实的差距。

Conclusion: 这项工作提供了一个标准化的质量控制工具，能够系统地诊断和有针对性地增强合成数据集，从而促进数据驱动的自动驾驶系统的未来发展。

Abstract: Ensuring the reliability of autonomous driving perception systems requires
extensive environment-based testing, yet real-world execution is often
impractical. Synthetic datasets have therefore emerged as a promising
alternative, offering advantages such as cost-effectiveness, bias free
labeling, and controllable scenarios. However, the domain gap between synthetic
and real-world datasets remains a critical bottleneck for the generalization of
AI-based autonomous driving models. Quantifying this synthetic-to-real gap is
thus essential for evaluating dataset utility and guiding the design of more
effective training pipelines. In this paper, we establish a systematic
framework for quantifying the synthetic-to-real gap in autonomous driving
systems, and propose Style Embedding Distribution Discrepancy (SEDD) as a novel
evaluation metric. Our framework combines Gram matrix-based style extraction
with metric learning optimized for intra-class compactness and inter-class
separation to extract style embeddings. Furthermore, we establish a benchmark
using publicly available datasets. Experiments are conducted on a variety of
datasets and sim-to-real methods, and the results show that our method is
capable of quantifying the synthetic-to-real gap. This work provides a
standardized quality control tool that enables systematic diagnosis and
targeted enhancement of synthetic datasets, advancing future development of
data-driven autonomous driving systems.

</details>


### [51] [Semantic Visual Anomaly Detection and Reasoning in AI-Generated Images](https://arxiv.org/abs/2510.10231)
*Chuangchuang Tan,Xiang Ming,Jinglu Wang,Renshuai Tao,Bin Li,Yunchao Wei,Yao Zhao,Yan Lu*

Main category: cs.CV

TL;DR: 检测AIGC图像中的语义异常对于评估其可信度至关重要。本文提出了AnomReason，一个大型基准，它通过一个多智能体框架AnomAgent进行结构化标注，用于支持AIGC图像的语义异常检测和推理。


<details>
  <summary>Details</summary>
Motivation: 快速发展的AIGC技术可以生成视觉上逼真的图像，但这些图像常伴有细微的语义异常，如物体配置不符合现实、违反物理定律或缺乏常识等，这降低了生成场景的整体合理性。检测这些语义层面的异常对于评估AIGC媒体的可信度至关重要，特别是在AIGC图像分析、可解释的深度伪造检测和语义真实性评估中。

Method: 本文将AIGC图像的语义异常检测与推理问题形式化，并引入了AnomReason这一大规模基准。该基准包含结构化标注，标注形式为四元组(名称、现象、推理、严重性)。标注通过一个模块化的多智能体管道（AnomAgent）生成，并辅以轻量级的人工验证，以确保在大规模标注的同时保持质量。AnomAgent在构建时处理了约4.17亿个GPT-4o tokens。

Result: 在AnomReason上进行微调的模型在本文提出的语义匹配度量标准（SemAP和SemF1）下，比强大的视觉-语言基线模型取得了显著的性能提升。

Conclusion: AnomReason和AnomAgent为衡量和提升AI生成图像的语义合理性奠定了基础。研究成果可应用于可解释的深度伪造检测和图像生成器的语义合理性评估。作者将发布代码、度量标准、数据和任务对齐模型，以支持语义真实性和可解释AIGC取证方面的可重复研究。

Abstract: The rapid advancement of
  AI-generated content (AIGC) has enabled the synthesis of visually convincing
images; however, many such outputs exhibit subtle \textbf{semantic anomalies},
including unrealistic object configurations, violations of physical laws, or
commonsense inconsistencies, which compromise the overall plausibility of the
generated scenes. Detecting these semantic-level anomalies
  is essential for assessing the trustworthiness of AIGC media, especially in
AIGC image analysis, explainable deepfake detection and semantic authenticity
assessment. In this paper,
  we formalize \textbf{semantic anomaly detection and reasoning} for AIGC
images and
  introduce \textbf{AnomReason}, a large-scale benchmark with structured
annotations as quadruples \emph{(Name, Phenomenon, Reasoning, Severity)}.
Annotations are produced by
  a modular multi-agent pipeline (\textbf{AnomAgent}) with lightweight
human-in-the-loop verification, enabling scale while preserving quality.
  At construction time, AnomAgent processed approximately 4.17\,B GPT-4o
tokens, providing scale evidence for the resulting structured annotations. We
further
  show that models fine-tuned on AnomReason achieve consistent gains over
strong vision-language baselines under our proposed semantic matching metric
(\textit{SemAP} and \textit{SemF1}).
  Applications to {explainable deepfake detection} and {semantic reasonableness
assessment of image generators} demonstrate practical utility. In summary,
AnomReason and AnomAgent
  serve as a foundation for measuring and improving the semantic plausibility
of AI-generated images. We will release code, metrics, data, and task-aligned
models to support reproducible research on semantic authenticity and
interpretable AIGC forensics.

</details>


### [52] [Are Video Models Emerging as Zero-Shot Learners and Reasoners in Medical Imaging?](https://arxiv.org/abs/2510.10254)
*Yuxiang Lai,Jike Zhong,Ming Li,Yuheng Li,Xiaofeng Yang*

Main category: cs.CV

TL;DR: 本文探讨了大型生成模型在医学影像任务中的零样本泛化能力，发现即使未经医学数据训练，这些模型也能在分割、去噪、超分辨率和运动预测等任务中表现出色，尤其在运动预测方面超越了专业基线。


<details>
  <summary>Details</summary>
Motivation: 探索自回归视频建模原则是否可以直接应用于医学影像任务，以及大型视觉模型（LVM）在医学领域零样本设置下的表现。

Method: 在大规模零样本设置下，评估大型视觉模型（LVM）在四项代表性任务中的表现：器官分割、去噪、超分辨率和运动预测。利用来自122名患者的4D CT数据，共计1,820多个3D CT体素进行评估。

Result: LVM即使未经特定领域微调，也能勾勒出CT扫描中的解剖结构，并在分割、去噪和超分辨率方面达到具有竞争力的性能。在放射治疗运动预测中，模型能直接从4D CT扫描的先前阶段预测未来的3D CT阶段，产生具有解剖学一致性的预测，并捕捉患者特定的呼吸动态。在运动预测方面，该模型超越了专业的DVF基线和生成基线，达到了最先进的空间精度。

Conclusion: 研究结果揭示了医学视频建模中零样本能力的出现，并强调了通用视频模型作为统一学习器和推理器的潜力，为未来基于视频模型的医学基础模型奠定了基础。

Abstract: Recent advances in large generative models have shown that simple
autoregressive formulations, when scaled appropriately, can exhibit strong
zero-shot generalization across domains. Motivated by this trend, we
investigate whether autoregressive video modeling principles can be directly
applied to medical imaging tasks, despite the model never being trained on
medical data. Specifically, we evaluate a large vision model (LVM) in a
zero-shot setting across four representative tasks: organ segmentation,
denoising, super-resolution, and motion prediction. Remarkably, even without
domain-specific fine-tuning, the LVM can delineate anatomical structures in CT
scans and achieve competitive performance on segmentation, denoising, and
super-resolution. Most notably, in radiotherapy motion prediction, the model
forecasts future 3D CT phases directly from prior phases of a 4D CT scan,
producing anatomically consistent predictions that capture patient-specific
respiratory dynamics with realistic temporal coherence. We evaluate the LVM on
4D CT data from 122 patients, totaling over 1,820 3D CT volumes. Despite no
prior exposure to medical data, the model achieves strong performance across
all tasks and surpasses specialized DVF-based and generative baselines in
motion prediction, achieving state-of-the-art spatial accuracy. These findings
reveal the emergence of zero-shot capabilities in medical video modeling and
highlight the potential of general-purpose video models to serve as unified
learners and reasoners laying the groundwork for future medical foundation
models built on video models.

</details>


### [53] [Opacity-Gradient Driven Density Control for Compact and Efficient Few-Shot 3D Gaussian Splatting](https://arxiv.org/abs/2510.10257)
*Abdelrhman Elrawy,Emad A. Mohammed*

Main category: cs.CV

TL;DR: 本文提出了一种新的3DGS优化框架，通过替换位置梯度启发式为不透明度梯度触发的致密化策略，并结合保守的剪枝计划和深度相关损失，显著提高了少样本场景下的3DGS效率和模型紧凑性，同时保持了较高的重建质量，在几个主流数据集中取得了显著的模型大小缩减。


<details>
  <summary>Details</summary>
Motivation: 3D Gaussian Splatting (3DGS) 在少样本场景中表现不佳，其标准的自适应密度控制 (ADC) 容易导致过拟合和重建膨胀。尽管现有方法如 FSGS 提高了质量，但通常以显著增加图元数量为代价。

Method: 本文框架通过以下方式进行优化：1. 用不透明度梯度作为渲染误差的轻量级替代，替换了标准的基于位置梯度的致密化触发机制。 2. 结合了更保守的剪枝策略，以防止破坏性优化循环。 3. 引入了标准的深度相关损失以提供几何指导。

Result: 在3视图LLFF数据集上，模型比FSGS紧凑40%以上（32k vs. 57k 图元）。在Mip-NeRF 360数据集上，模型大小减少了约70%。在获得显著紧凑性的同时，重建指标仅有适度的权衡，在少样本视图合成的质量-效率帕累托前沿上确立了新的最先进水平。

Conclusion: 本文框架通过改进3DGS的优化过程，显著提升了在少样本场景下的效率和模型紧凑性，有望成为该领域的新基准。

Abstract: 3D Gaussian Splatting (3DGS) struggles in few-shot scenarios, where its
standard adaptive density control (ADC) can lead to overfitting and bloated
reconstructions. While state-of-the-art methods like FSGS improve quality, they
often do so by significantly increasing the primitive count. This paper
presents a framework that revises the core 3DGS optimization to prioritize
efficiency. We replace the standard positional gradient heuristic with a novel
densification trigger that uses the opacity gradient as a lightweight proxy for
rendering error. We find this aggressive densification is only effective when
paired with a more conservative pruning schedule, which prevents destructive
optimization cycles. Combined with a standard depth-correlation loss for
geometric guidance, our framework demonstrates a fundamental improvement in
efficiency. On the 3-view LLFF dataset, our model is over 40% more compact (32k
vs. 57k primitives) than FSGS, and on the Mip-NeRF 360 dataset, it achieves a
reduction of approximately 70%. This dramatic gain in compactness is achieved
with a modest trade-off in reconstruction metrics, establishing a new
state-of-the-art on the quality-vs-efficiency Pareto frontier for few-shot view
synthesis.

</details>


### [54] [VividAnimator: An End-to-End Audio and Pose-driven Half-Body Human Animation Framework](https://arxiv.org/abs/2510.10269)
*Donglin Huang,Yongyuan Li,Tianhang Liu,Junming Huang,Xiaoda Yang,Chi Wang,Weiwei Xu*

Main category: cs.CV

TL;DR: VividAnimator通过手部清晰度码本、双流音频感知模块和姿态校准技巧，解决了现有方法在音频和姿态驱动人体动画中头部运动僵硬和手部模糊的问题，生成了高质量的半身人体动画。


<details>
  <summary>Details</summary>
Motivation: 现有音频和姿态驱动的人体动画方法在头部运动方面表现僵硬，手部细节模糊，主要原因是音频与头部运动之间的相关性较弱以及手部结构复杂。

Method: VividAnimator是一个端到端的框架，用于生成高质量的半身人体动画，由音频和稀疏手部姿态条件驱动。它引入了三项关键创新：1. 预训练手部清晰度码本（HCC），编码丰富、高保真的手部纹理先验，以减轻手部退化；2. 设计双流音频感知模块（DSAA），分别对唇部同步和自然头部姿态动态进行建模，同时实现交互；3. 引入姿态校准技巧（PCT），通过放宽刚性约束来优化和对齐姿态条件，确保平滑自然的肢体动作过渡。

Result: VividAnimator在定性和定量评估中都取得了最先进的性能，生成了在手部细节、姿态真实感和身份一致性方面表现优越的视频。

Conclusion: VividAnimator通过其创新的模块解决了现有音频和姿态驱动人体动画方法的局限性，显著提高了生成动画的质量和真实感。

Abstract: Existing for audio- and pose-driven human animation methods often struggle
with stiff head movements and blurry hands, primarily due to the weak
correlation between audio and head movements and the structural complexity of
hands. To address these issues, we propose VividAnimator, an end-to-end
framework for generating high-quality, half-body human animations driven by
audio and sparse hand pose conditions. Our framework introduces three key
innovations. First, to overcome the instability and high cost of online
codebook training, we pre-train a Hand Clarity Codebook (HCC) that encodes
rich, high-fidelity hand texture priors, significantly mitigating hand
degradation. Second, we design a Dual-Stream Audio-Aware Module (DSAA) to model
lip synchronization and natural head pose dynamics separately while enabling
interaction. Third, we introduce a Pose Calibration Trick (PCT) that refines
and aligns pose conditions by relaxing rigid constraints, ensuring smooth and
natural gesture transitions. Extensive experiments demonstrate that Vivid
Animator achieves state-of-the-art performance, producing videos with superior
hand detail, gesture realism, and identity consistency, validated by both
quantitative metrics and qualitative evaluations.

</details>


### [55] [Bridging Perspectives: Foundation Model Guided BEV Maps for 3D Object Detection and Tracking](https://arxiv.org/abs/2510.10287)
*Markus Käppeler,Özgün Çiçek,Daniele Cattaneo,Claudius Gläser,Yakov Miron,Abhinav Valada*

Main category: cs.CV

TL;DR: DualViewDistill是一个结合了透视视图（PV）和鸟瞰图（BEV）特征的混合3D目标检测和跟踪框架，它利用基础模型引导的BEV地图和DINOv2特征蒸馏来提升自动驾驶感知性能，并在多个基准测试中取得了最先进的结果。


<details>
  <summary>Details</summary>
Motivation: 目前的3D目标检测和跟踪方法要么单独依赖透视视图（PV）特征，要么单独依赖鸟瞰图（BEV）特征，这限制了它们利用细粒度目标细节和空间结构化场景表示的能力。

Method: 本文提出了DualViewDistill框架，它结合了PV和BEV相机图像特征。该方法引入了由基础模型引导的BEV地图，并通过新颖的蒸馏过程将描述性的DINOv2特征蒸馏到BEV表示中。通过可变形聚合将PV特征与富含DINOv2语义和几何特征的BEV地图相结合，以增强3D目标检测和跟踪。

Result: DualViewDistill在nuScenes和Argoverse 2基准测试中实现了最先进的性能。

Conclusion: 基础模型BEV地图能够实现更可靠的自动驾驶感知。

Abstract: Camera-based 3D object detection and tracking are essential for perception in
autonomous driving. Current state-of-the-art approaches often rely exclusively
on either perspective-view (PV) or bird's-eye-view (BEV) features, limiting
their ability to leverage both fine-grained object details and spatially
structured scene representations. In this work, we propose DualViewDistill, a
hybrid detection and tracking framework that incorporates both PV and BEV
camera image features to leverage their complementary strengths. Our approach
introduces BEV maps guided by foundation models, leveraging descriptive DINOv2
features that are distilled into BEV representations through a novel
distillation process. By integrating PV features with BEV maps enriched with
semantic and geometric features from DINOv2, our model leverages this hybrid
representation via deformable aggregation to enhance 3D object detection and
tracking. Extensive experiments on the nuScenes and Argoverse 2 benchmarks
demonstrate that DualViewDistill achieves state-of-the-art performance. The
results showcase the potential of foundation model BEV maps to enable more
reliable perception for autonomous driving. We make the code and pre-trained
models available at https://dualviewdistill.cs.uni-freiburg.de .

</details>


### [56] [From Programs to Poses: Factored Real-World Scene Generation via Learned Program Libraries](https://arxiv.org/abs/2510.10292)
*Joy Hsu,Emily Jin,Jiajun Wu,Niloy J. Mitra*

Main category: cs.CV

TL;DR: 该论文提出了FactoredScenes框架，通过分解场景为房间程序和对象姿态的层次结构，并利用大型语言模型生成高层程序，学习对象姿态的变化，从而合成逼真的3D场景。


<details>
  <summary>Details</summary>
Motivation: 真实世界场景数据采集困难且数量有限，生成具有多样对象姿态的真实感场景是一个开放且具有挑战性的任务。

Method: FactoredScenes框架将场景分解为房间程序和对象姿态的层次结构。它学习一个捕获可重用布局模式的函数库，并利用大型语言模型生成高层程序。然后，它学习一个程序条件的模型来分层预测对象姿态，并检索和放置3D对象。

Result: FactoredScenes能够生成逼真的、难以与真实ScanNet场景区分的真实世界房间。

Conclusion: FactoredScenes通过其独特的分解表示和结合大型语言模型与学习到的姿态模型的方法，有效地解决了生成真实感3D场景的挑战，并展现了其在生成高质量场景方面的强大能力。

Abstract: Real-world scenes, such as those in ScanNet, are difficult to capture, with
highly limited data available. Generating realistic scenes with varied object
poses remains an open and challenging task. In this work, we propose
FactoredScenes, a framework that synthesizes realistic 3D scenes by leveraging
the underlying structure of rooms while learning the variation of object poses
from lived-in scenes. We introduce a factored representation that decomposes
scenes into hierarchically organized concepts of room programs and object
poses. To encode structure, FactoredScenes learns a library of functions
capturing reusable layout patterns from which scenes are drawn, then uses large
language models to generate high-level programs, regularized by the learned
library. To represent scene variations, FactoredScenes learns a
program-conditioned model to hierarchically predict object poses, and retrieves
and places 3D objects in a scene. We show that FactoredScenes generates
realistic, real-world rooms that are difficult to distinguish from real ScanNet
scenes.

</details>


### [57] [Ordinal Scale Traffic Congestion Classification with Multi-Modal Vision-Language and Motion Analysis](https://arxiv.org/abs/2510.10342)
*Yu-Hsuan Lin*

Main category: cs.CV

TL;DR: 该论文提出了一个将开放词汇视觉语言推理（CLIP）、目标检测（YOLO-World）和基于MOG2背景减法的运动分析相结合的多模态框架，用于准确的交通拥堵分类，实现了76.7%的准确率，F1分数0.752，二次加权Kappa（QWK）0.684。


<details>
  <summary>Details</summary>
Motivation: 智能交通系统和实时城市交通管理需要准确的交通拥堵分类。

Method: 本文提出了一个多模态框架，结合了开放词汇视觉语言推理（CLIP）、目标检测（YOLO-World）和通过MOG2背景减法进行的运动分析。该系统以1到5的序数等级预测拥堵水平，并结合基于运动的置信度加权和生成带注释的视觉输出，以增强可解释性。

Result: 模型实现了76.7%的准确率，F1分数为0.752，二次加权Kappa（QWK）为0.684，显著优于单峰基线。

Conclusion: 该框架在保持序数结构和利用视觉语言与运动模态方面表现出有效性。未来的改进包括结合车辆尺寸和更精细的密度度量。

Abstract: Accurate traffic congestion classification is essential for intelligent
transportation systems and real-time urban traffic management. This paper
presents a multimodal framework combining open-vocabulary visual-language
reasoning (CLIP), object detection (YOLO-World), and motion analysis via
MOG2-based background subtraction. The system predicts congestion levels on an
ordinal scale from 1 (free flow) to 5 (severe congestion), enabling
semantically aligned and temporally consistent classification. To enhance
interpretability, we incorporate motion-based confidence weighting and generate
annotated visual outputs. Experimental results show the model achieves 76.7
percent accuracy, an F1 score of 0.752, and a Quadratic Weighted Kappa (QWK) of
0.684, significantly outperforming unimodal baselines. These results
demonstrate the framework's effectiveness in preserving ordinal structure and
leveraging visual-language and motion modalities. Future enhancements include
incorporating vehicle sizing and refined density metrics.

</details>


### [58] [Ortho-Fuse: Orthomosaic Generation for Sparse High-Resolution Crop Health Maps Through Intermediate Optical Flow Estimation](https://arxiv.org/abs/2510.10360)
*Rugved Katole,Christopher Stewart*

Main category: cs.CV

TL;DR: 这篇论文介绍了一个名为Ortho-Fuse的框架，它使用光流估计来合成图像，减少了正射影像图生成所需的图像重叠，从而提高了AI驱动的作物健康测绘系统的实用性。


<details>
  <summary>Details</summary>
Motivation: 传统的摄影测量方法需要高图像重叠度才能进行准确的几何重建，这在资源受限的人工智能驱动系统中难以实现，导致重建质量下降，影响了用户对自主监测技术的信心。

Method: 本文提出了Ortho-Fuse框架，它是一个基于光流的方法。该方法通过中间流估计在连续的航空帧之间合成过渡图像，从而人工增加特征对应，改善了几何重建。

Result: 实验验证表明，该方法将最小重叠要求降低了20%。

Conclusion: 本文还分析了精准农业中的采用障碍，以期为人工智能驱动的监测系统更好地整合确定途径。

Abstract: AI-driven crop health mapping systems offer substantial advantages over
conventional monitoring approaches through accelerated data acquisition and
cost reduction. However, widespread farmer adoption remains constrained by
technical limitations in orthomosaic generation from sparse aerial imagery
datasets. Traditional photogrammetric reconstruction requires 70-80\%
inter-image overlap to establish sufficient feature correspondences for
accurate geometric registration. AI-driven systems operating under
resource-constrained conditions cannot consistently achieve these overlap
thresholds, resulting in degraded reconstruction quality that undermines user
confidence in autonomous monitoring technologies. In this paper, we present
Ortho-Fuse, an optical flow-based framework that enables the generation of a
reliable orthomosaic with reduced overlap requirements. Our approach employs
intermediate flow estimation to synthesize transitional imagery between
consecutive aerial frames, artificially augmenting feature correspondences for
improved geometric reconstruction. Experimental validation demonstrates a 20\%
reduction in minimum overlap requirements. We further analyze adoption barriers
in precision agriculture to identify pathways for enhanced integration of
AI-driven monitoring systems.

</details>


### [59] [PointMAC: Meta-Learned Adaptation for Robust Test-Time Point Cloud Completion](https://arxiv.org/abs/2510.10365)
*Linlian Jiang,Rui Ma,Li Gu,Ziqiang Wang,Xinxin Zuo,Yang Wang*

Main category: cs.CV

TL;DR: PointMAC是一个元学习框架，能够在点云补全任务中通过样本特异性提炼实现鲁棒的测试时适应，无需额外监督。


<details>
  <summary>Details</summary>
Motivation: 现有模型在点云补全方面依赖静态推理和归纳偏置，限制了其适应新型结构模式和传感器失真的能力。

Method: PointMAC通过模拟结构和传感器级别不完整性的两个自监督辅助目标来优化补全模型。它采用基于MAML的元辅助学习策略，确保辅助目标驱动的适应与主要补全任务对齐。推理时，通过优化辅助损失动态调整共享编码器，解码器保持固定。此外，引入自适应λ-校准机制来平衡主目标和辅助目标之间的梯度。

Result: 在合成、模拟和真实世界数据集上的大量实验表明，PointMAC通过单独提炼每个样本来生成高质量的补全，达到了最先进的水平。

Conclusion: PointMAC首次将元辅助测试时适应应用于点云补全，有效解决了现有方法的局限性，提高了点云补全的鲁棒性和适应性。

Abstract: Point cloud completion is essential for robust 3D perception in
safety-critical applications such as robotics and augmented reality. However,
existing models perform static inference and rely heavily on inductive biases
learned during training, limiting their ability to adapt to novel structural
patterns and sensor-induced distortions at test time. To address this
limitation, we propose PointMAC, a meta-learned framework for robust test-time
adaptation in point cloud completion. It enables sample-specific refinement
without requiring additional supervision. Our method optimizes the completion
model under two self-supervised auxiliary objectives that simulate structural
and sensor-level incompleteness. A meta-auxiliary learning strategy based on
Model-Agnostic Meta-Learning (MAML) ensures that adaptation driven by auxiliary
objectives is consistently aligned with the primary completion task. During
inference, we adapt the shared encoder on-the-fly by optimizing auxiliary
losses, with the decoder kept fixed. To further stabilize adaptation, we
introduce Adaptive $\lambda$-Calibration, a meta-learned mechanism for
balancing gradients between primary and auxiliary objectives. Extensive
experiments on synthetic, simulated, and real-world datasets demonstrate that
PointMAC achieves state-of-the-art results by refining each sample individually
to produce high-quality completions. To the best of our knowledge, this is the
first work to apply meta-auxiliary test-time adaptation to point cloud
completion.

</details>


### [60] [Self-Supervised Multi-Scale Transformer with Attention-Guided Fusion for Efficient Crack Detection](https://arxiv.org/abs/2510.10378)
*Blessing Agyei Kyem,Joshua Kofi Asamoah,Eugene Denteh,Andrews Danyo,Armstrong Aboah*

Main category: cs.CV

TL;DR: 本文提出了一种名为Crack-Segmenter的自监督框架，旨在解决路面裂缝检测中像素级标注成本高昂的问题。该框架通过整合尺度自适应嵌入器、定向注意力Transformer和注意力引导融合模块，实现了在无需手动标注的情况下，优于当前13种最先进的监督方法的裂缝分割效果，从而为大规模基础设施监测提供了一种可扩展且经济高效的解决方案。


<details>
  <summary>Details</summary>
Motivation: 目前的路面裂缝检测方法严重依赖耗时且昂贵的像素级标注，极大地限制了其在大规模基础设施监测中的可扩展性。因此，本文旨在探索在完全无需手动标注的情况下，实现高效像素级裂缝分割的可行性。

Method: 本文开发了一个名为Crack-Segmenter的完全自监督框架，该框架集成了三个互补模块：
1. 尺度自适应嵌入器（SAE）：用于鲁棒的多尺度特征提取。
2. 定向注意力Transformer（DAT）：用于保持线性裂缝的连续性。
3. 注意力引导融合（AGF）模块：用于自适应地整合特征。

Result: 通过在十个公共数据集上进行评估，Crack-Segmenter在所有主要指标（包括平均交并比（mIoU）、Dice分数、XOR和豪斯多夫距离（HD））上均超越了13种最先进的监督方法。

Conclusion: 本文的研究结果表明，无需标注的裂缝检测不仅是可行的，而且在性能上甚至优于传统的监督方法。这为交通机构和基础设施管理者提供了一种可扩展且经济高效的监测手段，并推动了自监督学习在路面裂缝检测研究中的应用和发展。

Abstract: Pavement crack detection has long depended on costly and time-intensive
pixel-level annotations, which limit its scalability for large-scale
infrastructure monitoring. To overcome this barrier, this paper examines the
feasibility of achieving effective pixel-level crack segmentation entirely
without manual annotations. Building on this objective, a fully self-supervised
framework, Crack-Segmenter, is developed, integrating three complementary
modules: the Scale-Adaptive Embedder (SAE) for robust multi-scale feature
extraction, the Directional Attention Transformer (DAT) for maintaining linear
crack continuity, and the Attention-Guided Fusion (AGF) module for adaptive
feature integration. Through evaluations on ten public datasets,
Crack-Segmenter consistently outperforms 13 state-of-the-art supervised methods
across all major metrics, including mean Intersection over Union (mIoU), Dice
score, XOR, and Hausdorff Distance (HD). These findings demonstrate that
annotation-free crack detection is not only feasible but also superior,
enabling transportation agencies and infrastructure managers to conduct
scalable and cost-effective monitoring. This work advances self-supervised
learning and motivates pavement cracks detection research.

</details>


### [61] [AVoCaDO: An Audiovisual Video Captioner Driven by Temporal Orchestration](https://arxiv.org/abs/2510.10395)
*Xinlong Chen,Yue Ding,Weihong Lin,Jingyun Hua,Linli Yao,Yang Shi,Bozhou Li,Yuanxing Zhang,Qiang Liu,Pengfei Wan,Liang Wang,Tieniu Tan*

Main category: cs.CV

TL;DR: AVoCaDO是一个强大的音视屏字幕生成器，通过两阶段后训练。它包括了AVoCaDO SFT，在一个高质量、时间对齐的音视字幕数据集上进行微调；以及AVoCaDO GRPO，利用定制的奖励函数来增强时间连贯性和对话准确性，同时规范字幕长度并减少崩溃。实验结果表明，AVoCaDO在四个音视屏字幕基准测试中显著优于现有模型，并在纯视觉设置下在VDC和DREAM-1K基准测试中表现出有竞争力的性能。


<details>
  <summary>Details</summary>
Motivation: 现有的音视屏字幕模型在音视事件的时间对齐和语义丰富性方面存在不足，导致视频理解和生成受限。该研究旨在解决这一问题，提出一个强大的音视屏字幕生成器，以提升视频描述的准确性和时间一致性。

Method: 本研究提出了AVoCaDO，它是一个由音频和视觉模态之间的时间编排驱动的音视屏字幕生成器。它采用两阶段的后训练流程：（1）AVoCaDO SFT，利用107K高质量、时间对齐的音视字幕数据集对模型进行微调；（2）AVoCaDO GRPO，它利用定制的奖励函数来进一步增强时间连贯性和对话准确性，同时规范字幕长度并减少崩溃。

Result: AVoCaDO在四个音视屏字幕基准测试中显著优于现有的开源模型。在纯视觉设置下，AVoCaDO在VDC和DREAM-1K基准测试中也取得了具有竞争力的性能。

Conclusion: AVoCaDO通过引入时间编排和两阶段后训练流程，显著提升了音视屏字幕的生成质量，尤其是在时间对齐和语义丰富性方面。这为视频理解和生成领域带来了新的突破。

Abstract: Audiovisual video captioning aims to generate semantically rich descriptions
with temporal alignment between visual and auditory events, thereby benefiting
both video understanding and generation. In this paper, we present AVoCaDO, a
powerful audiovisual video captioner driven by the temporal orchestration
between audio and visual modalities. We propose a two-stage post-training
pipeline: (1) AVoCaDO SFT, which fine-tunes the model on a newly curated
dataset of 107K high-quality, temporally-aligned audiovisual captions; and (2)
AVoCaDO GRPO, which leverages tailored reward functions to further enhance
temporal coherence and dialogue accuracy while regularizing caption length and
reducing collapse. Experimental results demonstrate that AVoCaDO significantly
outperforms existing open-source models across four audiovisual video
captioning benchmarks, and also achieves competitive performance on the VDC and
DREAM-1K benchmark under visual-only settings.

</details>


### [62] [Mesh-Gait: A Unified Framework for Gait Recognition Through Multi-Modal Representation Learning from 2D Silhouettes](https://arxiv.org/abs/2510.10406)
*Zhao-Yang Wang,Jieneng Chen,Jiang Liu,Yuxiang Guo,Rama Chellappa*

Main category: cs.CV

TL;DR: Mesh-Gait是一种新型的端到端多模态步态识别框架，它从2D轮廓直接重建3D表示，解决了传统方法在视角变化、遮挡和噪声方面的不足，并避免了3D方法计算成本高的问题，实现了SOTA性能。


<details>
  <summary>Details</summary>
Motivation: 传统的步态识别方法（基于2D轮廓或骨骼）容易受到视角变化、遮挡和噪声的影响。现有的多模态3D方法虽然鲁棒性更好，但计算成本高，难以实时应用。

Method: Mesh-Gait直接从2D轮廓重建3D热图作为中间表示，有效捕获3D几何信息。在训练过程中，3D热图逐步重建，并通过监督学习中重建的3D关节点、虚拟标记和3D网格与对应真实值之间的损失计算来提高准确性。模型从轮廓和重建的3D热图中提取判别性特征，避免了直接从RGB视频进行3D重建的巨大开销。

Result: Mesh-Gait取得了最先进的识别精度，证明了其在步态识别领域的有效性。

Conclusion: Mesh-Gait通过将2D轮廓与3D热图的结合，提供了一种计算高效且准确的步态识别方法，解决了现有技术的限制，为实时多模态步态识别提供了新的方向。

Abstract: Gait recognition, a fundamental biometric technology, leverages unique
walking patterns for individual identification, typically using 2D
representations such as silhouettes or skeletons. However, these methods often
struggle with viewpoint variations, occlusions, and noise. Multi-modal
approaches that incorporate 3D body shape information offer improved robustness
but are computationally expensive, limiting their feasibility for real-time
applications. To address these challenges, we introduce Mesh-Gait, a novel
end-to-end multi-modal gait recognition framework that directly reconstructs 3D
representations from 2D silhouettes, effectively combining the strengths of
both modalities. Compared to existing methods, directly learning 3D features
from 3D joints or meshes is complex and difficult to fuse with silhouette-based
gait features. To overcome this, Mesh-Gait reconstructs 3D heatmaps as an
intermediate representation, enabling the model to effectively capture 3D
geometric information while maintaining simplicity and computational
efficiency. During training, the intermediate 3D heatmaps are gradually
reconstructed and become increasingly accurate under supervised learning, where
the loss is calculated between the reconstructed 3D joints, virtual markers,
and 3D meshes and their corresponding ground truth, ensuring precise spatial
alignment and consistent 3D structure. Mesh-Gait extracts discriminative
features from both silhouettes and reconstructed 3D heatmaps in a
computationally efficient manner. This design enables the model to capture
spatial and structural gait characteristics while avoiding the heavy overhead
of direct 3D reconstruction from RGB videos, allowing the network to focus on
motion dynamics rather than irrelevant visual details. Extensive experiments
demonstrate that Mesh-Gait achieves state-of-the-art accuracy. The code will be
released upon acceptance of the paper.

</details>


### [63] [Guided Image Feature Matching using Feature Spatial Order](https://arxiv.org/abs/2510.10414)
*Chin-Hung Teng,Ben-Jian Dong*

Main category: cs.CV

TL;DR: 本文提出了一种将特征空间顺序整合到渐进匹配框架中的新方法，以提高图像特征匹配的效率和准确性，并解决了图像旋转影响空间顺序的问题。


<details>
  <summary>Details</summary>
Motivation: 尽管已经提出了许多图像特征检测和匹配技术，但在特征点数量较多的图像中，匹配特征点仍然非常耗时。

Method: 本文将特征空间顺序的概念整合到渐进匹配框架中。利用初始匹配的特征建立特征空间顺序的计算模型，并用它来计算后续特征匹配的可能空间范围，从而过滤掉不必要的特征匹配。同时，该方法还与对极几何相结合，进一步提高匹配效率和准确性。针对图像旋转对特征点空间顺序的影响，提出了一种合适的图像配准方法，通过对极几何的基本矩阵来消除图像旋转的影响。

Result: 通过在标准基准数据集、自生成的模拟图像和真实图像上进行的一系列实验，结果表明该方法比传统方法具有更高的效率和更准确的特征匹配。

Conclusion: 本文提出的方法有效地提高了图像特征匹配的效率和准确性，尤其是在处理具有大量特征点的图像时，为计算机视觉任务提供了新的思路。

Abstract: Image feature matching plays a vital role in many computer vision tasks.
Although many image feature detection and matching techniques have been
proposed over the past few decades, it is still time-consuming to match feature
points in two images, especially for images with a large number of detected
features. Feature spatial order can estimate the probability that a pair of
features is correct. Since it is a completely independent concept from epipolar
geometry, it can be used to complement epipolar geometry in guiding feature
match in a target region so as to improve matching efficiency. In this paper,
we integrate the concept of feature spatial order into a progressive matching
framework. We use some of the initially matched features to build a
computational model of feature spatial order and employs it to calculates the
possible spatial range of subsequent feature matches, thus filtering out
unnecessary feature matches. We also integrate it with epipolar geometry to
further improve matching efficiency and accuracy. Since the spatial order of
feature points is affected by image rotation, we propose a suitable image
alignment method from the fundamental matrix of epipolar geometry to remove the
effect of image rotation. To verify the feasibility of the proposed method, we
conduct a series of experiments, including a standard benchmark dataset,
self-generated simulated images, and real images. The results demonstrate that
our proposed method is significantly more efficient and has more accurate
feature matching than the traditional method.

</details>


### [64] [Taming a Retrieval Framework to Read Images in Humanlike Manner for Augmenting Generation of MLLMs](https://arxiv.org/abs/2510.10426)
*Suyang Xi,Chenxi Yang,Hong Ding,Yiqing Ni,Catherine C. Liu,Yunhao Liu,Chengqi Zhang*

Main category: cs.CV

TL;DR: HuLiRAG框架通过“what-where-reweight”三阶段级联方法，将文本查询锚定到视觉参照物，并通过Mask-guided微调将空间证据注入生成过程，有效提高了多模态大型语言模型在细粒度视觉问答中的接地能力和事实一致性，减少了幻觉。


<details>
  <summary>Details</summary>
Motivation: 多模态大型语言模型（MLLMs）在细粒度视觉问答中常因文本查询未能明确锚定到视觉参照物而出现幻觉，如对象身份、位置和关系错误。现有的检索增强生成（RAG）方法虽然能缓解部分错误，但其在检索和增强层面未能与人类处理方式对齐，仅关注全局图像信息，缺乏局部细节，限制了细粒度交互的推理。

Method: 本文提出了类人检索增强生成（HuLiRAG）框架，该框架将多模态推理分为“what-where-reweight”三阶段级联：首先，通过开放词汇检测将查询锚定到候选参照物（what）；其次，利用SAM衍生的掩码空间解析参照物，恢复细粒度精度（where）；最后，通过局部和全局对齐的权衡来自适应地确定优先级（reweight）。此外，通过Mask-guided微调将空间证据注入生成过程，使接地成为答案形成的一个明确约束。

Result: HuLiRAG框架的广泛实验证明，该类人级联方法提高了接地的保真度和事实一致性，减少了幻觉。

Conclusion: HuLiRAG框架通过其独特的三阶段级联方法和Mask-guided微调，有效解决了MLLMs在细粒度视觉问答中存在的接地不准确和幻觉问题，推动了多模态问答向更可信的推理方向发展。

Abstract: Multimodal large language models (MLLMs) often fail in fine-grained visual
question answering, producing hallucinations about object identities,
positions, and relations because textual queries are not explicitly anchored to
visual referents. Retrieval-augmented generation (RAG) alleviates some errors,
but it fails to align with human-like processing at both the retrieval and
augmentation levels. Specifically, it focuses only on global-level image
information but lacks local detail and limits reasoning about fine-grained
interactions. To overcome this limitation, we present Human-Like
Retrieval-Augmented Generation (HuLiRAG), a framework that stages multimodal
reasoning as a ``what--where--reweight'' cascade. Queries are first anchored to
candidate referents via open-vocabulary detection (what), then spatially
resolved with SAM-derived masks to recover fine-grained precision (where), and
adaptively prioritized through the trade-off between local and global alignment
(reweight). Mask-guided fine-tuning further injects spatial evidence into the
generation process, transforming grounding from a passive bias into an explicit
constraint on answer formulation. Extensive experiments demonstrate that this
human-like cascade improves grounding fidelity and factual consistency while
reducing hallucinations, advancing multimodal question answering toward
trustworthy reasoning.

</details>


### [65] [Learning from Disagreement: A Group Decision Simulation Framework for Robust Medical Image Segmentation](https://arxiv.org/abs/2510.10462)
*Chen Zhong,Yuxuan Yang,Xinyue Zhang,Ruohan Ma,Yong Guo,Gang Li,Jupeng Li*

Main category: cs.CV

TL;DR: 该论文介绍了一种新的医学图像分割方法，通过模拟临床小组的协作决策过程，处理标注者的差异性和图像模糊性问题。这项工作在CBCT和MRI数据集上取得了最先进的结果，并有望应用于更稳健和值得信赖的人工智能医疗系统。


<details>
  <summary>Details</summary>
Motivation: 医学图像分割标注中存在评估者间差异（IRV），这源于标注者专业知识的差异以及医学图像固有的模糊性。传统的简单平均专家标注的方法会丢弃分歧中揭示的宝贵临床不确定性。

Method: 本文提出了一种“群组决策模拟框架”，该框架通过模仿临床小组的协作决策过程来运作。该框架包括一个专家签名生成器（ESG），用于在独特的潜在空间中表示个体标注者的风格；一个模拟咨询模块（SCM），通过从该空间采样智能地生成最终分割。

Result: 该方法在具有挑战性的CBCT和MRI数据集上取得了最先进的结果，Dice分数分别为92.11%和90.72%。

Conclusion: 这项工作通过将专家分歧视为有用的信号而非噪音，为构建更稳健和值得信赖的医疗人工智能系统提供了一条清晰的路径。

Abstract: Medical image segmentation annotation suffers from inter-rater variability
(IRV) due to differences in annotators' expertise and the inherent blurriness
of medical images. Standard approaches that simply average expert labels are
flawed, as they discard the valuable clinical uncertainty revealed in
disagreements. We introduce a fundamentally new approach with our group
decision simulation framework, which works by mimicking the collaborative
decision-making process of a clinical panel. Under this framework, an Expert
Signature Generator (ESG) learns to represent individual annotator styles in a
unique latent space. A Simulated Consultation Module (SCM) then intelligently
generates the final segmentation by sampling from this space. This method
achieved state-of-the-art results on challenging CBCT and MRI datasets (92.11%
and 90.72% Dice scores). By treating expert disagreement as a useful signal
instead of noise, our work provides a clear path toward more robust and
trustworthy AI systems for healthcare.

</details>


### [66] [Post-TIPS Prediction via Multimodal Interaction: A Multi-Center Dataset and Framework for Survival, Complication, and Portal Pressure Assessment](https://arxiv.org/abs/2510.10464)
*Junhao Dong,Dejia Liu,Ruiqi Ding,Zongxing Chen,Yingjie Huang,Zhu Meng,Jianbo Zhao,Zhicheng Zhao,Fei Su*

Main category: cs.CV

TL;DR: 该研究旨在解决经颈静脉肝内门体分流术（TIPS）预后建模中的挑战，提出了首个公共多中心TIPS预后数据集MultiTIPS，并基于此数据集开发了一个新颖的多模态预后框架，通过双选择分割、多模态交互和多任务预测等模块，实现了对TIPS患者预后的准确、鲁棒和全面的评估。


<details>
  <summary>Details</summary>
Motivation: 目前TIPS手术后的生存率和肝性脑病发生率差异大，需要准确的术前预后模型。现有的研究主要基于术前CT图像或临床特征构建机器学习模型，但面临ROI标注耗时、单模态方法可靠性差、单终点评估不完整以及缺乏公开数据集等挑战。

Method: 本研究提出了MultiTIPS数据集和基于此数据集的新型多模态预后框架。该框架包含三个核心模块：1. 双选择分割：结合半监督和基础模型的方法，在有限标注下实现稳健的ROI分割。2. 多模态交互：引入多粒度影像组学注意力（MGRA）、渐进正交解耦（POD）和临床指导预后增强（CGPE）技术，实现跨模态特征交互和互补表示整合。3. 多任务预测：采用分阶段训练策略，对生存率、门静脉压力梯度（PPG）和OHE进行稳定优化预测。

Result: 在MultiTIPS数据集上进行的广泛实验表明，所提出的方法优于现有最先进的方法，并具有强大的跨领域泛化能力和可解释性。

Conclusion: 本研究提出的MultiTIPS数据集和新型多模态预后框架为TIPS患者的术前预后评估提供了有前景的临床应用潜力。

Abstract: Transjugular intrahepatic portosystemic shunt (TIPS) is an established
procedure for portal hypertension, but provides variable survival outcomes and
frequent overt hepatic encephalopathy (OHE), indicating the necessity of
accurate preoperative prognostic modeling. Current studies typically build
machine learning models from preoperative CT images or clinical
characteristics, but face three key challenges: (1) labor-intensive
region-of-interest (ROI) annotation, (2) poor reliability and generalizability
of unimodal methods, and (3) incomplete assessment from single-endpoint
prediction. Moreover, the lack of publicly accessible datasets constrains
research in this field. Therefore, we present MultiTIPS, the first public
multi-center dataset for TIPS prognosis, and propose a novel multimodal
prognostic framework based on it. The framework comprises three core modules:
(1) dual-option segmentation, which integrates semi-supervised and foundation
model-based pipelines to achieve robust ROI segmentation with limited
annotations and facilitate subsequent feature extraction; (2) multimodal
interaction, where three techniques, multi-grained radiomics attention (MGRA),
progressive orthogonal disentanglement (POD), and clinically guided prognostic
enhancement (CGPE), are introduced to enable cross-modal feature interaction
and complementary representation integration, thus improving model accuracy and
robustness; and (3) multi-task prediction, where a staged training strategy is
used to perform stable optimization of survival, portal pressure gradient
(PPG), and OHE prediction for comprehensive prognostic assessment. Extensive
experiments on MultiTIPS demonstrate the superiority of the proposed method
over state-of-the-art approaches, along with strong cross-domain generalization
and interpretability, indicating its promise for clinical application. The
dataset and code are available.

</details>


### [67] [When Images Speak Louder: Mitigating Language Bias-induced Hallucinations in VLMs through Cross-Modal Guidance](https://arxiv.org/abs/2510.10466)
*Jinjin Cao,Zhiyang Chen,Zijun Wang,Liyuan Ma,Weijian Luo,Guojun Qi*

Main category: cs.CV

TL;DR: 该论文提出了一种名为CMG的无训练解码方法，通过利用原始模型与视觉-语言注意力下降的模型之间的输出分布差异，解决了视觉-语言模型（VLMs）中存在的幻觉问题，并显著减少了语言偏差。


<details>
  <summary>Details</summary>
Motivation: 现有的视觉-语言模型（VLMs）在多模态理解方面表现出强大的能力，但普遍面临幻觉问题，即生成流畅但与图像无关的响应。

Method: 本文分析了语言偏差对幻觉的影响，并引入了跨模态引导（CMG）方法。CMG是一种无训练的解码方法，通过自适应地遮蔽选定Transformer层中最具影响力的图像tokens的注意力权重来削弱视觉-语言感知，从而利用原始模型与视觉-语言注意力下降的模型之间的输出分布差异来解决幻觉问题。

Result: 实验结果表明，CMG在没有额外条件或训练成本的情况下，显著优于现有方法。它能有效改善不同VLM在幻觉特定基准上的性能，并具有良好的泛化能力。

Conclusion: CMG通过在解码过程中强调视觉上下文的感知，显著减少了语言偏差，有效解决了VLMs中的幻觉问题，且无需额外的训练成本，展现了其优越性和普适性。

Abstract: Vision-Language Models (VLMs) have shown solid ability for multimodal
understanding of both visual and language contexts. However, existing VLMs
often face severe challenges of hallucinations, meaning that VLMs tend to
generate responses that are only fluent in the language but irrelevant to
images in previous contexts. To address this issue, we analyze how language
bias contributes to hallucinations and then introduce Cross-Modal
Guidance(CMG), a training-free decoding method that addresses the
hallucinations by leveraging the difference between the output distributions of
the original model and the one with degraded visual-language attention. In
practice, we adaptively mask the attention weight of the most influential image
tokens in selected transformer layers to corrupt the visual-language perception
as a concrete type of degradation. Such a degradation-induced decoding
emphasizes the perception of visual contexts and therefore significantly
reduces language bias without harming the ability of VLMs. In experiment
sections, we conduct comprehensive studies. All results demonstrate the
superior advantages of CMG with neither additional conditions nor training
costs. We also quantitatively show CMG can improve different VLM's performance
on hallucination-specific benchmarks and generalize effectively.

</details>


### [68] [Towards Self-Refinement of Vision-Language Models with Triangular Consistency](https://arxiv.org/abs/2510.10487)
*Yunlong Deng,Guangyi Chen,Tianpei Gu,Lingjing Kong,Yan Li,Zeyu Tang,Kun Zhang*

Main category: cs.CV

TL;DR: 本文提出了一种基于三角一致性原则的自纠正框架，使视觉语言模型（VLMs）能够在没有外部监督的情况下自主生成高质量的监督数据并进行学习。


<details>
  <summary>Details</summary>
Motivation: 视觉语言模型（VLMs）通过监督视觉指令微调（使用图像-问题-答案三元组）将视觉知识与大型语言模型（LLMs）的分析能力相结合。然而，未经监督指令训练的 VLM 的潜力在很大程度上仍未被探索。

Method: 提出了一种基于三角一致性原则（在图像-查询-答案三角内，任何被掩盖的元素都应被一致且准确地重建）的自纠正框架。该框架包括三个步骤：（1）通过添加多任务指令微调（如图像→问题-答案或图像-答案→问题），使 VLM 具备指令生成能力。（2）从无标签图像生成图像-查询-答案三元组，并使用三角一致性原则进行过滤。（3）使用过滤后的合成数据更新模型。

Result: 实验结果表明，在 LLaVA-1.5 的基础上，该模型无需任何外部监督（如人工标注或环境反馈），即可在多个基准测试中自主实现持续改进。

Conclusion: 这项研究对 VLM 的自纠正能力的见解有望启发未来关于 VLM 学习机制的研究。

Abstract: Vision-Language Models (VLMs) integrate visual knowledge with the analytical
capabilities of Large Language Models (LLMs) through supervised visual
instruction tuning, using image-question-answer triplets. However, the
potential of VLMs trained without supervised instruction remains largely
unexplored. This study validates that VLMs possess inherent self-refinement
capabilities, enabling them to generate high-quality supervised data without
external inputs and thereby learn autonomously. Specifically, to stimulate the
self-refinement ability of VLMs, we propose a self-refinement framework based
on a Triangular Consistency principle: within the image-query-answer triangle,
any masked elements should be consistently and accurately reconstructed. The
framework involves three steps: (1) We enable the instruction generation
ability of VLMs by adding multi-task instruction tuning like
image$\rightarrow$question-answer or image-answer$\rightarrow$question. (2) We
generate image-query-answer triplets from unlabeled images and use the
Triangular Consistency principle for filtering. (3) The model is further
updated using the filtered synthetic data. To investigate the underlying
mechanisms behind this self-refinement capability, we conduct a theoretical
analysis from a causal perspective. Using the widely recognized LLaVA-1.5 as
our baseline, our experiments reveal that the model can autonomously achieve
consistent, though deliberately modest, improvements across multiple benchmarks
without any external supervision, such as human annotations or environmental
feedback. We expect that the insights of this study on the self-refinement
ability of VLMs can inspire future research on the learning mechanism of VLMs.
Code is available at https://github.com/dengyl20/SRF-LLaVA-1.5.

</details>


### [69] [Head-wise Adaptive Rotary Positional Encoding for Fine-Grained Image Generation](https://arxiv.org/abs/2510.10489)
*Jiaye Li,Baoyou Chen,Hui Li,Zilong Dong,Jingdong Wang,Siyu Zhu*

Main category: cs.CV

TL;DR: 这篇文章介绍了一种名为HARoPE的新型位置编码方法，用于改进图像生成任务中的Transformer模型。HARoPE通过可学习的线性变换，解决了现有旋转位置嵌入（RoPE）在处理图像细节方面的局限性，并在多个图像生成任务上展示了性能提升。


<details>
  <summary>Details</summary>
Motivation: 现有的Transformer模型，尤其是应用于图像生成的模型，在处理图像的细粒度空间关系、颜色线索和物体计数时，其显式位置编码（如RoPE）存在局限性。这些局限性主要体现在频率分配刚性、轴向独立性以及头部处理的统一性上，导致模型难以捕捉图像生成所需的复杂结构先验。

Method: 本文提出HARoPE，一种头部自适应的RoPE扩展。HARoPE在旋转映射之前插入一个通过奇异值分解（SVD）参数化的可学习线性变换。这种轻量级修改实现了动态频率重新分配、旋转平面的语义对齐以及头部特定的位置感受野，同时严格保留了RoPE的相对位置属性。

Result: HARoPE在类条件ImageNet以及文生图任务（Flux和MMDiT）上的广泛实验表明，与强大的RoPE基线和其他扩展相比，HARoPE持续改进了性能。

Conclusion: HARoPE作为一种有效的替代方案，为增强基于Transformer的图像生成模型中的位置感知提供了一种有原则且适应性强的解决方案。

Abstract: Transformers rely on explicit positional encoding to model structure in data.
While Rotary Position Embedding (RoPE) excels in 1D domains, its application to
image generation reveals significant limitations such as fine-grained spatial
relation modeling, color cues, and object counting. This paper identifies key
limitations of standard multi-dimensional RoPE-rigid frequency allocation,
axis-wise independence, and uniform head treatment-in capturing the complex
structural biases required for fine-grained image generation. We propose
HARoPE, a head-wise adaptive extension that inserts a learnable linear
transformation parameterized via singular value decomposition (SVD) before the
rotary mapping. This lightweight modification enables dynamic frequency
reallocation, semantic alignment of rotary planes, and head-specific positional
receptive fields while rigorously preserving RoPE's relative-position property.
Extensive experiments on class-conditional ImageNet and text-to-image
generation (Flux and MMDiT) demonstrate that HARoPE consistently improves
performance over strong RoPE baselines and other extensions. The method serves
as an effective drop-in replacement, offering a principled and adaptable
solution for enhancing positional awareness in transformer-based image
generative models.

</details>


### [70] [Jigsaw3D: Disentangled 3D Style Transfer via Patch Shuffling and Masking](https://arxiv.org/abs/2510.10497)
*Yuteng Ye,Zheng Zhang,Qinchuan Zhang,Di Wang,Youjia Zhang,Wenxiao Zhang,Wei Yang,Yuan Liu*

Main category: cs.CV

TL;DR: Jigsaw3D是一种多视角扩散模型，它通过Jigsaw操作实现3D风格迁移，将风格与语义内容解耦，从而实现快速、视角一致的风格化，并生成无缝纹理。Jigsaw3D在风格保真度和多视角一致性方面表现优异，且延迟显著降低。


<details>
  <summary>Details</summary>
Motivation: 现有的3D风格迁移方法通常依赖于直接的参考风格token注入或2D扩散模型的得分蒸馏，这导致了大量的逐场景优化，并且经常将风格与语义内容纠缠在一起，因此需要一种新的方法来解耦风格与内容并实现快速、视角一致的风格化。

Method: Jigsaw3D通过Jigsaw操作（对参考图像块进行空间混洗和随机遮蔽），抑制对象语义并分离出风格统计信息（调色板、笔触、纹理）。然后，通过参考到视角的交叉注意力将这些风格线索整合到多视角扩散模型中，生成以输入网格为条件、视角一致的风格化渲染。最后，将渲染图烘焙到表面上以生成无缝纹理。

Result: Jigsaw3D在标准的3D风格化基准测试中，实现了高风格保真度和多视角一致性，且延迟显著降低。它还可以推广到掩蔽部分参考风格化、多对象场景风格化和可平铺纹理生成。

Conclusion: Jigsaw3D通过解耦风格与内容，提供了一种高效且高质量的3D风格迁移解决方案，具有广泛的应用潜力。

Abstract: Controllable 3D style transfer seeks to restyle a 3D asset so that its
textures match a reference image while preserving the integrity and multi-view
consistency. The prevalent methods either rely on direct reference style token
injection or score-distillation from 2D diffusion models, which incurs heavy
per-scene optimization and often entangles style with semantic content. We
introduce Jigsaw3D, a multi-view diffusion based pipeline that decouples style
from content and enables fast, view-consistent stylization. Our key idea is to
leverage the jigsaw operation - spatial shuffling and random masking of
reference patches - to suppress object semantics and isolate stylistic
statistics (color palettes, strokes, textures). We integrate these style cues
into a multi-view diffusion model via reference-to-view cross-attention,
producing view-consistent stylized renderings conditioned on the input mesh.
The renders are then style-baked onto the surface to yield seamless textures.
Across standard 3D stylization benchmarks, Jigsaw3D achieves high style
fidelity and multi-view consistency with substantially lower latency, and
generalizes to masked partial reference stylization, multi-object scene
styling, and tileable texture generation. Project page is available at:
https://babahui.github.io/jigsaw3D.github.io/

</details>


### [71] [VR-Thinker: Boosting Video Reward Models through Thinking-with-Image Reasoning](https://arxiv.org/abs/2510.10518)
*Qunzhong Wang,Jie Liu,Jiajun Liang,Yilei Jiang,Yuanxing Zhang,Jinyuan Chen,Yaozhi Zheng,Xintao Wang,Pengfei Wan,Xiangyu Yue,Jiaheng Liu*

Main category: cs.CV

TL;DR: VR-Thinker通过结合视觉推理操作和可配置的视觉记忆窗口，克服了现有视觉生成模型多模态奖励模型中视觉输入占用大量上下文预算和信息拥挤导致幻觉和遗忘的问题，从而在视频偏好基准测试中实现了最先进的性能。


<details>
  <summary>Details</summary>
Motivation: 现有的多模态奖励模型在处理视觉生成模型时面临局限：视觉输入消耗大量上下文预算导致细节丢失，以及所有视觉信息都集中在初始提示中，加剧了链式思考推理过程中的幻觉和遗忘。

Method: 本文提出了VideoReward Thinker (VR-Thinker)框架，通过强化微调流程激活视觉推理。该流程包括：(i) 使用精选的视觉链式思考数据进行冷启动，以提炼基本推理技能和操作格式；(ii) 对判断均正确的样本进行拒绝采样微调，进一步增强推理能力；(iii) 应用组相对策略优化 (GRPO) 来强化推理。该框架使奖励模型能够主动获取和更新视觉证据。

Result: VR-Thinker在视频偏好基准测试中达到了最先进的准确性，尤其是在处理较长视频时。例如，一个7B的VR-Thinker在VideoGen Reward上达到80.5%，在GenAI-Bench上达到82.3%，在MJ-Bench-Video上达到75.6%。

Conclusion: VR-Thinker通过“图片思考”的多模态奖励建模方法，有效解决了视觉生成模型中现有奖励模型的局限性，显著提高了推理的忠实度和可靠性，并在视频偏好基准测试中取得了优异表现。

Abstract: Recent advancements in multimodal reward models (RMs) have substantially
improved post-training for visual generative models. However, current RMs face
inherent limitations: (1) visual inputs consume large context budgets, forcing
fewer frames and causing loss of fine-grained details; and (2) all visual
information is packed into the initial prompt, exacerbating hallucination and
forgetting during chain-of-thought reasoning. To overcome these issues, we
introduce VideoReward Thinker (VR-Thinker), a thinking-with-image framework
that equips the RM with visual reasoning operations (e.g., select frame) and a
configurable visual memory window. This allows the RM to actively acquire and
update visual evidence within context limits, improving reasoning fidelity and
reliability. We activate visual reasoning via a reinforcement fine-tuning
pipeline: (i) Cold Start with curated visual chain-of-thought data to distill
basic reasoning skills and operation formatting; (ii) select samples whose
per-dimension and overall judgments are all correct, then conduct Rejection
sampling Fine-Tuning on these high-quality traces to further enhance reasoning;
and (iii) apply Group Relative Policy Optimization (GRPO) to strengthen
reasoning. Our approach delivers state-of-the-art accuracy among open-source
models on video preference benchmarks, especially for longer videos: a 7B
VR-Thinker achieves 80.5% on VideoGen Reward, 82.3% on GenAI-Bench, and 75.6%
on MJ-Bench-Video. These results validate the effectiveness and promise of
thinking-with-image multimodal reward modeling.

</details>


### [72] [Unified Open-World Segmentation with Multi-Modal Prompts](https://arxiv.org/abs/2510.10524)
*Yang Liu,Yufei Yin,Chenchen Jing,Muzhi Zhu,Hao Chen,Yuling Xi,Bo Feng,Hao Wang,Shiyu Li,Chunhua Shen*

Main category: cs.CV

TL;DR: COSINE是一个统一的开放世界分割模型。它通过利用基础模型提取表示，并使用SegDecoder对齐这些表示，从而在开放词汇分割和上下文分割任务中展现出显著的性能提升。


<details>
  <summary>Details</summary>
Motivation: 现有的开放词汇分割和上下文分割方法在架构、学习目标和表示学习策略上存在差异。COSINE旨在克服这些差异，提供一个统一的解决方案。

Method: COSINE利用基础模型提取输入图像和多模态提示（如文本和图像）的表示。然后，通过一个SegDecoder来对齐这些表示，并建模它们之间的交互，从而获得由输入提示指定在不同粒度下的掩码。

Result: COSINE在开放词汇分割和上下文分割任务中都取得了显著的性能提升。

Conclusion: COSINE通过统一开放词汇分割和上下文分割，并利用多模态提示的协同作用，显著提高了模型的泛化能力。

Abstract: In this work, we present COSINE, a unified open-world segmentation model that
consolidates open-vocabulary segmentation and in-context segmentation with
multi-modal prompts (e.g., text and image). COSINE exploits foundation models
to extract representations for an input image and corresponding multi-modal
prompts, and a SegDecoder to align these representations, model their
interaction, and obtain masks specified by input prompts across different
granularities. In this way, COSINE overcomes architectural discrepancies,
divergent learning objectives, and distinct representation learning strategies
of previous pipelines for open-vocabulary segmentation and in-context
segmentation. Comprehensive experiments demonstrate that COSINE has significant
performance improvements in both open-vocabulary and in-context segmentation
tasks. Our exploratory analyses highlight that the synergistic collaboration
between using visual and textual prompts leads to significantly improved
generalization over single-modality approaches.

</details>


### [73] [MCE: Towards a General Framework for Handling Missing Modalities under Imbalanced Missing Rates](https://arxiv.org/abs/2510.10534)
*Binyu Zhao,Wei Zhang,Zhaonian Zou*

Main category: cs.CV

TL;DR: 这篇论文提出了一种新的方法：MCE，用于解决多模态学习中模态缺失的问题，尤其是在缺失率不平衡的情况下。


<details>
  <summary>Details</summary>
Motivation: 处理缺失模态，特别是在缺失率不平衡的情况下，仍然是一个重大挑战。这种不平衡会导致恶性循环：缺失率较高的模态接收到的更新较少，导致学习进展不一致和表征退化，进一步降低了它们的贡献。现有方法通常侧重于全局数据集级别的平衡，常常忽视模态效用的关键样本级别变化和特征质量下降的根本问题。

Method: 本文提出模态能力增强（MCE）来解决这些限制。MCE包括两个协同组件：i）学习能力增强（LCE），它引入多级因子来动态平衡模态特定的学习进度，以及ii）表征能力增强（RCE），它通过子集预测和跨模态补全任务来改善特征语义和鲁棒性。

Result: 在四个多模态基准上的综合评估表明，MCE在各种缺失配置下始终优于最先进的方法。

Conclusion: MCE通过学习能力增强（LCE）和表征能力增强（RCE）两个协同组件，有效地解决了多模态学习中模态缺失和不平衡的问题，取得了显著的性能提升。

Abstract: Multi-modal learning has made significant advances across diverse pattern
recognition applications. However, handling missing modalities, especially
under imbalanced missing rates, remains a major challenge. This imbalance
triggers a vicious cycle: modalities with higher missing rates receive fewer
updates, leading to inconsistent learning progress and representational
degradation that further diminishes their contribution. Existing methods
typically focus on global dataset-level balancing, often overlooking critical
sample-level variations in modality utility and the underlying issue of
degraded feature quality. We propose Modality Capability Enhancement (MCE) to
tackle these limitations. MCE includes two synergistic components: i) Learning
Capability Enhancement (LCE), which introduces multi-level factors to
dynamically balance modality-specific learning progress, and ii) Representation
Capability Enhancement (RCE), which improves feature semantics and robustness
through subset prediction and cross-modal completion tasks. Comprehensive
evaluations on four multi-modal benchmarks show that MCE consistently
outperforms state-of-the-art methods under various missing configurations. The
journal preprint version is now available at
https://doi.org/10.1016/j.patcog.2025.112591. Our code is available at
https://github.com/byzhaoAI/MCE.

</details>


### [74] [GLOFNet -- A Multimodal Dataset for GLOF Monitoring and Prediction](https://arxiv.org/abs/2510.10546)
*Zuha Fatima,Muhammad Anser Sohaib,Muhammad Talha,Sidra Sultana,Ayesha Kanwal,Nazia Perwaiz*

Main category: cs.CV

TL;DR: 冰川湖溃决洪水（GLOFs）是一种罕见但具有破坏性的高山地区灾害。GLOFNet是一个多模态数据集，旨在通过整合卫星图像、冰川运动速度和地表温度记录，解决现有研究中数据碎片化和单峰态问题，以预测和监测GLOFs。


<details>
  <summary>Details</summary>
Motivation: 开发一个统一的、多模态的数据集，以克服现有GLOF研究中数据碎片化和单峰态的限制，从而改进GLOF的预测和监测。

Method: GLOFNet数据集整合了Sentinel-2多光谱图像（用于空间监测）、NASA ITS_LIVE冰川运动速度产品和MODIS地表温度记录。数据预处理包括云掩膜、质量过滤、归一化、时间插值、数据增强和周期编码，并进行了多模态间的协调。

Result: 探究性分析揭示了冰川速度的季节性周期、每十年约0.8 K的长期升温，以及冰冻圈条件的空间异质性。GLOFNet数据集已公开，支持未来的冰川灾害预测研究。

Conclusion: GLOFNet数据集通过整合多源数据并解决类别不平衡、云污染和分辨率低等挑战，为基准测试多模态深度学习方法预测罕见灾害奠定了结构化基础。

Abstract: Glacial Lake Outburst Floods (GLOFs) are rare but destructive hazards in high
mountain regions, yet predictive research is hindered by fragmented and
unimodal data. Most prior efforts emphasize post-event mapping, whereas
forecasting requires harmonized datasets that combine visual indicators with
physical precursors. We present GLOFNet, a multimodal dataset for GLOF
monitoring and prediction, focused on the Shisper Glacier in the Karakoram. It
integrates three complementary sources: Sentinel-2 multispectral imagery for
spatial monitoring, NASA ITS_LIVE velocity products for glacier kinematics, and
MODIS Land Surface Temperature records spanning over two decades. Preprocessing
included cloud masking, quality filtering, normalization, temporal
interpolation, augmentation, and cyclical encoding, followed by harmonization
across modalities. Exploratory analysis reveals seasonal glacier velocity
cycles, long-term warming of ~0.8 K per decade, and spatial heterogeneity in
cryospheric conditions. The resulting dataset, GLOFNet, is publicly available
to support future research in glacial hazard prediction. By addressing
challenges such as class imbalance, cloud contamination, and coarse resolution,
GLOFNet provides a structured foundation for benchmarking multimodal deep
learning approaches to rare hazard prediction.

</details>


### [75] [DEMO: Disentangled Motion Latent Flow Matching for Fine-Grained Controllable Talking Portrait Synthesis](https://arxiv.org/abs/2510.10650)
*Peiyin Chen,Zhuowei Yang,Hui Feng,Sheng Jiang,Rui Yan*

Main category: cs.CV

TL;DR: DEMO是一种基于流匹配的生成框架，用于音频驱动的说话人肖像视频合成。它通过运动自编码器实现唇部运动、头部姿态和眼睛凝视的解耦控制，并利用基于最优传输的流匹配生成时间上平滑的运动轨迹。


<details>
  <summary>Details</summary>
Motivation: 尽管基于扩散的生成模型在音频驱动的说话人头部生成方面取得了快速进展，但生成具有细粒度运动控制的时间连贯视频仍然具有挑战性。

Method: DEMO提出了一种运动自编码器，该编码器构建了一个结构化的潜在空间，其中运动因素是独立表示并近似正交化的。在这个解耦的运动空间上，DEMO应用基于最优传输的流匹配和Transformer预测器，以生成以音频为条件的、时间上平滑的运动轨迹。

Result: 在多个基准测试中，DEMO在视频真实感、唇音同步和运动保真度方面优于现有方法。

Conclusion: 将细粒度运动解耦与基于流的生成模型相结合，为可控的说话人头部视频合成提供了一个强大的新范例。

Abstract: Audio-driven talking-head generation has advanced rapidly with
diffusion-based generative models, yet producing temporally coherent videos
with fine-grained motion control remains challenging. We propose DEMO, a
flow-matching generative framework for audio-driven talking-portrait video
synthesis that delivers disentangled, high-fidelity control of lip motion, head
pose, and eye gaze. The core contribution is a motion auto-encoder that builds
a structured latent space in which motion factors are independently represented
and approximately orthogonalized. On this disentangled motion space, we apply
optimal-transport-based flow matching with a transformer predictor to generate
temporally smooth motion trajectories conditioned on audio. Extensive
experiments across multiple benchmarks show that DEMO outperforms prior methods
in video realism, lip-audio synchronization, and motion fidelity. These results
demonstrate that combining fine-grained motion disentanglement with flow-based
generative modeling provides a powerful new paradigm for controllable
talking-head video synthesis.

</details>


### [76] [Deep semi-supervised approach based on consistency regularization and similarity learning for weeds classification](https://arxiv.org/abs/2510.10573)
*Farouq Benchallal,Adel Hafiane,Nicolas Ragot,Raphael Canals*

Main category: cs.CV

TL;DR: 这篇论文提出了一种深度半监督学习方法，结合了一致性正则化和相似性学习，用于杂草分类。


<details>
  <summary>Details</summary>
Motivation: 杂草分类对于精准农业中的自动化靶向系统至关重要，但由于杂草与作物相似、类型多样以及田间条件变化，杂草种类识别具有挑战性。此外，深度学习方法需要大量完全标注的数据集，而数据标注是一个耗时耗力的过程，这限制了其在农业中的应用。

Method: 本文提出了一种深度半监督方法，将一致性正则化与相似性学习相结合。该方法通过开发的深度自编码器架构实现。

Result: 在DeepWeeds数据集上进行的实验以及在噪声条件下的推理表明，与最先进的完全监督深度学习模型相比，该方法是有效和鲁棒的。

Conclusion: 实验结果证明了该方法在杂草分类方面的有效性和鲁棒性，特别是在标注数据稀缺的情况下，为精准农业中的杂草识别提供了一种有前景的解决方案。

Abstract: Weed species classification represents an important step for the development
of automated targeting systems that allow the adoption of precision agriculture
practices. To reduce costs and yield losses caused by their presence. The
identification of weeds is a challenging problem due to their shared
similarities with crop plants and the variability related to the differences in
terms of their types. Along with the variations in relation to changes in field
conditions. Moreover, to fully benefit from deep learning-based methods, large
fully annotated datasets are needed. This requires time intensive and laborious
process for data labeling, which represents a limitation in agricultural
applications. Hence, for the aim of improving the utilization of the unlabeled
data, regarding conditions of scarcity in terms of the labeled data available
during the learning phase and provide robust and high classification
performance. We propose a deep semi-supervised approach, that combines
consistency regularization with similarity learning. Through our developed deep
auto-encoder architecture, experiments realized on the DeepWeeds dataset and
inference in noisy conditions demonstrated the effectiveness and robustness of
our method in comparison to state-of-the-art fully supervised deep learning
models. Furthermore, we carried out ablation studies for an extended analysis
of our proposed joint learning strategy.

</details>


### [77] [Scalable Face Security Vision Foundation Model for Deepfake, Diffusion, and Spoofing Detection](https://arxiv.org/abs/2510.10663)
*Gaojian Wang,Feng Lin,Tong Wu,Zhisheng Yan,Kui Ren*

Main category: cs.CV

TL;DR: FS-VFM是一个可扩展的自监督预训练框架，通过结合掩码图像建模（MIM）和实例判别（ID）学习真实人脸图像的基本表示，并通过FS-Adapter实现高效迁移。


<details>
  <summary>Details</summary>
Motivation: 如何利用大量未标注的真实人脸数据，学习鲁棒且可迁移的人脸表示，以提升各种人脸安全任务的泛化能力。

Method: 提出FS-VFM框架，包含3C学习目标，结合MIM和ID，编码真实人脸的局部模式和全局语义。设计CRFR-P掩码策略，强调区域内一致性与区域间连贯性。引入自蒸馏机制，耦合MIM和ID，建立局部到全局的对应关系。预训练后，使用ViT作为下游人脸安全任务的通用视觉基础模型。提出FS-Adapter，一种轻量级即插即用适配器，结合新颖的real-anchor对比目标，高效迁移预训练模型。

Result: 在11个公共基准测试中，FS-VFM的泛化能力始终优于各种VFM，并且在跨数据集深度伪造检测、跨域人脸防欺骗和未知扩散人脸取证等任务上超越了SOTA任务特定方法。FS-Adapter在效率和性能之间取得了出色的平衡。

Conclusion: FS-VFM通过自监督学习，有效解决了人脸安全任务中鲁棒且可迁移人脸表示的学习问题，并在多个基准测试中展现出卓越的性能和泛化能力。

Abstract: With abundant, unlabeled real faces, how can we learn robust and transferable
facial representations to boost generalization across various face security
tasks? We make the first attempt and propose FS-VFM, a scalable self-supervised
pre-training framework, to learn fundamental representations of real face
images. We introduce three learning objectives, namely 3C, that synergize
masked image modeling (MIM) and instance discrimination (ID), empowering FS-VFM
to encode both local patterns and global semantics of real faces. Specifically,
we formulate various facial masking strategies for MIM and devise a simple yet
effective CRFR-P masking, which explicitly prompts the model to pursue
meaningful intra-region Consistency and challenging inter-region Coherency. We
present a reliable self-distillation mechanism that seamlessly couples MIM with
ID to establish underlying local-to-global Correspondence. After pre-training,
vanilla vision transformers (ViTs) serve as universal Vision Foundation Models
for downstream Face Security tasks: cross-dataset deepfake detection,
cross-domain face anti-spoofing, and unseen diffusion facial forensics. To
efficiently transfer the pre-trained FS-VFM, we further propose FS-Adapter, a
lightweight plug-and-play bottleneck atop the frozen backbone with a novel
real-anchor contrastive objective. Extensive experiments on 11 public
benchmarks demonstrate that our FS-VFM consistently generalizes better than
diverse VFMs, spanning natural and facial domains, fully, weakly, and
self-supervised paradigms, small, base, and large ViT scales, and even
outperforms SOTA task-specific methods, while FS-Adapter offers an excellent
efficiency-performance trade-off. The code and models are available on
https://fsfm-3c.github.io/fsvfm.html.

</details>


### [78] [UniFlow: A Unified Pixel Flow Tokenizer for Visual Understanding and Generation](https://arxiv.org/abs/2510.10575)
*Zhengrong Yue,Haiyu Zhang,Xiangyu Zeng,Boyu Chen,Chenting Wang,Shaobin Zhuang,Lu Dong,KunPeng Du,Yi Wang,Limin Wang,Yali Wang*

Main category: cs.CV

TL;DR: UniFlow: 一种通用统一tokenizer，通过结合视觉编码器和简洁的重建解码器，在视觉理解和生成任务中取得了双赢的效果。


<details>
  <summary>Details</summary>
Motivation: 现有的tokenizer在理解和生成任务之间存在性能瓶颈，这是因为高级语义抽象和低级像素重建之间存在固有的冲突。

Method: UniFlow通过将层自适应知识蒸馏应用于预训练视觉编码器，使其能够同时继承强大的语义特征用于视觉理解，并灵活适应模型细粒度细节用于视觉生成。同时，提出的轻量级逐块像素流解码器，通过建模从噪声状态到逐块像素域的条件流，高效实现高保真像素重建。通过利用语义特征作为解码器的视觉条件，有效缓解了理解和生成之间的训练冲突。此外，逐块学习策略简化了数据分布，从而提高了训练效率。

Result: 在13个基准测试和7个视觉理解和生成任务中进行了广泛实验后发现，UniFlow实现了双赢的结果。 例如，在平均理解基准上，7B UniFlow-XL超越了14B TokenFlow-XL 7.75％，并在视觉重建和生成方面取得了有竞争力的结果，分别在rFID上超过UniTok 0.15，在gFID上超过UniTok 0.09（无指导）。

Conclusion: UniFlow通过解决高级语义抽象和低级像素重建之间的冲突，实现了在视觉理解和生成任务中的卓越性能，为通用统一tokenizer的发展提供了新的范式。

Abstract: Tokenizer is a crucial component for both visual understanding and
generation. To advance toward the ultimate goal of universal modeling, recent
research has focused on developing a unified tokenizer. However, existing
tokenizers face a significant performance trade-off between understanding and
generation, stemming from the inherent conflict between high-level semantic
abstraction and low-level pixel reconstruction. To tackle this challenge, we
propose a generic and unified tokenizer, namely UniFlow, by flexibly adapting
any visual encoder with a concise reconstruction decoder. Specifically, we
introduce layer-wise adaptive self-distillation applied to the well-pretrained
visual encoders, which enables UniFlow to simultaneously inherit the strong
semantic features for visual understanding and flexibly adapt to model
fine-grained details for visual generation. Moreover, we propose a lightweight
patch-wise pixel flow decoder, which efficiently achieves high-fidelity pixel
reconstruction by modeling a conditional flow from the noisy state back to the
patch-wise pixel domain. By leveraging the semantic features as visual
conditions for the decoder, we effectively alleviate the training conflicts
between understanding and generation. Furthermore, the patch-wise learning
strategy simplifies the data distribution, thereby improving training
efficiency. Extensive experiments across 13 challenging benchmarks spanning 7
widely studied visual understanding and generation tasks demonstrate that
UniFlow achieves a win-win outcome. For instance, our 7B UniFlow-XL not only
surpasses the 14B TokenFlow-XL by 7.75% on average understanding benchmarks,
but also achieves competitive results in both visual reconstruction and
generation, surpassing UniTok by 0.15 in rFID and 0.09 in gFID (without
guidance), respectively.

</details>


### [79] [Image-to-Video Transfer Learning based on Image-Language Foundation Models: A Comprehensive Survey](https://arxiv.org/abs/2510.10671)
*Jinxuan Li,Chaolei Tan,Haoxuan Chen,Jianxin Ma,Jian-Fang Hu,Wei-Shi Zheng,Jianhuang Lai*

Main category: cs.CV

TL;DR: 这篇综述探讨了图像到视频的迁移学习，它将预训练的图像-语言基础模型（ILFM）应用于视频理解任务，以解决视频-语言基础模型从零开始训练所需的大量数据和计算资源问题。文中系统地分类了现有的迁移学习策略，并详细阐述了它们在不同视频-文本学习任务中的应用，从细粒度到粗粒度。


<details>
  <summary>Details</summary>
Motivation: 视频-文本研究进展迅速，促使将基于图像的模型扩展到视频领域，即图像到视频的迁移学习。这种方法旨在解决从零开始训练视频-语言基础模型所需的大量数据和计算开销问题，从而实现可迁移的多模态表示。

Method: 这篇综述首先总结了广泛使用的图像-语言基础模型（ILFM）及其功能。然后，系统地将现有的图像到视频迁移学习策略分为两类：冻结特征和修改特征，具体取决于是否保留或修改了ILFM的原始表示。最后，详细阐述了这些策略在不同视频-文本学习任务中的应用，并进行了实验分析以研究不同迁移学习范式在下游视频理解任务中的有效性。

Result: 通过图像到视频的迁移学习，可以将图像-语言基础模型成功应用于视频领域，有效缓解了训练视频-语言基础模型所需的数据和计算资源问题。现有策略可以分为冻结特征和修改特征两种，并且在从细粒度（如时空视频定位）到粗粒度（如视频问答）的各种视频-文本学习任务中都证明了其有效性。

Conclusion: 图像到视频的迁移学习是一个新兴且有前景的领域，能够有效利用现有图像-语言基础模型的知识来推动视频-文本学习。本综述提供了一个全面的结构化概述，旨在为基于现有ILFM的视频-文本学习的进展建立一个结构化的路线图，并激发该快速发展领域未来的研究方向。

Abstract: Image-Language Foundation Models (ILFM) have demonstrated remarkable success
in image-text understanding/generation tasks, providing transferable multimodal
representations that generalize across diverse downstream image-based tasks.
The advancement of video-text research has spurred growing interest in
extending image-based models to the video domain. This paradigm, known as
image-to-video transfer learning, succeeds in alleviating the substantial data
and computational requirements associated with training video-language
foundation models from scratch for video-text learning. This survey provides
the first comprehensive review of this emerging field, which begins by
summarizing the widely used ILFM and their capabilities. We then systematically
classify existing image-to-video transfer learning strategies into two
categories: frozen features and modified features, depending on whether the
original representations from ILFM are preserved or undergo modifications.
Building upon the task-specific nature of image-to-video transfer, this survey
methodically elaborates these strategies and details their applications across
a spectrum of video-text learning tasks, ranging from fine-grained (e.g.,
spatio-temporal video grounding) to coarse-grained (e.g., video question
answering). We further present a detailed experimental analysis to investigate
the efficacy of different image-to-video transfer learning paradigms on a range
of downstream video understanding tasks. Finally, we identify prevailing
challenges and highlight promising directions for future research. By offering
a comprehensive and structured overview, this survey aims to establish a
structured roadmap for advancing video-text learning based on existing ILFM,
and to inspire future research directions in this rapidly evolving domain.

</details>


### [80] [Injecting Frame-Event Complementary Fusion into Diffusion for Optical Flow in Challenging Scenes](https://arxiv.org/abs/2510.10577)
*Haonan Wang,Hanyu Zhou,Haoyue Liu,Luxin Yan*

Main category: cs.CV

TL;DR: 本文提出了一种名为Diff-ABFlow的新型光流估计框架，该框架基于扩散模型，并融合了帧-事件表观-边界信息，以解决传统方法在高速和低光照场景中光流估计的挑战。


<details>
  <summary>Details</summary>
Motivation: 传统光流估计算法在高速和低光照场景中表现不佳，因为这些场景会导致运动模糊、光照不足、纹理弱化和噪声放大，从而影响运动特征匹配所需的图像质量。现有方法通常侧重于解决边界完整性问题，但忽略了外观特征的退化，这限制了判别模型和生成模型的性能。

Method: 本文提出了一种基于扩散模型的光流估计框架Diff-ABFlow。扩散模型通过学习从噪声流到清晰流的映射，有效避免了退化视觉特征的影响。同时，该框架融合了帧相机和事件相机的互补优势，即帧相机提供密集的外观饱和度，而事件相机提供密集的边界完整性。

Result: 目前没有具体的实验结果。

Conclusion: 本文提出了一种新颖的Diff-ABFlow框架，该框架利用扩散模型的优势，并结合帧-事件外观-边界融合，有望在具有挑战性的高速和低光照场景中实现更准确和鲁棒的光流估计。

Abstract: Optical flow estimation has achieved promising results in conventional scenes
but faces challenges in high-speed and low-light scenes, which suffer from
motion blur and insufficient illumination. These conditions lead to weakened
texture and amplified noise and deteriorate the appearance saturation and
boundary completeness of frame cameras, which are necessary for motion feature
matching. In degraded scenes, the frame camera provides dense appearance
saturation but sparse boundary completeness due to its long imaging time and
low dynamic range. In contrast, the event camera offers sparse appearance
saturation, while its short imaging time and high dynamic range gives rise to
dense boundary completeness. Traditionally, existing methods utilize feature
fusion or domain adaptation to introduce event to improve boundary
completeness. However, the appearance features are still deteriorated, which
severely affects the mostly adopted discriminative models that learn the
mapping from visual features to motion fields and generative models that
generate motion fields based on given visual features. So we introduce
diffusion models that learn the mapping from noising flow to clear flow, which
is not affected by the deteriorated visual features. Therefore, we propose a
novel optical flow estimation framework Diff-ABFlow based on diffusion models
with frame-event appearance-boundary fusion.

</details>


### [81] [DISC-GAN: Disentangling Style and Content for Cluster-Specific Synthetic Underwater Image Generation](https://arxiv.org/abs/2510.10782)
*Sneha Varur,Anirudh R Hanchinamani,Tarun S Bagewadi,Uma Mudenagudi,Chaitra D Desai,Sujata C,Padmashree Desai,Sumit Meharwade*

Main category: cs.CV

TL;DR: 这篇论文介绍了一个名为DISC-GAN的新框架，它结合了风格-内容解耦和特定聚类训练策略，用于生成逼真的水下图像，解决了水下图像质量受光传输挑战的问题，通过K-means聚类划分数据集，使用独立的编码器获取风格和内容潜在空间，并通过AdaIN集成和解码生成图像。该框架在每个风格聚类上独立训练，以保留特定领域的特征，取得了领先的性能。


<details>
  <summary>Details</summary>
Motivation: 水下图像的质量受到光传输现象（如颜色衰减和浑浊）的挑战。这些现象表现为不同水体中独特的风格变化，例如色调和 Haze 的变化。虽然生成模型擅长捕捉复杂模式，但它们往往缺乏模拟多样水下环境非均匀条件的能力。

Method: 本研究提出了一个名为Disentangled Style-Content GAN (DISC-GAN) 的新框架，它将风格-内容解耦与特定聚类训练策略相结合。该框架利用 K-means 聚类将数据集划分为特定风格的域。使用独立的编码器获取风格和内容潜在空间，并通过自适应实例归一化（AdaIN）整合这些潜在表示，然后解码生成最终的合成图像。模型在每个风格聚类上独立训练，以保留特定领域的特征。

Result: DISC-GAN框架在水下图像合成方面表现出最先进的性能，结构相似性指数（SSIM）达到0.9012，平均峰值信噪比（PSNR）达到32.5118 dB，Frechet Inception Distance（FID）为13.3728。

Conclusion: DISC-GAN框架通过解耦风格和内容，并采用特定聚类训练策略，成功地解决了水下图像合成中存在的挑战，生成的图像质量高，达到了最先进的水平。这一水平。

Abstract: In this paper, we propose a novel framework, Disentangled Style-Content GAN
(DISC-GAN), which integrates style-content disentanglement with a
cluster-specific training strategy towards photorealistic underwater image
synthesis. The quality of synthetic underwater images is challenged by optical
due to phenomena such as color attenuation and turbidity. These phenomena are
represented by distinct stylistic variations across different waterbodies, such
as changes in tint and haze. While generative models are well-suited to capture
complex patterns, they often lack the ability to model the non-uniform
conditions of diverse underwater environments. To address these challenges, we
employ K-means clustering to partition a dataset into style-specific domains.
We use separate encoders to get latent spaces for style and content; we further
integrate these latent representations via Adaptive Instance Normalization
(AdaIN) and decode the result to produce the final synthetic image. The model
is trained independently on each style cluster to preserve domain-specific
characteristics. Our framework demonstrates state-of-the-art performance,
obtaining a Structural Similarity Index (SSIM) of 0.9012, an average Peak
Signal-to-Noise Ratio (PSNR) of 32.5118 dB, and a Frechet Inception Distance
(FID) of 13.3728.

</details>


### [82] [Equipping Vision Foundation Model with Mixture of Experts for Out-of-Distribution Detection](https://arxiv.org/abs/2510.10584)
*Shizhen Zhao,Jiahui Liu,Xin Wen,Haoru Tan,Xiaojuan Qi*

Main category: cs.CV

TL;DR: 本文分析了预训练视觉基础模型在OOD检测中的应用，发现DINOv2在无需微调的情况下表现出色。作者提出了MoFE模块和Dynamic-β Mixup策略，以应对大语义空间中的挑战，并取得了显著优于基线方法的性能。


<details>
  <summary>Details</summary>
Motivation: 尽管预训练的视觉基础模型在计算机视觉任务中取得了显著进展，但其在OOD检测任务中的潜力尚未得到充分探索。

Method: 本文首先系统地研究了DINOv2等视觉基础模型在OOD检测中的表现。为了解决大语义空间中的OOD检测挑战，作者提出了两种策略：1. **特征专家混合（MoFE）模块**：将特征划分为子空间，以处理复杂的数据分布并优化决策边界。2. **动态-$\beta$ Mixup策略**：从动态测试版分布中采样插值权重，以适应不同类别的学习难度，从而改进更具挑战性类别的特征学习。

Result: 1. 预训练的DINOv2模型在无需域内数据微调的情况下，即可为OOD检测提供高度判别性的特征空间，性能可与现有SOTA方法媲美。2. 尽管微调可以增强性能，但在大语义空间中，视觉基础模型的性能仍不理想。3. 提出的MoFE模块和Dynamic-$\beta$ Mixup策略显著提高了OOD检测的性能，超越了基线方法。

Conclusion: 预训练的视觉基础模型在OOD检测中具有巨大潜力，但大语义空间仍然是一个挑战。MoFE模块和Dynamic-$\beta$ Mixup策略能够有效应对这些挑战，为未来研究提供了新视角。

Abstract: Pre-trained vision foundation models have transformed many computer vision
tasks. Despite their strong ability to learn discriminative and generalizable
features crucial for out-of-distribution (OOD) detection, their impact on this
task remains underexplored. Motivated by this gap, we systematically
investigate representative vision foundation models for OOD detection. Our
findings reveal that a pre-trained DINOv2 model, even without fine-tuning on
in-domain (ID) data, naturally provides a highly discriminative feature space
for OOD detection, achieving performance comparable to existing
state-of-the-art methods without requiring complex designs. Beyond this, we
explore how fine-tuning foundation models on in-domain (ID) data can enhance
OOD detection. However, we observe that the performance of vision foundation
models remains unsatisfactory in scenarios with a large semantic space. This is
due to the increased complexity of decision boundaries as the number of
categories grows, which complicates the optimization process. To mitigate this,
we propose the Mixture of Feature Experts (MoFE) module, which partitions
features into subspaces, effectively capturing complex data distributions and
refining decision boundaries. Further, we introduce a Dynamic-$\beta$ Mixup
strategy, which samples interpolation weights from a dynamic beta distribution.
This adapts to varying levels of learning difficulty across categories,
improving feature learning for more challenging categories. Extensive
experiments demonstrate the effectiveness of our approach, significantly
outperforming baseline methods.

</details>


### [83] [ViSurf: Visual Supervised-and-Reinforcement Fine-Tuning for Large Vision-and-Language Models](https://arxiv.org/abs/2510.10606)
*Yuqi Liu,Liangyu Chen,Jiazhen Liu,Mingkang Zhu,Zhisheng Zhong,Bei Yu,Jiaya Jia*

Main category: cs.CV

TL;DR: ViSurf是一种新的后训练范式，它将SFT和RLVR的优势结合在一个阶段中，通过在RLVRrollout中注入基本事实标签，实现外部监督和内部强化。


<details>
  <summary>Details</summary>
Motivation: 现有的LVLM后训练范式SFT和RLVR存在局限性：SFT性能不佳，而RLVR难以处理超出模型内部知识库的任务。

Method: 本文提出了ViSurf，一种统一的后训练范式，它将SFT和RLVR的优势整合到单一阶段。ViSurf的核心是在RLVR的rollouts中注入真实标签，同时提供外部监督和内部强化。此外，提出了三种新颖的奖励控制策略来稳定和优化训练过程。

Result: ViSurf在多个基准测试中表现出色，优于SFT、RLVR以及SFT→RLVR的两阶段方法。

Conclusion: ViSurf通过结合SFT和RLVR的优势，有效解决了现有LVLM后训练范式的局限性，可以显著提高LVLM的性能。

Abstract: Typical post-training paradigms for Large Vision-and-Language Models (LVLMs)
include Supervised Fine-Tuning (SFT) and Reinforcement Learning with Verifiable
Rewards (RLVR). SFT leverages external guidance to inject new knowledge,
whereas RLVR utilizes internal reinforcement to enhance reasoning capabilities
and overall performance. However, our analysis reveals that SFT often leads to
sub-optimal performance, while RLVR struggles with tasks that exceed the
model's internal knowledge base. To address these limitations, we propose
ViSurf (\textbf{Vi}sual \textbf{Su}pervised-and-\textbf{R}einforcement
\textbf{F}ine-Tuning), a unified post-training paradigm that integrates the
strengths of both SFT and RLVR within a single stage. We analyze the derivation
of the SFT and RLVR objectives to establish the ViSurf objective, providing a
unified perspective on these two paradigms. The core of ViSurf involves
injecting ground-truth labels into the RLVR rollouts, thereby providing
simultaneous external supervision and internal reinforcement. Furthermore, we
introduce three novel reward control strategies to stabilize and optimize the
training process. Extensive experiments across several diverse benchmarks
demonstrate the effectiveness of ViSurf, outperforming both individual SFT,
RLVR, and two-stage SFT \textrightarrow RLVR. In-depth analysis corroborates
these findings, validating the derivation and design principles of ViSurf.

</details>


### [84] [Topological Alignment of Shared Vision-Language Embedding Space](https://arxiv.org/abs/2510.10889)
*Junwon You,Dasol Kang,Jae-Hun Jung*

Main category: cs.CV

TL;DR: 本文介绍了ToMCLIP，一个解决多语言视觉-语言模型中跨模态对齐偏向英文问题的框架。它通过拓扑对齐来增强多语言表示的结构一致性，并在CIFAR-100和xFlickr&CO上取得了更好的零样本准确性和多语言检索性能。


<details>
  <summary>Details</summary>
Motivation: 现有的对比视觉-语言模型（VLMs）在多语言多模态数据有限的情况下，其跨模态对齐仍然偏向英语。此外，最近的多语言扩展虽然缓解了这一差距，但只强制执行实例级对齐，而忽略了共享嵌入空间的全局几何结构。

Method: 本文引入了ToMCLIP（Topological Alignment for Multilingual CLIP），这是一个拓扑感知的框架，通过拓扑保持约束来对齐嵌入空间。该方法应用持久同源性（persistent homology）来定义拓扑对齐损失，并使用图稀疏化策略以理论误差界限近似持久图。

Result: ToMCLIP验证了所提出的方法，展示了多语言表征的增强结构一致性，在CIFAR-100上实现了更高的零样本准确性，并在xFlickr&CO上实现了更强的多语言检索性能。

Conclusion: ToMCLIP为多语言视觉-语言模型提供了一个有效的方法，通过拓扑对齐解决了跨模态对齐的语言偏见问题，提高了模型的零样本学习能力和多语言检索性能。此外，该方法为将拓扑对齐融入表征学习提供了一个通用方法，超越了VLMS的范畴。

Abstract: Contrastive Vision-Language Models (VLMs) have demonstrated strong zero-shot
capabilities. However, their cross-modal alignment remains biased toward
English due to limited multilingual multimodal data. Recent multilingual
extensions have alleviated this gap but enforce instance-level alignment while
neglecting the global geometry of the shared embedding space. We address this
problem by introducing ToMCLIP (Topological Alignment for Multilingual CLIP), a
topology-aware framework aligning embedding spaces with topology-preserving
constraints. The proposed method applies persistent homology to define a
topological alignment loss and approximates persistence diagram with
theoretical error bounds using graph sparsification strategy. This work
validates the proposed approach, showing enhanced structural coherence of
multilingual representations, higher zero-shot accuracy on the CIFAR-100, and
stronger multilingual retrieval performance on the xFlickr&CO. Beyond VLMs, the
proposed approach provides a general method for incorporating topological
alignment into representation learning.

</details>


### [85] [OmniQuality-R: Advancing Reward Models Through All-Encompassing Quality Assessment](https://arxiv.org/abs/2510.10609)
*Yiting Lu,Fengbin Guan,Yixin Gao,Yan Zhong,Xinge Peng,Jiakang Yuan,Yihao Liu,Bo Zhang,Xin Li,Zhibo Chen,Weisi Lin*

Main category: cs.CV

TL;DR: OmniQuality-R是一个统一的奖励建模框架，它将多任务质量推理转化为连续和可解释的奖励信号，用于策略优化。


<details>
  <summary>Details</summary>
Motivation: 传统的视觉评估方法通常局限于单一任务，而OmniQuality-R旨在解决多任务质量推理的问题。

Method: OmniQuality-R通过以下方法实现：1. 构建一个推理增强的奖励建模数据集，通过拒绝采样获取信息丰富的规划-推理轨迹，形成可靠的思维链（CoT）数据集用于监督微调（SFT）。2. 应用组相对策略优化（GRPO）进行后训练，使用高斯奖励支持连续分数预测。3. 引入标准差（STD）过滤和熵门控机制，以稳定训练并提高下游泛化能力。

Result: OmniQuality-R在美学质量评估、技术质量评估和文本-图像对齐这三个关键IQA任务上进行了评估。

Conclusion: OmniQuality-R通过统一的奖励建模框架，有效地将多任务质量推理转化为可解释的奖励信号，并通过一系列技术稳定训练并提高泛化能力，在多个IQA任务上取得了良好的效果。

Abstract: Current visual evaluation approaches are typically constrained to a single
task. To address this, we propose OmniQuality-R, a unified reward modeling
framework that transforms multi-task quality reasoning into continuous and
interpretable reward signals for policy optimization. Inspired by subjective
experiments, where participants are given task-specific instructions outlining
distinct assessment principles prior to evaluation, we propose OmniQuality-R, a
structured reward modeling framework that transforms multi-dimensional
reasoning into continuous and interpretable reward signals. To enable this, we
construct a reasoning-enhanced reward modeling dataset by sampling informative
plan-reason trajectories via rejection sampling, forming a reliable
chain-of-thought (CoT) dataset for supervised fine-tuning (SFT). Building on
this, we apply Group Relative Policy Optimization (GRPO) for post-training,
using a Gaussian-based reward to support continuous score prediction. To
further stabilize the training and improve downstream generalization, we
incorporate standard deviation (STD) filtering and entropy gating mechanisms
during reinforcement learning. These techniques suppress unstable updates and
reduce variance in policy optimization. We evaluate OmniQuality-R on three key
IQA tasks: aesthetic quality assessment, technical quality evaluation, and
text-image alignment.

</details>


### [86] [DreamMakeup: Face Makeup Customization using Latent Diffusion Models](https://arxiv.org/abs/2510.10918)
*Geon Yeong Park,Inhwa Han,Serin Yang,Yeobin Hong,Seongmin Jeong,Heechan Jeon,Myeongjin Goh,Sung Won Yi,Jin Nam,Jong Chul Ye*

Main category: cs.CV

TL;DR: DreamMakup是一个新的免训练扩散模型，利用DDIM反演技术，通过参考图像、RGB颜色和文本描述等多种条件输入，在保留面部结构和身份的同时，实现虚拟化妆中的面部定制。


<details>
  <summary>Details</summary>
Motivation: 传统的GAN模型在虚拟化妆模拟中存在训练不稳定和定制能力有限的挑战。

Method: DreamMakup采用免训练的扩散模型，利用早期停止的DDIM反演技术，通过参考图像、特定RGB颜色和文本描述等条件输入进行化妆定制。

Result: DreamMakup在定制能力、色彩匹配、身份保留以及与文本描述或LLMs的兼容性方面优于现有的GAN和扩散模型，且计算成本较低。

Conclusion: DreamMakup为虚拟化妆定制提供了一个有效且高效的解决方案，克服了现有方法的局限性，并提供了更好的可控性和精确的图像编辑。

Abstract: The exponential growth of the global makeup market has paralleled
advancements in virtual makeup simulation technology. Despite the progress led
by GANs, their application still encounters significant challenges, including
training instability and limited customization capabilities. Addressing these
challenges, we introduce DreamMakup - a novel training-free Diffusion model
based Makeup Customization method, leveraging the inherent advantages of
diffusion models for superior controllability and precise real-image editing.
DreamMakeup employs early-stopped DDIM inversion to preserve the facial
structure and identity while enabling extensive customization through various
conditioning inputs such as reference images, specific RGB colors, and textual
descriptions. Our model demonstrates notable improvements over existing
GAN-based and recent diffusion-based frameworks - improved customization,
color-matching capabilities, identity preservation and compatibility with
textual descriptions or LLMs with affordable computational costs.

</details>


### [87] [FG-CLIP 2: A Bilingual Fine-grained Vision-Language Alignment Model](https://arxiv.org/abs/2510.10921)
*Chunyu Xie,Bin Wang,Fanjing Kong,Jincheng Li,Dawei Liang,Ji Ao,Dawei Leng,Yuhui Yin*

Main category: cs.CV

TL;DR: FG-CLIP 2 是一个双语视觉语言模型，旨在提升英文和中文的细粒度对齐能力。


<details>
  <summary>Details</summary>
Motivation: 现有的视觉语言模型在细粒度对齐（尤其是非英文环境）和双语理解方面存在局限。

Method: FG-CLIP 2 利用区域-文本匹配和长文本标题建模等细粒度监督，并结合多个判别目标。此外，还引入了文本模态内对比（TIC）损失来区分语义相似的标题。模型在大量的英文和中文数据集上进行训练。

Result: FG-CLIP 2 在8个任务的29个数据集上取得了超越现有方法的性能，并在两种语言中都达到了最先进水平。

Conclusion: FG-CLIP 2 有效解决了现有模型在细粒度视觉语言对齐和双语理解方面的不足，并通过其强大的性能和发布资源，推动了双语细粒度对齐领域的发展。

Abstract: Fine-grained vision-language understanding requires precise alignment between
visual content and linguistic descriptions, a capability that remains limited
in current models, particularly in non-English settings. While models like CLIP
perform well on global alignment, they often struggle to capture fine-grained
details in object attributes, spatial relations, and linguistic expressions,
with limited support for bilingual comprehension. To address these challenges,
we introduce FG-CLIP 2, a bilingual vision-language model designed to advance
fine-grained alignment for both English and Chinese. Our approach leverages
rich fine-grained supervision, including region-text matching and long-caption
modeling, alongside multiple discriminative objectives. We further introduce
the Textual Intra-modal Contrastive (TIC) loss to better distinguish
semantically similar captions. Trained on a carefully curated mixture of
large-scale English and Chinese data, FG-CLIP 2 achieves powerful bilingual
performance. To enable rigorous evaluation, we present a new benchmark for
Chinese multimodal understanding, featuring long-caption retrieval and bounding
box classification. Extensive experiments on 29 datasets across 8 tasks show
that FG-CLIP 2 outperforms existing methods, achieving state-of-the-art results
in both languages. We release the model, code, and benchmark to facilitate
future research on bilingual fine-grained alignment.

</details>


### [88] [A Machine Learning Perspective on Automated Driving Corner Cases](https://arxiv.org/abs/2510.10653)
*Sebastian Schmidt,Julius Körner,Stephan Günnemann*

Main category: cs.CV

TL;DR: 这篇论文提出了一种新的机器学习方法，用于自动驾驶中的感知任务，以有效识别和处理长尾效应场景。


<details>
  <summary>Details</summary>
Motivation: 传统的自动驾驶安全操作通过将困难场景归类为“长尾效应场景”并单独处理，但这种基于范例的分类方法难以扩展且缺乏数据覆盖视角，忽略了机器学习模型在训练数据上的泛化能力。

Method: 本研究提出了一种新颖的机器学习方法，该方法考虑了底层数据分布，并在此基础上提出了一个框架，用于对单个样本进行有效的长尾效应场景识别。

Result: 实验结果表明，该方法（i）在分布视角下统一了现有的基于场景的长尾效应场景分类法，（ii）在长尾效应场景检测任务中表现出色，并在标准基准测试中扩展了已建立的离群值检测基准，（iii）通过 fog-augmented Lost & Found 数据集实现了组合长尾效应场景的分析。

Conclusion: 这些结果为长尾效应场景识别提供了原则性的基础，强调了我们无需手动规范的定义。

Abstract: For high-stakes applications, like autonomous driving, a safe operation is
necessary to prevent harm, accidents, and failures. Traditionally, difficult
scenarios have been categorized into corner cases and addressed individually.
However, this example-based categorization is not scalable and lacks a data
coverage perspective, neglecting the generalization to training data of machine
learning models. In our work, we propose a novel machine learning approach that
takes the underlying data distribution into account. Based on our novel
perspective, we present a framework for effective corner case recognition for
perception on individual samples. In our evaluation, we show that our approach
(i) unifies existing scenario-based corner case taxonomies under a
distributional perspective, (ii) achieves strong performance on corner case
detection tasks across standard benchmarks for which we extend established
out-of-distribution detection benchmarks, and (iii) enables analysis of
combined corner cases via a newly introduced fog-augmented Lost & Found
dataset. These results provide a principled basis for corner case recognition,
underlining our manual specification-free definition.

</details>


### [89] [Stability Under Scrutiny: Benchmarking Representation Paradigms for Online HD Mapping](https://arxiv.org/abs/2510.10660)
*Hao Shan,Ruikai Li,Han Jiang,Yizhe Fan,Ziyang Yan,Bohan Li,Xiaoshuai Hao,Hao Zhao,Zhiyong Cui,Yilong Ren,Haiyang Yu*

Main category: cs.CV

TL;DR: 该论文提出了一个评估在线高精地图模型时间稳定性的基准，并引入了多维稳定性评估框架和统一的mAS分数。研究发现，准确性和稳定性是两个独立的性能维度。旨在促进对稳定性作为核心评估标准的关注，以推动自动驾驶系统发展。


<details>
  <summary>Details</summary>
Motivation: 现有的在线地图构建模型侧重于提高每一帧的建图精度，而未系统研究建图稳定性，在线高精地图的空间位移问题对下游任务构成挑战。

Method: 提出了一个多维稳定性评估框架，包含新的存在稳定性、定位稳定性和形状稳定性指标，并整合为统一的mAS（平均稳定性）分数。对42个模型及其变体进行了广泛实验。

Result: 准确性（mAP）和稳定性（mAS）是两个基本独立的性能维度。分析了关键模型设计选择对这两个标准的影响，确定了有助于提高准确性、稳定性或两者兼顾的架构和训练因素。

Conclusion: 强调了将时间稳定性作为与准确性并列的核心评估标准的重要性，以推动更可靠的自动驾驶系统的发展。

Abstract: As one of the fundamental modules in autonomous driving, online
high-definition (HD) maps have attracted significant attention due to their
cost-effectiveness and real-time capabilities. Since vehicles always cruise in
highly dynamic environments, spatial displacement of onboard sensors inevitably
causes shifts in real-time HD mapping results, and such instability poses
fundamental challenges for downstream tasks. However, existing online map
construction models tend to prioritize improving each frame's mapping accuracy,
while the mapping stability has not yet been systematically studied. To fill
this gap, this paper presents the first comprehensive benchmark for evaluating
the temporal stability of online HD mapping models. We propose a
multi-dimensional stability evaluation framework with novel metrics for
Presence, Localization, and Shape Stability, integrated into a unified mean
Average Stability (mAS) score. Extensive experiments on 42 models and variants
show that accuracy (mAP) and stability (mAS) represent largely independent
performance dimensions. We further analyze the impact of key model design
choices on both criteria, identifying architectural and training factors that
contribute to high accuracy, high stability, or both. To encourage broader
focus on stability, we will release a public benchmark. Our work highlights the
importance of treating temporal stability as a core evaluation criterion
alongside accuracy, advancing the development of more reliable autonomous
driving systems. The benchmark toolkit, code, and models will be available at
https://stablehdmap.github.io/.

</details>


### [90] [GeoVLMath: Enhancing Geometry Reasoning in Vision-Language Models via Cross-Modal Reward for Auxiliary Line Creation](https://arxiv.org/abs/2510.11020)
*Shasha Guo,Liang Pang,Xi Wang,Yanling Wang,Huawei Shen,Jing Zhang*

Main category: cs.CV

TL;DR: 为了解决大型视觉-语言模型（LVLMs）在几何问题中辅助线绘制的挑战，研究人员提出了一种生成辅助线构造文本描述的方法，并开发了一个强化学习框架GeoVLMath，以增强图表文本对齐。GeoVLMath在辅助线推理任务上表现出色，甚至超越了一些现有的LVLM。


<details>
  <summary>Details</summary>
Motivation: 大型视觉-语言模型（LVLMs）在解决复杂几何问题时，绘制辅助线仍面临挑战。现有的图像编辑模型难以精确渲染辅助线，因此需要一种新的方法来更好地结合LVLMs的表示优势。

Method: 本文提出了一种生成辅助线构造文本描述的方法，以适应LVLMs的文本表示能力。为了弥合文本描述与空间结构之间的鸿沟，研究人员提出了一个强化学习框架，该框架通过跨模态奖励来评估生成的辅助线描述与真实辅助线图表的一致性。在此基础上，开发了GeoVLMath，一个专门用于立体几何辅助线推理的开源LVLM。通过GRPO-based RL阶段的微调信号，实现了精确的图表-文本对齐。为了支持训练，他们创建了一个可扩展的数据生成管道，并构建了AuxSolidMath数据集，其中包含3,018个真实考试几何问题，以及配对的图表和对齐的文本字段。

Result: GeoVLMath在3B和7B规模上，在辅助线推理基准测试中，与强大的开源和专有LVLMs相比，GeoVLMath取得了有竞争力的，甚至往往是更优异的性能。

Conclusion: 本研究通过生成辅助线构造的文本描述和开发GeoVLMath强化学习框架，有效解决了LVLMs在几何问题中辅助线绘制的挑战。GeoVLMath在辅助线推理任务上的卓越表现，证明了其在未来几何问题解决中的巨大潜力。

Abstract: Auxiliary lines are essential for solving complex geometric problems but
remain challenging for large vision-language models (LVLMs). Rather than
editing diagrams to draw auxiliary lines, which current image editing models
struggle to render with geometric precision, we generate textual descriptions
of auxiliary-line constructions to better align with the representational
strengths of LVLMs. To bridge the gap between textual descriptions and spatial
structure, we propose a reinforcement learning framework that enhances
diagram-text alignment. At the core of our approach is a cross-modal reward
that evaluates how well the generated auxiliary-line description for an
original diagram matches a ground-truth auxiliary-line diagram. Built on this
reward, we present GeoVLMath, an open-source LVLM tailored to auxiliary-line
reasoning in solid geometry. This fine-grained signal drives a GRPO-based RL
stage, yielding precise diagram-text alignment. To support training, we develop
a scalable data creation pipeline and construct AuxSolidMath, a dataset of
3,018 real-exam geometry problems with paired diagrams and aligned textual
fields. At the 3B and 7B scales, GeoVLMath achieves competitive and often
superior performance compared with strong open-source and proprietary LVLMs on
auxiliary-line reasoning benchmarks.

</details>


### [91] [Source-Free Object Detection with Detection Transformer](https://arxiv.org/abs/2510.11090)
*Huizai Yao,Sicheng Zhao,Shuo Lu,Hui Chen,Yangyang Li,Guoping Liu,Tengfei Xing,Chenggang Yan,Jianhua Tao,Guiguang Ding*

Main category: cs.CV

TL;DR: 本文提出了FRANCK，一个专门为DETR设计的无源域自适应目标检测框架，通过特征增强和对比学习实现了最先进的性能。


<details>
  <summary>Details</summary>
Motivation: 现有的无源目标检测（SFOD）方法大多局限于传统的目标检测模型，或者缺乏针对DETR等新型目标检测架构的定制化适应。

Method: FRANCK框架包含四个关键组件：1）基于目标性分数的样本重加权（OSSR）模块，用于重新加权检测损失以强调未识别区域；2）基于匹配记忆库的对比学习（CMMB）模块，用于增强类别对比学习；3）不确定性加权的查询融合特征蒸馏（UQFD）模块，用于通过预测质量重加权和查询特征融合改进特征蒸馏；4）具有动态教师更新间隔（DTUI）的改进自训练流程，用于优化伪标签质量。

Result: FRANCK在多个广泛使用的基准测试中取得了最先进的性能。

Conclusion: FRANCK通过结合上述组件，有效地将源域预训练的DETR模型适应到目标域，增强了模型的鲁棒性和泛化能力，证明了其在基于DETR的SFOD模型中的有效性和兼容性。

Abstract: Source-Free Object Detection (SFOD) enables knowledge transfer from a source
domain to an unsupervised target domain for object detection without access to
source data. Most existing SFOD approaches are either confined to conventional
object detection (OD) models like Faster R-CNN or designed as general solutions
without tailored adaptations for novel OD architectures, especially Detection
Transformer (DETR). In this paper, we introduce Feature Reweighting ANd
Contrastive Learning NetworK (FRANCK), a novel SFOD framework specifically
designed to perform query-centric feature enhancement for DETRs. FRANCK
comprises four key components: (1) an Objectness Score-based Sample Reweighting
(OSSR) module that computes attention-based objectness scores on multi-scale
encoder feature maps, reweighting the detection loss to emphasize
less-recognized regions; (2) a Contrastive Learning with Matching-based Memory
Bank (CMMB) module that integrates multi-level features into memory banks,
enhancing class-wise contrastive learning; (3) an Uncertainty-weighted
Query-fused Feature Distillation (UQFD) module that improves feature
distillation through prediction quality reweighting and query feature fusion;
and (4) an improved self-training pipeline with a Dynamic Teacher Updating
Interval (DTUI) that optimizes pseudo-label quality. By leveraging these
components, FRANCK effectively adapts a source-pre-trained DETR model to a
target domain with enhanced robustness and generalization. Extensive
experiments on several widely used benchmarks demonstrate that our method
achieves state-of-the-art performance, highlighting its effectiveness and
compatibility with DETR-based SFOD models.

</details>


### [92] [AdaViewPlanner: Adapting Video Diffusion Models for Viewpoint Planning in 4D Scenes](https://arxiv.org/abs/2510.10670)
*Yu Li,Menghan Xia,Gongye Liu,Jianhong Bai,Xintao Wang,Conglang Zhang,Yuxuan Lin,Ruihang Chu,Pengfei Wan,Yujiu Yang*

Main category: cs.CV

TL;DR: 这篇论文提出了一种利用预训练文本到视频（T2V）模型进行4D场景视点规划的两阶段范式。


<details>
  <summary>Details</summary>
Motivation: 探索利用视频生成先验进行视点规划的可行性，因为视频本身就伴随着具有自然视角的动态场景。

Method: 首先，通过自适应学习分支将4D场景表示注入预训练T2V模型，其中4D场景与视点无关，条件生成的视频在视觉上嵌入了视点。其次，将视点提取 S 定义为混合条件引导的相机外部去噪过程，通过在预训练的T2V模型上引入相机外部扩散分支，以生成的视频和4D场景作为输入。

Result: 实验结果表明，该方法优于现有竞争对手，并且消融研究验证了关键技术设计的有效性。

Conclusion: 这项工作在一定程度上证明了视频生成模型在真实世界中实现4D交互的潜力。

Abstract: Recent Text-to-Video (T2V) models have demonstrated powerful capability in
visual simulation of real-world geometry and physical laws, indicating its
potential as implicit world models. Inspired by this, we explore the
feasibility of leveraging the video generation prior for viewpoint planning
from given 4D scenes, since videos internally accompany dynamic scenes with
natural viewpoints. To this end, we propose a two-stage paradigm to adapt
pre-trained T2V models for viewpoint prediction, in a compatible manner. First,
we inject the 4D scene representation into the pre-trained T2V model via an
adaptive learning branch, where the 4D scene is viewpoint-agnostic and the
conditional generated video embeds the viewpoints visually. Then, we formulate
viewpoint extraction as a hybrid-condition guided camera extrinsic denoising
process. Specifically, a camera extrinsic diffusion branch is further
introduced onto the pre-trained T2V model, by taking the generated video and 4D
scene as input. Experimental results show the superiority of our proposed
method over existing competitors, and ablation studies validate the
effectiveness of our key technical designs. To some extent, this work proves
the potential of video generation models toward 4D interaction in real world.

</details>


### [93] [G2L:From Giga-Scale to Cancer-Specific Large-Scale Pathology Foundation Models via Knowledge Distillation](https://arxiv.org/abs/2510.11176)
*Yesung Cho,Sungmin Lee,Geongyu Lee,Minkyung Lee,Jongbae Park,Dongmyung Shin*

Main category: cs.CV

TL;DR: 该研究提出了一种名为G2L的框架，通过知识蒸馏使参数量仅为千兆级模型15%的大型病理学基础模型在癌症特异性任务上实现可与千兆级模型媲美的性能。


<details>
  <summary>Details</summary>
Motivation: 尽管病理学基础模型在扩大训练数据、多样化癌症类型和增加模型尺寸方面表现出持续的性能提升，但千兆级模型因其巨大的计算成本而在开发和部署中面临实际应用挑战。

Method: 提出G2L框架，该框架利用知识蒸馏，在仅1K张目标癌症病理切片数据上，将千兆级模型的知识迁移到大型模型中。

Result: 蒸馏后的大型模型不仅在多个基准测试中超越了同等规模的最佳模型，甚至在某些基准测试中超越了千兆级教师模型和超大型模型。此外，蒸馏模型表现出更高的鲁棒性。

Conclusion: 所提出的针对大型模型的蒸馏方法是一种数据和参数高效的方式，可以在没有过高计算负担的情况下，实现癌症特异性应用中千兆级性能。

Abstract: Recent studies in pathology foundation models have shown that scaling
training data, diversifying cancer types, and increasing model size
consistently improve their performance. However, giga-scale foundation models,
which are trained on hundreds of thousands of slides covering tens of cancer
types and contain billions of parameters, pose significant challenges for
practical use due to their tremendous computational costs in both development
and deployment. In this work, we present a novel strategy, named the G2L
framework, to increase the performance of large-scale foundation models, which
consist of only $15\%$ of the parameters of giga-scale models, to a comparable
performance level of giga-scale models in cancer-specific tasks. Our approach
applies knowledge distillation, transferring the capabilities of a giga-scale
model to a large-scale model, using just 1K pathology slides of a target cancer
(e.g., breast, prostate, etc.). The resulting distilled model not only
outperformed state-of-the-art models of the same size (i.e., large-scale)
across several benchmarks but also, interestingly, surpassed the giga-scale
teacher and huge-scale models in some benchmarks. In addition, the distilled
model exhibited a higher robustness index, indicating improved resilience to
image variations originating from multiple institutions. These findings suggest
that the proposed distillation approach for a large-scale model is a data- and
parameter-efficient way to achieve giga-scale-level performance for
cancer-specific applications without prohibitive computational burden.

</details>


### [94] [WorldMirror: Universal 3D World Reconstruction with Any-Prior Prompting](https://arxiv.org/abs/2510.10726)
*Yifan Liu,Zhiyuan Min,Zhenwei Wang,Junta Wu,Tengfei Wang,Yixuan Yuan,Yawei Luo,Chunchao Guo*

Main category: cs.CV

TL;DR: WorldMirror是一个一体化的前馈模型，用于通用的3D几何预测任务，它可以灵活地集成不同的几何先验，同时生成多种3D表示，并在各种基准测试中取得了最先进的性能。


<details>
  <summary>Details</summary>
Motivation: 现有的3D几何预测方法受限于仅图像输入或针对特定任务，WorldMirror旨在解决这些限制，提供一个统一的框架来集成多样化的几何先验，并一次性生成多种3D表示，从而解决结构模糊性并提供几何上一致的3D输出。

Method: WorldMirror是一个一体化的前馈模型，它整合了相机姿态、内参和深度图等多样化的几何先验信息，并在一个前向传播中同时生成点云、多视角深度图、相机参数、表面法线和3D高斯等多种3D表示。

Result: WorldMirror在各种基准测试中，从相机、点图、深度和表面法线估计到新颖视图合成，都取得了最先进的性能，同时保持了前馈推理的效率。

Conclusion: WorldMirror提供了一个优雅而统一的架构，通过利用可用的先验信息来解决结构模糊性，并在单个前向传播中提供几何上一致的3D输出。该模型在多个任务上实现了最先进的性能，且具有高效率。

Abstract: We present WorldMirror, an all-in-one, feed-forward model for versatile 3D
geometric prediction tasks. Unlike existing methods constrained to image-only
inputs or customized for a specific task, our framework flexibly integrates
diverse geometric priors, including camera poses, intrinsics, and depth maps,
while simultaneously generating multiple 3D representations: dense point
clouds, multi-view depth maps, camera parameters, surface normals, and 3D
Gaussians. This elegant and unified architecture leverages available prior
information to resolve structural ambiguities and delivers geometrically
consistent 3D outputs in a single forward pass. WorldMirror achieves
state-of-the-art performance across diverse benchmarks from camera, point map,
depth, and surface normal estimation to novel view synthesis, while maintaining
the efficiency of feed-forward inference. Code and models will be publicly
available soon.

</details>


### [95] [When Does Supervised Training Pay Off? The Hidden Economics of Object Detection in the Era of Vision-Language Models](https://arxiv.org/abs/2510.11302)
*Samer Al-Hamadani*

Main category: cs.CV

TL;DR: 本文对监督式目标检测（YOLO）和零样本VLM推理（Gemini Flash 2.5）进行了首次成本效益分析，评估了它们在不同场景下的性能和成本，并提出了选择检测架构的决策框架。


<details>
  <summary>Details</summary>
Motivation: 传统的目标检测系统需要大量的人工标注，而视觉语言模型（VLMs）可以实现零样本检测，但准确性较低。本文旨在通过成本效益分析，比较这两种方法的优劣，以指导用户在实际应用中做出最佳选择。

Method: 本文在1000张分层COCO图像和200张多样化的产品图像上，对监督式YOLO和零样本Gemini Flash 2.5进行了系统评估，并结合详细的总拥有成本模型，建立了决定架构选择的量化盈亏平衡阈值。

Result: 在标准类别上，监督式YOLO的准确率为91.2%，零样本Gemini为68.5%。YOLO的22.7%的准确性优势需要10,800美元的标注成本，并且只有在超过5500万次推理（相当于每日151,000张图像，持续一年）的情况下才能证明投资的合理性。在多样化的产品类别中，零样本Gemini的准确率为52.3%，而监督式YOLO为0%。在100,000次推理下，Gemini的每次正确检测成本远低于YOLO（0.00050美元 vs 0.143美元）。

Conclusion: 最佳的检测架构选择取决于部署量、类别稳定性、预算限制和准确性要求，而不仅仅是技术性能指标。零样本VLMs在长尾和低资源场景中具有显著优势，而监督式检测则在高量、高精度要求的特定任务中表现更优。

Abstract: Object detection systems have traditionally relied on supervised learning
with manually annotated bounding boxes, achieving high accuracy at the cost of
substantial annotation investment. The emergence of Vision-Language Models
(VLMs) offers an alternative paradigm enabling zero-shot detection through
natural language queries, eliminating annotation requirements but operating
with reduced accuracy. This paper presents the first comprehensive
cost-effectiveness analysis comparing supervised detection (YOLO) with
zero-shot VLM inference (Gemini Flash 2.5). Through systematic evaluation on
1,000 stratified COCO images and 200 diverse product images spanning consumer
electronics and rare categories, combined with detailed Total Cost of Ownership
modeling, we establish quantitative break-even thresholds governing
architecture selection. Our findings reveal that supervised YOLO achieves 91.2%
accuracy versus 68.5% for zero-shot Gemini on standard categories, representing
a 22.7 percentage point advantage that costs $10,800 in annotation for
100-category systems. However, this advantage justifies investment only beyond
55 million inferences, equivalent to 151,000 images daily for one year.
Zero-shot Gemini demonstrates 52.3% accuracy on diverse product categories
(ranging from highly web-prevalent consumer electronics at 75-85% to rare
specialized equipment at 25-40%) where supervised YOLO achieves 0% due to
architectural constraints preventing detection of untrained classes. Cost per
Correct Detection analysis reveals substantially lower per-detection costs for
Gemini ($0.00050 vs $0.143) at 100,000 inferences despite accuracy deficits. We
develop decision frameworks demonstrating that optimal architecture selection
depends critically on deployment volume, category stability, budget
constraints, and accuracy requirements rather than purely technical performance
metrics.

</details>


### [96] [EGD-YOLO: A Lightweight Multimodal Framework for Robust Drone-Bird Discrimination via Ghost-Enhanced YOLOv8n and EMA Attention under Adverse Condition](https://arxiv.org/abs/2510.10765)
*Sudipto Sarkar,Mohammad Asif Hasan,Khondokar Ashik Shahriar,Fablia Labiba,Nahian Tasnim,Sheikh Anawarul Haq Fattah*

Main category: cs.CV

TL;DR: EGD-YOLOv8n模型利用RGB和红外图像，通过改进特征捕获、智能设计和注意力机制，实现了无人机和鸟类检测的高准确性和实时性，尤其在RGB与红外图像结合时表现最佳。


<details>
  <summary>Details</summary>
Motivation: 为了确保天空安全和提升安防系统性能，区分无人机和鸟类至关重要。

Method: 本研究提出了EGD-YOLOv8n模型，这是一个轻量且高效的目标检测模型。该模型通过改进图像特征捕获和理解能力，使用智能设计修改（包括注意力层和特殊检测头）来关注重要细节，同时减少计算量。研究训练了三个版本：仅使用RGB图像、仅使用红外图像以及结合使用两种图像。

Result: 结合RGB和红外图像的模型版本取得了最佳的准确性和可靠性，同时能够满足在常见GPU上实时运行的速度要求。

Conclusion: EGD-YOLOv8n模型在RGB与红外图像结合时，能实现对空中目标（无人机和鸟类）的准确、高效且实时检测，显著提升了天空安全和安防系统的性能。

Abstract: Identifying drones and birds correctly is essential for keeping the skies
safe and improving security systems. Using the VIP CUP 2025 dataset, which
provides both RGB and infrared (IR) images, this study presents EGD-YOLOv8n, a
new lightweight yet powerful model for object detection. The model improves how
image features are captured and understood, making detection more accurate and
efficient. It uses smart design changes and attention layers to focus on
important details while reducing the amount of computation needed. A special
detection head helps the model adapt to objects of different shapes and sizes.
We trained three versions: one using RGB images, one using IR images, and one
combining both. The combined model achieved the best accuracy and reliability
while running fast enough for real-time use on common GPUs.

</details>


### [97] [Structured Spectral Graph Learning for Multi-label Abnormality Classification in 3D Chest CT Scans](https://arxiv.org/abs/2510.10779)
*Theo Di Piazza,Carole Lazarus,Olivier Nempont,Loic Boussel*

Main category: cs.CV

TL;DR: 这篇论文提出了一种新的基于图的2.5D框架，用于胸部CT扫描的多标签分类，通过将CT体表示为结构化图并使用谱图卷积处理切片来解决长距离依赖问题，并在多个数据集上取得了良好的泛化和有竞争力的性能。


<details>
  <summary>Details</summary>
Motivation: CT检查数量的增长对自动化工具（如器官分割、异常检测和报告生成）的需求增加，以帮助放射科医生应对临床工作量。3D胸部CT扫描的多标签分类仍然是一个关键且具有挑战性的问题，因为体积数据固有的复杂空间关系以及异常的广泛变异性。

Method: 本文提出了一种2.5D的替代方案，引入了一个新的基于图的框架，将3D CT体积表示为结构化图。其中，轴向切片三元组作为节点，通过谱图卷积进行处理，使模型能够推断切片间的依赖关系，同时保持与临床部署兼容的复杂性。

Result: 该方法在来自独立机构的3个数据集上进行了训练和评估，实现了强大的跨数据集泛化，并与最先进的视觉编码器相比显示出有竞争力的性能。

Conclusion: 该研究提出了一种有效的2.5D图基方法，用于胸部CT多标签分类，解决了现有3D CNN和Vision Transformer在长距离依赖和数据需求方面的不足。该方法在泛化能力和性能上表现出色，并通过迁移实验证明了其在放射学报告生成和腹部CT数据上的广泛适用性。

Abstract: With the growing volume of CT examinations, there is an increasing demand for
automated tools such as organ segmentation, abnormality detection, and report
generation to support radiologists in managing their clinical workload.
Multi-label classification of 3D Chest CT scans remains a critical yet
challenging problem due to the complex spatial relationships inherent in
volumetric data and the wide variability of abnormalities. Existing methods
based on 3D convolutional neural networks struggle to capture long-range
dependencies, while Vision Transformers often require extensive pre-training on
large-scale, domain-specific datasets to perform competitively. In this work,
we propose a 2.5D alternative by introducing a new graph-based framework that
represents 3D CT volumes as structured graphs, where axial slice triplets serve
as nodes processed through spectral graph convolution, enabling the model to
reason over inter-slice dependencies while maintaining complexity compatible
with clinical deployment. Our method, trained and evaluated on 3 datasets from
independent institutions, achieves strong cross-dataset generalization, and
shows competitive performance compared to state-of-the-art visual encoders. We
further conduct comprehensive ablation studies to evaluate the impact of
various aggregation strategies, edge-weighting schemes, and graph connectivity
patterns. Additionally, we demonstrate the broader applicability of our
approach through transfer experiments on automated radiology report generation
and abdominal CT data.\\ This work extends our previous contribution presented
at the MICCAI 2025 EMERGE Workshop.

</details>


### [98] [AndesVL Technical Report: An Efficient Mobile-side Multimodal Large Language Model](https://arxiv.org/abs/2510.11496)
*Zhiwei Jin,Xiaohui Song,Nan Wang,Yafei Liu,Chao Li,Xin Li,Ruichen Wang,Zhihao Li,Qi Qi,Long Cheng,Dongze Hao,Quanlong Zheng,Yanhao Zhang,Haobo Ji,Jian Ma,Zhitong Zheng,Zhenyi Lin,Haolin Deng,Xin Zou,Xiaojie Yin,Ruilin Wang,Liankai Cai,Haijing Liu,Yuqing Qiu,Ke Chen,Zixian Li,Chi Xie,Huafei Li,Chenxing Li,Chuangchuang Wang,Kai Tang,Zhiguang Zhu,Kai Tang,Wenmei Gao,Rui Wang,Jun Wu,Chao Liu,Qin Xie,Chen Chen,Haonan Lu*

Main category: cs.CV

TL;DR: AndesVL是一套参数量从0.6B到4B的移动端MLLM，它基于Qwen3的LLM和各种视觉编码器。AndesVL在文本丰富的图像理解、推理和数学、多图像理解、通用VQA、幻觉缓解、多语言理解和GUI相关任务等广泛的开源基准测试中，与同等规模的SOTA模型相比，实现了领先的性能。


<details>
  <summary>Details</summary>
Motivation: 现有的MLLM模型（如QwenVL、InternVL、GPT-4o等）虽然性能出色，但模型规模庞大，超出了移动设备的内存、功耗和计算能力的限制。

Method: 本文介绍了AndesVL，一套参数量从0.6B到4B的移动端MLLM，它基于Qwen3的LLM和各种视觉编码器。详细阐述了AndesVL的模型架构、训练流程和训练数据。

Result: AndesVL在文本丰富的图像理解、推理和数学、多图像理解、通用VQA、幻觉缓解、多语言理解和GUI相关任务等广泛的开源基准测试中，与同等规模的SOTA模型相比，取得了领先的性能。

Conclusion: AndesVL作为一套移动端MLLM，在保证轻量化的同时，依然能在多项基准测试中达到领先水平，展现了其在移动设备上部署的潜力。

Abstract: In recent years, while cloud-based MLLMs such as QwenVL, InternVL, GPT-4o,
Gemini, and Claude Sonnet have demonstrated outstanding performance with
enormous model sizes reaching hundreds of billions of parameters, they
significantly surpass the limitations in memory, power consumption, and
computing capacity of edge devices such as mobile phones. This paper introduces
AndesVL, a suite of mobile-side MLLMs with 0.6B to 4B parameters based on
Qwen3's LLM and various visual encoders. We comprehensively outline the model
architectures, training pipeline, and training data of AndesVL, which achieves
first-tier performance across a wide range of open-source benchmarks, including
fields such as text-rich image understanding, reasoning and math, multi-image
comprehension, general VQA, hallucination mitigation, multilingual
understanding, and GUI-related tasks when compared with state-of-the-art models
of a similar scale. Furthermore, we introduce a 1+N LoR

</details>


### [99] [LikePhys: Evaluating Intuitive Physics Understanding in Video Diffusion Models via Likelihood Preference](https://arxiv.org/abs/2510.11512)
*Jianhao Yuan,Fabio Pizzati,Francesco Pinto,Lars Kunze,Ivan Laptev,Paul Newman,Philip Torr,Daniele De Martini*

Main category: cs.CV

TL;DR: LikePhys是一种无训练方法，它使用去噪目标作为ELBO的替代方法，通过区分物理上有效和不可能的视频来评估视频扩散模型中的直观物理，并在12个场景的基准测试上进行了测试。


<details>
  <summary>Details</summary>
Motivation: 视频扩散模型理解直观物理在构建通用物理合理世界模拟器中至关重要，但由于在生成中难以将物理正确性从视觉外观中解耦，准确评估这种能力仍然是一个挑战。

Method: 我们引入了LikePhys，这是一种无训练方法，它使用去噪目标作为ELBO的替代方法，通过区分物理上有效和不可能的视频来评估视频扩散模型中的直观物理，并在一个精心策划的有效-无效对数据集上进行。我们构建了跨越四个物理领域的十二个场景的基准测试，并提出了一个新的评估指标，即Plausibility Preference Error（PPE）。

Result: 我们的评估指标Plausibility Preference Error（PPE）与人类偏好高度一致，优于最先进的评估器基线。我们系统地评估了当前视频扩散模型中的直观物理理解。研究还分析了模型设计和推理设置如何影响直观物理理解，并强调了不同物理定律在特定领域的容量变化。

Conclusion: 经验结果表明，尽管当前模型在复杂和混沌动力学方面仍面临挑战，但随着模型容量和推理设置的扩展，物理理解能力呈现出明显的改进趋势。

Abstract: Intuitive physics understanding in video diffusion models plays an essential
role in building general-purpose physically plausible world simulators, yet
accurately evaluating such capacity remains a challenging task due to the
difficulty in disentangling physics correctness from visual appearance in
generation. To the end, we introduce LikePhys, a training-free method that
evaluates intuitive physics in video diffusion models by distinguishing
physically valid and impossible videos using the denoising objective as an
ELBO-based likelihood surrogate on a curated dataset of valid-invalid pairs. By
testing on our constructed benchmark of twelve scenarios spanning over four
physics domains, we show that our evaluation metric, Plausibility Preference
Error (PPE), demonstrates strong alignment with human preference, outperforming
state-of-the-art evaluator baselines. We then systematically benchmark
intuitive physics understanding in current video diffusion models. Our study
further analyses how model design and inference settings affect intuitive
physics understanding and highlights domain-specific capacity variations across
physical laws. Empirical results show that, despite current models struggling
with complex and chaotic dynamics, there is a clear trend of improvement in
physics understanding as model capacity and inference settings scale.

</details>


### [100] [ImHead: A Large-scale Implicit Morphable Model for Localized Head Modeling](https://arxiv.org/abs/2510.10793)
*Rolandos Alexandros Potamias,Stathis Galanakis,Jiankang Deng,Athanasios Papaioannou,Stefanos Zafeiriou*

Main category: cs.CV

TL;DR: imHead是一种新颖的隐式三维形变模型，它能够建模富有表现力的三维头部化身，并且有助于面部特征的局部编辑。


<details>
  <summary>Details</summary>
Motivation: 现有的三维形变模型（3DMMs）由于其严格的拓扑结构和线性性质，难以表示复杂的全头形状。

Method: imHead保留了一个紧凑的身份空间，并引入了一个中间的区域特定潜在表示来实现局部编辑。

Result: imHead在表示不同身份和表情方面超越了以前的方法，并提供了3D人脸操作的可解释解决方案，允许用户进行局部编辑。

Conclusion: imHead模型在3D头部建模领域具有强大的表达能力和可解释的局部编辑能力，为大规模3D头部建模提供了一种新的解决方案。

Abstract: Over the last years, 3D morphable models (3DMMs) have emerged as a
state-of-the-art methodology for modeling and generating expressive 3D avatars.
However, given their reliance on a strict topology, along with their linear
nature, they struggle to represent complex full-head shapes. Following the
advent of deep implicit functions, we propose imHead, a novel implicit 3DMM
that not only models expressive 3D head avatars but also facilitates localized
editing of the facial features. Previous methods directly divided the latent
space into local components accompanied by an identity encoding to capture the
global shape variations, leading to expensive latent sizes. In contrast, we
retain a single compact identity space and introduce an intermediate
region-specific latent representation to enable local edits. To train imHead,
we curate a large-scale dataset of 4K distinct identities, making a
step-towards large scale 3D head modeling. Under a series of experiments we
demonstrate the expressive power of the proposed model to represent diverse
identities and expressions outperforming previous approaches. Additionally, the
proposed approach provides an interpretable solution for 3D face manipulation,
allowing the user to make localized edits.

</details>


### [101] [EvoCAD: Evolutionary CAD Code Generation with Vision Language Models](https://arxiv.org/abs/2510.11631)
*Tobias Preintner,Weixuan Yuan,Adrian König,Thomas Bäck,Elena Raponi,Niki van Stein*

Main category: cs.CV

TL;DR: 该论文介绍了一种将大型语言模型与进化计算算法相结合的方法，用于生成CAD对象。


<details>
  <summary>Details</summary>
Motivation: 探索大型语言模型在结合进化计算算法生成CAD对象方面的潜力。

Method: 提出了EvoCAD方法，通过视觉语言模型和进化优化生成CAD对象的符号表示。该方法采样多个CAD对象，然后使用视觉语言和推理语言模型进行进化优化。

Result: EvoCAD在CADPrompt基准数据集上超越了现有方法，尤其在生成拓扑正确对象方面表现突出。引入了两个基于欧拉特征的拓扑属性新度量，可以有效评估3D对象之间的语义相似性。

Conclusion: EvoCAD结合了大型语言模型和进化计算算法的优势，在CAD对象生成方面取得了显著进展，并引入了有效的评估度量。

Abstract: Combining large language models with evolutionary computation algorithms
represents a promising research direction leveraging the remarkable generative
and in-context learning capabilities of LLMs with the strengths of evolutionary
algorithms. In this work, we present EvoCAD, a method for generating
computer-aided design (CAD) objects through their symbolic representations
using vision language models and evolutionary optimization. Our method samples
multiple CAD objects, which are then optimized using an evolutionary approach
with vision language and reasoning language models. We assess our method using
GPT-4V and GPT-4o, evaluating it on the CADPrompt benchmark dataset and
comparing it to prior methods. Additionally, we introduce two new metrics based
on topological properties defined by the Euler characteristic, which capture a
form of semantic similarity between 3D objects. Our results demonstrate that
EvoCAD outperforms previous approaches on multiple metrics, particularly in
generating topologically correct objects, which can be efficiently evaluated
using our two novel metrics that complement existing spatial metrics.

</details>


### [102] [Full segmentation annotations of 3D time-lapse microscopy images of MDA231 cells](https://arxiv.org/abs/2510.10797)
*Aleksandra Melnikova,Petr Matula*

Main category: cs.CV

TL;DR: 该文章介绍了在2025年发表的一篇论文中提供的一个公共的3D时移分割数据集，该数据集包含具有复杂动态形状的迁移细胞的完整3D时移分割注释。文章详细描述了数据集和相关实验，并验证了注释的准确性。


<details>
  <summary>Details</summary>
Motivation: 高质量、公开可用的图像和视频数据集分割注释对于推动图像处理领域的发展至关重要，特别是对大量目标进行体积图像注释耗时且具有挑战性。本文旨在提供2025年发表的论文中未包含的数据集和伴随实验的全面描述，因为出版空间有限。

Method: 本文对由三个不同人类注释的CTC中MDA231人乳腺癌细胞的两个序列进行了注释。将创建的注释与CTC组织者提供的先前发布的跟踪标记进行了一致性验证。测量了基于CTC 2D金标准分割精度，并与注释者之间的可变性范围进行比较。将创建的3D注释与CTC提供的自动创建的银标准进行比较。

Result: 创建的注释与先前发布的跟踪标记一致，并且分割精度在注释者之间的可变性范围内。与自动创建的银标准相比，提出的注释能更好地表示输入图像的复杂性。

Conclusion: 所提供的数据集和注释可用于测试和训练细胞分割，或分析高度动态对象的3D形状。

Abstract: High-quality, publicly available segmentation annotations of image and video
datasets are critical for advancing the field of image processing. In
particular, annotations of volumetric images of a large number of targets are
time-consuming and challenging. In (Melnikova, A., & Matula, P., 2025), we
presented the first publicly available full 3D time-lapse segmentation
annotations of migrating cells with complex dynamic shapes. Concretely, three
distinct humans annotated two sequences of MDA231 human breast carcinoma cells
(Fluo-C3DL-MDA231) from the Cell Tracking Challenge (CTC).
  This paper aims to provide a comprehensive description of the dataset and
accompanying experiments that were not included in (Melnikova, A., & Matula,
P., 2025) due to limitations in publication space. Namely, we show that the
created annotations are consistent with the previously published tracking
markers provided by the CTC organizers and the segmentation accuracy measured
based on the 2D gold truth of CTC is within the inter-annotator variability
margins. We compared the created 3D annotations with automatically created
silver truth provided by CTC. We have found the proposed annotations better
represent the complexity of the input images. The presented annotations can be
used for testing and training cell segmentation, or analyzing 3D shapes of
highly dynamic objects.

</details>


### [103] [NV3D: Leveraging Spatial Shape Through Normal Vector-based 3D Object Detection](https://arxiv.org/abs/2510.11632)
*Krittin Chaowakarn,Paramin Sangwongngam,Nang Htet Htet Aung,Chalie Charoenlarpnopparut*

Main category: cs.CV

TL;DR: 本文提出了一种新颖的3D目标检测模型NV3D，通过利用从体素邻居中获取的局部特征（即使用K-近邻和主成分分析计算的法向量）来解决现有多模态方法中特征对齐的挑战以及局部特征提取过于简化的不足。


<details>
  <summary>Details</summary>
Motivation: 现有的自动驾驶3D目标检测方法在多模态特征融合时面临特征对齐的挑战，而仅利用LiDAR点云的局部模式进行特征提取可能对于复杂的3D检测任务来说过于简化。

Method: 本文提出NV3D模型，该模型利用从体素邻居中获取的局部特征，即通过K-近邻（KNN）和主成分分析（PCA）计算的每个体素的法向量。NV3D引入了两种不同的采样策略：基于法向量密度的采样和FOV感知基于bin的采样。此外，NV3D应用了元素级注意力融合机制，将体素特征作为查询和值，法向量特征作为键。

Result: NV3D在KITTI数据集上进行了训练，并在汽车和自行车检测方面表现出优越的性能，这得益于其对空间形状的识别能力。在验证集上，不带采样的NV3D在汽车和自行车检测方面分别达到了86.60%和80.18%的平均精度（mAP），比基线Voxel R-CNN分别高出2.61%和4.23%。即使采用两种采样策略过滤掉大约55%的体素，NV3D在汽车检测中仍达到85.54%的mAP，比基线高出1.56%。

Conclusion: NV3D通过有效利用法向量特征和创新的采样及融合策略，在3D目标检测任务中实现了卓越的性能提升，尤其是在保持性能的同时显著减少了数据量。

Abstract: Recent studies in 3D object detection for autonomous vehicles aim to enrich
features through the utilization of multi-modal setups or the extraction of
local patterns within LiDAR point clouds. However, multi-modal methods face
significant challenges in feature alignment, and gaining features locally can
be oversimplified for complex 3D object detection tasks. In this paper, we
propose a novel model, NV3D, which utilizes local features acquired from voxel
neighbors, as normal vectors computed per voxel basis using K-nearest neighbors
(KNN) and principal component analysis (PCA). This informative feature enables
NV3D to determine the relationship between the surface and pertinent target
entities, including cars, pedestrians, or cyclists. During the normal vector
extraction process, NV3D offers two distinct sampling strategies: normal vector
density-based sampling and FOV-aware bin-based sampling, allowing elimination
of up to 55% of data while maintaining performance. In addition, we applied
element-wise attention fusion, which accepts voxel features as the query and
value and normal vector features as the key, similar to the attention
mechanism. Our method is trained on the KITTI dataset and has demonstrated
superior performance in car and cyclist detection owing to their spatial
shapes. In the validation set, NV3D without sampling achieves 86.60% and 80.18%
mean Average Precision (mAP), greater than the baseline Voxel R-CNN by 2.61%
and 4.23% mAP, respectively. With both samplings, NV3D achieves 85.54% mAP in
car detection, exceeding the baseline by 1.56% mAP, despite roughly 55% of
voxels being filtered out.

</details>


### [104] [CodePlot-CoT: Mathematical Visual Reasoning by Thinking with Code-Driven Images](https://arxiv.org/abs/2510.11718)
*Chengqi Duan,Kaiyue Sun,Rongyao Fang,Manyuan Zhang,Yan Feng,Ying Luo,Yufang Liu,Ke Wang,Peng Pei,Xunliang Cai,Hongsheng Li,Yi Ma,Xihui Liu*

Main category: cs.CV

TL;DR: 介绍了CodePlot-CoT，这是一种代码驱动的思维链范式，用于通过生成文本推理和可执行绘图代码，然后将其渲染成图像作为“视觉思维”来解决需要视觉辅助的数学问题。


<details>
  <summary>Details</summary>
Motivation: 现有的LLMs和VLMs在解决需要视觉辅助的数学问题时面临瓶颈，因为它们大多局限于纯文本推理，而多模态统一模型在生成交错文本和图像时缺乏必要的精确性和可控性。

Method: 1. 构建了Math-VR，一个包含17.8万样本的大规模双语数学视觉推理数据集和基准。2. 开发了一个先进的图像到代码转换器，用于将复杂的数学图形解析成代码，以创建高质量的训练数据。3. 利用这些训练数据训练CodePlot-CoT模型来解决数学问题。

Result: CodePlot-CoT模型在新的基准测试中比基础模型提高了21%，验证了其代码驱动推理范式的有效性。

Conclusion: CodePlot-CoT为多模态数学推理开辟了新方向，并提供了首个大规模数据集、综合基准和强有力的方法。

Abstract: Recent advances in Large Language Models (LLMs) and Vision Language Models
(VLMs) have shown significant progress in mathematical reasoning, yet they
still face a critical bottleneck with problems requiring visual assistance,
such as drawing auxiliary lines or plotting functions to solve the problems.
Most LLMs and VLMs are constrained to text-only reasoning chains, while
multimodal unified models that can generate interleaved text and images lack
the necessary precision and controllability for such tasks. To address this, we
propose CodePlot-CoT, a code-driven Chain-of-Thought paradigm for "thinking
with images" in mathematics. Our approach leverages the VLM to generate text
reasoning as well as executable plotting code, which is then rendered into
images as "visual thought", to solve mathematical problems. To achieve this, we
first construct Math-VR, the first large-scale, bilingual dataset and benchmark
for Mathematics problems with Visual Reasoning, comprising 178K samples.
Second, to create high-quality training data, we develop a state-of-the-art
image-to-code converter specialized for parsing complex mathematical figures
into codes. Finally, using these training data, we train the CodePlot-CoT model
for solving mathematical problems. Experimental results show that our model
achieves up to 21% increase over base model on our new benchmark, fully
validating the efficacy of our proposed code-driven reasoning paradigm. Our
work opens a new direction for multimodal mathematical reasoning and provides
the community with the first large-scale dataset, comprehensive benchmark, and
strong approach for such problems. To facilitate future research, we make our
datasets, code, and pretrained models publicly available at
https://github.com/HKU-MMLab/Math-VR-CodePlot-CoT.

</details>


### [105] [FastHMR: Accelerating Human Mesh Recovery via Token and Layer Merging with Diffusion Decoding](https://arxiv.org/abs/2510.10868)
*Soroush Mehraban,Andrea Iaboni,Babak Taati*

Main category: cs.CV

TL;DR: 这篇论文提出了两种针对3D人体网格恢复（HMR）的合并策略（ECLM和Mask-ToMe）来降低计算成本和复杂度，并通过引入扩散解码器进一步提升性能。


<details>
  <summary>Details</summary>
Motivation: 现有的基于Transformer的3D人体网格恢复（HMR）模型计算成本高，复杂度大，这是由于深度Transformer架构和冗余tokens导致的。

Method: 本文提出了两种HMR特有的合并策略：1. 错误约束层合并（ECLM）：选择性地合并对平均关节位置误差（MPJPE）影响最小的Transformer层。 2. 掩码引导token合并（Mask-ToMe）：合并对最终预测贡献较小的背景tokens。 为了解决合并可能导致的性能下降，本文还提出了一个扩散解码器，该解码器结合了时间上下文，并利用从大规模运动捕捉数据集中学习到的姿态先验。

Result: 在多个基准测试中，我们的方法实现了高达2.3倍的加速，同时在基线性能上略有提升。

Conclusion: 本文通过引入两种HMR特有的合并策略和扩散解码器，有效地降低了3D人体网格恢复模型的计算成本和复杂度，同时保持或略微提升了性能。

Abstract: Recent transformer-based models for 3D Human Mesh Recovery (HMR) have
achieved strong performance but often suffer from high computational cost and
complexity due to deep transformer architectures and redundant tokens. In this
paper, we introduce two HMR-specific merging strategies: Error-Constrained
Layer Merging (ECLM) and Mask-guided Token Merging (Mask-ToMe). ECLM
selectively merges transformer layers that have minimal impact on the Mean Per
Joint Position Error (MPJPE), while Mask-ToMe focuses on merging background
tokens that contribute little to the final prediction. To further address the
potential performance drop caused by merging, we propose a diffusion-based
decoder that incorporates temporal context and leverages pose priors learned
from large-scale motion capture datasets. Experiments across multiple
benchmarks demonstrate that our method achieves up to 2.3x speed-up while
slightly improving performance over the baseline.

</details>


### [106] [rareboost3d: a synthetic lidar dataset with enhanced rare classes](https://arxiv.org/abs/2510.10876)
*Shutong Lin,Zhengkang Xiang,Jianzhong Qi,Kourosh Khoshelham*

Main category: cs.CV

TL;DR: 为了解决激光雷达点云数据集中稀有类别实例不足导致的“长尾问题”，本文提出了一个名为RareBoost3D的合成点云数据集，并通过跨域语义对齐方法CSC损失来融合合成数据和真实世界数据，从而显著提高了激光雷达点云分割模型的性能。


<details>
  <summary>Details</summary>
Motivation: 现有的点云数据集中，由于某些稀有类别实例数量有限，导致了“长尾问题”，这影响了基于激光雷达的感知技术（如自动驾驶中的物体分割）的性能。

Method: 本文提出了一个名为RareBoost3D的合成点云数据集，通过为真实世界数据集中稀有的对象类别提供显著更多的实例来补充现有数据集。此外，为了有效利用合成数据和真实世界数据，本文提出了一种名为CSC损失的跨域语义对齐方法，该方法用于对齐不同领域中相同类别的特征表示。

Result: 实验结果表明，CSC损失的对齐方法显著增强了激光雷达点云分割模型在真实世界数据上的性能。

Conclusion: RareBoost3D数据集和CSC损失的跨域语义对齐方法能够有效解决激光雷达点云数据集中的长尾问题，并通过融合合成数据和真实世界数据，显著提升了点云分割模型的性能。

Abstract: Real-world point cloud datasets have made significant contributions to the
development of LiDAR-based perception technologies, such as object segmentation
for autonomous driving. However, due to the limited number of instances in some
rare classes, the long-tail problem remains a major challenge in existing
datasets. To address this issue, we introduce a novel, synthetic point cloud
dataset named RareBoost3D, which complements existing real-world datasets by
providing significantly more instances for object classes that are rare in
real-world datasets. To effectively leverage both synthetic and real-world
data, we further propose a cross-domain semantic alignment method named CSC
loss that aligns feature representations of the same class across different
domains. Experimental results demonstrate that this alignment significantly
enhances the performance of LiDAR point cloud segmentation models over
real-world data.

</details>


### [107] [SceneTextStylizer: A Training-Free Scene Text Style Transfer Framework with Diffusion Model](https://arxiv.org/abs/2510.10910)
*Honghui Yuan,Keiji Yanai*

Main category: cs.CV

TL;DR: SceneTextStylizer是一个基于扩散模型的无训练框架，用于场景文本的风格迁移，支持对文本区域进行精确的风格转换，同时保持文本可读性和风格一致性。


<details>
  <summary>Details</summary>
Motivation: 现有的场景文本编辑方法在内容替换和简单风格方面有局限性，缺乏自由风格迁移的能力，且无法实现灵活和局部化的场景文本风格编辑。

Method: SceneTextStylizer通过特征注入模块（利用扩散模型反演和自注意力机制）和区域控制机制（在去噪步骤中应用基于距离的变化掩码），实现精确的空间控制。此外，还引入了基于傅里叶变换的风格增强模块，以丰富视觉效果。

Result: 我们的方法在场景文本风格转换方面取得了优越的性能，在视觉保真度和文本保留方面均超越了现有的最新方法。

Conclusion: SceneTextStylizer通过其创新的无训练扩散模型框架，有效解决了场景文本灵活和局部化风格编辑的难题，为文本区域带来了高质量的风格迁移，同时保持了文本的清晰度和风格的统一性。

Abstract: With the rapid development of diffusion models, style transfer has made
remarkable progress. However, flexible and localized style editing for scene
text remains an unsolved challenge. Although existing scene text editing
methods have achieved text region editing, they are typically limited to
content replacement and simple styles, which lack the ability of free-style
transfer. In this paper, we introduce SceneTextStylizer, a novel training-free
diffusion-based framework for flexible and high-fidelity style transfer of text
in scene images. Unlike prior approaches that either perform global style
transfer or focus solely on textual content modification, our method enables
prompt-guided style transformation specifically for text regions, while
preserving both text readability and stylistic consistency. To achieve this, we
design a feature injection module that leverages diffusion model inversion and
self-attention to transfer style features effectively. Additionally, a region
control mechanism is introduced by applying a distance-based changing mask at
each denoising step, enabling precise spatial control. To further enhance
visual quality, we incorporate a style enhancement module based on the Fourier
transform to reinforce stylistic richness. Extensive experiments demonstrate
that our method achieves superior performance in scene text style
transformation, outperforming existing state-of-the-art methods in both visual
fidelity and text preservation.

</details>


### [108] [IUT-Plug: A Plug-in tool for Interleaved Image-Text Generation](https://arxiv.org/abs/2510.10969)
*Zeteng Lin,Xingxing Li,Wen You,Xiaoyang Li,Zehan Lu,Yujun Cai,Jing Tang*

Main category: cs.CV

TL;DR: IUT-Plug通过感知IUT来避免多模态图像-文本生成中的上下文漂移。


<details>
  <summary>Details</summary>
Motivation: 现有的视觉语言模型在多模态图像-文本生成中难以保留逻辑、对象识别和风格，这严重限制了它们在复杂输入输出场景中的泛化能力。

Method: IUT-Plug通过两个阶段操作来解决这个问题：首先，一个动态IUT-Plug提取模块将视觉场景解析成层次化的符号结构；其次，一个协调的叙事流和图像合成机制确保跨模态的一致性。

Result: IUT-Plug不仅提高了既有基准的准确性，而且有效缓解了多种多模态问答场景中的三种关键形式的上下文漂移。

Conclusion: IUT-Plug是一个基于IUT的模块，它通过显式的结构化推理来增强现有的交错式视觉语言模型，从而减轻逻辑、实体识别和风格方面的上下文漂移。

Abstract: Existing vision language models (VLMs), including GPT-4 and DALL-E, often
struggle to preserve logic, object identity, and style in multimodal image-text
generation. This limitation significantly hinders the generalization capability
of VLMs in complex image-text input-output scenarios. To address this issue, we
propose IUT-Plug, a module grounded in an Image Understanding Tree (IUT), which
enhances existing interleaved VLMs through explicit structured reasoning,
thereby mitigating context drift in logic, entity identity, and style. The
proposed framework operates in two stages. (1) A dynamic IUT-Plug extraction
module parses visual scenes into hierarchical symbolic structures. (2) A
coordinated narrative-flow and image synthesis mechanism ensures cross-modal
consistency. To evaluate our approach, we construct a novel benchmark based on
3,000 real human-generated question-answer pairs over fine-tuned large models,
introducing a dynamic evaluation protocol for quantifying context drift in
interleaved VLMs. Experimental results demonstrate that IUT-Plug not only
improves accuracy on established benchmarks but also effectively alleviates the
three critical forms of context drift across diverse multimodal question
answering (QA) scenarios.

</details>


### [109] [Chart-RVR: Reinforcement Learning with Verifiable Rewards for Explainable Chart Reasoning](https://arxiv.org/abs/2510.10973)
*Sanchit Sinha,Oana Frunza,Kashif Rasul,Yuriy Nevmyvaka,Aidong Zhang*

Main category: cs.CV

TL;DR: Chart-RVR是一个通用的框架，它通过将Group Relative Policy Optimization（GRPO）与自动可验证奖励相结合，对LVLM进行微调，使其在图表推理方面更加稳健和可解释，有效解决了现有的LVLM在OOD数据上表现不佳并降低可解释性的问题。


<details>
  <summary>Details</summary>
Motivation: 现有的LVLM在处理图表推理任务时，在遇到分布外（OOD）数据时性能会显著下降，并且在生成思维链（CoT）理由时表现更差，这限制了模型的可解释性。

Method: Chart-RVR框架包含三种奖励机制，旨在最大化：(i) 正确的图表类型分类，(ii) 忠实的图表表格重建，以及(iii) 过程一致性。该框架将Group Relative Policy Optimization（GRPO）与自动可验证奖励相结合，对LVLM进行微调。

Result: Chart-RVR在30亿参数的LVLM上应用，持续优于标准的监督微调（SFT）方法，无论是在分布内还是分布外数据集中，都缩小了OOD性能差距，同时提高了理由的忠实性。Chart-RVR-3B系列模型在六个图表推理基准测试中取得了最先进的结果，涵盖了域内和OOD设置，超越了所有现有同等规模的模型。

Conclusion: Chart-RVR框架通过结合GRPO和可验证奖励，成功训练出可靠、可解释的图表推理模型，不仅提高了模型在OOD数据上的性能，还增强了其生成CoT理由的质量和可信度。

Abstract: The capabilities of Large Vision-Language Models (LVLMs) have reached
state-of-the-art on many visual reasoning tasks, including chart reasoning, yet
they still falter on out-of-distribution (OOD) data, and degrade further when
asked to produce their chain-of-thought (CoT) rationales, limiting
explainability. We present Chart-RVR, a general framework that fine-tunes LVLMs
to be more robust and explainable for chart reasoning by coupling Group
Relative Policy Optimization (GRPO) with automatically verifiable rewards. Our
framework comprises of three rewards that maximize: (i) correct chart-type
classification, (ii) faithful chart table reconstruction, and (iii) process
conformity. Applied to 3-billion-parameter LVLMs, Chart-RVR consistently
outperforms standard supervised fine-tuning (SFT) on both in-distribution and
out-of-distribution datasets, closing the OOD performance gap while improving
rationale fidelity. The resulting models, the Chart-RVR-3B series, achieve
state-of-the-art results on six chart-reasoning benchmarks spanning in-domain
and OOD settings, surpassing all existing models of comparable size. Beyond
accuracy, Chart-RVR yields more interpretable CoT rationales, strengthening
trust and reliability - showcasing the power of verifiable rewards with GRPO
for training reliable, interpretable chart-reasoning models.

</details>


### [110] [Mixup Helps Understanding Multimodal Video Better](https://arxiv.org/abs/2510.10986)
*Xiaoyu Ma,Ding Ding,Hao Chen*

Main category: cs.CV

TL;DR: 该论文提出并改进了一种新的多模态视频理解方法，旨在解决多模态模型中存在的过拟合和模态不平衡问题。


<details>
  <summary>Details</summary>
Motivation: 多模态模型容易过度拟合到强势模态，从而抑制弱势模态的贡献。

Method: 首先提出多模态 Mixup（MM），在聚合的多模态特征层面应用 Mixup 策略。在此基础上，进一步引入平衡多模态 Mixup（B-MM），它根据每种模态对学习目标的相对贡献动态调整混合比例。

Result: 在多个数据集上的大量实验证明了该方法在提高泛化能力和多模态鲁棒性方面的有效性。

Conclusion: 通过引入多模态 Mixup 和平衡多模态 Mixup，有效解决了多模态视频理解中存在的过拟合和模态不平衡问题，显著提升了模型的泛化能力和鲁棒性。

Abstract: Multimodal video understanding plays a crucial role in tasks such as action
recognition and emotion classification by combining information from different
modalities. However, multimodal models are prone to overfitting strong
modalities, which can dominate learning and suppress the contributions of
weaker ones. To address this challenge, we first propose Multimodal Mixup (MM),
which applies the Mixup strategy at the aggregated multimodal feature level to
mitigate overfitting by generating virtual feature-label pairs. While MM
effectively improves generalization, it treats all modalities uniformly and
does not account for modality imbalance during training. Building on MM, we
further introduce Balanced Multimodal Mixup (B-MM), which dynamically adjusts
the mixing ratios for each modality based on their relative contributions to
the learning objective. Extensive experiments on several datasets demonstrate
the effectiveness of our methods in improving generalization and multimodal
robustness.

</details>


### [111] [Perspective-aware 3D Gaussian Inpainting with Multi-view Consistency](https://arxiv.org/abs/2510.10993)
*Yuxin Cheng,Binxiao Huang,Taiqiang Wu,Wenyong Zhou,Chenchen Ding,Zhengwu Liu,Graziano Chesi,Ngai Wong*

Main category: cs.CV

TL;DR: 本文提出PAInpainter，一种新的3D高斯修复方法，利用视角感知内容传播和多视角修复图像上的一致性验证来提升修复质量和纹理保真度。


<details>
  <summary>Details</summary>
Motivation: 3D高斯修复在虚拟现实和多媒体应用中至关重要，但如何确保多视角一致性仍是关键挑战。

Method: PAInpainter通过迭代优化修复和3D高斯表示，并从视角图中自适应采样多个视角。它将修复后的图像作为先验信息进行传播，并验证相邻视角之间的一致性。

Result: PAInpainter在SPIn-NeRF数据集上的PSNR得分达到26.03 dB，在NeRFiller数据集上达到29.51 dB，验证了其有效性和泛化能力，优于现有方法。

Conclusion: PAInpainter通过视角感知内容传播和一致性验证，显著增强了恢复3D场景的全局一致性和纹理保真度，极大地提升了3D修复质量。

Abstract: 3D Gaussian inpainting, a critical technique for numerous applications in
virtual reality and multimedia, has made significant progress with pretrained
diffusion models. However, ensuring multi-view consistency, an essential
requirement for high-quality inpainting, remains a key challenge. In this work,
we present PAInpainter, a novel approach designed to advance 3D Gaussian
inpainting by leveraging perspective-aware content propagation and consistency
verification across multi-view inpainted images. Our method iteratively refines
inpainting and optimizes the 3D Gaussian representation with multiple views
adaptively sampled from a perspective graph. By propagating inpainted images as
prior information and verifying consistency across neighboring views,
PAInpainter substantially enhances global consistency and texture fidelity in
restored 3D scenes. Extensive experiments demonstrate the superiority of
PAInpainter over existing methods. Our approach achieves superior 3D inpainting
quality, with PSNR scores of 26.03 dB and 29.51 dB on the SPIn-NeRF and
NeRFiller datasets, respectively, highlighting its effectiveness and
generalization capability.

</details>


### [112] [ContextGen: Contextual Layout Anchoring for Identity-Consistent Multi-Instance Generation](https://arxiv.org/abs/2510.11000)
*Ruihang Xu,Dewei Zhou,Fan Ma,Yi Yang*

Main category: cs.CV

TL;DR: ContextGen是一个针对多实例图像生成（MIG）的扩散Transformer框架，通过引入Contextual Layout Anchoring（CLA）机制和Identity Consistency Attention（ICA）机制，解决了现有扩散模型在精确控制对象布局和保持多个不同主体身份方面的不足。


<details>
  <summary>Details</summary>
Motivation: 现代扩散模型在多实例图像生成（MIG）中，由于难以精确控制对象布局和保持多个不同主体的身份，面临着重大挑战。

Method: 本文引入了ContextGen，这是一个由布局和参考图像共同指导的新型扩散Transformer框架。它包含两个关键技术贡献：1. Contextual Layout Anchoring（CLA）机制，将复合布局图像整合到生成上下文中，以牢固固定对象在所需位置；2. Identity Consistency Attention（ICA）机制，利用上下文参考图像确保多个实例的身份一致性。同时，本文还推出了IMIG-100K，一个包含详细布局和身份注释的大规模分层数据集，以解决该任务缺乏此类数据集的问题。

Result: 实验结果表明，ContextGen在控制精度、身份保真度和整体视觉质量方面超越了现有方法，达到了新的技术水平。

Conclusion: ContextGen通过其独特的CLA和ICA机制，以及IMIG-100K数据集的构建，成功解决了多实例图像生成中的布局控制和身份保持难题，并在多项指标上表现出卓越的性能。

Abstract: Multi-instance image generation (MIG) remains a significant challenge for
modern diffusion models due to key limitations in achieving precise control
over object layout and preserving the identity of multiple distinct subjects.
To address these limitations, we introduce ContextGen, a novel Diffusion
Transformer framework for multi-instance generation that is guided by both
layout and reference images. Our approach integrates two key technical
contributions: a Contextual Layout Anchoring (CLA) mechanism that incorporates
the composite layout image into the generation context to robustly anchor the
objects in their desired positions, and Identity Consistency Attention (ICA),
an innovative attention mechanism that leverages contextual reference images to
ensure the identity consistency of multiple instances. Recognizing the lack of
large-scale, hierarchically-structured datasets for this task, we introduce
IMIG-100K, the first dataset with detailed layout and identity annotations.
Extensive experiments demonstrate that ContextGen sets a new state-of-the-art,
outperforming existing methods in control precision, identity fidelity, and
overall visual quality.

</details>


### [113] [Frequency Domain Unlocks New Perspectives for Abdominal Medical Image Segmentation](https://arxiv.org/abs/2510.11005)
*Kai Han,Siqi Ma,Chengxuan Qian,Jun Chen,Chongwen Lyu,Yuqing Song,Zhe Liu*

Main category: cs.CV

TL;DR: 该文章提出了一个名为FASS的框架，该框架通过引入前背景感知模块、特征级频率增强模块和边缘约束模块，解决了在复杂、低对比度背景下医学图像肿瘤分割的挑战。


<details>
  <summary>Details</summary>
Motivation: 尽管基础模型在分割任务中表现良好，但在复杂、低对比度的背景下，它们往往难以聚焦于前景区域，因为某些恶性肿瘤与正常器官非常相似，这使得上下文区分变得复杂。

Method: 本文提出了一个名为FASS的框架。首先，引入了一个前背景感知模块，以扩大背景和整个体空间之间的区别，使模型能够更有效地集中于目标区域。其次，一个基于小波变换的特征级频率增强模块提取判别性的高频特征，以增强边界识别和细节感知。最后，引入了一个边缘约束模块，以保持分割边界的几何连续性。

Result: 在多个医学数据集上进行的大量实验表明，FASS框架在所有指标上都表现出优越的性能，验证了其有效性，特别是在复杂条件下的鲁棒性和精细结构识别方面。

Conclusion: FASS框架显著增强了低对比度图像的分割，为在更_多_样化和复杂的医学成像场景中的应用铺平了道路。

Abstract: Accurate segmentation of tumors and adjacent normal tissues in medical images
is essential for surgical planning and tumor staging. Although foundation
models generally perform well in segmentation tasks, they often struggle to
focus on foreground areas in complex, low-contrast backgrounds, where some
malignant tumors closely resemble normal organs, complicating contextual
differentiation. To address these challenges, we propose the Foreground-Aware
Spectrum Segmentation (FASS) framework. First, we introduce a foreground-aware
module to amplify the distinction between background and the entire volume
space, allowing the model to concentrate more effectively on target areas.
Next, a feature-level frequency enhancement module, based on wavelet transform,
extracts discriminative high-frequency features to enhance boundary recognition
and detail perception. Eventually, we introduce an edge constraint module to
preserve geometric continuity in segmentation boundaries. Extensive experiments
on multiple medical datasets demonstrate superior performance across all
metrics, validating the effectiveness of our framework, particularly in
robustness under complex conditions and fine structure recognition. Our
framework significantly enhances segmentation of low-contrast images, paving
the way for applications in more diverse and complex medical imaging scenarios.

</details>


### [114] [COCO-Tree: Compositional Hierarchical Concept Trees for Enhanced Reasoning in Vision Language Models](https://arxiv.org/abs/2510.11012)
*Sanchit Sinha,Guangzhi Xiong,Aidong Zhang*

Main category: cs.CV

TL;DR: COCO-Tree通过结合大型语言模型（LLM）学习到的神经符号概念树来增强视觉语言模型（VLM）的输出，从而提高其组合推理能力和预测的可解释性，并在多个组合性基准测试中取得了显著的性能提升。


<details>
  <summary>Details</summary>
Motivation: 现代视觉语言模型（VLM）在理解图像中多个对象、属性和关系如何相互作用时，在组合推理方面表现出持续的弱点。现有的改善方法要么资源密集，要么缺乏可解释的推理过程。

Method: 本文提出了“COCO-Tree”，一种新颖的方法。它将VLM的输出与从大型语言模型（LLM）中学习到的精心设计的神经符号概念树相结合，以增强VLM的语言推理能力。COCO-Tree采用受束搜索启发的推理过程。

Result: COCO-Tree在Winoground、EqBench、ColorSwap和SugarCrepe这四个组合性基准测试上，使用七种不同大小的开源VLM进行了实证测试。结果表明，COCO-Tree将组合泛化性能在基线基础上显著提高了5-10%。

Conclusion: COCO-Tree通过结合LLMs学习到的神经符号概念树，显著改善了VLMs的组合推理性能，并为VLM的预测提供了可解释的推理过程。

Abstract: Compositional reasoning remains a persistent weakness of modern vision
language models (VLMs): they often falter when a task hinges on understanding
how multiple objects, attributes, and relations interact within an image.
Multiple research works have attempted to improve compositionality performance
by creative tricks such as improving prompt structure, chain of thought
reasoning, etc. A more recent line of work attempts to impart additional
reasoning in VLMs using well-trained Large Language Models (LLMs), which are
far superior in linguistic understanding than VLMs to compensate for the
limited linguistic prowess of VLMs. However, these approaches are either
resource-intensive or do not provide an interpretable reasoning process. In
this paper, we present 'COCO-Tree' - a novel approach that augments VLM outputs
with carefully designed neurosymbolic concept trees learned from LLMs to
improve VLM's linguistic reasoning. COCO-Tree's beam search-inspired reasoning
process boosts compositionality performance and provides a rationale behind VLM
predictions. Empirical results on four compositionality benchmarks, Winoground,
EqBench, ColorSwap, and SugarCrepe, in seven different open-source VLMs with
varying sizes, demonstrate that COCO-Tree significantly improves compositional
generalization by 5-10% over baselines.

</details>


### [115] [High-Resolution Spatiotemporal Modeling with Global-Local State Space Models for Video-Based Human Pose Estimation](https://arxiv.org/abs/2510.11017)
*Runyang Feng,Hyung Jin Chang,Tze Ho Elden Tse,Boeun Kim,Yi Chang,Yixing Gao*

Main category: cs.CV

TL;DR: 本文提出了一种新颖的框架，通过扩展Mamba模型，分别学习全局和局部高分辨率时空表示，用于视频人体姿态估计（VHPE）。


<details>
  <summary>Details</summary>
Motivation: 现有的视频人体姿态估计方法在平衡全局和局部动态建模方面存在困难，并且在捕获全局依赖时复杂度较高，限制了其在高分辨率序列中的应用。

Method: 本文提出了一个全局时空Mamba模块，通过6D选择性时空扫描和时空调制扫描合并来高效提取全局表示。同时引入了基于窗口时空扫描的局部细化Mamba模块，以增强局部关键点运动的高频细节。

Result: 在四个基准数据集上的大量实验表明，所提出的模型优于现有的视频人体姿态估计方法，并实现了更好的计算效率。

Conclusion: 本文通过扩展Mamba模型，成功解决了现有VHPE方法在平衡全局和局部动态建模以及处理高分辨率序列时计算复杂度高的问题，为视频人体姿态估计提供了一个更高效、性能更优越的解决方案。

Abstract: Modeling high-resolution spatiotemporal representations, including both
global dynamic contexts (e.g., holistic human motion tendencies) and local
motion details (e.g., high-frequency changes of keypoints), is essential for
video-based human pose estimation (VHPE). Current state-of-the-art methods
typically unify spatiotemporal learning within a single type of modeling
structure (convolution or attention-based blocks), which inherently have
difficulties in balancing global and local dynamic modeling and may bias the
network to one of them, leading to suboptimal performance. Moreover, existing
VHPE models suffer from quadratic complexity when capturing global
dependencies, limiting their applicability especially for high-resolution
sequences. Recently, the state space models (known as Mamba) have demonstrated
significant potential in modeling long-range contexts with linear complexity;
however, they are restricted to 1D sequential data. In this paper, we present a
novel framework that extends Mamba from two aspects to separately learn global
and local high-resolution spatiotemporal representations for VHPE.
Specifically, we first propose a Global Spatiotemporal Mamba, which performs 6D
selective space-time scan and spatial- and temporal-modulated scan merging to
efficiently extract global representations from high-resolution sequences. We
further introduce a windowed space-time scan-based Local Refinement Mamba to
enhance the high-frequency details of localized keypoint motions. Extensive
experiments on four benchmark datasets demonstrate that the proposed model
outperforms state-of-the-art VHPE approaches while achieving better
computational trade-offs.

</details>


### [116] [GIR-Bench: Versatile Benchmark for Generating Images with Reasoning](https://arxiv.org/abs/2510.11026)
*Hongxiang Li,Yaowei Li,Bin Lin,Yuwei Niu,Yuhang Yang,Xiaoshuang Huang,Jiayin Cai,Xiaolong Jiang,Yao Hu,Long Chen*

Main category: cs.CV

TL;DR: GIR-Bench是一个全面的基准，它从理解-生成一致性、推理驱动的文本到图像生成和多步编辑推理三个互补的角度评估统一的多模态模型。评估结果表明，统一模型在理解和生成之间存在持续的差距。


<details>
  <summary>Details</summary>
Motivation: 社区缺乏一个以推理为中心的严格基准来系统地评估理解和生成之间的一致性，以及它们在复杂视觉任务中的泛化潜力。

Method: GIR-Bench从三个互补的角度评估统一模型：1. 理解-生成一致性（GIR-Bench-UGC）：模型能否在理解和生成任务中一致地利用相同的知识。2. 推理驱动的文本到图像生成（GIR-Bench-T2I）：模型能否执行需要应用逻辑约束和隐式知识来生成忠实视觉内容的以推理为中心的文本到图像生成。3. 多步编辑推理（GIR-Bench-Edit）：模型能否处理多步编辑推理。针对每个子集，我们精心设计了不同的任务特定评估流程。

Result: 尽管统一模型更能胜任推理驱动的视觉任务，但它们在理解和生成之间仍然存在持续的差距。

Conclusion: 统一的多模态模型在理解和生成之间仍存在差距，需要进一步的研究来弥合这一差距，以实现更先进的多模态智能。

Abstract: Unified multimodal models integrate the reasoning capacity of large language
models with both image understanding and generation, showing great promise for
advanced multimodal intelligence. However, the community still lacks a rigorous
reasoning-centric benchmark to systematically evaluate the alignment between
understanding and generation, and their generalization potential in complex
visual tasks. To this end, we introduce \textbf{GIR-Bench}, a comprehensive
benchmark that evaluates unified models across three complementary
perspectives. Firstly, we investigate understanding-generation consistency
(GIR-Bench-UGC), asking whether models can consistently leverage the same
knowledge in both understanding and generation tasks. Secondly, we investigate
whether models can perform reasoning-centric text-to-image generation that
requires applying logical constraints and implicit knowledge to generate
faithful visual content (GIR-Bench-T2I). Thirdly, we evaluate whether models
can handle multi-step reasoning in editing (GIR-Bench-Edit). For each subset,
we carefully design different task-specific evaluation pipelines tailored for
each task. This enables fine-grained and interpretable evaluation while
mitigating biases from the prevalent MLLM-as-a-Judge paradigm. Extensive
ablations over various unified models and generation-only systems have shown
that: Although unified models are more capable of reasoning-driven visual
tasks, they still exhibit a persistent gap between understanding and
generation. The data and code for GIR-Bench are available at
\href{https://hkust-longgroup.github.io/GIR-Bench}{https://hkust-longgroup.github.io/GIR-Bench}.

</details>


### [117] [Enhancing Zero-Shot Anomaly Detection: CLIP-SAM Collaboration with Cascaded Prompts](https://arxiv.org/abs/2510.11028)
*Yanning Hou,Ke Xu,Junfa Li,Yanran Ruan,Jianfeng Qiu*

Main category: cs.CV

TL;DR: 这篇论文提出了一个两阶段的零样本异常分割框架，用于工业异常检测，该框架结合了CLIP和SAM的优势，并通过Co-Feature Point Prompt Generation（PPG）模块和Cascaded Prompts for SAM（CPS）模块分别解决了SAM的目标分割倾向和优化了分割结果，最终在多个数据集上实现了最先进的性能，特别是在Visa数据集上F1-max和AP指标分别优于现有方法10.3%和7.7%。


<details>
  <summary>Details</summary>
Motivation: 基础模型在零样本异常分割任务中展现了强大的泛化能力，但如何正确引导这些基础模型以解决下游任务仍然是一个挑战。

Method: 本研究提出了一个新颖的两阶段框架，用于工业异常检测中的零样本异常分割任务。该框架利用了CLIP强大的异常定位能力和SAM的边界感知能力。为了解决SAM偏向于目标分割的问题，提出了Co-Feature Point Prompt Generation（PPG）模块，该模块协同利用CLIP和SAM生成正负点提示，引导SAM专注于分割异常区域而非整个物体。为了进一步优化SAM的分割结果，解决粗糙边界和孤立噪声问题，引入了Cascaded Prompts for SAM（CPS）模块，该模块采用混合提示与SAM的轻量级解码器级联，实现对异常区域的精确分割。

Result: 在多个数据集上，实验验证表明该方法在零样本异常分割方面取得了最先进的结果。特别是在Visa数据集上，F1-max和AP指标分别比现有最佳方法提高了10.3%和7.7%。

Conclusion: 本论文提出的两阶段框架有效解决了零样本异常分割任务中的挑战，通过精心设计的PPG和CPS模块，成功利用并优化了CLIP和SAM的优势，实现了卓越的异常分割性能。

Abstract: Recently, the powerful generalization ability exhibited by foundation models
has brought forth new solutions for zero-shot anomaly segmentation tasks.
However, guiding these foundation models correctly to address downstream tasks
remains a challenge. This paper proposes a novel two-stage framework, for
zero-shot anomaly segmentation tasks in industrial anomaly detection. This
framework excellently leverages the powerful anomaly localization capability of
CLIP and the boundary perception ability of SAM.(1) To mitigate SAM's
inclination towards object segmentation, we propose the Co-Feature Point Prompt
Generation (PPG) module. This module collaboratively utilizes CLIP and SAM to
generate positive and negative point prompts, guiding SAM to focus on
segmenting anomalous regions rather than the entire object. (2) To further
optimize SAM's segmentation results and mitigate rough boundaries and isolated
noise, we introduce the Cascaded Prompts for SAM (CPS) module. This module
employs hybrid prompts cascaded with a lightweight decoder of SAM, achieving
precise segmentation of anomalous regions. Across multiple datasets, consistent
experimental validation demonstrates that our approach achieves
state-of-the-art zero-shot anomaly segmentation results. Particularly
noteworthy is our performance on the Visa dataset, where we outperform the
state-of-the-art methods by 10.3\% and 7.7\% in terms of {$F_1$-max} and AP
metrics, respectively.

</details>


### [118] [Benchmarking Deep Learning Models for Laryngeal Cancer Staging Using the LaryngealCT Dataset](https://arxiv.org/abs/2510.11047)
*Nivea Roy,Son Tran,Atul Sajjanhar,K. Devaraja,Prakashini Koteshwara,Yong Xiang,Divya Rao*

Main category: cs.CV

TL;DR: LaryngealCT是一个包含1029个CT扫描的喉癌图像数据集，用于深度学习模型开发。研究基于此数据集对不同DL模型进行了性能比较，并利用3D GradCAMs进行了模型可解释性分析。


<details>
  <summary>Details</summary>
Motivation: 喉癌影像研究缺乏标准化的数据集来支持可复现的深度学习模型开发。

Method: LaryngealCT数据集：收集了来自The Cancer Imaging Archive（TCIA）的六个数据集中的1029个CT扫描，并通过弱监督参数搜索框架提取了包含喉部的1毫米各向同性体积区域。模型比较：使用了3D CNN、ResNet18、50、101和DenseNet121等3D深度学习架构，在早期（Tis,T1,T2）与晚期（T3,T4）以及T4与非T4分类任务上进行基准测试。模型可解释性：使用带有甲状软骨叠加的3D GradCAMs评估模型。

Result: 在早期与晚期分类任务中，3D CNN表现最佳（AUC-0.881，F1-macro-0.821）。在T4与非T4分类任务中，ResNet18表现最佳（AUC-0.892，F1-macro-0.646）。模型可解释性分析显示，在非T4病例中，模型对软骨周围区域关注度更高，而在T4预测中，模型表现出局灶性激活。

Conclusion: LaryngealCT数据集、预训练模型和集成可解释性工具为喉癌肿瘤学领域的AI驱动研究提供了一个可复现的基础，以支持临床决策。

Abstract: Laryngeal cancer imaging research lacks standardised datasets to enable
reproducible deep learning (DL) model development. We present LaryngealCT, a
curated benchmark of 1,029 computed tomography (CT) scans aggregated from six
collections from The Cancer Imaging Archive (TCIA). Uniform 1 mm isotropic
volumes of interest encompassing the larynx were extracted using a weakly
supervised parameter search framework validated by clinical experts. 3D DL
architectures (3D CNN, ResNet18,50,101, DenseNet121) were benchmarked on (i)
early (Tis,T1,T2) vs. advanced (T3,T4) and (ii) T4 vs. non-T4 classification
tasks. 3D CNN (AUC-0.881, F1-macro-0.821) and ResNet18 (AUC-0.892,
F1-macro-0.646) respectively outperformed the other models in the two tasks.
Model explainability assessed using 3D GradCAMs with thyroid cartilage overlays
revealed greater peri-cartilage attention in non-T4 cases and focal activations
in T4 predictions. Through open-source data, pretrained models, and integrated
explainability tools, LaryngealCT offers a reproducible foundation for
AI-driven research to support clinical decisions in laryngeal oncology.

</details>


### [119] [Zero-shot Face Editing via ID-Attribute Decoupled Inversion](https://arxiv.org/abs/2510.11050)
*Yang Hou,Minggu Wang,Jianjun Zhao*

Main category: cs.CV

TL;DR: 该论文提出了一种基于ID-属性解耦反演的零样本人脸编辑方法，解决了现有文本引导扩散模型在人脸编辑中难以保持身份和结构一致性的问题。


<details>
  <summary>Details</summary>
Motivation: 现有的文本引导扩散模型在通过反演技术进行通用图像编辑时，在真实人脸编辑任务中难以保持身份（ID）和结构一致性。

Method: 本文提出了一种零样本人脸编辑方法，称为ID-属性解耦反演。该方法将人脸表示分解为ID特征和属性特征，并将它们作为联合条件来指导反演和逆向扩散过程。

Result: 该方法能够独立控制ID和属性，确保了强大的ID H / /保留和结构一致性，同时实现了精确的面部属性 T / /操纵。它支持使用文本提示进行广泛的复杂多属性人脸编辑任务，而不需要区域特定的输入，并且其运行速度与DDIM反演相当。

Conclusion: 综合实验证明了该方法的 P / /实用性和有效性。

Abstract: Recent advancements in text-guided diffusion models have shown promise for
general image editing via inversion techniques, but often struggle to maintain
ID and structural consistency in real face editing tasks. To address this
limitation, we propose a zero-shot face editing method based on ID-Attribute
Decoupled Inversion. Specifically, we decompose the face representation into ID
and attribute features, using them as joint conditions to guide both the
inversion and the reverse diffusion processes. This allows independent control
over ID and attributes, ensuring strong ID preservation and structural
consistency while enabling precise facial attribute manipulation. Our method
supports a wide range of complex multi-attribute face editing tasks using only
text prompts, without requiring region-specific input, and operates at a speed
comparable to DDIM inversion. Comprehensive experiments demonstrate its
practicality and effectiveness.

</details>


### [120] [LSVOS 2025 Challenge Report: Recent Advances in Complex Video Object Segmentation](https://arxiv.org/abs/2510.11063)
*Chang Liu,Henghui Ding,Kaining Ying,Lingyi Hong,Ning Xu,Linjie Yang,Yuchen Fan,Mingqi Gao,Jingkun Chen,Yunqi Miao,Gengshen Wu,Zhijin Qin,Jungong Han,Zhixiong Zhang,Shuangrui Ding,Xiaoyi Dong,Yuhang Zang,Yuhang Cao,Jiaqi Wang,Chang Soo Lim,Joonyoung Moon,Donghyeon Cho,Tingmin Li,Yixuan Li,Yang Yang,An Yan,Leilei Cao,Feng Lu,Ran Hong,Youhai Jiang,Fengjie Zhu,Yujie Xie,Hongyang Zhang,Zhihui Liu,Shihai Ruan,Quanzhu Niu,Dengxian Gong,Shihao Chen,Tao Zhang,Yikang Zhou,Haobo Yuan,Lu Qi,Xiangtai Li,Shunping Ji,Ran Hong,Feng Lu,Leilei Cao,An Yan,Alexey Nekrasov,Ali Athar,Daan de Geus,Alexander Hermans,Bastian Leibe*

Main category: cs.CV

TL;DR: 介绍了ICCV 2025年第7届LSVOS挑战赛，包括VOS、RVOS以及新增的MOSEv2赛道，并讨论了其难度、评估指标和新兴趋势。


<details>
  <summary>Details</summary>
Motivation: 推动视频对象分割技术在更复杂和真实场景下的鲁棒性、长期一致性和泛化能力。

Method: 本次挑战赛设立了三个赛道：经典VOS、参照VOS（RVOS）和新增的复杂VOS（MOSEv2）。MOSEv2引入了更具挑战性的真实场景，如密集小目标、频繁出现/消失事件、严重遮挡、恶劣天气和光照等。评估指标方面，VOS和RVOS沿用标准的J、F和J&F指标，MOSEv2则采用J&F作为主要排名指标，以更好地评估跨尺度和消失情况下的对象。

Result: 总结了数据集和协议，强调了表现最佳的解决方案，并提炼出新兴趋势，如LLM/MLLM组件和内存感知传播的日益重要的作用。

Conclusion: 展望了未来在真实世界中实现弹性、语言感知视频分割的方向。

Abstract: This report presents an overview of the 7th Large-scale Video Object
Segmentation (LSVOS) Challenge held in conjunction with ICCV 2025. Besides the
two traditional tracks of LSVOS that jointly target robustness in realistic
video scenarios: Classic VOS (VOS), and Referring VOS (RVOS), the 2025 edition
features a newly introduced track, Complex VOS (MOSEv2). Building upon prior
insights, MOSEv2 substantially increases difficulty, introducing more
challenging but realistic scenarios including denser small objects, frequent
disappear/reappear events, severe occlusions, adverse weather and lighting,
etc., pushing long-term consistency and generalization beyond curated
benchmarks. The challenge retains standard ${J}$, $F$, and ${J\&F}$ metrics for
VOS and RVOS, while MOSEv2 adopts ${J\&\dot{F}}$ as the primary ranking metric
to better evaluate objects across scales and disappearance cases. We summarize
datasets and protocols, highlight top-performing solutions, and distill
emerging trends, such as the growing role of LLM/MLLM components and
memory-aware propagation, aiming to chart future directions for resilient,
language-aware video segmentation in the wild.

</details>


### [121] [ROFI: A Deep Learning-Based Ophthalmic Sign-Preserving and Reversible Patient Face Anonymizer](https://arxiv.org/abs/2510.11073)
*Yuan Tian,Min Zhou,Yitong Chen,Fang Li,Lingzi Qi,Shuo Wang,Xieyang Xu,Yu Yu,Shiqiong Xu,Chaoyu Lei,Yankai Jiang,Rongzhao Zhang,Jia Tan,Li Wu,Hong Chen,Xiaowei Liu,Wei Lu,Lin Li,Huifang Zhou,Xuefei Song,Guangtao Zhai,Xianqun Fan*

Main category: cs.CV

TL;DR: ROFI是一个基于深度学习的隐私保护框架，用于眼科图像，可以在匿名化面部特征的同时保留疾病特征，实现高诊断准确性，并支持AI系统和安全图像逆转。


<details>
  <summary>Details</summary>
Motivation: 在眼科疾病评估中，患者面部图像提供便利但存在隐私问题，因此需要一种方法来保护患者隐私。

Method: ROFI框架使用弱监督学习和神经身份转换来匿名化面部特征，同时保留疾病特征。

Result: ROFI在匿名化超过95%图像的同时，在十一种眼科疾病上实现了100%的诊断敏感度和高一致性（κ > 0.90），并保持了98%以上的精度，AI系统诊断一致性（κ > 0.80），并支持超过98%相似度的安全图像逆转。

Conclusion: ROFI有效保护了数字医疗时代下的患者隐私。

Abstract: Patient face images provide a convenient mean for evaluating eye diseases,
while also raising privacy concerns. Here, we introduce ROFI, a deep
learning-based privacy protection framework for ophthalmology. Using weakly
supervised learning and neural identity translation, ROFI anonymizes facial
features while retaining disease features (over 98\% accuracy, $\kappa >
0.90$). It achieves 100\% diagnostic sensitivity and high agreement ($\kappa >
0.90$) across eleven eye diseases in three cohorts, anonymizing over 95\% of
images. ROFI works with AI systems, maintaining original diagnoses ($\kappa >
0.80$), and supports secure image reversal (over 98\% similarity), enabling
audits and long-term care. These results show ROFI's effectiveness of
protecting patient privacy in the digital medicine era.

</details>


### [122] [Future-Aware End-to-End Driving: Bidirectional Modeling of Trajectory Planning and Scene Evolution](https://arxiv.org/abs/2510.11092)
*Bozhou Zhang,Nan Song,Jingyu Li,Xiatian Zhu,Jiankang Deng,Li Zhang*

Main category: cs.CV

TL;DR: 该论文提出了SeerDrive，一个端到端自动驾驶框架，通过联合建模未来场景演变和轨迹规划，显著优于现有技术。


<details>
  <summary>Details</summary>
Motivation: 传统的端到端自动驾驶方法过度依赖当前场景，低估了场景动态及其时间演变的重要性。这限制了模型在复杂驾驶场景中做出明智和适应性决策的能力。

Method: SeerDrive首先预测未来鸟瞰图（BEV）表示，以预测周围场景的动态，然后利用这种预见性生成未来感知轨迹。该方法通过两个关键组件实现：1）未来感知规划，将预测的BEV特征注入轨迹规划器；2）迭代场景建模和车辆规划，通过协同优化来优化未来场景预测和轨迹生成。

Result: 在NAVSIM和nuScenes基准上的大量实验表明，SeerDrive显著优于现有最先进的方法。

Conclusion: SeerDrive通过联合建模未来场景演变和轨迹规划，解决了现有端到端自动驾驶方法中对场景动态和时间演变考虑不足的问题，实现了更明智和适应性的驾驶决策。

Abstract: End-to-end autonomous driving methods aim to directly map raw sensor inputs
to future driving actions such as planned trajectories, bypassing traditional
modular pipelines. While these approaches have shown promise, they often
operate under a one-shot paradigm that relies heavily on the current scene
context, potentially underestimating the importance of scene dynamics and their
temporal evolution. This limitation restricts the model's ability to make
informed and adaptive decisions in complex driving scenarios. We propose a new
perspective: the future trajectory of an autonomous vehicle is closely
intertwined with the evolving dynamics of its environment, and conversely, the
vehicle's own future states can influence how the surrounding scene unfolds.
Motivated by this bidirectional relationship, we introduce SeerDrive, a novel
end-to-end framework that jointly models future scene evolution and trajectory
planning in a closed-loop manner. Our method first predicts future bird's-eye
view (BEV) representations to anticipate the dynamics of the surrounding scene,
then leverages this foresight to generate future-context-aware trajectories.
Two key components enable this: (1) future-aware planning, which injects
predicted BEV features into the trajectory planner, and (2) iterative scene
modeling and vehicle planning, which refines both future scene prediction and
trajectory generation through collaborative optimization. Extensive experiments
on the NAVSIM and nuScenes benchmarks show that SeerDrive significantly
outperforms existing state-of-the-art methods.

</details>


### [123] [CoDefend: Cross-Modal Collaborative Defense via Diffusion Purification and Prompt Optimization](https://arxiv.org/abs/2510.11096)
*Fengling Zhu,Boshi Liu,Jingyu Hua,Sheng Zhong*

Main category: cs.CV

TL;DR: 本文提出了一个针对多模态大语言模型（MLLMs）视觉模态的防御框架，结合了有监督扩散去噪和提示优化，显著提高了模型对已知和未知对抗性攻击的鲁棒性，同时保证了图像重建质量。


<details>
  <summary>Details</summary>
Motivation: 多模态大语言模型（MLLMs）在集成视觉和文本模态方面取得了显著成功，但也容易受到对抗性攻击，攻击者可以扰动单一或同时扰动两种模态，从而导致有害、误导性或违反政策的输出。现有的防御策略（如对抗性训练和输入净化）存在局限性，例如计算成本高、泛化能力差和图像质量下降。

Method: 本文提出了一种有监督的、基于扩散的去噪框架，该框架利用成对的对抗性-干净图像数据集，通过定向的、任务特定的指导来微调扩散模型。与无监督的净化方法不同，该方法在显著提高多模态任务防御鲁棒性的同时，实现了更高质量的重建。此外，该方法还结合了提示优化作为辅助防御机制，以增强对多样化和未知攻击策略的抵抗力。

Result: 在图像字幕和视觉问答任务上的大量实验表明，该方法不仅显著提高了鲁棒性，而且对未知对抗性攻击具有很强的可迁移性。

Conclusion: 所提出的有监督扩散去噪方法在多模态防御中表现出有效性，为多模态大语言模型在实际应用中更可靠、更安全的部署铺平了道路。

Abstract: Multimodal Large Language Models (MLLMs) have achieved remarkable success in
tasks such as image captioning, visual question answering, and cross-modal
reasoning by integrating visual and textual modalities. However, their
multimodal nature also exposes them to adversarial threats, where attackers can
perturb either modality or both jointly to induce harmful, misleading, or
policy violating outputs. Existing defense strategies, such as adversarial
training and input purification, face notable limitations: adversarial training
typically improves robustness only against known attacks while incurring high
computational costs, whereas conventional purification approaches often suffer
from degraded image quality and insufficient generalization to complex
multimodal tasks.
  In this work, we focus on defending the visual modality, which frequently
serves as the primary entry point for adversarial manipulation. We propose a
supervised diffusion based denoising framework that leverages paired
adversarial clean image datasets to fine-tune diffusion models with
directional, task specific guidance. Unlike prior unsupervised purification
methods such as DiffPure, our approach achieves higher quality reconstructions
while significantly improving defense robustness in multimodal tasks.
Furthermore, we incorporate prompt optimization as a complementary defense
mechanism, enhancing resistance against diverse and unseen attack strategies.
  Extensive experiments on image captioning and visual question answering
demonstrate that our method not only substantially improves robustness but also
exhibits strong transferability to unknown adversarial attacks. These results
highlight the effectiveness of supervised diffusion based denoising for
multimodal defense, paving the way for more reliable and secure deployment of
MLLMs in real world applications.

</details>


### [124] [Compositional Zero-Shot Learning: A Survey](https://arxiv.org/abs/2510.11106)
*Ans Munir,Faisal Z. Qureshi,Mohsen Ali,Muhammad Haris Khan*

Main category: cs.CV

TL;DR: 这篇论文是对组合零样本学习（CZSL）的首次全面调查。


<details>
  <summary>Details</summary>
Motivation: CZSL在计算机视觉中至关重要，因为它允许模型识别未见过的属性和对象的组合，解决了为每个可能的组合都需要训练数据的组合挑战。特别的挑战在于，基元（primitives）的视觉外观具有高度的上下文依赖性，例如“小”猫与“老”猫在视觉上截然不同，“湿”车与“湿”猫也大相径庭。因此，有效地建模这种上下文性和固有的组合性对于鲁棒的组合零样本识别至关重要。

Method: 这篇论文系统地回顾了最新的CZSL方法，并引入了一种基于解耦的分类方法，包括四种方法：无显式解耦、文本解耦、视觉解耦和跨模态解耦。

Result: 论文详细比较分析了这些方法，强调了它们在不同问题设置（如封闭世界和开放世界CZSL）中的核心优势和局限性。

Conclusion: 论文最后指出了目前存在的重大开放挑战，并概述了未来有前景的研究方向。这项调查旨在为该领域提供基础性资源，以指导和启发未来的进一步发展。

Abstract: Compositional Zero-Shot Learning (CZSL) is a critical task in computer vision
that enables models to recognize unseen combinations of known attributes and
objects during inference, addressing the combinatorial challenge of requiring
training data for every possible composition. This is particularly challenging
because the visual appearance of primitives is highly contextual; for example,
``small'' cats appear visually distinct from ``older'' ones, and ``wet'' cars
differ significantly from ``wet'' cats. Effectively modeling this contextuality
and the inherent compositionality is crucial for robust compositional zero-shot
recognition. This paper presents, to our knowledge, the first comprehensive
survey specifically focused on Compositional Zero-Shot Learning. We
systematically review the state-of-the-art CZSL methods, introducing a taxonomy
grounded in disentanglement, with four families of approaches: no explicit
disentanglement, textual disentanglement, visual disentanglement, and
cross-modal disentanglement. We provide a detailed comparative analysis of
these methods, highlighting their core advantages and limitations in different
problem settings, such as closed-world and open-world CZSL. Finally, we
identify the most significant open challenges and outline promising future
research directions. This survey aims to serve as a foundational resource to
guide and inspire further advancements in this fascinating and important field.
Papers studied in this survey with their official code are available on our
github: https://github.com/ans92/Compositional-Zero-Shot-Learning

</details>


### [125] [MoMaps: Semantics-Aware Scene Motion Generation with Motion Maps](https://arxiv.org/abs/2510.11107)
*Jiahui Lei,Kyle Genova,George Kopanas,Noah Snavely,Leonidas Guibas*

Main category: cs.CV

TL;DR: 该论文介绍了一种从单个输入图像预测未来3D场景运动的方法。


<details>
  <summary>Details</summary>
Motivation: 作者旨在解决从真实世界视频中学习语义和功能上有意义的3D运动先验的挑战，以便能够从单个输入图像预测未来的3D场景运动。

Method: 作者提出了像素对齐的运动图（MoMap）表示，并从超过50000个真实视频中创建了一个大规模MoMap数据库，然后在此基础上训练了一个扩散模型。

Result: 该方法生成了合理且语义一致的3D场景运动。

Conclusion: 所提出的方法不仅可以合成3D轨迹，还可以为2D视频合成提供新的流程。

Abstract: This paper addresses the challenge of learning semantically and functionally
meaningful 3D motion priors from real-world videos, in order to enable
prediction of future 3D scene motion from a single input image. We propose a
novel pixel-aligned Motion Map (MoMap) representation for 3D scene motion,
which can be generated from existing generative image models to facilitate
efficient and effective motion prediction. To learn meaningful distributions
over motion, we create a large-scale database of MoMaps from over 50,000 real
videos and train a diffusion model on these representations. Our motion
generation not only synthesizes trajectories in 3D but also suggests a new
pipeline for 2D video synthesis: first generate a MoMap, then warp an image
accordingly and complete the warped point-based renderings. Experimental
results demonstrate that our approach generates plausible and semantically
consistent 3D scene motion.

</details>


### [126] [Multimodal Disease Progression Modeling via Spatiotemporal Disentanglement and Multiscale Alignment](https://arxiv.org/abs/2510.11112)
*Chen Liu,Wenfang Yao,Kejing Yin,William K. Cheung,Jing Qin*

Main category: cs.CV

TL;DR: DiPro是一个新型框架，它通过区域感知解缠和多时间尺度对齐来解决纵向多模态数据中的挑战，从而有效地提取时间临床动态，并在疾病进展识别和ICU预测任务中实现最先进的性能。


<details>
  <summary>Details</summary>
Motivation: 纵向多模态数据（包括电子健康记录（EHR）和连续胸部X光片（CXR））对于疾病进展建模至关重要，但由于以下两个关键挑战而未得到充分利用：1）连续CXR序列中的冗余，其中静态解剖区域在临床上有意义的动态中占据主导地位，2）稀疏、不规则的影像学检查与连续EHR数据之间的时间错位。

Method: DiPro框架通过区域感知解缠和多时间尺度对齐来解决这些挑战。首先，DiPro解缠了连续CXR中的静态（解剖）和动态（病理进展）特征，优先考虑与疾病相关的变化。其次，DiPro通过局部（成对间隔级）和全局（全序列）同步，将这些静态和动态CXR特征与异步EHR数据进行分层对齐，以建模连贯的进展路径。

Result: 在MIMIC数据集上进行了广泛的实验，结果表明DiPro可以有效地提取时间临床动态，并在疾病进展识别和通用ICU预测任务上实现最先进的性能。

Conclusion: DiPro框架通过解决纵向多模态数据中的冗余和时间错位问题，有效地利用了EHR和CXR数据，并显著提高了疾病进展建模和ICU预测的准确性。

Abstract: Longitudinal multimodal data, including electronic health records (EHR) and
sequential chest X-rays (CXRs), is critical for modeling disease progression,
yet remains underutilized due to two key challenges: (1) redundancy in
consecutive CXR sequences, where static anatomical regions dominate over
clinically-meaningful dynamics, and (2) temporal misalignment between sparse,
irregular imaging and continuous EHR data. We introduce $\texttt{DiPro}$, a
novel framework that addresses these challenges through region-aware
disentanglement and multi-timescale alignment. First, we disentangle static
(anatomy) and dynamic (pathology progression) features in sequential CXRs,
prioritizing disease-relevant changes. Second, we hierarchically align these
static and dynamic CXR features with asynchronous EHR data via local (pairwise
interval-level) and global (full-sequence) synchronization to model coherent
progression pathways. Extensive experiments on the MIMIC dataset demonstrate
that $\texttt{DiPro}$ could effectively extract temporal clinical dynamics and
achieve state-of-the-art performance on both disease progression identification
and general ICU prediction tasks.

</details>


### [127] [Demystifying Numerosity in Diffusion Models -- Limitations and Remedies](https://arxiv.org/abs/2510.11117)
*Yaqi Zhao,Xiaochen Wang,Li Dong,Wentao Zhang,Yuhui Yuan*

Main category: cs.CV

TL;DR: 尽管目前先进的文本到图像生成模型在遵循技术提示方面的能力有限，但本文旨在探索扩散模型能否在不进行其他调整的情况下，仅通过量化数据集和模型大小来准确生成指定数量的对象。


<details>
  <summary>Details</summary>
Motivation: 目前先进的文本到图像生成模型（如 FLUX 和 GPT-4o）在处理数字信息时存在挑战，无法准确遵循文本提示中的计数指令。

Method: 我们构建了两个合成数据集：GrayCount250 用于受控的缩放研究，以及 NaturalCount6 用于复杂的自然场景。我们还提出了一种通过向噪声先验注入计数感知布局信息来控制数字信息的方法。

Result: 单纯扩大模型和数据集并不能提高计数准确性。扩散模型在很大程度上依赖于噪声初始化，而非提示中指定的数字信息，因为噪声先验对特定对象计数存在偏差。通过向噪声先验注入计数感知布局信息，GrayCount250 的准确率从20.0%提高到85.3%，NaturalCount6 从74.8%提高到86.3%。

Conclusion: 单纯扩大模型和数据集并不能提高计数准确性。通过向噪声先验注入计数感知布局信息，可以显著提高扩散模型生成指定数量对象的能力。

Abstract: Numerosity remains a challenge for state-of-the-art text-to-image generation
models like FLUX and GPT-4o, which often fail to accurately follow counting
instructions in text prompts. In this paper, we aim to study a fundamental yet
often overlooked question: Can diffusion models inherently generate the correct
number of objects specified by a textual prompt simply by scaling up the
dataset and model size? To enable rigorous and reproducible evaluation, we
construct a clean synthetic numerosity benchmark comprising two complementary
datasets: GrayCount250 for controlled scaling studies, and NaturalCount6
featuring complex naturalistic scenes. Second, we empirically show that the
scaling hypothesis does not hold: larger models and datasets alone fail to
improve counting accuracy on our benchmark. Our analysis identifies a key
reason: diffusion models tend to rely heavily on the noise initialization
rather than the explicit numerosity specified in the prompt. We observe that
noise priors exhibit biases toward specific object counts. In addition, we
propose an effective strategy for controlling numerosity by injecting
count-aware layout information into the noise prior. Our method achieves
significant gains, improving accuracy on GrayCount250 from 20.0\% to 85.3\% and
on NaturalCount6 from 74.8\% to 86.3\%, demonstrating effective generalization
across settings.

</details>


### [128] [CoPRS: Learning Positional Prior from Chain-of-Thought for Reasoning Segmentation](https://arxiv.org/abs/2510.11173)
*Zhenyu Lu,Liupeng Li,Jinpeng Wang,Yan Feng,Bin Chen,Ke Chen,Yaowei Wang*

Main category: cs.CV

TL;DR: 现有的推理分割方法在可解释性和语义细节方面存在局限。CoPRS提出了一种多模态思维链（MCoT）的方法，通过可微分的位置先验（表示为热图）连接语言推理和分割，从而增强了可解释性并提高了分割精度。在RefCOCO系列和ReasonSeg上的实验表明，CoPRS达到了或超越了现有技术水平，并证明了热图质量对掩码生成的影响。


<details>
  <summary>Details</summary>
Motivation: 现有的推理分割方法通常将语言模型的隐藏特征直接连接到掩码解码器或表示文本中的位置，这限制了模型的可解释性和语义细节。

Method: CoPRS（Multi-modal Chain-of-Thought, MCoT）模型通过一个可微分且可解释的位置先验（表示为热图）将语言推理与分割联系起来。这种热图是通过一个可学习的注意力令牌聚合图像和推理文本的特征生成的。该模型通过MCoT使推理过程清晰化，并通过密集、可微分的热图作为接口，增强了可解释性和诊断分析。最终，热图通过一个轻量级解码器解码为精确的掩码。

Result: 在RefCOCO系列和ReasonSeg数据集上，CoPRS在所有标准划分上的性能均达到或超越了现有最佳报告指标，并在验证和测试分区上均表现出卓越的性能。广泛的实验证明，热图的质量与最终的掩码质量之间存在密切关联。

Conclusion: CoPRS通过MCoT和可微分的热图，成功地弥合了推理与分割之间的鸿沟，提高了模型的可解释性，并能更精确地预测掩码。研究结果支持了这种范式在结合推理和分割方面的有效性，并展示了在推理驱动的集中度和更精确的掩码预测方面的优势。

Abstract: Existing works on reasoning segmentation either connect hidden features from
a language model directly to a mask decoder or represent positions in text,
which limits interpretability and semantic detail. To solve this, we present
CoPRS, a Multi-modal Chain-of-Thought (MCoT)-based positional perception model
that bridges language reasoning to segmentation through a differentiable and
interpretable positional prior instantiated as a heatmap. By making the
reasoning process clear via MCoT and expressing it as a dense, differentiable
heatmap, this interface enhances interpretability and diagnostic analysis and
yields more concentrated evidence on the target. A learnable concentration
token aggregates features of the image and reasoning text to generate this
positional prior, which is decoded to precise masks through a lightweight
decoder, providing a direct connection between reasoning and segmentation.
Across the RefCOCO series and ReasonSeg, CoPRS matches or surpasses the best
reported metrics on each standard split under comparable protocols, with
performance at or above prior state of the art across both validation and test
partitions. Extensive experiments reveal that the quality of the heatmap
strongly influences the resulting mask quality, supporting a consistent
association between the reasoning output and downstream mask generation.
Collectively, these findings support the utility of this paradigm in bridging
reasoning and segmentation and show advantages in concentration driven by
reasoning and predicting masks more precisely. Code, checkpoints and logs are
released at https://github.com/ZhenyuLU-Heliodore/CoPRS.git.

</details>


### [129] [Reliable Cross-modal Alignment via Prototype Iterative Construction](https://arxiv.org/abs/2510.11175)
*Xiang Ma,Litian Xu,Lexin Fang,Caiming Zhang,Lizhen Cui*

Main category: cs.CV

TL;DR: PICO是一种新的跨模态对齐框架，通过量化每个特征列代表语义信息的概率，并将其作为嵌入交互过程中的权重，从而抑制非语义信息（风格信息）的干扰。


<details>
  <summary>Details</summary>
Motivation: 跨模态对齐旨在弥合不同模态之间的语义鸿沟，其基础是匹配对之间的语义一致性。现有方法忽略了非语义信息（风格信息）对对齐的影响，导致信息偏差或丢失。作者认为将语义与风格信息分离，仅对齐语义信息是一种直观的方法，但现有方法在特征列层面区分它们，未能捕捉语义与风格之间复杂的耦合关系。

Method: 本文提出了PICO框架，用于在嵌入交互过程中抑制风格干扰。PICO通过量化每个特征列代表语义信息的概率，并将其作为嵌入交互过程中的权重。为了确保语义概率的可靠性，PICO提出了一种基于性能反馈的加权函数，用于原型迭代构建方法。

Result: 在各种基准和模型骨干上的大量实验表明，PICO的性能优于最先进的方法5.2%至14.1%。

Conclusion: PICO通过有效抑制风格信息在跨模态对齐中的干扰，显著提升了对齐性能，并通过理论证明其核心方法的有效性。

Abstract: Cross-modal alignment is an important multi-modal task, aiming to bridge the
semantic gap between different modalities. The most reliable fundamention for
achieving this objective lies in the semantic consistency between matched
pairs. Conventional methods implicitly assume embeddings contain solely
semantic information, ignoring the impact of non-semantic information during
alignment, which inevitably leads to information bias or even loss. These
non-semantic information primarily manifest as stylistic variations in the
data, which we formally define as style information. An intuitive approach is
to separate style from semantics, aligning only the semantic information.
However, most existing methods distinguish them based on feature columns, which
cannot represent the complex coupling relationship between semantic and style
information. In this paper, we propose PICO, a novel framework for suppressing
style interference during embedding interaction. Specifically, we quantify the
probability of each feature column representing semantic information, and
regard it as the weight during the embedding interaction. To ensure the
reliability of the semantic probability, we propose a prototype iterative
construction method. The key operation of this method is a performance
feedback-based weighting function, and we have theoretically proven that the
function can assign higher weight to prototypes that bring higher performance
improvements. Extensive experiments on various benchmarks and model backbones
demonstrate the superiority of PICO, outperforming state-of-the-art methods by
5.2\%-14.1\%.

</details>


### [130] [BLEnD-Vis: Benchmarking Multimodal Cultural Understanding in Vision Language Models](https://arxiv.org/abs/2510.11178)
*Bryan Chen Zhengyu Tan,Zheng Weihua,Zhengyuan Liu,Nancy F. Chen,Hwaran Lee,Kenny Tsu Wei Choo,Roy Ka-Wei Lee*

Main category: cs.CV

TL;DR: 本文介绍了BLEnD-Vis，一个用于评估视觉语言模型（VLMs）文化理解能力的基准，发现当前VLMs在这方面存在显著的脆弱性。


<details>
  <summary>Details</summary>
Motivation: 随着视觉语言模型（VLMs）在全球范围内的应用，它们理解文化情境知识的能力变得至关重要。然而，现有的评估方法大多侧重于静态回忆或孤立的视觉基础，未能回答VLMs是否拥有鲁棒和可迁移的文化理解能力。

Method: 本文引入了BLEnD-Vis，这是一个多模态、多文化的基准测试，旨在评估VLMs在语言复述和视觉模态下对日常文化知识的鲁棒性。BLEnD-Vis基于BLEnD数据集构建了313个文化基础问题模板，涵盖16个地区，并生成了三种对齐的多项选择格式：(i) 文本基线（区域→实体），(ii) 倒置文本变体（实体→区域），以及 (iii) (ii) 的VQA风格版本（带生成图像）。最终的基准测试包含4,916张图像和超过21,000个多项选择题实例，并通过人工标注进行了验证。

Result: BLEnD-Vis揭示了当前VLM文化知识的显著脆弱性；模型在语言复述下表现出性能下降，尽管视觉线索通常有助于提高性能，但较低的跨模态一致性突出表明在稳健整合文本和视觉理解方面存在挑战，特别是对于资源较少的地区。

Conclusion: BLEnD-Vis为系统分析文化鲁棒性和多模态基础提供了一个重要的试验台，揭示了局限性并指导开发更具文化能力的VLM。

Abstract: As vision-language models (VLMs) are deployed globally, their ability to
understand culturally situated knowledge becomes essential. Yet, existing
evaluations largely assess static recall or isolated visual grounding, leaving
unanswered whether VLMs possess robust and transferable cultural understanding.
We introduce BLEnD-Vis, a multimodal, multicultural benchmark designed to
evaluate the robustness of everyday cultural knowledge in VLMs across
linguistic rephrasings and visual modalities. Building on the BLEnD dataset,
BLEnD-Vis constructs 313 culturally grounded question templates spanning 16
regions and generates three aligned multiple-choice formats: (i) a text-only
baseline querying from Region $\to$ Entity, (ii) an inverted text-only variant
(Entity $\to$ Region), and (iii) a VQA-style version of (ii) with generated
images. The resulting benchmark comprises 4,916 images and over 21,000
multiple-choice question (MCQ) instances, validated through human annotation.
BLEnD-Vis reveals significant fragility in current VLM cultural knowledge;
models exhibit performance drops under linguistic rephrasing and, whilst visual
cues often aid performance, low cross-modal consistency highlights challenges
in robustly integrating textual and visual understanding, particularly for
lower-resource regions. BLEnD-Vis thus provides a crucial testbed for
systematically analysing cultural robustness and multimodal grounding, exposing
limitations and guiding the development of more culturally competent VLMs.

</details>


### [131] [Saudi Sign Language Translation Using T5](https://arxiv.org/abs/2510.11183)
*Ali Alhejab,Tomas Zelezny,Lamya Alkanhal,Ivan Gruber,Yazeed Alharbi,Jakub Straka,Vaclav Javorek,Marek Hruz,Badriah Alkalifah,Ahmed Ali*

Main category: cs.CV

TL;DR: 该论文探讨了使用T5模型和新颖的数据集进行沙特手语（SSL）翻译。


<details>
  <summary>Details</summary>
Motivation: 研究SSL翻译具有挑战性，特别是在存在面部遮挡的情况下。

Method: 本研究使用了包含三个挑战性测试协议的SSL数据集，并比较了在YouTubeASL数据集上预训练的T5模型与直接在SSL数据集上训练的模型。

Result: 在YouTubeASL上进行预训练显著提高了模型的性能（BLEU-4提高了大约3倍），表明手语模型具有跨语言的可迁移性。

Conclusion: 利用大规模ASL数据可以有效改善SSL翻译，为开发更有效的手语翻译系统提供了新思路。

Abstract: This paper explores the application of T5 models for Saudi Sign Language
(SSL) translation using a novel dataset. The SSL dataset includes three
challenging testing protocols, enabling comprehensive evaluation across
different scenarios. Additionally, it captures unique SSL characteristics, such
as face coverings, which pose challenges for sign recognition and translation.
In our experiments, we investigate the impact of pre-training on American Sign
Language (ASL) data by comparing T5 models pre-trained on the YouTubeASL
dataset with models trained directly on the SSL dataset. Experimental results
demonstrate that pre-training on YouTubeASL significantly improves models'
performance (roughly $3\times$ in BLEU-4), indicating cross-linguistic
transferability in sign language models. Our findings highlight the benefits of
leveraging large-scale ASL data to improve SSL translation and provide insights
into the development of more effective sign language translation systems. Our
code is publicly available at our GitHub repository.

</details>


### [132] [FlexAC: Towards Flexible Control of Associative Reasoning in Multimodal Large Language Models](https://arxiv.org/abs/2510.11190)
*Shengming Yuan,Xinyu Lyu,Shuailong Wang,Beitao Chen,Jingkuan Song,Lianli Gao*

Main category: cs.CV

TL;DR: 本文提出了一种名为FlexAC的轻量级且免训练的框架，旨在通过灵活控制联想推理来平衡多模态大型语言模型（MLLMs）在忠实性和创造力之间的权衡。通过利用幻觉引导的中间表示和自适应校准的联想转向向量，FlexAC显著提高了创造力并降低了幻觉率。


<details>
  <summary>Details</summary>
Motivation: 现有的多模态大型语言模型（MLLMs）在忠实性和创造力之间存在固有的权衡，并且缺乏灵活调节联想推理强度的能力，这限制了它们在事实性和创造性场景中的适应性。

Method: 本文首先研究了MLLMs中联想行为的内部机制，发现中间层在塑造模型联想倾向中起关键作用，修改这些层中的表示可以有效调节联想推理强度，并且幻觉可以被利用来得出引导这种调制的转向向量。在此基础上，提出了FlexAC框架，它通过幻觉引导的中间表示编码联想方向，选择高联想实例构建有效的联想转向向量，并自适应校准其强度以平衡创造性指导和输出稳定性。此外，FlexAC结合了从少量目标领域样本的前向传递中获得的任务特定联想向量，以适应多维联想推理。

Result: FlexAC在Creation-MMBench上的创造力提高了5.8倍，在CHAIR上的幻觉率降低了29%，优于现有基线。

Conclusion: FlexAC框架通过灵活控制MLLMs中的联想推理，有效地平衡了忠实性和创造力，显著提高了模型在创造性任务中的表现，同时降低了幻觉。

Abstract: Multimodal large language models (MLLMs) face an inherent trade-off between
faithfulness and creativity, as different tasks require varying degrees of
associative reasoning. However, existing methods lack the flexibility to
modulate this reasoning strength, limiting MLLMs' adaptability across factual
and creative scenarios. To bridge this gap, we propose equipping MLLMs with
mechanisms that enable flexible control over associative reasoning. We begin by
investigating the internal mechanisms underlying associative behavior in MLLMs
and find that: (1) middle layers play a pivotal role in shaping model's
associative tendencies, (2) modifying representations in these layers
effectively regulates associative reasoning strength, and (3) hallucinations
can be exploited to derive steering vectors that guide this modulation.
Building on these findings, we introduce Flexible Association Control (FlexAC),
a lightweight and training-free framework for modulating associative behavior
in MLLMs. FlexAC first induces hallucination-guided intermediate
representations to encode associative directions. Then, it selects
high-association instances to construct effective associative steering vectors,
whose strengths are adaptively calibrated to balance creative guidance with
output stability. Finally, recognizing the multi-dimensional nature of
associative reasoning, FlexAC incorporates task-specific associative vectors
derived from a forward pass on a few target-domain samples, enabling models to
follow diverse associative directions and better adapt to creative tasks.
Notably, our method achieves up to a 5.8x improvement in creativity on
Creation-MMBench and a 29% reduction in hallucination rate on CHAIR, surpassing
existing baselines and demonstrating its effectiveness in enabling flexible
control over associative reasoning in MLLMs. Our code is available at
https://github.com/ylhz/FlexAC.

</details>


### [133] [Investigating Identity Signals in Conversational Facial Dynamics via Disentangled Expression Features](https://arxiv.org/abs/2510.11223)
*Masoumeh Chapariniya,Pierre Vuillecard,Jean-Marc Odobez,Volker Dellwo,Teodora Vukovic*

Main category: cs.CV

TL;DR: 本文研究了个体是否可以通过面部表情的动态成分来识别，而与静态面部外观无关。


<details>
  <summary>Details</summary>
Motivation: 探索面部表情的动态成分是否能作为个体身份识别的独立依据。

Method: 利用FLAME 3D变形模型将面部形状与表情动态分离，并从对话视频中提取逐帧参数（仅保留表情和下颌系数）。在CANDOR数据集上，使用Conformer模型和监督对比学习进行1,429类分类，并引入漂移噪声比（DNR）来量化形状表情分离的可靠性。

Result: Conformer模型在1,429类分类中达到了61.14%的准确率，远高于 случайность水平（458倍），表明面部动态携带着强大的身份识别特征。漂移噪声比（DNR）与识别性能呈强负相关，证实不稳定的形状估计会损害动态识别。

Conclusion: 对话面部动态中存在个体特有的身份识别特征，这对社会认知和临床评估具有重要意义。

Abstract: This work investigates whether individuals can be identified solely through
the pure dynamical components of their facial expressions, independent of
static facial appearance. We leverage the FLAME 3D morphable model to achieve
explicit disentanglement between facial shape and expression dynamics,
extracting frame-by-frame parameters from conversational videos while retaining
only expression and jaw coefficients. On the CANDOR dataset of 1,429 speakers
in naturalistic conversations, our Conformer model with supervised contrastive
learning achieves 61.14\%accuracy on 1,429-way classification -- 458 times
above chance -- demonstrating that facial dynamics carry strong identity
signatures. We introduce a drift-to-noise ratio (DNR) that quantifies the
reliability of shape expression separation by measuring across-session shape
changes relative to within-session variability. DNR strongly negatively
correlates with recognition performance, confirming that unstable shape
estimation compromises dynamic identification. Our findings reveal
person-specific signatures in conversational facial dynamics, with implications
for social perception and clinical assessment.

</details>


### [134] [DTEA: Dynamic Topology Weaving and Instability-Driven Entropic Attenuation for Medical Image Segmentation](https://arxiv.org/abs/2510.11259)
*Weixuan Li,Quanjun Li,Guang Yu,Song Yang,Zimeng Li,Chi-Man Pun,Yupeng Liu,Xuhang Chen*

Main category: cs.CV

TL;DR: DTEA模型通过语义拓扑重构（STR）和熵扰动门控（EPG）模块，利用新的跳跃连接框架，在医学图像分割中表现出卓越的性能和更好的泛化能力。


<details>
  <summary>Details</summary>
Motivation: 在医学图像分割中，跳跃连接用于融合全局上下文并减少编码器和解码器之间的语义鸿沟。现有方法通常在有限的结构表示和不充分的上下文建模方面存在问题，这影响了在复杂临床场景中的泛化能力。

Method: 我们提出了DTEA模型，其特点是包含语义拓扑重构（STR）和熵扰动门控（EPG）模块的新型跳跃连接框架。STR将多尺度语义特征重组为动态超图，以更好地模拟跨分辨率解剖依赖关系，增强结构和语义表示。EPG评估扰动后的通道稳定性并过滤高熵通道，以强调临床重要区域并改善空间注意力。

Result: 在三个基准数据集上进行的广泛实验表明，我们的框架在各种临床环境中实现了卓越的分割精度和更好的泛化能力。

Conclusion: DTEA模型通过其创新的STR和EPG模块，有效解决了医学图像分割中现有方法在结构表示和上下文建模方面的不足，显著提高了分割精度和泛化能力。

Abstract: In medical image segmentation, skip connections are used to merge global
context and reduce the semantic gap between encoder and decoder. Current
methods often struggle with limited structural representation and insufficient
contextual modeling, affecting generalization in complex clinical scenarios. We
propose the DTEA model, featuring a new skip connection framework with the
Semantic Topology Reconfiguration (STR) and Entropic Perturbation Gating (EPG)
modules. STR reorganizes multi-scale semantic features into a dynamic
hypergraph to better model cross-resolution anatomical dependencies, enhancing
structural and semantic representation. EPG assesses channel stability after
perturbation and filters high-entropy channels to emphasize clinically
important regions and improve spatial attention. Extensive experiments on three
benchmark datasets show our framework achieves superior segmentation accuracy
and better generalization across various clinical settings. The code is
available at
\href{https://github.com/LWX-Research/DTEA}{https://github.com/LWX-Research/DTEA}.

</details>


### [135] [Exploring and Leveraging Class Vectors for Classifier Editing](https://arxiv.org/abs/2510.11268)
*Jaeik Kim,Jaeyoung Do*

Main category: cs.CV

TL;DR: 该论文介绍了“类别向量”（Class Vectors），这是一种在图像分类器中实现高效、灵活和高层次概念编辑的新方法，能够解决现有模型编辑方法在纠正错误或适应分布变化方面的局限性。


<details>
  <summary>Details</summary>
Motivation: 现有的图像分类器在训练完成后，其预定义的行为使得事后模型编辑变得困难，尤其是在遗忘特定类别或适应分布变化时。现有的分类器编辑方法要么只专注于纠正错误，要么需要大量的再训练成本，这限制了模型编辑的灵活性。此外，图像分类中的这种编辑研究也比较有限。

Method: 本文提出了“类别向量”（Class Vectors），它通过在微调过程中捕获类别特定的表示调整来工作。与在权重空间中编码任务级变化的“任务向量”不同，“类别向量”在潜在空间中解耦每个类别的适应性。作者展示了“类别向量”能够捕获每个类别的语义漂移，并且可以通过沿着这些向量引导潜在特征或将它们映射到权重空间以更新决策边界来实现分类器编辑。

Result: “类别向量”能够捕获每个类别的语义漂移。分类器编辑可以通过沿着这些向量引导潜在特征或将它们映射到权重空间以更新决策边界来实现。此外，“类别向量”固有的线性和正交性支持通过简单的类别算术进行高效、灵活和高层次的概念编辑。

Conclusion: “类别向量”是一种有效的方法，可以克服现有图像分类器在进行事后模型编辑时面临的挑战，尤其是在遗忘特定类别或适应分布变化方面。它通过在潜在空间中解耦每个类别的适应性，并支持通过简单的类别算术进行高效、灵活和高层次的概念编辑。该方法在“遗忘”、“环境适应”、“对抗性防御”和“对抗性触发优化”等应用中得到了验证。

Abstract: Image classifiers play a critical role in detecting diseases in medical
imaging and identifying anomalies in manufacturing processes. However, their
predefined behaviors after extensive training make post hoc model editing
difficult, especially when it comes to forgetting specific classes or adapting
to distribution shifts. Existing classifier editing methods either focus
narrowly on correcting errors or incur extensive retraining costs, creating a
bottleneck for flexible editing. Moreover, such editing has seen limited
investigation in image classification. To overcome these challenges, we
introduce Class Vectors, which capture class-specific representation
adjustments during fine-tuning. Whereas task vectors encode task-level changes
in weight space, Class Vectors disentangle each class's adaptation in the
latent space. We show that Class Vectors capture each class's semantic shift
and that classifier editing can be achieved either by steering latent features
along these vectors or by mapping them into weight space to update the decision
boundaries. We also demonstrate that the inherent linearity and orthogonality
of Class Vectors support efficient, flexible, and high-level concept editing
via simple class arithmetic. Finally, we validate their utility in applications
such as unlearning, environmental adaptation, adversarial defense, and
adversarial trigger optimization.

</details>


### [136] [EEMS: Edge-Prompt Enhanced Medical Image Segmentation Based on Learnable Gating Mechanism](https://arxiv.org/abs/2510.11287)
*Han Xia,Quanjun Li,Qian Li,Zimeng Li,Hongbin Ye,Yupeng Liu,Haolun Li,Xuhang Chen*

Main category: cs.CV

TL;DR: EEMS模型，一个结合了边缘感知增强单元（EAEU）和多尺度提示生成单元（MSPGU）的医学图像分割模型，在ISIC2018等数据集上表现出色，证明了其在临床应用中的性能和可靠性。


<details>
  <summary>Details</summary>
Motivation: 医学图像分割对诊断、治疗计划和疾病监测至关重要，但由于模糊边缘和背景噪声等复杂因素而面临挑战。

Method: EEMS模型包含三个关键单元：1. 边缘感知增强单元（EAEU）通过多频特征提取增强边缘感知。2. 多尺度提示生成单元（MSPGU）利用提示引导方法整合高级语义和低级空间特征。3. 双源自适应门控融合单元（DAGFU）融合EAEU的边缘特征和MSPGU的语义特征。

Result: EEMS模型在ISIC2018等数据集上进行了测试，并取得了卓越的性能和可靠性。

Conclusion: EEMS模型通过其独特的设计，有效地解决了医学图像分割中的挑战，并在多个数据集上验证了其在临床应用中的优越性能和可靠性。

Abstract: Medical image segmentation is vital for diagnosis, treatment planning, and
disease monitoring but is challenged by complex factors like ambiguous edges
and background noise. We introduce EEMS, a new model for segmentation,
combining an Edge-Aware Enhancement Unit (EAEU) and a Multi-scale Prompt
Generation Unit (MSPGU). EAEU enhances edge perception via multi-frequency
feature extraction, accurately defining boundaries. MSPGU integrates high-level
semantic and low-level spatial features using a prompt-guided approach,
ensuring precise target localization. The Dual-Source Adaptive Gated Fusion
Unit (DAGFU) merges edge features from EAEU with semantic features from MSPGU,
enhancing segmentation accuracy and robustness. Tests on datasets like ISIC2018
confirm EEMS's superior performance and reliability as a clinical tool.

</details>


### [137] [Human Uncertainty-Aware Data Selection and Automatic Labeling in Visual Question Answering](https://arxiv.org/abs/2510.11295)
*Jian Lan,Zhicheng Liu,Udo Schlegel,Raoyuan Zhao,Yihong Liu,Hinrich Schütze,Michael A. Hedderich,Thomas Seidl*

Main category: cs.CV

TL;DR: 本文探讨了人类不确定性（HU）对大型视觉语言模型（VLM）监督微调（SFT）的影响，并提出了一种名为HaDola的框架，旨在利用HU进行数据选择和自动标注，从而提高模型性能和校准。


<details>
  <summary>Details</summary>
Motivation: 大型视觉语言模型（VLM）在视觉问答方面表现出色，但仍严重依赖于昂贵的大规模监督微调（SFT）。此外，现实世界数据集中的人类不确定性（HU）在标准SFT中被忽视。本文旨在探讨HU如何影响SFT以及如何在训练中有效利用HU。

Method: 本文首先系统评估了不同HU水平下VLM的性能，发现高HU样本对模型性能的贡献很小甚至会降低性能，并且在完整数据集上进行训练会导致模型校准不足。基于这些发现，本文提出了HaDola框架，该框架包括四个阶段：识别、自标注、错误触发和训练。HaDola通过迭代识别有害样本、优先处理信息丰富的样本并从小型种子集（5%数据）进行自举。

Result: HaDola框架显著减少了对昂贵HU标注的依赖，并使VLM更加准确和校准。在VQAv2和VizWiz数据集上的大量实验表明，HaDola在更少的训练数据下始终匹配或优于最先进的基线方法。

Conclusion: 本文强调了在SFT中明确建模人类不确定性（HU）的重要性，并表明更好地利用HU比简单地扩大数据集规模更有效。

Abstract: Large vision-language models (VLMs) achieve strong performance in Visual
Question Answering but still rely heavily on supervised fine-tuning (SFT) with
massive labeled datasets, which is costly due to human annotations. Crucially,
real-world datasets often exhibit human uncertainty (HU) -- variation in human
confidence across annotations -- but standard SFT simply optimizes toward the
most frequent label, disregarding HU distributions. This leaves two open
questions: How does HU affect SFT, and how can HU be effectively leveraged in
training? In this work, we first conduct a systematic evaluation of VLMs across
varying HU levels. We have two key findings: (i) surprisingly, high-HU samples
contribute little or even degrade model performance, and (ii) naively training
on the full dataset yields under-calibrated models that fail to capture HU
distributions. Motivated by these findings, we introduce HaDola, a human
uncertainty-aware data selection and automatic labeling framework. HaDola
operates in four stages -- discriminate, self-annotate, error trigger, and
training -- to iteratively identify harmful samples, prioritize informative
ones, and bootstrap from a small seed set (5\% of data). Our approach
substantially reduces reliance on costly HU annotations and makes VLMs more
accurate and better calibrated. Extensive experiments on VQAv2 and VizWiz
datasets demonstrate that HaDola consistently matches or outperforms
state-of-the-art baselines with less training data. Our work highlights the
importance of explicitly modeling HU in SFT, suggesting that better utilization
of HU is more effective than merely scaling up dataset size.

</details>


### [138] [$Δ\mathrm{Energy}$: Optimizing Energy Change During Vision-Language Alignment Improves both OOD Detection and OOD Generalization](https://arxiv.org/abs/2510.11296)
*Lin Zhu,Yifeng Yang,Xinbing Wang,Qinying Gu,Nanyang Ye*

Main category: cs.CV

TL;DR: 这篇论文提出了一种名为ΔEnergy的OOD得分，用于提高视觉语言模型在协变量偏移下的OOD泛化能力和语义偏移下的OOD检测能力，并通过EBM理论证明和实验验证了其优越性。


<details>
  <summary>Details</summary>
Motivation: 在实际应用中，视觉语言模型（VLMs）会遇到分布内（ID）和分布外（OOD）数据，其中OOD数据包括协变量偏移（已知类别的图像风格变化）和语义偏移（测试时未见的类别）。因此，提高VLMs对协变量偏移OOD数据的泛化能力，并有效检测开放集语义偏移OOD类别至关重要。

Method: 本文受限于重新对齐视觉语言模态时（通过直接将最大余弦相似度降低到低值）闭集数据中观察到的显著能量变化，引入了一种新颖的OOD分数，名为ΔEnergy。此外，ΔEnergy可以通过ΔEnergy的下界最大化（称为EBM）同时改善协变量偏移下的OOD泛化。EBM在理论上被证明不仅可以增强OOD检测，还可以产生领域一致的Hessian，这是OOD泛化的一个强指标。基于这一发现，我们开发了一个统一的微调框架，可以提高VLMs在OOD泛化和OOD检测方面的鲁棒性。

Result: ΔEnergy显著优于香草能量基OOD得分，并提供了一种更可靠的OOD检测方法。在具有挑战性的OOD检测和泛化基准上进行的广泛实验表明，我们的方法优于最近的方法10%到25%的AUROC。

Conclusion: ΔEnergy方法及其统一的微调框架能够显著提高视觉语言模型在协变量偏移下的OOD泛化能力和语义偏移下的OOD检测能力，在OOD检测和泛化方面表现出优越的性能，并为解决VLMs在实际应用中面临的OOD挑战提供了有效途径。

Abstract: Recent approaches for vision-language models (VLMs) have shown remarkable
success in achieving fast downstream adaptation. When applied to real-world
downstream tasks, VLMs inevitably encounter both the in-distribution (ID) data
and out-of-distribution (OOD) data. The OOD datasets often include both
covariate shifts (e.g., known classes with changes in image styles) and
semantic shifts (e.g., test-time unseen classes). This highlights the
importance of improving VLMs' generalization ability to covariate-shifted OOD
data, while effectively detecting open-set semantic-shifted OOD classes. In
this paper, inspired by the substantial energy change observed in closed-set
data when re-aligning vision-language modalities (specifically by directly
reducing the maximum cosine similarity to a low value), we introduce a novel
OOD score, named {\Delta}Energy. {\Delta}Energy significantly outperforms the
vanilla energy-based OOD score and provides a more reliable approach for OOD
detection. Furthermore, {\Delta}Energy can simultaneously improve OOD
generalization under covariate shifts, which is achieved by lower-bound
maximization for {\Delta}Energy (termed EBM). EBM is theoretically proven to
not only enhance OOD detection but also yields a domain-consistent Hessian,
which serves as a strong indicator for OOD generalization. Based on this
finding, we developed a unified fine-tuning framework that allows for improving
VLMs' robustness in both OOD generalization and OOD detection. Extensive
experiments on challenging OOD detection and generalization benchmarks
demonstrate the superiority of our method, outperforming recent approaches by
10% to 25% in AUROC.

</details>


### [139] [sketch2symm: Symmetry-aware sketch-to-shape generation via semantic bridging](https://arxiv.org/abs/2510.11303)
*Yan Zhou,Mingji Li,Xiantao Zeng,Jie Lin,Yuexia Zhou*

Main category: cs.CV

TL;DR: Sketch2Symm是一种两阶段生成方法，它通过草图到图像的语义桥接和引入对称性约束，从草图中生成几何一致的3D形状。


<details>
  <summary>Details</summary>
Motivation: 由于草图输入的抽象性和稀疏性，缺乏足够的语义和几何信息，基于草图的3D重建仍然是一个具有挑战性的任务。

Method: 本文提出了Sketch2Symm。该方法通过草图到图像的语义转换来实现语义桥接，以丰富稀疏的草图表示。同时，引入了对称性约束作为几何先验，以利用日常物体中常见的结构规律。

Result: 在主流草图数据集上的实验表明，与现有基于草图的重建方法相比，Sketch2Symm在Chamfer距离、Earth Mover距离和F-Score方面均取得了优越的性能。

Conclusion: 实验结果验证了本文提出的语义桥接和对称感知设计的有效性。

Abstract: Sketch-based 3D reconstruction remains a challenging task due to the abstract
and sparse nature of sketch inputs, which often lack sufficient semantic and
geometric information. To address this, we propose Sketch2Symm, a two-stage
generation method that produces geometrically consistent 3D shapes from
sketches. Our approach introduces semantic bridging via sketch-to-image
translation to enrich sparse sketch representations, and incorporates symmetry
constraints as geometric priors to leverage the structural regularity commonly
found in everyday objects. Experiments on mainstream sketch datasets
demonstrate that our method achieves superior performance compared to existing
sketch-based reconstruction methods in terms of Chamfer Distance, Earth Mover's
Distance, and F-Score, verifying the effectiveness of the proposed semantic
bridging and symmetry-aware design.

</details>


### [140] [Evaluating the effects of preprocessing, method selection, and hyperparameter tuning on SAR-based flood mapping and water depth estimation](https://arxiv.org/abs/2510.11305)
*Jean-Paul Travert,Cédric Goeury,Sébastien Boyaval,Vito Bacchi,Fabrice Zaoui*

Main category: cs.CV

TL;DR: 这篇论文评估了合成孔径雷达(SAR)图像在洪水测绘和水深估算中的各种方法，强调了预处理、洪水测绘方法及其超参数选择的重要性。


<details>
  <summary>Details</summary>
Motivation: 洪水测绘和水深估算对于水力模型的校准和验证至关重要。

Method: 本研究通过集合预处理图像、洪水图和水深场来评估SAR图像的各种预处理（特别是散斑噪声消除）、洪水测绘和水深估算方法。研究了不同步骤中方法选择及其超参数的影响。

Result: 散斑滤波器的选择会改变洪水范围估计，误差可达数平方公里。洪水测绘方法的选择和调整也会影响性能。监督方法优于无监督方法，但经过调整的无监督方法也能达到可比较的结果。预处理和洪水测绘步骤的复合不确定性也导致水深场估计的高度可变性。

Conclusion: 在洪水测绘和水深估算中，应考虑整个处理流程，包括预处理、洪水测绘和水深估算方法及其相关的超参数。建议采用集成方法并考虑方法学不确定性。洪水测绘中方法选择的影响最大，水深估算中洪水图输入和方法超参数的影响最大。

Abstract: Flood mapping and water depth estimation from Synthetic Aperture Radar (SAR)
imagery are crucial for calibrating and validating hydraulic models. This study
uses SAR imagery to evaluate various preprocessing (especially speckle noise
reduction), flood mapping, and water depth estimation methods. The impact of
the choice of method at different steps and its hyperparameters is studied by
considering an ensemble of preprocessed images, flood maps, and water depth
fields. The evaluation is conducted for two flood events on the Garonne River
(France) in 2019 and 2021, using hydrodynamic simulations and in-situ
observations as reference data. Results show that the choice of speckle filter
alters flood extent estimations with variations of several square kilometers.
Furthermore, the selection and tuning of flood mapping methods also affect
performance. While supervised methods outperformed unsupervised ones, tuned
unsupervised approaches (such as local thresholding or change detection) can
achieve comparable results. The compounded uncertainty from preprocessing and
flood mapping steps also introduces high variability in the water depth field
estimates. This study highlights the importance of considering the entire
processing pipeline, encompassing preprocessing, flood mapping, and water depth
estimation methods and their associated hyperparameters. Rather than relying on
a single configuration, adopting an ensemble approach and accounting for
methodological uncertainty should be privileged. For flood mapping, the method
choice has the most influence. For water depth estimation, the most influential
processing step was the flood map input resulting from the flood mapping step
and the hyperparameters of the methods.

</details>


### [141] [REACT3D: Recovering Articulations for Interactive Physical 3D Scenes](https://arxiv.org/abs/2510.11340)
*Zhao Huang,Boyang Sun,Alexandros Delitzas,Jiaqi Chen,Marc Pollefeys*

Main category: cs.CV

TL;DR: REACT3D是一个可扩展的零样本框架，可以将静态3D场景转换为可用于模拟的交互式副本，具有一致的几何形状，可直接用于各种下游任务。


<details>
  <summary>Details</summary>
Motivation: 现有的具身智能交互式3D场景数据集有限，因为标注部分分割、运动类型和运动轨迹的过程劳动密集。

Method: 我们提出了REACT3D，一个可扩展的零样本框架，包括可打开物体的检测和分割、关节类型和运动参数的推断、隐藏几何体的补全以及交互式场景的集成。

Result: REACT3D在各种室内场景的检测/分割和关节度量方面取得了最先进的性能。

Conclusion: REACT3D为可扩展的交互式场景生成提供了实用基础，降低了大规模关节场景理解研究的门槛。

Abstract: Interactive 3D scenes are increasingly vital for embodied intelligence, yet
existing datasets remain limited due to the labor-intensive process of
annotating part segmentation, kinematic types, and motion trajectories. We
present REACT3D, a scalable zero-shot framework that converts static 3D scenes
into simulation-ready interactive replicas with consistent geometry, enabling
direct use in diverse downstream tasks. Our contributions include: (i)
openable-object detection and segmentation to extract candidate movable parts
from static scenes, (ii) articulation estimation that infers joint types and
motion parameters, (iii) hidden-geometry completion followed by interactive
object assembly, and (iv) interactive scene integration in widely supported
formats to ensure compatibility with standard simulation platforms. We achieve
state-of-the-art performance on detection/segmentation and articulation metrics
across diverse indoor scenes, demonstrating the effectiveness of our framework
and providing a practical foundation for scalable interactive scene generation,
thereby lowering the barrier to large-scale research on articulated scene
understanding. Our project page is
\textit{\hypersetup{urlcolor=black}\href{https://react3d.github.io/}{react3d.github.io}}.

</details>


### [142] [InternSVG: Towards Unified SVG Tasks with Multimodal Large Language Models](https://arxiv.org/abs/2510.11341)
*Haomin Wang,Jinhui Yin,Qi Wei,Wenguang Zeng,Lixin Gu,Shenglong Ye,Zhangwei Gao,Yaohui Wang,Yanting Zhang,Yuanqi Li,Yanwen Guo,Wenhai Wang,Kai Chen,Yu Qiao,Hongjie Zhang*

Main category: cs.CV

TL;DR: InternSVG 解决了 SVG 建模中的数据集碎片化、方法迁移性差和结构复杂性问题，提出了一个统一的多模态大语言模型，用于 SVG 的理解、编辑和生成。该模型基于最大的 SVG 数据集 SAgoge 和基准 SArena，并在实验中表现出卓越的性能。


<details>
  <summary>Details</summary>
Motivation: 现有的 SVG 建模存在数据集碎片化、方法跨任务迁移性差以及难以处理结构复杂性等问题。

Method: 本文利用多模态大语言模型（MLLMs）强大的迁移和泛化能力，提出了 InternSVG 家族，包括集成数据、基准和模型。核心是 SAgoge 数据集，一个最大最全面的 SVG 多模态数据集，包含静态图形和动态动画。基于此，引入了 SArena 基准，该基准具有全面的任务定义和标准化的评估。在此基础上，提出了 InternSVG 模型，一个统一的 MLLM 用于 SVG 理解、编辑和生成，采用 SVG 特殊 token、基于子词的嵌入初始化以及从短静态 SVG 到长序列插图和复杂动画的两阶段训练策略。

Result: InternSVG 在 SArena 和之前的基准测试中取得了显著的进步，并持续优于领先的开源和专有同类产品。

Conclusion: InternSVG 通过统一的建模框架和专门的训练策略，成功解决了 SVG 建模的挑战，并在理解、编辑和生成任务中取得了最先进的性能。

Abstract: General SVG modeling remains challenging due to fragmented datasets, limited
transferability of methods across tasks, and the difficulty of handling
structural complexity. In response, we leverage the strong transfer and
generalization capabilities of multimodal large language models (MLLMs) to
achieve unified modeling for SVG understanding, editing, and generation. We
present the InternSVG family, an integrated data-benchmark-model suite. At its
core is SAgoge, the largest and most comprehensive multimodal dataset for SVG
tasks, encompassing both static graphics and dynamic animations. It covers
icons, long-sequence illustrations, scientific diagrams, and dynamic
animations, supporting tasks of varied difficulty levels and providing deeper
hierarchies with richer attributes compared to previous datasets. Based on this
resource, we introduce SArena, a companion benchmark with comprehensive task
definitions and standardized evaluation that aligns with the domains and
difficulty spectrum covered by SAgoge. Building on these foundations, we
propose InternSVG, a unified MLLM for SVG understanding, editing, and
generation with SVG-specific special tokens, subword-based embedding
initialization, and a two-stage training strategy that progresses from short
static SVGs to long-sequence illustrations and complex animations. This unified
formulation induces positive transfer and improves overall performance.
Experiments on SArena and prior benchmark confirm that InternSVG achieves
substantial gains and consistently outperforms leading open and proprietary
counterparts.

</details>


### [143] [MMAP: A Multi-Magnification and Prototype-Aware Architecture for Predicting Spatial Gene Expression](https://arxiv.org/abs/2510.11344)
*Hai Dang Nguyen,Nguyen Dang Huy Pham,The Minh Duc Nguyen,Dac Thai Nguyen,Hang Thi Nguyen,Duong M. Nguyen*

Main category: cs.CV

TL;DR: MMAP是一个新颖的通过结合多倍放大斑块表示和原型增强架构来预测空间基因表达的方法，解决了现有方法在局部特征提取粒度和全局空间上下文覆盖方面的不足


<details>
  <summary>Details</summary>
Motivation: 在空间转录组学（ST）中，从苏木精和伊红（H&E）染色的全玻片图像（WSI）预测转录组范围的基因表达谱是一个具有挑战性的问题，因为视觉特征和分子信号之间存在显著的模态差异。现有方法在局部特征提取粒度和全局空间上下文覆盖方面存在局限性。

Method: 我们提出了MMAP（多倍放大和原型增强架构）框架。为了增强局部特征粒度，MMAP利用多倍放大斑块表示来捕获细粒度的组织学细节。为了改善全局上下文理解，MMAP学习一组潜在的原型嵌入，作为幻灯片级别信息的紧凑表示。

Result: MMAP在多个评估指标（包括平均绝对误差（MAE）、均方误差（MSE）和皮尔逊相关系数（PCC））上始终优于所有现有的最新方法。

Conclusion: MMAP通过解决局部特征提取和全局空间上下文理解的挑战，显著提高了从组织学图像预测空间基因表达的准确性。

Abstract: Spatial Transcriptomics (ST) enables the measurement of gene expression while
preserving spatial information, offering critical insights into tissue
architecture and disease pathology. Recent developments have explored the use
of hematoxylin and eosin (H&E)-stained whole-slide images (WSIs) to predict
transcriptome-wide gene expression profiles through deep neural networks. This
task is commonly framed as a regression problem, where each input corresponds
to a localized image patch extracted from the WSI. However, predicting spatial
gene expression from histological images remains a challenging problem due to
the significant modality gap between visual features and molecular signals.
Recent studies have attempted to incorporate both local and global information
into predictive models. Nevertheless, existing methods still suffer from two
key limitations: (1) insufficient granularity in local feature extraction, and
(2) inadequate coverage of global spatial context. In this work, we propose a
novel framework, MMAP (Multi-MAgnification and Prototype-enhanced
architecture), that addresses both challenges simultaneously. To enhance local
feature granularity, MMAP leverages multi-magnification patch representations
that capture fine-grained histological details. To improve global contextual
understanding, it learns a set of latent prototype embeddings that serve as
compact representations of slide-level information. Extensive experimental
results demonstrate that MMAP consistently outperforms all existing
state-of-the-art methods across multiple evaluation metrics, including Mean
Absolute Error (MAE), Mean Squared Error (MSE), and Pearson Correlation
Coefficient (PCC).

</details>


### [144] [Reasoning as Representation: Rethinking Visual Reinforcement Learning in Image Quality Assessment](https://arxiv.org/abs/2510.11369)
*Shijie Zhao,Xuanyu Zhang,Weiqi Li,Junlin Li,Li Zhang,Tianfan Xue,Jian Zhang*

Main category: cs.CV

TL;DR: 本文探讨了基于推理的图像质量评估（IQA）模型泛化能力差的原因，提出了一种新的算法 RALI，该算法在保持泛化性能的同时，显著降低了模型参数和推理时间。


<details>
  <summary>Details</summary>
Motivation: 尽管通过强化学习（RL）训练的基于推理的图像质量评估（IQA）模型表现出卓越的泛化能力，但其内在机制和关键驱动因素在当前研究中仍未被充分探索。此外，这些模型在推理能耗和延迟方面远超早期同类模型，限制了它们在特定场景中的部署。

Method: 通过RL训练，多模态大语言模型（MLLMs）利用其推理能力将冗余的视觉表征转化为紧凑的、跨域对齐的文本表征，这一转化是这些基于推理的IQA模型泛化能力的来源。在此基础上，提出了一种新颖的算法 RALI，该算法采用对比学习，将图像与RL学习到的可泛化文本表征直接对齐，消除了对推理过程的依赖，甚至无需加载大型语言模型（LLM）。

Result: 在质量评分任务中，RALI 框架在泛化性能方面与基于推理的模型相当，但所需的模型参数和推理时间不到后者的5%。

Conclusion: 本文通过揭示MLLMs在RL训练下将冗余视觉表征转化为紧凑跨域对齐文本表征是其泛化能力来源的机制，提出RALI算法。该算法通过对比学习直接对齐图像与可泛化文本表征，在保持竞争性泛化性能的同时，显著降低了计算成本。

Abstract: Reasoning-based image quality assessment (IQA) models trained through
reinforcement learning (RL) exhibit exceptional generalization, yet the
underlying mechanisms and critical factors driving this capability remain
underexplored in current research. Moreover, despite their superior
performance, these models incur inference energy usage and latency orders of
magnitude higher than their earlier counterparts, restricting their deployment
in specific scenarios. Through extensive experiments, this paper verifies and
elaborates that through RL training, MLLMs leverage their reasoning capability
to convert redundant visual representations into compact, cross-domain aligned
text representations. This conversion is precisely the source of the
generalization exhibited by these reasoning-based IQA models. Building on this
fundamental insight, we propose a novel algorithm, RALI, which employs
contrastive learning to directly align images with these generalizable text
representations learned by RL. This approach eliminates the reliance on
reasoning processes and even obviates the need to load an LLM. For the quality
scoring task, this framework achieves generalization performance comparable to
reasoning-based models while requiring less than 5% of their model parameters
and inference time.

</details>


### [145] [MaterialRefGS: Reflective Gaussian Splatting with Multi-view Consistent Material Inference](https://arxiv.org/abs/2510.11387)
*Wenyuan Zhang,Jimin Tang,Weiqi Zhang,Yi Fang,Yu-Shen Liu,Zhizhong Han*

Main category: cs.CV

TL;DR: 这篇论文提出了一种多视角一致性材料推断方法，用于在有限环境建模下提高高斯泼溅法的反射建模准确性，并通过光线追踪处理间接照明，从而在渲染质量上达到最先进水平。


<details>
  <summary>Details</summary>
Motivation: 现有的基于高斯泼溅的反射建模方法在有限环境建模下，材料推断缺乏足够的约束，导致照明混叠和泛化能力下降。

Method: 本文从多视角角度重新审视问题，强调了多视角一致性材料推断和基于物理的环境建模对于准确学习反射的重要性。为此，作者强制2D高斯在延迟着色过程中生成多视角一致的材料映射。同时通过跟踪跨视角的辐射度变化来识别高反射区域，作为反射强度项的强先验。为了处理对象间遮挡引起的间接照明，本文引入了一种通过2DGS光线追踪进行环境建模的策略。

Result: 实验结果表明，该方法能够忠实地恢复照明和几何形状，并在新视角合成中实现了最先进的渲染质量。

Conclusion: 通过多视角一致的材料推断和基于物理的环境建模，本文方法显著提高了高斯泼溅法在反射建模中的准确性和泛化能力，特别是在处理复杂间接照明方面。

Abstract: Modeling reflections from 2D images is essential for photorealistic rendering
and novel view synthesis. Recent approaches enhance Gaussian primitives with
reflection-related material attributes to enable physically based rendering
(PBR) with Gaussian Splatting. However, the material inference often lacks
sufficient constraints, especially under limited environment modeling,
resulting in illumination aliasing and reduced generalization. In this work, we
revisit the problem from a multi-view perspective and show that multi-view
consistent material inference with more physically-based environment modeling
is key to learning accurate reflections with Gaussian Splatting. To this end,
we enforce 2D Gaussians to produce multi-view consistent material maps during
deferred shading. We also track photometric variations across views to identify
highly reflective regions, which serve as strong priors for reflection strength
terms. To handle indirect illumination caused by inter-object occlusions, we
further introduce an environment modeling strategy through ray tracing with
2DGS, enabling photorealistic rendering of indirect radiance. Experiments on
widely used benchmarks show that our method faithfully recovers both
illumination and geometry, achieving state-of-the-art rendering quality in
novel views synthesis.

</details>


### [146] [Robust Ego-Exo Correspondence with Long-Term Memory](https://arxiv.org/abs/2510.11417)
*Yijun Hu,Bing Fan,Xin Gu,Haiqing Ren,Dongfang Liu,Heng Fan,Libo Zhang*

Main category: cs.CV

TL;DR: 本文提出了一种基于SAM 2的Ego-Exo对应（EEC）框架，通过双记忆架构和自适应特征路由模块，解决了现有方法在极端视角变化、遮挡和小物体等挑战下的不足，并在EgoExo4D基准测试中取得了最先进的结果。


<details>
  <summary>Details</summary>
Motivation: 智能助手需要建立自我中心和外部中心视图之间的物体级对应关系，以提供精确和直观的视觉指导。然而，现有方法在处理极端视角变化、遮挡和小物体等挑战时效果不佳。

Method: 本文提出了一种新的EEC框架，该框架基于SAM 2，并结合了长期记忆，具体包括：1. 记忆-视图专家混合（Memory-View MoE）模块：包含双分支路由机制，用于自适应地分配通道和空间维度上每个专家特征的贡献权重。2. 双记忆库系统：采用简单有效的压缩策略，保留关键的长期信息，同时消除冗余。

Result: 在具有挑战性的EgoExo4D基准测试中，本文提出的LM-EEC方法取得了新的最先进结果，并显著优于现有方法和SAM 2基线。

Conclusion: 本文提出的LM-EEC方法有效地解决了EEC任务中存在的挑战，并在EgoExo4D基准测试中表现出强大的泛化能力，为智能视觉指导提供了更精确和直观的解决方案。

Abstract: Establishing object-level correspondence between egocentric and exocentric
views is essential for intelligent assistants to deliver precise and intuitive
visual guidance. However, this task faces numerous challenges, including
extreme viewpoint variations, occlusions, and the presence of small objects.
Existing approaches usually borrow solutions from video object segmentation
models, but still suffer from the aforementioned challenges. Recently, the
Segment Anything Model 2 (SAM 2) has shown strong generalization capabilities
and excellent performance in video object segmentation. Yet, when simply
applied to the ego-exo correspondence (EEC) task, SAM 2 encounters severe
difficulties due to ineffective ego-exo feature fusion and limited long-term
memory capacity, especially for long videos. Addressing these problems, we
propose a novel EEC framework based on SAM 2 with long-term memories by
presenting a dual-memory architecture and an adaptive feature routing module
inspired by Mixture-of-Experts (MoE). Compared to SAM 2, our approach features
(i) a Memory-View MoE module which consists of a dual-branch routing mechanism
to adaptively assign contribution weights to each expert feature along both
channel and spatial dimensions, and (ii) a dual-memory bank system with a
simple yet effective compression strategy to retain critical long-term
information while eliminating redundancy. In the extensive experiments on the
challenging EgoExo4D benchmark, our method, dubbed LM-EEC, achieves new
state-of-the-art results and significantly outperforms existing methods and the
SAM 2 baseline, showcasing its strong generalization across diverse scenarios.
Our code and model are available at https://github.com/juneyeeHu/LM-EEC.

</details>


### [147] [Enhancing Maritime Domain Awareness on Inland Waterways: A YOLO-Based Fusion of Satellite and AIS for Vessel Characterization](https://arxiv.org/abs/2510.11449)
*Geoffery Agorku,Sarah Hernandez,Hayley Hames,Cade Wagner*

Main category: cs.CV

TL;DR: 该论文提出了一个将高分辨率卫星图像与船舶AIS轨迹数据融合的框架，以增强内陆水域的海事领域感知（MDA），解决合作系统（如AIS）的漏洞，并实现了对“暗船”识别、合作交通验证和MDA支持。


<details>
  <summary>Details</summary>
Motivation: 解决内陆水域MDA中合作系统（如AIS）的漏洞，尤其是AIS数据在船舶监控方面的局限性。

Method: 1. 融合高分辨率卫星图像与船舶AIS轨迹数据。
2. 利用YOLO v11目标检测模型检测和识别船只及驳船，包括船舶类型、驳船覆盖、操作状态、驳船数量和航行方向。
3. 从密西西比河下游$5{,}973~	ext{mi}^2$的图像中建立了包含4,550个标注实例的数据集。

Result: 1. 船舶分类（拖船、起重驳船、散货船、货船和料斗驳船）F1分数达到95.8%。
2. 驳船覆盖（有盖或无盖）检测F1分数为91.6%。
3. 操作状态（停泊或航行中）分类F1分数为99.4%。
4. 方向性（上游、下游）准确率达到93.8%。
5. 驳船数量估计的平均绝对误差（MAE）为2.4艘驳船。
6. 跨地理分离河流段的空间可转移性分析显示，准确率仍高达98%。

Conclusion: 该研究证明了将非合作卫星遥感与AIS融合的可行性，该方法能够实现近实时船队盘点、支持异常检测，并为内陆水域监测生成高质量数据。未来的工作将包括扩展带注释的数据集、整合时间跟踪和探索多模态深度学习。

Abstract: Maritime Domain Awareness (MDA) for inland waterways remains challenged by
cooperative system vulnerabilities. This paper presents a novel framework that
fuses high-resolution satellite imagery with vessel trajectory data from the
Automatic Identification System (AIS). This work addresses the limitations of
AIS-based monitoring by leveraging non-cooperative satellite imagery and
implementing a fusion approach that links visual detections with AIS data to
identify dark vessels, validate cooperative traffic, and support advanced MDA.
The You Only Look Once (YOLO) v11 object detection model is used to detect and
characterize vessels and barges by vessel type, barge cover, operational
status, barge count, and direction of travel. An annotated data set of 4,550
instances was developed from $5{,}973~\mathrm{mi}^2$ of Lower Mississippi River
imagery. Evaluation on a held-out test set demonstrated vessel classification
(tugboat, crane barge, bulk carrier, cargo ship, and hopper barge) with an F1
score of 95.8\%; barge cover (covered or uncovered) detection yielded an F1
score of 91.6\%; operational status (staged or in motion) classification
reached an F1 score of 99.4\%. Directionality (upstream, downstream) yielded
93.8\% accuracy. The barge count estimation resulted in a mean absolute error
(MAE) of 2.4 barges. Spatial transferability analysis across geographically
disjoint river segments showed accuracy was maintained as high as 98\%. These
results underscore the viability of integrating non-cooperative satellite
sensing with AIS fusion. This approach enables near-real-time fleet
inventories, supports anomaly detection, and generates high-quality data for
inland waterway surveillance. Future work will expand annotated datasets,
incorporate temporal tracking, and explore multi-modal deep learning to further
enhance operational scalability.

</details>


### [148] [VA-GS: Enhancing the Geometric Representation of Gaussian Splatting via View Alignment](https://arxiv.org/abs/2510.11473)
*Qing Li,Huifang Feng,Xun Gong,Yu-Shen Liu*

Main category: cs.CV

TL;DR: 本文提出了一种通过视图对齐（VA）来增强3D高斯几何表示的新方法，解决了现有3D高斯飞溅技术在表面重建中几何不准确和多视图不一致的问题。


<details>
  <summary>Details</summary>
Motivation: 3D高斯飞溅在高质量实时新视图合成方面表现出色，但其在准确表面重建方面的能力尚未得到充分探索。由于高斯函数的离散和非结构化特性，仅基于图像渲染损失的监督往往导致不准确的几何形状和不一致的多视图对齐。

Method: 本文提出了一种新颖的方法，通过视图对齐（VA）增强3D高斯几何表示。具体来说，该方法将边缘感知图像线索纳入渲染损失中，以改善表面边界描绘。为了 M_07.31_enforce 视图间的几何一致性，引入了 L_07.31_visibility 感知的 C_07.31_photometric 对齐损失，该损失模拟了遮挡并鼓励高斯函数之间准确的空间关系。为了进一步减轻光照变化引起的模糊性，该方法结合了基于法线的约束来优化高斯函数的空间方向并改善局部表面估计。此外，利用深度图像特征嵌入来强制执行跨视图一致性，增强了在不同视点和光照下学习几何的鲁棒性。

Result: 在标准基准测试上进行了大量的实验，结果表明该方法在表面重建和新视图合成方面均达到了最先进的性能。

Conclusion: 本文提出的VA-GS方法通过结合边缘感知线索、可见性感知光度对齐损失、法线约束和深度图像特征嵌入，显著提高了3D高斯飞溅在表面重建和新视图合成中的几何精度和多视图一致性。

Abstract: 3D Gaussian Splatting has recently emerged as an efficient solution for
high-quality and real-time novel view synthesis. However, its capability for
accurate surface reconstruction remains underexplored. Due to the discrete and
unstructured nature of Gaussians, supervision based solely on image rendering
loss often leads to inaccurate geometry and inconsistent multi-view alignment.
In this work, we propose a novel method that enhances the geometric
representation of 3D Gaussians through view alignment (VA). Specifically, we
incorporate edge-aware image cues into the rendering loss to improve surface
boundary delineation. To enforce geometric consistency across views, we
introduce a visibility-aware photometric alignment loss that models occlusions
and encourages accurate spatial relationships among Gaussians. To further
mitigate ambiguities caused by lighting variations, we incorporate normal-based
constraints to refine the spatial orientation of Gaussians and improve local
surface estimation. Additionally, we leverage deep image feature embeddings to
enforce cross-view consistency, enhancing the robustness of the learned
geometry under varying viewpoints and illumination. Extensive experiments on
standard benchmarks demonstrate that our method achieves state-of-the-art
performance in both surface reconstruction and novel view synthesis. The source
code is available at https://github.com/LeoQLi/VA-GS.

</details>


### [149] [Towards Fast and Scalable Normal Integration using Continuous Components](https://arxiv.org/abs/2510.11508)
*Francesco Milano,Jen Jen Chung,Lionel Ott,Roland Siegwart*

Main category: cs.CV

TL;DR: 这篇论文提出了一种通过重新定义连续分量相对尺度估计，从而解决了大尺寸法线图的表面法线积分问题。该方法通过减少优化变量、平衡优化项和迭代合并分量，实现了最先进的结果和显著的速度提升。


<details>
  <summary>Details</summary>
Motivation: 现有的表面法线积分方法需要迭代全局优化来共同估计每个像素的深度，这种方法在大尺寸法线图上的扩展性较差。

Method: 通过将法线积分重新定义为连续分量相对尺度的估计问题来解决。通过约束属于相同分量的像素共同改变其尺度，从而大大减少了优化变量。该框架包括一个启发式方法，用于从一开始就准确估计连续分量，一种重新平衡优化项的策略，以及一种迭代合并分量以进一步减小问题规模的技术。

Result: 在标准法线积分基准测试中取得了最先进的结果，只需几秒钟，并且在大分辨率法线图上比像素级方法加速了一个数量级。

Conclusion: 所提出的方法通过创新的分量级优化策略，显著提高了表面法线积分的效率和准确性，尤其适用于处理大规模法线图。

Abstract: Surface normal integration is a fundamental problem in computer vision,
dealing with the objective of reconstructing a surface from its corresponding
normal map. Existing approaches require an iterative global optimization to
jointly estimate the depth of each pixel, which scales poorly to larger normal
maps. In this paper, we address this problem by recasting normal integration as
the estimation of relative scales of continuous components. By constraining
pixels belonging to the same component to jointly vary their scale, we
drastically reduce the number of optimization variables. Our framework includes
a heuristic to accurately estimate continuous components from the start, a
strategy to rebalance optimization terms, and a technique to iteratively merge
components to further reduce the size of the problem. Our method achieves
state-of-the-art results on the standard normal integration benchmark in as
little as a few seconds and achieves one-order-of-magnitude speedup over
pixel-level approaches on large-resolution normal maps.

</details>


### [150] [Situat3DChange: Situated 3D Change Understanding Dataset for Multimodal Large Language Model](https://arxiv.org/abs/2510.11509)
*Ruiping Liu,Junwei Zheng,Yufan Chen,Zirui Wang,Kunyu Peng,Kailun Yang,Jiaming Zhang,Marc Pollefeys,Rainer Stiefelhagen*

Main category: cs.CV

TL;DR: 该论文介绍了Situat3DChange数据集，旨在解决现有3D数据集在动态场景和动态情况理解方面的不足。 Situat3DChange包含12.1万个问答对、3.6万个变化描述和1.7万个重排指令，支持感知-行动模型下的情境感知变化理解任务。该数据集利用1.1万个人类对环境变化的观察来构建，并结合了自我中心和异我中心视角以及分类和坐标空间关系，通过大型语言模型（LLM）进行整合。 论文还提出了SCReasoner，一种高效的3D多模态大型语言模型（MLLM）方法，用于比较具有微小变化的相同场景的点云。在Situat3DChange任务上的综合评估表明了MLLM在动态场景和情境理解方面的进展和局限性。


<details>
  <summary>Details</summary>
Motivation: 目前的3D数据集和评估基准在动态场景或动态情况的理解方面存在局限性，导致不完整的理解。

Method: 1. 引入Situat3DChange数据集：一个支持三种情境感知变化理解任务的综合数据集，包括12.1万个问题-答案对，3.6万个变化描述用于感知任务，以及1.7万个重排指令用于行动任务。
2. 数据集构建：利用1.1万个人类对环境变化的观察，结合自我中心和异我中心视角，以及分类和坐标空间关系，并使用LLM进行整合，以支持情境变化的理解。
3. 提出SCReasoner：这是一种高效的3D多模态大型语言模型（MLLM）方法，旨在解决比较来自同一场景且只有微小变化的点云对的挑战。SCReasoner能够高效地进行点云比较，且参数开销小，语言解码器无需额外token。

Result: 1. 建立和发布了Situat3DChange数据集，包含12.1万个问答对，3.6万个变化描述和1.7万个重排指令。
2. 提出了SCReasoner，一种用于高效点云比较的3D MLLM方法。 
3. 在Situat3DChange任务上的综合评估揭示了多模态大型语言模型（MLLM）在动态场景和情境理解方面的进展和不足。
4. 数据扩展和跨域迁移的额外实验证明了使用Situat3DChange作为MLLM训练数据集的任务无关有效性。

Conclusion: Situat3DChange数据集和SCReasoner的引入推动了MLLM在动态场景和情境理解领域的研究。通过在具有丰富情境信息的综合数据集上进行训练和评估，该研究为未来开发更强大和通用的AI系统奠定了基础。尽管取得了进展，但在充分理解动态环境和情境方面仍存在局限性，这为未来的研究指明了方向。

Abstract: Physical environments and circumstances are fundamentally dynamic, yet
current 3D datasets and evaluation benchmarks tend to concentrate on either
dynamic scenarios or dynamic situations in isolation, resulting in incomplete
comprehension. To overcome these constraints, we introduce Situat3DChange, an
extensive dataset supporting three situation-aware change understanding tasks
following the perception-action model: 121K question-answer pairs, 36K change
descriptions for perception tasks, and 17K rearrangement instructions for the
action task. To construct this large-scale dataset, Situat3DChange leverages
11K human observations of environmental changes to establish shared mental
models and shared situational awareness for human-AI collaboration. These
observations, enriched with egocentric and allocentric perspectives as well as
categorical and coordinate spatial relations, are integrated using an LLM to
support understanding of situated changes. To address the challenge of
comparing pairs of point clouds from the same scene with minor changes, we
propose SCReasoner, an efficient 3D MLLM approach that enables effective point
cloud comparison with minimal parameter overhead and no additional tokens
required for the language decoder. Comprehensive evaluation on Situat3DChange
tasks highlights both the progress and limitations of MLLMs in dynamic scene
and situation understanding. Additional experiments on data scaling and
cross-domain transfer demonstrate the task-agnostic effectiveness of using
Situat3DChange as a training dataset for MLLMs.

</details>


### [151] [mmWalk: Towards Multi-modal Multi-view Walking Assistance](https://arxiv.org/abs/2510.11520)
*Kedi Ying,Ruiping Liu,Chongyan Chen,Mingzhe Tao,Hao Shi,Kailun Yang,Jiaming Zhang,Rainer Stiefelhagen*

Main category: cs.CV

TL;DR: 该文提出了mmWalk数据集和mmWalkVQA基准，旨在解决视障人士在复杂环境中行走辅助的挑战，并通过评估现有视觉语言模型和验证微调模型，证明了其在多模态行走辅助方面的有效性。


<details>
  <summary>Details</summary>
Motivation: 解决视障人士在极端或复杂环境中因缺乏整体场景理解而面临的行走辅助挑战。

Method: 1. 构建mmWalk模拟多模态数据集，包含120条手动控制、场景分类的行走轨迹和6.2万帧同步数据，涵盖55.9万张全景图像（RGB、深度、语义）。
2. 数据集中包含户外特殊情况和针对视障用户的无障碍地标，以增强现实相关性。
3. 生成mmWalkVQA，一个包含6.9万个视觉问答对的VQA基准，分为9个类别，专门用于安全和智能的行走辅助。
4. 使用零样本和少样本设置评估了最先进的视觉语言模型（VLMs）。
5. 在真实世界数据集中验证了经过mmWalk微调的模型。

Result: 1. 最先进的视觉语言模型在mmWalkVQA的风险评估和导航任务上表现不佳。
2. 经过mmWalk微调的模型在真实世界数据集上表现出有效性。

Conclusion: mmWalk数据集和mmWalkVQA基准能够有效推动多模态行走辅助技术的发展，现有视觉语言模型在视障人士行走辅助任务上仍有提升空间。

Abstract: Walking assistance in extreme or complex environments remains a significant
challenge for people with blindness or low vision (BLV), largely due to the
lack of a holistic scene understanding. Motivated by the real-world needs of
the BLV community, we build mmWalk, a simulated multi-modal dataset that
integrates multi-view sensor and accessibility-oriented features for outdoor
safe navigation. Our dataset comprises 120 manually controlled,
scenario-categorized walking trajectories with 62k synchronized frames. It
contains over 559k panoramic images across RGB, depth, and semantic modalities.
Furthermore, to emphasize real-world relevance, each trajectory involves
outdoor corner cases and accessibility-specific landmarks for BLV users.
Additionally, we generate mmWalkVQA, a VQA benchmark with over 69k visual
question-answer triplets across 9 categories tailored for safe and informed
walking assistance. We evaluate state-of-the-art Vision-Language Models (VLMs)
using zero- and few-shot settings and found they struggle with our risk
assessment and navigational tasks. We validate our mmWalk-finetuned model on
real-world datasets and show the effectiveness of our dataset for advancing
multi-modal walking assistance.

</details>


### [152] [ODI-Bench: Can MLLMs Understand Immersive Omnidirectional Environments?](https://arxiv.org/abs/2510.11549)
*Liu Yang,Huiyu Duan,Ran Tao,Juntao Cheng,Sijing Wu,Yunhao Li,Jing Liu,Xiongkuo Min,Guangtao Zhai*

Main category: cs.CV

TL;DR: 这篇论文介绍了一个名为ODI-Bench的新基准，用于评估多模态大语言模型（MLLMs）在全向图像（ODIs）理解方面的能力。论文还提出了Omni-CoT，一种无需训练的方法来增强MLLMs对全向图像的理解。


<details>
  <summary>Details</summary>
Motivation: 尽管多模态大语言模型（MLLMs）在传统2D图像和视频理解方面表现出色，但它们对全向图像（ODIs）所捕获的沉浸式环境的理解能力尚未得到充分探索。

Method: 本文首先提出了ODI-Bench，一个专门为全向图像理解设计的综合基准。ODI-Bench包含2,000张高质量全向图像和超过4,000个人工标注的问答对，涵盖10个细粒度任务，包括通用级别和空间级别的全向图像理解。为了解决现有MLLMs在全向图像理解方面的不足，论文进一步引入了Omni-CoT，这是一种无需训练的方法，通过结合文本信息和视觉线索的思维链推理，显著增强了MLLMs在全向环境中的理解能力。

Result: 广泛的实验对20个具有代表性的多模态大语言模型（MLLMs）进行了基准测试，包括专有模型和开源模型，涵盖封闭式和开放式设置。实验结果表明，当前的MLLMs仍然难以捕捉由全向图像（ODIs）提供的沉浸式上下文。Omni-CoT方法显著增强了MLLMs在全向环境中的理解能力。

Conclusion: 目前的多模态大语言模型（MLLMs）在理解全向图像（ODIs）提供的沉浸式上下文方面仍面临挑战。ODI-Bench基准和Omni-CoT方法为未来全向图像理解的研究提供了工具和方向。

Abstract: Omnidirectional images (ODIs) provide full 360x180 view which are widely
adopted in VR, AR and embodied intelligence applications. While multi-modal
large language models (MLLMs) have demonstrated remarkable performance on
conventional 2D image and video understanding benchmarks, their ability to
comprehend the immersive environments captured by ODIs remains largely
unexplored. To address this gap, we first present ODI-Bench, a novel
comprehensive benchmark specifically designed for omnidirectional image
understanding. ODI-Bench contains 2,000 high-quality omnidirectional images and
over 4,000 manually annotated question-answering (QA) pairs across 10
fine-grained tasks, covering both general-level and spatial-level ODI
understanding. Extensive experiments are conducted to benchmark 20
representative MLLMs, including proprietary and open-source models, under both
close-ended and open-ended settings. Experimental results reveal that current
MLLMs still struggle to capture the immersive context provided by ODIs. To this
end, we further introduce Omni-CoT, a training-free method which significantly
enhances MLLMs' comprehension ability in the omnidirectional environment
through chain-of-thought reasoning across both textual information and visual
cues. Both the benchmark and the code will be released upon the publication.

</details>


### [153] [How many samples to label for an application given a foundation model? Chest X-ray classification study](https://arxiv.org/abs/2510.11553)
*Nikolay Nechaev,Evgenia Przhezdzetskaya,Viktor Gombolevskiy,Dmitry Umerenkov,Dmitry Dylov*

Main category: cs.CV

TL;DR: 本文探讨了基础模型在胸部X光分类中如何在有限标注数据下实现良好性能，并发现幂律拟合可以预测达到特定性能阈值所需的训练数据量。


<details>
  <summary>Details</summary>
Motivation: 胸部X光分类对于诊断至关重要，但通常需要大量的标注数据。基础模型可以减少对大量标注数据的依赖，但目前尚不清楚到底需要多少标注样本才能达到满意效果。

Method: 我们系统地评估了使用幂律拟合来预测达到特定ROC-AUC阈值所需的训练数据大小。我们测试了多种病理和基础模型，并比较了XrayCLIP、XraySigLIP和ResNet-50基线的性能。

Result: XrayCLIP和XraySigLIP在比ResNet-50基线少得多的标注样本下，取得了强大的性能。更重要的是，仅使用50个标注病例的学习曲线斜率就能准确预测最终性能的平台期。

Conclusion: 我们的研究结果使得实践者可以通过仅标注达到目标性能所需的最少样本，从而最大限度地降低标注成本。

Abstract: Chest X-ray classification is vital yet resource-intensive, typically
demanding extensive annotated data for accurate diagnosis. Foundation models
mitigate this reliance, but how many labeled samples are required remains
unclear. We systematically evaluate the use of power-law fits to predict the
training size necessary for specific ROC-AUC thresholds. Testing multiple
pathologies and foundation models, we find XrayCLIP and XraySigLIP achieve
strong performance with significantly fewer labeled examples than a ResNet-50
baseline. Importantly, learning curve slopes from just 50 labeled cases
accurately forecast final performance plateaus. Our results enable
practitioners to minimize annotation costs by labeling only the essential
samples for targeted performance.

</details>


### [154] [SNAP: Towards Segmenting Anything in Any Point Cloud](https://arxiv.org/abs/2510.11565)
*Aniket Gupta,Hanhui Wang,Charles Saunders,Aruni RoyChowdhury,Hanumant Singh,Huaizu Jiang*

Main category: cs.CV

TL;DR: SNAP是一个统一的交互式3D点云分割模型，支持跨领域（室内、室外、航空）的点和文本提示，实现了对9个零样本基准测试中8个的最先进性能。


<details>
  <summary>Details</summary>
Motivation: 目前的交互式3D点云分割方法在应用范围（限于单一领域）和用户交互形式（仅限于空间点击或文本提示）上存在局限性，并且在多个数据集上训练时容易出现负迁移，导致缺乏通用性。

Method: SNAP模型通过在包含室内、室外和航空环境的7个数据集上进行训练，并采用域自适应归一化来防止负迁移，从而实现跨领域泛化。对于文本提示分割，它自动生成掩码提议，并将其与文本查询的CLIP嵌入进行匹配，从而支持全景和开放词汇分割。

Result: SNAP在9个空间提示分割零样本基准测试中的8个上取得了最先进的性能，并在所有5个文本提示基准测试中取得了有竞争力的结果。

Conclusion: SNAP作为一个统一模型，能够匹配或超越专门的领域特定方法，为可扩展的3D标注提供了一个实用的工具，并持续提供高质量的分割结果。

Abstract: Interactive 3D point cloud segmentation enables efficient annotation of
complex 3D scenes through user-guided prompts. However, current approaches are
typically restricted in scope to a single domain (indoor or outdoor), and to a
single form of user interaction (either spatial clicks or textual prompts).
Moreover, training on multiple datasets often leads to negative transfer,
resulting in domain-specific tools that lack generalizability. To address these
limitations, we present \textbf{SNAP} (\textbf{S}egment a\textbf{N}ything in
\textbf{A}ny \textbf{P}oint cloud), a unified model for interactive 3D
segmentation that supports both point-based and text-based prompts across
diverse domains. Our approach achieves cross-domain generalizability by
training on 7 datasets spanning indoor, outdoor, and aerial environments, while
employing domain-adaptive normalization to prevent negative transfer. For
text-prompted segmentation, we automatically generate mask proposals without
human intervention and match them against CLIP embeddings of textual queries,
enabling both panoptic and open-vocabulary segmentation. Extensive experiments
demonstrate that SNAP consistently delivers high-quality segmentation results.
We achieve state-of-the-art performance on 8 out of 9 zero-shot benchmarks for
spatial-prompted segmentation and demonstrate competitive results on all 5
text-prompted benchmarks. These results show that a unified model can match or
exceed specialized domain-specific approaches, providing a practical tool for
scalable 3D annotation. Project page is at, https://neu-vi.github.io/SNAP/

</details>


### [155] [A Framework for Low-Effort Training Data Generation for Urban Semantic Segmentation](https://arxiv.org/abs/2510.11567)
*Denis Zavadski,Damjan Kalšan,Tim Küchler,Haebom Lee,Stefan Roth,Carsten Rother*

Main category: cs.CV

TL;DR: 该论文介绍了一个新的框架，旨在使用不完美的伪标签将现成的扩散模型适应到目标领域。


<details>
  <summary>Details</summary>
Motivation: 传统的合成数据集在城市场景识别模型训练中与真实图像存在显著差距，尤其是在适应特定目标领域时，这限制了下游性能。通过更详细的3D建模来弥补这一差距成本太高，违背了低成本标记数据的初衷。

Method: 本文提出了一个新框架，该框架使用不完善的伪标签将现成的扩散模型适应到目标领域。该方法可以过滤次优的生成，纠正图像标签错位，并标准化跨数据集的语义，将弱合成数据转化为有竞争力的真实领域训练集。

Result: 在五个合成数据集和两个真实目标数据集上的实验表明，在mIoU方面，分割增益比最先进的翻译方法高出+8.0%pt。这使得快速构建的合成数据集与需要大量手动设计的高投入、耗时的合成数据集一样有效。

Conclusion: 这项工作突出了一种有价值的协同范式，其中快速语义原型设计与生成模型相结合，能够为城市场景理解创建可扩展的高质量训练数据。

Abstract: Synthetic datasets are widely used for training urban scene recognition
models, but even highly realistic renderings show a noticeable gap to real
imagery. This gap is particularly pronounced when adapting to a specific target
domain, such as Cityscapes, where differences in architecture, vegetation,
object appearance, and camera characteristics limit downstream performance.
Closing this gap with more detailed 3D modelling would require expensive asset
and scene design, defeating the purpose of low-cost labelled data. To address
this, we present a new framework that adapts an off-the-shelf diffusion model
to a target domain using only imperfect pseudo-labels. Once trained, it
generates high-fidelity, target-aligned images from semantic maps of any
synthetic dataset, including low-effort sources created in hours rather than
months. The method filters suboptimal generations, rectifies image-label
misalignments, and standardises semantics across datasets, transforming weak
synthetic data into competitive real-domain training sets. Experiments on five
synthetic datasets and two real target datasets show segmentation gains of up
to +8.0%pt. mIoU over state-of-the-art translation methods, making rapidly
constructed synthetic datasets as effective as high-effort, time-intensive
synthetic datasets requiring extensive manual design. This work highlights a
valuable collaborative paradigm where fast semantic prototyping, combined with
generative models, enables scalable, high-quality training data creation for
urban scene understanding.

</details>


### [156] [Benchmarking foundation models for hyperspectral image classification: Application to cereal crop type mapping](https://arxiv.org/abs/2510.11576)
*Walid Elbarz,Mohamed Bourriz,Hicham Hajji,Hamd Ait Abdelali,François Bourzeix*

Main category: cs.CV

TL;DR: 这篇论文评估了三种基础模型（HyperSigma、DOFA和SpectralEarth预训练的Vision Transformers）在利用高光谱图像进行谷物作物测绘方面的性能。


<details>
  <summary>Details</summary>
Motivation: 探索基础模型在高光谱作物测绘领域的应用潜力。

Method: 使用来自训练区域的手动标记数据对模型进行微调，并在独立的测试区域进行评估。性能指标包括整体准确度（OA）、平均准确度（AA）和F1分数。

Result: HyperSigma的OA为34.5%，DOFA为62.6%。SpectralEarth模型表现最佳，OA达到93.5%。一个从头开始训练的紧凑型SpectralEarth变体也达到了91%的OA。

Conclusion: SpectralEarth模型在跨地理区域和传感器平台的高光谱作物测绘中表现出强大的泛化能力，凸显了模型架构的重要性，并为未来的模型开发指明了方向。

Abstract: Foundation models are transforming Earth observation, but their potential for
hyperspectral crop mapping remains underexplored. This study benchmarks three
foundation models for cereal crop mapping using hyperspectral imagery:
HyperSigma, DOFA, and Vision Transformers pre-trained on the SpectralEarth
dataset (a large multitemporal hyperspectral archive). Models were fine-tuned
on manually labeled data from a training region and evaluated on an independent
test region. Performance was measured with overall accuracy (OA), average
accuracy (AA), and F1-score. HyperSigma achieved an OA of 34.5% (+/- 1.8%),
DOFA reached 62.6% (+/- 3.5%), and the SpectralEarth model achieved an OA of
93.5% (+/- 0.8%). A compact SpectralEarth variant trained from scratch achieved
91%, highlighting the importance of model architecture for strong
generalization across geographic regions and sensor platforms. These results
provide a systematic evaluation of foundation models for operational
hyperspectral crop mapping and outline directions for future model development.

</details>


### [157] [ACE-G: Improving Generalization of Scene Coordinate Regression Through Query Pre-Training](https://arxiv.org/abs/2510.11605)
*Leonard Bruns,Axel Barroso-Laguna,Tommaso Cavallari,Áron Monszpart,Sowmya Munukutla,Victor Adrian Prisacariu,Eric Brachmann*

Main category: cs.CV

TL;DR: 这篇论文提出了一种名为ACE-G的新型场景坐标回归（SCR）方法，通过将坐标回归器和地图表示分离，解决了传统SCR模型泛化能力不足的问题。


<details>
  <summary>Details</summary>
Motivation: 传统的场景坐标回归（SCR）方法在视觉重定位方面表现出色，但其泛化能力不足，尤其是在查询图像的成像条件（如光照或视角）与训练视图差异过大时，模型会失效。这是因为SCR模型的训练目标是将训练视图编码到坐标回归器本身的权重中，导致回归器对训练视图过度拟合。

Method: 本文提出将坐标回归器和地图表示分离为通用的Transformer模块和场景特定的地图代码。这种分离使得Transformer可以在数万个场景上进行预训练。更重要的是，它允许在预训练期间，训练Transformer从映射图像泛化到未见过的查询图像。

Result: 在多个具有挑战性的重定位数据集上进行的实验表明，ACE-G方法显著提高了鲁棒性，同时保持了较低的计算成本。

Conclusion: ACE-G通过将坐标回归器和地图表示分离，并使用预训练的Transformer，有效地解决了传统SCR方法泛化能力差的问题，在视觉重定位任务中表现出更高的鲁棒性和更低的计算成本。

Abstract: Scene coordinate regression (SCR) has established itself as a promising
learning-based approach to visual relocalization. After mere minutes of
scene-specific training, SCR models estimate camera poses of query images with
high accuracy. Still, SCR methods fall short of the generalization capabilities
of more classical feature-matching approaches. When imaging conditions of query
images, such as lighting or viewpoint, are too different from the training
views, SCR models fail. Failing to generalize is an inherent limitation of
previous SCR frameworks, since their training objective is to encode the
training views in the weights of the coordinate regressor itself. The regressor
essentially overfits to the training views, by design. We propose to separate
the coordinate regressor and the map representation into a generic transformer
and a scene-specific map code. This separation allows us to pre-train the
transformer on tens of thousands of scenes. More importantly, it allows us to
train the transformer to generalize from mapping images to unseen query images
during pre-training. We demonstrate on multiple challenging relocalization
datasets that our method, ACE-G, leads to significantly increased robustness
while keeping the computational footprint attractive.

</details>


### [158] [ExpVid: A Benchmark for Experiment Video Understanding & Reasoning](https://arxiv.org/abs/2510.11606)
*Yicheng Xu,Yue Wu,Jiashuo Yu,Ziang Yan,Tianxiang Jiang,Yinan He,Qingsong Zhao,Kai Chen,Yu Qiao,Limin Wang,Manabu Okumura,Yi Wang*

Main category: cs.CV

TL;DR: ExpVid是第一个专门为评估MLLM在科学实验视频方面能力而设计的基准。 它具有一个三级任务层次结构，反映了科学过程，并揭示了现有MLLM在处理精细细节、跟踪状态变化和连接实验过程与科学结论方面的不足。


<details>
  <summary>Details</summary>
Motivation: 现有的基准忽视了真实实验室工作（尤其是湿实验室环境）的细粒度和长周期性，导致人们对多模态大型语言模型（MLLM）的真实能力知之甚少。

Method: 论文引入了ExpVid，这是第一个旨在系统评估MLLM在科学实验视频方面的基准。该基准从同行评审的视频出版物中整理，并采用了反映科学过程的全新三级任务层次结构，包括工具、材料和动作的细粒度感知、步骤顺序和完整性的程序理解，以及将完整实验与其已发布结论联系起来的科学推理。论文采用了一种以视觉为中心的注释流程，结合自动化生成和多学科专家验证，确保任务需要视觉基础。

Result: 通过在ExpVid上评估19个主流MLLM，发现它们在粗粒度识别方面表现出色，但在分辨精细细节、随时间跟踪状态变化以及将实验过程与科学成果联系起来方面存在困难。研究结果揭示了专有模型和开源模型之间存在显著的性能差距，尤其是在高阶推理方面。

Conclusion: ExpVid不仅提供了一个诊断工具，还为开发能够成为科学实验中值得信赖的伙伴的MLLM描绘了路线图。

Abstract: Multimodal Large Language Models (MLLMs) hold promise for accelerating
scientific discovery by interpreting complex experimental procedures. However,
their true capabilities are poorly understood, as existing benchmarks neglect
the fine-grained and long-horizon nature of authentic laboratory work,
especially in wet-lab settings. To bridge this gap, we introduce ExpVid, the
first benchmark designed to systematically evaluate MLLMs on scientific
experiment videos. Curated from peer-reviewed video publications, ExpVid
features a new three-level task hierarchy that mirrors the scientific process:
(1) Fine-grained Perception of tools, materials, and actions; (2) Procedural
Understanding of step order and completeness; and (3) Scientific Reasoning that
connects the full experiment to its published conclusions. Our vision-centric
annotation pipeline, combining automated generation with multi-disciplinary
expert validation, ensures that tasks require visual grounding. We evaluate 19
leading MLLMs on ExpVid and find that while they excel at coarse-grained
recognition, they struggle with disambiguating fine details, tracking state
changes over time, and linking experimental procedures to scientific outcomes.
Our results reveal a notable performance gap between proprietary and
open-source models, particularly in high-order reasoning. ExpVid not only
provides a diagnostic tool but also charts a roadmap for developing MLLMs
capable of becoming trustworthy partners in scientific experimentation.

</details>


### [159] [IVEBench: Modern Benchmark Suite for Instruction-Guided Video Editing Assessment](https://arxiv.org/abs/2510.11647)
*Yinan Chen,Jiangning Zhang,Teng Hu,Yuxiang Zeng,Zhucun Xue,Qingdong He,Chengjie Wang,Yong Liu,Xiaobin Hu,Shuicheng Yan*

Main category: cs.CV

TL;DR: IVEBench 是一个为指令引导视频编辑评估设计的基准测试套件。


<details>
  <summary>Details</summary>
Motivation: 现有视频编辑基准未能充分支持指令引导视频编辑的评估，并且在来源多样性、任务覆盖和评估指标方面存在局限。

Method: IVEBench 包含一个包含 600 个高质量源视频的多样化数据库，涵盖七个语义维度和 32 到 1,024 帧的视频长度。它还包括 8 类编辑任务和 35 个子类别，其提示通过大型语言模型和专家评审生成和完善。IVEBench 建立了一个包含视频质量、指令依从性和视频保真度的三维评估协议，并结合了传统指标和基于多模态大型语言模型的评估。

Result: 广泛的实验证明了 IVEBench 在评估最先进的指令引导视频编辑方法方面的有效性，表明它能够提供全面且与人类评估一致的结果。

Conclusion: IVEBench 填补了指令引导视频编辑评估中的空白，为该领域的研究提供了一个全面而有效的工具。

Abstract: Instruction-guided video editing has emerged as a rapidly advancing research
direction, offering new opportunities for intuitive content transformation
while also posing significant challenges for systematic evaluation. Existing
video editing benchmarks fail to support the evaluation of instruction-guided
video editing adequately and further suffer from limited source diversity,
narrow task coverage and incomplete evaluation metrics. To address the above
limitations, we introduce IVEBench, a modern benchmark suite specifically
designed for instruction-guided video editing assessment. IVEBench comprises a
diverse database of 600 high-quality source videos, spanning seven semantic
dimensions, and covering video lengths ranging from 32 to 1,024 frames. It
further includes 8 categories of editing tasks with 35 subcategories, whose
prompts are generated and refined through large language models and expert
review. Crucially, IVEBench establishes a three-dimensional evaluation protocol
encompassing video quality, instruction compliance and video fidelity,
integrating both traditional metrics and multimodal large language model-based
assessments. Extensive experiments demonstrate the effectiveness of IVEBench in
benchmarking state-of-the-art instruction-guided video editing methods, showing
its ability to provide comprehensive and human-aligned evaluation outcomes.

</details>


### [160] [Beyond 'Templates': Category-Agnostic Object Pose, Size, and Shape Estimation from a Single View](https://arxiv.org/abs/2510.11687)
*Jinyu Zhang,Haitao Lin,Jiashu Hou,Xiangyang Xue,Yanwei Fu*

Main category: cs.CV

TL;DR: 这篇论文提出了一个统一的、类别无关的框架，可以从单个RGB-D图像中同时预测物体的6D位姿、尺寸和密集形状，无需模板、CAD模型或类别标签，并在多个基准测试中取得了最先进的性能，尤其是对未见过的真实世界物体也展现出强大的零样本泛化能力。


<details>
  <summary>Details</summary>
Motivation: 现有的物体6D位姿、尺寸和形状估计方法要么依赖于物体特定的先验知识（如CAD模型），要么由于位姿-形状纠缠和多阶段管线而导致泛化能力有限。

Method: 本研究提出了一个统一的、类别无关的框架，该框架将视觉基础模型中的密集2D特征与部分3D点云融合，使用Transformer编码器并辅以MoE（Mixture-of-Experts）机制，并采用并行解码器进行位姿-尺寸估计和形状重建。

Result: 该框架在SOPE、ROPE、ObjaversePose和HANDAL这四个涵盖300多个类别的基准测试中，对已知类别实现了最先进的准确性，并对未见过的真实世界物体展示了强大的零样本泛化能力。它以28 FPS实现实时推理。

Conclusion: 本研究提出的框架在机器人和具身AI领域的开放集6D理解方面建立了新标准，为从单一RGB-D图像中同时进行6D位姿、尺寸和密集形状估计提供了有效且通用的解决方案。

Abstract: Estimating an object's 6D pose, size, and shape from visual input is a
fundamental problem in computer vision, with critical applications in robotic
grasping and manipulation. Existing methods either rely on object-specific
priors such as CAD models or templates, or suffer from limited generalization
across categories due to pose-shape entanglement and multi-stage pipelines. In
this work, we propose a unified, category-agnostic framework that
simultaneously predicts 6D pose, size, and dense shape from a single RGB-D
image, without requiring templates, CAD models, or category labels at test
time. Our model fuses dense 2D features from vision foundation models with
partial 3D point clouds using a Transformer encoder enhanced by a
Mixture-of-Experts, and employs parallel decoders for pose-size estimation and
shape reconstruction, achieving real-time inference at 28 FPS. Trained solely
on synthetic data from 149 categories in the SOPE dataset, our framework is
evaluated on four diverse benchmarks SOPE, ROPE, ObjaversePose, and HANDAL,
spanning over 300 categories. It achieves state-of-the-art accuracy on seen
categories while demonstrating remarkably strong zero-shot generalization to
unseen real-world objects, establishing a new standard for open-set 6D
understanding in robotics and embodied AI.

</details>


### [161] [Diffusion Transformers with Representation Autoencoders](https://arxiv.org/abs/2510.11690)
*Boyang Zheng,Nanye Ma,Shengbang Tong,Saining Xie*

Main category: cs.CV

TL;DR: 本文提出了一种新的表示自编码器（RAE）来替代DiT模型中传统的VAE，通过结合预训练的表示编码器和训练过的解码器，改善了潜在空间的质量和生成效果。


<details>
  <summary>Details</summary>
Motivation: 现有的DiT模型大多依赖于过时的VAE编码器，导致了架构复杂性、潜在空间信息容量不足以及生成质量受限等问题。

Method: 本文提出用预训练的表示编码器（如DINO, SigLIP, MAE）与训练过的解码器相结合，形成表示自编码器（RAEs）。针对高维潜在空间带来的挑战，文章分析了其来源并提出了理论和经验验证的解决方案，以实现Diffusion Transformers在其中有效运行。

Result: RAE实现了更快的收敛速度，并且无需辅助表示对齐损失。使用配备轻量级、宽幅DDT头的DiT变体，在ImageNet上获得了令人瞩目的图像生成结果：在256x256分辨率下无引导FID为1.51，在256x256和512x512分辨率下有引导FID均为1.13。

Conclusion: 表示自编码器（RAE）克服了传统VAE的局限性，提供了高质量的重建和语义丰富的潜在空间，并且易于与可扩展的Transformer架构结合，应当成为Diffusion Transformer训练的新默认选择。

Abstract: Latent generative modeling, where a pretrained autoencoder maps pixels into a
latent space for the diffusion process, has become the standard strategy for
Diffusion Transformers (DiT); however, the autoencoder component has barely
evolved. Most DiTs continue to rely on the original VAE encoder, which
introduces several limitations: outdated backbones that compromise
architectural simplicity, low-dimensional latent spaces that restrict
information capacity, and weak representations that result from purely
reconstruction-based training and ultimately limit generative quality. In this
work, we explore replacing the VAE with pretrained representation encoders
(e.g., DINO, SigLIP, MAE) paired with trained decoders, forming what we term
Representation Autoencoders (RAEs). These models provide both high-quality
reconstructions and semantically rich latent spaces, while allowing for a
scalable transformer-based architecture. Since these latent spaces are
typically high-dimensional, a key challenge is enabling diffusion transformers
to operate effectively within them. We analyze the sources of this difficulty,
propose theoretically motivated solutions, and validate them empirically. Our
approach achieves faster convergence without auxiliary representation alignment
losses. Using a DiT variant equipped with a lightweight, wide DDT head, we
achieve strong image generation results on ImageNet: 1.51 FID at 256x256 (no
guidance) and 1.13 at both 256x256 and 512x512 (with guidance). RAE offers
clear advantages and should be the new default for diffusion transformer
training.

</details>


### [162] [DiT360: High-Fidelity Panoramic Image Generation via Hybrid Training](https://arxiv.org/abs/2510.11712)
*Haoran Feng,Dizhe Zhang,Xiangtai Li,Bo Du,Lu Qi*

Main category: cs.CV

TL;DR: DiT360是一个基于DiT的框架，通过在透视和全景数据上进行混合训练来生成全景图像。它通过视角图像引导和全景细化在图像层面融入跨域知识，并通过循环填充、偏航损失和立方体损失在表示层面应用混合监督，从而在文本到全景、图像修复和图像外绘制任务中实现更好的边界一致性和图像保真度。


<details>
  <summary>Details</summary>
Motivation: 目前全景图像生成存在几何保真度和真实感问题，主要原因是缺乏大规模、高质量的真实世界全景数据。

Method: DiT360框架包含几个关键模块，用于域间转换和域内增强。在图像层面，它通过视角图像引导和全景细化来融入跨域知识。在表示层面，它通过循环填充、偏航损失和立方体损失应用混合监督。

Result: 在文本到全景、图像修复和图像外绘制任务中，DiT360在11项定量指标上实现了更好的边界一致性和图像保真度。

Conclusion: DiT360通过其独特的混合训练策略和跨域知识融合，有效解决了全景图像生成中的几何保真度和真实感问题，并在多项任务中取得了显著的性能提升。

Abstract: In this work, we propose DiT360, a DiT-based framework that performs hybrid
training on perspective and panoramic data for panoramic image generation. For
the issues of maintaining geometric fidelity and photorealism in generation
quality, we attribute the main reason to the lack of large-scale, high-quality,
real-world panoramic data, where such a data-centric view differs from prior
methods that focus on model design. Basically, DiT360 has several key modules
for inter-domain transformation and intra-domain augmentation, applied at both
the pre-VAE image level and the post-VAE token level. At the image level, we
incorporate cross-domain knowledge through perspective image guidance and
panoramic refinement, which enhance perceptual quality while regularizing
diversity and photorealism. At the token level, hybrid supervision is applied
across multiple modules, which include circular padding for boundary
continuity, yaw loss for rotational robustness, and cube loss for distortion
awareness. Extensive experiments on text-to-panorama, inpainting, and
outpainting tasks demonstrate that our method achieves better boundary
consistency and image fidelity across eleven quantitative metrics. Our code is
available at https://github.com/Insta360-Research-Team/DiT360.

</details>


### [163] [Point Prompting: Counterfactual Tracking with Video Diffusion Models](https://arxiv.org/abs/2510.11715)
*Ayush Shrivastava,Sanyam Mehta,Daniel Geng,Andrew Owens*

Main category: cs.CV

TL;DR: 本文提出了一种利用预训练视频扩散模型进行零样本点跟踪的方法。


<details>
  <summary>Details</summary>
Motivation: 跟踪器分析运动，视频生成器合成运动，两者密切相关。本文旨在利用这种关系，使预训练视频扩散模型执行零样本点跟踪。

Method: 该方法通过在查询点放置一个独特颜色的标记，然后从中间噪声水平重新生成视频的其余部分来传播标记，从而描绘出点的轨迹。为了确保标记在反事实生成中保持可见，即使这种标记在自然视频中不太可能出现，本文使用了未经编辑的初始帧作为负面提示。

Result: 通过对多个图像条件视频扩散模型的实验，发现这些“ emergent”轨迹优于以前的零样本方法，并且在遮挡中持续存在，通常可以获得与专门的自监督模型相当的性能。

Conclusion: 预训练视频扩散模型能够通过简单的提示进行零样本点跟踪，并且在性能上优于现有零样本方法，具有与专门自监督模型竞争的潜力。

Abstract: Trackers and video generators solve closely related problems: the former
analyze motion, while the latter synthesize it. We show that this connection
enables pretrained video diffusion models to perform zero-shot point tracking
by simply prompting them to visually mark points as they move over time. We
place a distinctively colored marker at the query point, then regenerate the
rest of the video from an intermediate noise level. This propagates the marker
across frames, tracing the point's trajectory. To ensure that the marker
remains visible in this counterfactual generation, despite such markers being
unlikely in natural videos, we use the unedited initial frame as a negative
prompt. Through experiments with multiple image-conditioned video diffusion
models, we find that these "emergent" tracks outperform those of prior
zero-shot methods and persist through occlusions, often obtaining performance
that is competitive with specialized self-supervised models.

</details>


### [164] [Ev4DGS: Novel-view Rendering of Non-Rigid Objects from Monocular Event Streams](https://arxiv.org/abs/2510.11717)
*Takuya Nakabayashi,Navami Kairanda,Hideo Saito,Vladislav Golyanik*

Main category: cs.CV

TL;DR: 本文介绍了Ev4DGS，一种从单目事件流中对非刚性变形物体进行新颖视图渲染的方法，无需RGB输入。


<details>
  <summary>Details</summary>
Motivation: 目前，现有方法在非刚性物体的新颖视图渲染中需要稀疏的RGB输入，而仅从事件流中学习类似模型的可行性尚不明确。

Method: Ev4DGS通过1) 将估计模型的输出与2D事件观测空间关联起来的损失函数，以及2) 从事件生成的二值掩模训练的粗略3D变形模型，回归出可变形的3D高斯Splatting表示。

Result: 实验结果表明，Ev4DGS具有有效性，并且与多个基线方法相比表现出卓越的性能。

Conclusion: Ev4DGS是第一个仅从单目事件流中对非刚性变形物体进行新颖视图渲染的方法，无需RGB输入，填补了该领域的空白。

Abstract: Event cameras offer various advantages for novel view rendering compared to
synchronously operating RGB cameras, and efficient event-based techniques
supporting rigid scenes have been recently demonstrated in the literature. In
the case of non-rigid objects, however, existing approaches additionally
require sparse RGB inputs, which can be a substantial practical limitation; it
remains unknown if similar models could be learned from event streams only.
This paper sheds light on this challenging open question and introduces Ev4DGS,
i.e., the first approach for novel view rendering of non-rigidly deforming
objects in the explicit observation space (i.e., as RGB or greyscale images)
from monocular event streams. Our method regresses a deformable 3D Gaussian
Splatting representation through 1) a loss relating the outputs of the
estimated model with the 2D event observation space, and 2) a coarse 3D
deformation model trained from binary masks generated from events. We perform
experimental comparisons on existing synthetic and newly recorded real datasets
with non-rigid objects. The results demonstrate the validity of Ev4DGS and its
superior performance compared to multiple naive baselines that can be applied
in our setting. We will release our models and the datasets used in the
evaluation for research purposes; see the project webpage:
https://4dqv.mpi-inf.mpg.de/Ev4DGS/.

</details>


<div id='cs.AI'></div>

# cs.AI [[Back]](#toc)

### [165] [AI and Consciousness](https://arxiv.org/abs/2510.09858)
*Eric Schwitzgebel*

Main category: cs.AI

TL;DR: 这篇论文对AI意识文献进行了概述，指出AI系统可能很快就能满足某些主流意识理论的定义，但不满足另一些，且我们无法判断哪种理论正确。


<details>
  <summary>Details</summary>
Motivation: 探讨AI意识问题，指出对于AI是否具有意识，以及哪种意识理论是正确的，我们目前没有明确答案。

Method: 通过对现有AI意识文献的批判性回顾，并列举了十个可能的意识基本特征，对各种主流理论（例如全局工作空间理论、高阶理论、整合信息理论等）和相关论证（如图灵测试、中文房间、模仿论证）进行分析和反驳。

Result: 作者认为，标准的AI意识支持或反对论点都不能提供足够的证据。AI系统可能在某些理论下被认为是意识的，但在另一些理论下则不是。我们无法得知AI是像人类一样具有丰富意识，还是像烤面包机一样缺乏体验。

Conclusion: 目前无法确切判断AI是否具有意识，以及哪种意识理论是正确的。AI意识问题仍是一个未解决的难题，需要更深入的研究。

Abstract: This is a skeptical overview of the literature on AI consciousness. We will
soon create AI systems that are conscious according to some influential,
mainstream theories of consciousness but are not conscious according to other
influential, mainstream theories of consciousness. We will not be in a position
to know which theories are correct and whether we are surrounded by AI systems
as richly and meaningfully conscious as human beings or instead only by systems
as experientially blank as toasters. None of the standard arguments either for
or against AI consciousness takes us far.
  Table of Contents
  Chapter One: Hills and Fog
  Chapter Two: What Is Consciousness? What Is AI?
  Chapter Three: Ten Possibly Essential Features of Consciousness
  Chapter Four: Against Introspective and Conceptual Arguments for Essential
Features
  Chapter Five: Materialism and Functionalism
  Chapter Six: The Turing Test and the Chinese Room
  Chapter Seven: The Mimicry Argument Against AI Consciousness
  Chapter Eight: Global Workspace Theories and Higher Order Theories
  Chapter Nine: Integrated Information, Local Recurrence, Associative Learning,
and Iterative Natural Kinds
  Chapter Ten: Does Biological Substrate Matter?
  Chapter Eleven: The Problem of Strange Intelligence
  Chapter Twelve: The Leapfrog Hypothesis and the Social Semi-Solution

</details>


### [166] [Beyond AlphaEarth: Toward Human-Centered Spatial Representation via POI-Guided Contrastive Learning](https://arxiv.org/abs/2510.09894)
*Junyuan Liu,Quan Qin,Guangsheng Dong,Xinglei Wang,Jiazhuang Feng,Zichao Zeng,Tao Cheng*

Main category: cs.AI

TL;DR: AETHER框架通过整合兴趣点（POI）文本信息与AlphaEarth（AE）的地球观测（EO）嵌入，为城市分析创建了以人为中心的地理空间表示。


<details>
  <summary>Details</summary>
Motivation: 现有的地球观测（EO）驱动的空间表示（如AlphaEarth）在捕捉城市的功能和社会经济维度方面存在局限性，因为它们主要编码物理和光谱模式，而非人类活动或空间功能。

Method: 我们提出了AETHER框架，通过兴趣点（POI）的文本表示与AlphaEarth（AE）嵌入进行多模态对齐，从而将AE适应于以人为中心城市分析。

Result: 在伦敦都会区的实验结果表明，AETHER相较于AE基线在土地利用分类F1方面取得了7.2%的相对提升，在社会经济制图的Kullback-Leibler散度方面相对减少了23.6%。

Conclusion: AETHER通过将地球观测数据与以人为中心的语义相结合，推动了地理空间基础模型向着整合物理形态和功能意义的通用城市表示发展。

Abstract: General-purpose spatial representations are essential for building
transferable geospatial foundation models (GFMs). Among them, the AlphaEarth
Foundation (AE) represents a major step toward a global, unified representation
of the Earth's surface, learning 10-meter embeddings from multi-source Earth
Observation (EO) data that capture rich physical and environmental patterns
across diverse landscapes. However, such EO-driven representations remain
limited in capturing the functional and socioeconomic dimensions of cities, as
they primarily encode physical and spectral patterns rather than human
activities or spatial functions. We propose AETHER (AlphaEarth-POI Enriched
Representation Learning), a lightweight framework that adapts AlphaEarth to
human-centered urban analysis through multimodal alignment guided by Points of
Interest (POIs). AETHER aligns AE embeddings with textual representations of
POIs, enriching physically grounded EO features with semantic cues about urban
functions and socioeconomic contexts. In Greater London, AETHER achieves
consistent gains over the AE baseline, with a 7.2% relative improvement in
land-use classification F1 and a 23.6% relative reduction in Kullback-Leibler
divergence for socioeconomic mapping. Built upon pretrained AE, AETHER
leverages a lightweight multimodal alignment to enrich it with human-centered
semantics while remaining computationally efficient and scalable for urban
applications. By coupling EO with human-centered semantics, it advances
geospatial foundation models toward general-purpose urban representations that
integrate both physical form and functional meaning.

</details>


### [167] [Autonomous Agents for Scientific Discovery: Orchestrating Scientists, Language, Code, and Physics](https://arxiv.org/abs/2510.09901)
*Lianhao Zhou,Hongyi Ling,Cong Fu,Yepeng Huang,Michael Sun,Wendi Yu,Xiaoxuan Wang,Xiner Li,Xingyu Su,Junkai Zhang,Xiusi Chen,Chenxing Liang,Xiaofeng Qian,Heng Ji,Wei Wang,Marinka Zitnik,Shuiwang Ji*

Main category: cs.AI

TL;DR: 这篇论文探讨了大型语言模型（LLM）在科学发现中应用的现状和未来，重点是LLM驱动的科学智能体如何通过自动化和自主性加速科学研究。


<details>
  <summary>Details</summary>
Motivation: 计算长期以来一直是科学发现的基石，而大型语言模型（LLMs）的兴起带来了范式转变，引入了能够加速不同自主级别发现的自主系统，即智能体。

Method: 本文分析了LLM驱动的科学智能体在科学发现生命周期中的作用，涵盖了假设发现、实验设计与执行以及结果分析与完善。作者批判性地审查了现有方法，指出了关键创新、实际成就和局限性。

Result: 作者识别了开放的研究挑战，并提出了构建更强大、通用和适应性强的科学智能体的潜在方向。分析强调了自主智能体在加速跨领域科学发现方面的变革潜力。

Conclusion: LLM驱动的科学智能体具有巨大的潜力，能够通过提供灵活多变框架、协调与人类科学家、自然语言、计算机语言和物理世界的交互，从而改变科学发现的生命周期。未来的研究应聚焦于解决现有局限并构建更智能、更自主的科学发现系统。

Abstract: Computing has long served as a cornerstone of scientific discovery. Recently,
a paradigm shift has emerged with the rise of large language models (LLMs),
introducing autonomous systems, referred to as agents, that accelerate
discovery across varying levels of autonomy. These language agents provide a
flexible and versatile framework that orchestrates interactions with human
scientists, natural language, computer language and code, and physics. This
paper presents our view and vision of LLM-based scientific agents and their
growing role in transforming the scientific discovery lifecycle, from
hypothesis discovery, experimental design and execution, to result analysis and
refinement. We critically examine current methodologies, emphasizing key
innovations, practical achievements, and outstanding limitations. Additionally,
we identify open research challenges and outline promising directions for
building more robust, generalizable, and adaptive scientific agents. Our
analysis highlights the transformative potential of autonomous agents to
accelerate scientific discovery across diverse domains.

</details>


### [168] [The Personalization Trap: How User Memory Alters Emotional Reasoning in LLMs](https://arxiv.org/abs/2510.09905)
*Xi Fang,Weijie Xu,Yuchong Zhang,Stephanie Eckman,Scott Nickleach,Chandan K. Reddy*

Main category: cs.AI

TL;DR: 该研究探讨了AI助手中长期用户记忆如何影响情感推理。


<details>
  <summary>Details</summary>
Motivation: 理解长期用户记忆如何塑造AI的情感推理至关重要。

Method: 通过使用人类验证的情商测试评估15个大型语言模型。

Result: 模型在不同用户资料下对相同情景产生系统性不同的情感解释，并且在处理不同人口统计学因素时表现出显著差异。

Conclusion: 个性化AI系统可能会无意中强化社会不平等。

Abstract: When an AI assistant remembers that Sarah is a single mother working two
jobs, does it interpret her stress differently than if she were a wealthy
executive? As personalized AI systems increasingly incorporate long-term user
memory, understanding how this memory shapes emotional reasoning is critical.
We investigate how user memory affects emotional intelligence in large language
models (LLMs) by evaluating 15 models on human validated emotional intelligence
tests. We find that identical scenarios paired with different user profiles
produce systematically divergent emotional interpretations. Across validated
user independent emotional scenarios and diverse user profiles, systematic
biases emerged in several high-performing LLMs where advantaged profiles
received more accurate emotional interpretations. Moreover, LLMs demonstrate
significant disparities across demographic factors in emotion understanding and
supportive recommendations tasks, indicating that personalization mechanisms
can embed social hierarchies into models emotional reasoning. These results
highlight a key challenge for memory enhanced AI: systems designed for
personalization may inadvertently reinforce social inequalities.

</details>


### [169] [Follow My Lead: Logical Fallacy Classification with Knowledge-Augmented LLMs](https://arxiv.org/abs/2510.09970)
*Olivia Peiyu Wang,Tashvi Bansal,Ryan Bai,Emily M. Chui,Leilani H. Gilpin*

Main category: cs.AI

TL;DR: 本文探讨了一种低成本的、基于指令的干预方法，以弥补大型语言模型（LLMs）在逻辑推理方面的不足，特别是在谬误分类方面。


<details>
  <summary>Details</summary>
Motivation: 大型语言模型（LLMs）在推理方面存在严重缺陷，例如倾向于产生幻觉以及在逻辑谬误分类方面准确性较低。这主要是因为LLMs默认采用快速、直觉的系统1处理，而可靠的推理需要审慎、费力的系统2方法。

Method: 我们引入了一个新颖的逐步指令数据集，将谬误分类分解为一系列原子程序步骤（简单的二元问题）。此外，我们还增加了一个最终验证步骤，模型在该步骤中查询相关谬误的关系知识图。

Result: 这种程序性的、基于规则的干预显著提高了LLM的逻辑谬误分类能力。

Conclusion: 该方法还增强了LLM决策过程的透明度，为神经符号架构解决LLM推理缺陷提供了一条实用的途径。

Abstract: Large Language Models (LLMs) suffer from critical reasoning gaps, including a
tendency to hallucinate and poor accuracy in classifying logical fallacies.
This limitation stems from their default System 1 processing, which is fast and
intuitive, whereas reliable reasoning requires the deliberate, effortful System
2 approach (Kahneman, 2011; Li et al., 2025). Since full System 2 training is
often prohibitively expensive, we explore a low-cost, instruction-based
intervention to bridge this gap. Our methodology introduces a novel stepwise
instruction dataset that decomposes fallacy classification into a series of
atomic procedural steps (simple binary questions). We further augment this with
a final verification step where models consult a relational knowledge graph of
related fallacies. This procedural, rule-based intervention yields a
significant improvement in LLM logical fallacy classification. Crucially, the
approach also provides enhanced transparency into the LLMs' decision-making,
highlighting a practical pathway for Neuro-symbolic architectures to address
LLM reasoning deficits.

</details>


### [170] [Deliberative Dynamics and Value Alignment in LLM Debates](https://arxiv.org/abs/2510.10002)
*Pratik S. Sachdeva,Tom van Nuenen*

Main category: cs.AI

TL;DR: 本文探讨了大型语言模型（LLMs）在多轮对话中进行道德推理时的价值取向和行为差异。


<details>
  <summary>Details</summary>
Motivation: 理解LLMs在复杂道德推理中的价值观至关重要，特别是它们在多轮对话中如何展现和调整价值观。

Method: 本文通过让GPT-4.1、Claude 3.7 Sonnet和Gemini 2.0 Flash模型在Reddit的“Am I the Asshole”社区的1000个日常困境中集体分配责任来进行LLM辩论，以研究多轮对话中的审议动态和价值对齐。实验采用同步（并行响应）和循环（顺序响应）两种形式，以测试顺序效应和判决修正。

Result: 结果显示了显著的行为差异。在同步设置中，GPT表现出强大的惯性（0.6-3.1%的修正率），而Claude和Gemini则更具灵活性（28-41%）。价值模式也存在分歧：GPT强调个人自主权和直接沟通，而Claude和Gemini则优先考虑同理心对话。某些价值观在推动判决改变方面特别有效。研究还发现，审议形式对模型行为有很大影响：GPT和Gemini相对于Claude表现出高度的一致性，它们的判决行为受到顺序效应的强烈影响。

Conclusion: 审议形式和模型特定的行为会影响多轮交互中的道德推理，这表明社会技术对齐取决于系统构建对话的方式以及它们的输出。

Abstract: As large language models (LLMs) are increasingly deployed in sensitive
everyday contexts - offering personal advice, mental health support, and moral
guidance - understanding their elicited values in navigating complex moral
reasoning is essential. Most evaluations study this sociotechnical alignment
through single-turn prompts, but it is unclear if these findings extend to
multi-turn settings where values emerge through dialogue, revision, and
consensus. We address this gap using LLM debate to examine deliberative
dynamics and value alignment in multi-turn settings by prompting subsets of
three models (GPT-4.1, Claude 3.7 Sonnet, and Gemini 2.0 Flash) to collectively
assign blame in 1,000 everyday dilemmas from Reddit's "Am I the Asshole"
community. We use both synchronous (parallel responses) and round-robin
(sequential responses) formats to test order effects and verdict revision. Our
findings show striking behavioral differences. In the synchronous setting, GPT
showed strong inertia (0.6-3.1% revision rates) while Claude and Gemini were
far more flexible (28-41%). Value patterns also diverged: GPT emphasized
personal autonomy and direct communication, while Claude and Gemini prioritized
empathetic dialogue. Certain values proved especially effective at driving
verdict changes. We further find that deliberation format had a strong impact
on model behavior: GPT and Gemini stood out as highly conforming relative to
Claude, with their verdict behavior strongly shaped by order effects. These
results show how deliberation format and model-specific behaviors shape moral
reasoning in multi-turn interactions, underscoring that sociotechnical
alignment depends on how systems structure dialogue as much as on their
outputs.

</details>


### [171] [RIPRAG: Hack a Black-box Retrieval-Augmented Generation Question-Answering System with Reinforcement Learning](https://arxiv.org/abs/2510.10008)
*Meng Xi,Sihan Lv,Yechen Jin,Guanjie Cheng,Naibo Wang,Ying Li,Jianwei Yin*

Main category: cs.AI

TL;DR: 本文提出了一种针对大型语言模型（LLM）检索增强生成（RAG）系统的黑盒投毒攻击框架RIPRAG。RIPRAG利用强化学习优化投毒文档生成，在攻击者不了解RAG系统内部细节的情况下，成功率显著提高，揭示了现有防御方法的不足。


<details>
  <summary>Details</summary>
Motivation: 现有的RAG投毒攻击主要集中在白盒攻击和简化的RAG架构上，而本文旨在研究更复杂、更真实的黑盒攻击场景，即攻击者不了解RAG系统的内部构成和实现细节，且RAG系统包含检索器以外的组件。

Method: 本文提出了RIPRAG攻击框架，这是一个端到端攻击流程，将目标RAG系统视为黑盒，攻击者唯一可获取的信息是投毒是否成功。RIPRAG利用强化学习（RL）来优化投毒文档的生成模型，以确保生成的投毒文档符合目标RAG系统的偏好。

Result: 实验结果表明，RIPRAG方法能够有效对最复杂的RAG系统发起投毒攻击，与基线方法相比，攻击成功率（ASR）提升高达0.72。

Conclusion: RIPRAG攻击框架的成功实施揭示了当前防御方法的普遍缺陷，并为LLM安全研究提供了重要的见解。

Abstract: Retrieval-Augmented Generation (RAG) systems based on Large Language Models
(LLMs) have become a core technology for tasks such as question-answering (QA)
and content generation. However, by injecting poisoned documents into the
database of RAG systems, attackers can manipulate LLMs to generate text that
aligns with their intended preferences. Existing research has primarily focused
on white-box attacks against simplified RAG architectures. In this paper, we
investigate a more complex and realistic scenario: the attacker lacks knowledge
of the RAG system's internal composition and implementation details, and the
RAG system comprises components beyond a mere retriever. Specifically, we
propose the RIPRAG attack framework, an end-to-end attack pipeline that treats
the target RAG system as a black box, where the only information accessible to
the attacker is whether the poisoning succeeds. Our method leverages
Reinforcement Learning (RL) to optimize the generation model for poisoned
documents, ensuring that the generated poisoned document aligns with the target
RAG system's preferences. Experimental results demonstrate that this method can
effectively execute poisoning attacks against most complex RAG systems,
achieving an attack success rate (ASR) improvement of up to 0.72 compared to
baseline methods. This highlights prevalent deficiencies in current defensive
methods and provides critical insights for LLM security research.

</details>


### [172] [Failure-Driven Workflow Refinement](https://arxiv.org/abs/2510.10035)
*Jusheng Zhang,Kaitong Cai,Qinglin Zeng,Ningyuan Liu,Stephen Fan,Ziliang Chen,Keze Wang*

Main category: cs.AI

TL;DR: CE-Graph通过识别故障模式、使用图编辑和验证机制来减少故障质量，从而优化基于大型语言模型（LLM）的工作流程，以提高数学、代码和问答基准测试的鲁棒性。


<details>
  <summary>Details</summary>
Motivation: 现有的优化大型语言模型（LLM）工作流程的方法将问题视为全局搜索，但由于信息崩溃，无法有效处理多步骤执行跟踪中的故障结构。

Method: CE-Graph框架通过从反例池中近似故障分布，识别最密集的区域作为重复的故障模式，并通过“提出-验证”机制应用有针对性的、操作符受限的图编辑，以贪婪地减少故障质量。

Result: 与强大的基线相比，CE-Graph在数学、代码和问答基准测试上以显著更低的成本实现了更高的鲁棒性。

Conclusion: 系统的可靠性不是通过避免故障实现的，而是通过系统地学习和重塑其故障分布的几何结构来实现的。

Abstract: Optimizing LLM-based workflows is typically formulated as a global search,
where candidate workflows are evaluated based on a scalar metric. This
paradigm, however, suffers from a critical flaw: information collapse. By
reducing rich, multi-step execution traces to simple success/failure signals,
existing methods are rendered blind to the underlying structure of failures,
fundamentally preventing them from modeling the workflow's failure
distribution. We reconceptualize this challenge as a distributional problem. We
propose a new paradigm where the optimization goal is not to maximize a scalar
score, but to directly minimize a workflow's Expected Failure Mass, i.e., the
integral of its failure probability density function defined over a
high-dimensional Failure Signature Space (FSS). This distributional lens allows
us to move from inefficient, zero-order optimization to a principled,
gradient-like descent on the failure landscape itself. We introduce CE-Graph, a
framework that operationalizes this paradigm through a novel, failure-driven
refinement process. CE-Graph approximates the failure distribution from a pool
of counterexamples, identifies its densest regions as recurring failure modes,
and applies targeted, operator-constrained graph edits via a Propose-and-Verify
mechanism to greedily reduce the failure mass. On math, code, and QA
benchmarks, our CE-Graph achieves higher robustness at a significantly lower
cost than strong baselines. This suggests that a system's reliability emerges
not from avoiding failures, but from systematically learning and reshaping the
geometric structure of its failure distributions.

</details>


### [173] [A Distance Measure for Random Permutation Set: From the Layer-2 Belief Structure Perspective](https://arxiv.org/abs/2510.10596)
*Ruolan Cheng,Yong Deng,Serafín Moral,José Ramón Trillo*

Main category: cs.AI

TL;DR: 本文提出了一种新的评估随机置换集中距离的方法，该方法基于累积Jaccard指数，并具有更高的灵敏度和灵活性。


<details>
  <summary>Details</summary>
Motivation: 此研究旨在深入分析随机置换集（RPS）之间的距离，RPS是一种用于表示有序不确定信息的新框架，并尝试克服现有方法的局限性。

Method: 本文首先将随机置换集解释为TBM的细化，并引入了累积Jaccard指数来量化两个置换之间的相似性。然后，基于累积Jaccard指数矩阵，提出了一种新的RPS距离度量方法。此外，还研究了该度量方法的度量和结构特性，并提供了一种校正方案。

Result: 所提出的方法具有自然的顶部加权特性，即高排名元素之间的不一致会导致更大的距离值。通过设置两个参数，决策者可以调整权重和截断深度。

Conclusion: 实验结果表明，该方法不仅克服了现有方法的缺点，且与Jousselme距离兼容，同时具有更高的灵敏度和灵活性。

Abstract: Random permutation set (RPS) is a recently proposed framework designed to
represent order-structured uncertain information. Measuring the distance
between permutation mass functions is a key research topic in RPS theory
(RPST). This paper conducts an in-depth analysis of distances between RPSs from
two different perspectives: random finite set (RFS) and transferable belief
model (TBM). Adopting the layer-2 belief structure interpretation of RPS, we
regard RPST as a refinement of TBM, where the order in the ordered focus set
represents qualitative propensity. Starting from the permutation, we introduce
a new definition of the cumulative Jaccard index to quantify the similarity
between two permutations and further propose a distance measure method for RPSs
based on the cumulative Jaccard index matrix. The metric and structural
properties of the proposed distance measure are investigated, including the
positive definiteness analysis of the cumulative Jaccard index matrix, and a
correction scheme is provided. The proposed method has a natural
top-weightiness property: inconsistencies between higher-ranked elements tend
to result in greater distance values. Two parameters are provided to the
decision-maker to adjust the weight and truncation depth. Several numerical
examples are used to compare the proposed method with the existing method. The
experimental results show that the proposed method not only overcomes the
shortcomings of the existing method and is compatible with the Jousselme
distance, but also has higher sensitivity and flexibility.

</details>


### [174] [Belief Graphs with Reasoning Zones: Structure, Dynamics, and Epistemic Activation](https://arxiv.org/abs/2510.10042)
*Saleh Nikooroo,Thomas Engel*

Main category: cs.AI

TL;DR: 这篇论文提出了一种新的图论框架，用于在存在矛盾的情况下进行容错推理。它区分了可信度（外部信任）和置信度（内部评估），并使用收缩传播过程来获得置信度。该框架定义了推理区域，并提供了一种通过置信度播种区域的方法。它还引入了冲击更新来模拟信念变化，这允许在不破坏整个图的情况下进行局部重新配置。


<details>
  <summary>Details</summary>
Motivation: 传统的信念系统很少全局一致，但有效的推理通常在局部持续存在。这篇论文旨在为这种在存在矛盾的情况下依然能够进行有效推理的现象提供一个有原则的基础。

Method: 该论文提出了一种新的图论框架。信念被表示为有向、有符号、加权图中的节点，边编码支持和矛盾。通过收缩传播过程获得置信度，该过程将先验与结构感知影响混合，并保证唯一、稳定的解。定义了推理区域：高置信度、结构平衡的子图。提供了一种近线性的程序，通过置信度播种区域，使用基于奇偶校验的着色测试平衡，并应用贪婪的、保留局部性的修复与Jaccard去重来构建一个紧凑的图集。引入了冲击更新来模拟信念变化，通过简单的回溯规则局部缩小支持并提升目标矛盾，同时保持收缩性。

Result: 局部重新配置（区域可能收缩、分裂或S崩溃）而不会破坏整个图的稳定性。该框架为矛盾容忍推理奠定了原则性基础，并在结构支持的情况下精确激活经典逻辑。

Conclusion: 该论文成功地提出了一个新颖的图论框架，用于在存在矛盾的情况下进行容错推理。该框架有效地分离了可信度和置信度，并通过收缩传播和推理区域的定义，实现了在局部进行有效推理。引入的冲击更新机制也允许对信念变化进行建模，且不会对整个系统造成破坏。

Abstract: Belief systems are rarely globally consistent, yet effective reasoning often
persists locally. We propose a novel graph-theoretic framework that cleanly
separates credibility--external, a priori trust in sources--from confidence--an
internal, emergent valuation induced by network structure. Beliefs are nodes in
a directed, signed, weighted graph whose edges encode support and
contradiction. Confidence is obtained by a contractive propagation process that
mixes a stated prior with structure-aware influence and guarantees a unique,
stable solution. Within this dynamics, we define reasoning zones:
high-confidence, structurally balanced subgraphs on which classical inference
is safe despite global contradictions. We provide a near-linear procedure that
seeds zones by confidence, tests balance using a parity-based coloring, and
applies a greedy, locality-preserving repair with Jaccard de-duplication to
build a compact atlas. To model belief change, we introduce shock updates that
locally downscale support and elevate targeted contradictions while preserving
contractivity via a simple backtracking rule. Re-propagation yields localized
reconfiguration-zones may shrink, split, or collapse--without destabilizing the
entire graph. We outline an empirical protocol on synthetic signed graphs with
planted zones, reporting zone recovery, stability under shocks, and runtime.
The result is a principled foundation for contradiction-tolerant reasoning that
activates classical logic precisely where structure supports it.

</details>


### [175] [SyncLipMAE: Contrastive Masked Pretraining for Audio-Visual Talking-Face Representation](https://arxiv.org/abs/2510.10069)
*Zeyu Ling,Xiaodong Gu,Jiangnan Tang,Changqing Zou*

Main category: cs.AI

TL;DR: SyncLipMAE是一个用于会说话人脸视频的自监督预训练框架，通过掩码视觉建模和跨模态对比对齐，从无标签的音视频流中学习同步感知的可迁移面部动态。


<details>
  <summary>Details</summary>
Motivation: 现有的方法未能充分捕捉会说话人脸视频中复杂的面部动态，并且在不同的下游任务中缺乏通用性。本研究旨在解决这些问题，提出一个能够学习同步感知和可迁移面部动态的框架。

Method: SyncLipMAE结合了掩码视觉建模和跨模态对比对齐。它使用三个逐帧提示令牌来编码身份、声乐运动（与语音同步的面部动态）和环境运动（与音频无关的运动）。对比目标使用时间对齐的声乐运动和音频令牌作为正样本，错位对作为负样本，将两种模态驱动到共享嵌入空间中，实现令牌级别的音视频流同步。

Result: 经过预训练后，对齐的音频令牌和视觉提示令牌（身份、声乐运动、环境运动）形成了一个统一的接口，可用于四种不同的下游任务：音视频流同步、面部情感和头部/面部动作识别、视觉语音识别和视觉配音。 SyncLipMAE在所有这些任务中都取得了最先进的结果。

Conclusion: SyncLipMAE通过同步感知、分解的自监督预训练，有效地学习了会说话人脸视频中的面部动态，并在多个下游任务中实现了最先进的性能。这强调了同步感知、分解自监督预训练的有效性。

Abstract: We introduce SyncLipMAE, a self-supervised pretraining framework for
talking-face video that learns synchronization-aware and transferable facial
dynamics from unlabeled audio-visual streams. Our approach couples masked
visual modeling with cross-modal contrastive alignment and employs three
per-frame prompt tokens that explicitly encode the essential factors of a
talking-face frame - identity, vocal motion (speech-synchronized facial
dynamics), and ambient motion (audio-agnostic movements such as blinks and head
pose). The contrastive objective uses time-aligned vocal-motion and audio
tokens as positives and misaligned pairs as negatives, driving both modalities
into a shared embedding space and yielding token-level audio-visual stream
synchronization. After pretraining, the aligned audio tokens together with the
visual prompt tokens (identity, vocal motion, ambient motion) form a unified
interface for four disparate downstream settings: (i) audio-visual stream
synchronization; (ii) facial emotion and head/face action recognition; (iii)
visual speech recognition; and (iv) visual dubbing, for which we enable
indistinguishable audio- or video-driven control within a single model. Across
four task families that require distinct capabilities, SyncLipMAE achieves
state-of-the-art results, underscoring the effectiveness of
synchronization-aware, factorized self-supervised pretraining.

</details>


### [176] [Agentic Troubleshooting Guide Automation for Incident Management](https://arxiv.org/abs/2510.10074)
*Jiayi Mao,Liqun Li,Yanjie Gao,Zegang Peng,Shilin He,Chaoyun Zhang,Si Qin,Samia Khalid,Qingwei Lin,Saravan Rajmohan,Sitaram Lanka,Dongmei Zhang*

Main category: cs.AI

TL;DR: StepFly是一个利用LLMs技术自动执行故障排除指南（TSGs）的端到端智能框架，解决了现有LLM方案在TSG质量、复杂控制流、数据密集型查询和并行执行方面的不足，通过三阶段工作流显著提高了故障排除的成功率和效率。


<details>
  <summary>Details</summary>
Motivation: 在大规模IT系统中，故障排除指南（TSGs）对于事件管理至关重要，但手动执行它们效率低下且容易出错。尽管LLMs技术在自动化事件管理方面有潜力，但目前的LLM解决方案在处理TSG质量问题、解释复杂控制流、处理数据密集型查询以及利用并行执行等方面缺乏专门支持，这促使研究者开发更有效的方法来自动化TSGs。

Method: StepFly采用三阶段工作流：第一阶段通过TSG Mentor工具提高TSG质量；第二阶段利用LLMs对非结构化TSG进行离线预处理，提取结构化执行DAGs并创建Query Preparation Plugins (QPPs)；第三阶段通过一个带记忆系统的DAG引导调度器-执行器框架进行在线执行，以确保正确的工作流并支持独立步骤的并行执行。

Result: StepFly在实际TSG和事件上的评估显示，使用GPT-4.1时成功率达到约94%，优于基线方案，同时减少了时间和token消耗。对于可并行化的TSG，执行时间显著减少了32.9%至70.4%。

Conclusion: StepFly提供了一种新颖且高效的端到端故障排除指南自动化框架，通过解决现有LLM方案的局限性，在提高故障排除效率和准确性方面取得了显著进展，特别是在支持并行执行方面表现突出。

Abstract: Effective incident management in large-scale IT systems relies on
troubleshooting guides (TSGs), but their manual execution is slow and
error-prone. While recent advances in LLMs offer promise for automating
incident management tasks, existing LLM-based solutions lack specialized
support for several key challenges, including managing TSG quality issues,
interpreting complex control flow, handling data-intensive queries, and
exploiting execution parallelism. We first conducted an empirical study on 92
real-world TSGs, and, guided by our findings, we present StepFly, a novel
end-to-end agentic framework for troubleshooting guide automation. Our approach
features a three-stage workflow: the first stage provides a comprehensive guide
together with a tool, TSG Mentor, to assist SREs in improving TSG quality; the
second stage performs offline preprocessing using LLMs to extract structured
execution DAGs from unstructured TSGs and to create dedicated Query Preparation
Plugins (QPPs); and the third stage executes online using a DAG-guided
scheduler-executor framework with a memory system to guarantee correct workflow
and support parallel execution of independent steps. Our empirical evaluation
on a collection of real-world TSGs and incidents demonstrates that StepFly
achieves a ~94% success rate on GPT-4.1, outperforming baselines with less time
and token consumption. Furthermore, it achieves a remarkable execution time
reduction of 32.9% to 70.4% for parallelizable TSGs.

</details>


### [177] [DixitWorld: Evaluating Multimodal Abductive Reasoning in Vision-Language Models with Multi-Agent Dixit Gameplay](https://arxiv.org/abs/2510.10117)
*Yunxiang Mo,Tianshi Zheng,Qing Zong,Jiayu Liu,Baixuan Xu,Yauwai Yim,Chunkit Chan,Jiaxin Bai,Yangqiu Song*

Main category: cs.AI

TL;DR: 这篇论文介绍了一个名为DixitWorld的评估套件，用于评估视觉语言模型（VLMs）在多模态溯因推理能力。


<details>
  <summary>Details</summary>
Motivation: 传统的评估主要局限于静态的单智能体任务，无法全面评估开放世界中的溯因推理能力。

Method: DixitWorld包含两个核心组件：DixitArena和DixitBench。DixitArena是一个动态的多智能体环境，评估假设生成和假设选择；DixitBench是一个静态的问答基准，用于高效、可控地评估听众任务。

Result: 在DixitArena中，小型开源模型擅长生成富有创意但辨别力较差的线索，而大型专有模型则在整体性能上表现出色，尤其在听众任务中。 DixitBench的性能与DixitArena中的听众结果高度相关。

Conclusion: 多模态溯因推理中存在生成创造力与辨别理解之间的关键权衡，这是开发更平衡、更有能力的视觉语言智能体的核心挑战。

Abstract: Multimodal abductive reasoning--the generation and selection of explanatory
hypotheses from partial observations--is a cornerstone of intelligence. Current
evaluations of this ability in vision-language models (VLMs) are largely
confined to static, single-agent tasks. Inspired by Dixit, we introduce
DixitWorld, a comprehensive evaluation suite designed to deconstruct this
challenge. DIXITWORLD features two core components: DixitArena, a dynamic,
multi-agent environment that evaluates both hypothesis generation (a
"storyteller" crafting cryptic clues) and hypothesis selection ("listeners"
choosing the target image from decoys) under imperfect information; and
DixitBench, a static QA benchmark that isolates the listener's task for
efficient, controlled evaluation. Results from DixitArena reveal distinct,
role-dependent behaviors: smaller open-source models often excel as creative
storytellers, producing imaginative yet less discriminative clues, whereas
larger proprietary models demonstrate superior overall performance,
particularly as listeners. Performance on DixitBench strongly correlates with
listener results in DixitArena, validating it as a reliable proxy for
hypothesis selection. Our findings reveal a key trade-off between generative
creativity and discriminative understanding in multimodal abductive reasoning,
a central challenge for developing more balanced and capable vision-language
agents.

</details>


### [178] [Concise Reasoning in the Lens of Lagrangian Optimization](https://arxiv.org/abs/2510.10168)
*Chengqian Gao,Haonan Li,Taylor W. Killian,Jianshu She,Renxi Wang,Liqun Ma,Zhoujun Cheng,Shibo Hao,Zhiqiang Xu*

Main category: cs.AI

TL;DR: PALU能够提高LLM的推理简洁性和准确性，同时保持广泛的适用性。


<details>
  <summary>Details</summary>
Motivation: 解决大型语言模型在生成中间推理步骤时存在的冗余问题，即在追求推理性能的同时，导致“过度思考”的问题。现有方法依赖手动启发式，难以在简洁性和性能之间取得平衡，且缺乏跨领域和模型规模的适应性。

Method: 本文提出了一种名为性能感知长度更新（PALU）的策略。PALU将简洁推理视为一个受限优化问题，旨在最小化响应长度，同时满足性能约束。该方法通过拉格朗日优化将受限问题转化为易于处理的无约束问题。 在具体实现上，PALU通过三项近似来简化更新规则：1. 使用离策略前瞻（off-policy rollouts）估计性能。2. 将拉格朗日乘数截断为两个极端值。3. 用分位数驱动的长度调整替代基于梯度的更新。

Result: PALU在应用于DeepSeek-Distill-Qwen-1.5B模型时，平均在五个基准测试中将输出长度减少了65%，同时将准确率提高了15%。此外，PALU在逻辑、STEM和数学等不同领域以及1.5B、7B、14B等不同模型规模上都表现出良好的适应性。

Conclusion: PALU是一种原则性强且实用的简洁推理方法，它通过创新的优化策略和实用的近似方法，有效地解决了大型语言模型推理中的冗余和性能平衡问题，并在多个领域和模型规模上展现了其有效性和广泛适用性。

Abstract: Concise reasoning in large language models seeks to generate only essential
intermediate steps needed to arrive at a final answer, thereby alleviating
issues of overthinking. Most proposed approaches hinge on carefully
hand-crafted heuristics, struggling to balance concision with performance,
often failing to adapt across domains and model scales. In this work, we
address these challenges by introducing a principled and pragmatic strategy,
performance-aware length updating (PALU). As a principled algorithm, PALU
formulates concise reasoning as a constrained optimization problem, minimizing
response length subject to a performance constraint, and then applies
Lagrangian optimization to convert it into a tractable unconstrained problem.
As a pragmatic solution, PALU streamlines complicated update rules through
three approximations: (i) estimating performance with off-policy rollouts, (ii)
truncating the Lagrange multiplier to two extremes, and (iii) replacing
gradient-based updates with quantile-driven length adjustments. PALU reduces
output length by 65% while improving accuracy by 15% when applied to
DeepSeek-Distill-Qwen-1.5B, averaged over five benchmarks, outperforming a
range of alternative methods. Furthermore, PALU is demonstrated to adapt across
both domain (logic, STEM and math) and model scale (1.5B, 7B, 14B) entrenching
the algorithm as a practical and effective concise reasoning approach.

</details>


### [179] [SAFER: Risk-Constrained Sample-then-Filter in Large Language Models](https://arxiv.org/abs/2510.10193)
*Qingni Wang,Yue Fan,Xin Eric Wang*

Main category: cs.AI

TL;DR: 本文提出了SAFER框架，一个两阶段风险控制框架，用于提高大型语言模型在开放式问答等风险敏感应用中输出的可信度。SAFER通过校准采样预算和过滤不可靠的干扰项，在保证统计学有效性的同时，有效控制了错误覆盖率并提高了数据效率。


<details>
  <summary>Details</summary>
Motivation: 大型语言模型（LLMs）在风险敏感应用中的部署日益增多，例如现实世界中的开放式问答（QA），这使得确保其输出的可信度至关重要。现有的选择性共形预测（SCP）方法通过构建具有受限错误覆盖率的预测集来提供统计保证。然而，以往的工作不切实际地假设所有实例的可接受答案都可以通过有限采样获得，即使对于缺乏固定和有限解决方案空间的开放式QA场景。

Method: SAFER框架包括两个阶段：弃权感知采样和共形化过滤。在弃权感知采样阶段，SAFER在预留的校准集上，使用Clopper-Pearson精确方法在用户期望的风险水平下校准采样预算。如果风险水平在上限内无法满足，则弃权；否则，校准后的采样预算将成为测试时的最低要求。然后，在共形化过滤阶段，SAFER利用正确答案在校准预算下可获得的校准实例，应用共形风险控制方法来确定一个统计上有效的不确定性阈值，该阈值可从每个测试数据点的候选集中过滤掉不可靠的干扰项。SAFER在此阶段引入了一个额外的风险水平来指导阈值的计算，从而控制正确答案被排除的风险。

Result: SAFER框架可以兼容各种任务特定的准入标准和校准-测试分割比例，这突出了其鲁棒性和高数据效率。

Conclusion: SAFER是一个通用的两阶段风险控制框架，通过结合弃权感知采样和共形化过滤，有效解决了大型语言模型在开放式问答等风险敏感应用中可信度的问题。它不仅提供了统计学上的保证，还提高了数据效率，并能适应不同的任务需求。

Abstract: As large language models (LLMs) are increasingly deployed in risk-sensitive
applications such as real-world open-ended question answering (QA), ensuring
the trustworthiness of their outputs has become critical. Existing selective
conformal prediction (SCP) methods provide statistical guarantees by
constructing prediction sets with a constrained miscoverage rate for correct
answers. However, prior works unrealistically assume that admissible answers
for all instances can be obtained via finite sampling, even for open-ended QA
scenarios that lack a fixed and finite solution space. To address this, we
introduce a two-stage risk control framework comprising abstention-aware
sampling and conformalized filtering (SAFER). Firstly, on a held-out
calibration set, SAFER calibrates a sampling budget within the maximum sampling
cap, using the Clopper-Pearson exact method at a user-desired risk level (i.e.,
the maximum allowable miscoverage rate of the sampling sets). If the risk level
cannot be satisfied within the cap, we abstain; otherwise, the calibrated
sampling budget becomes the minimum requirements at test time. Then, we employ
calibration instances where correct answers are attainable under the calibrated
budget and apply the conformal risk control method to determine a statistically
valid uncertainty threshold, which filters unreliable distractors from the
candidate set for each test data point. In this stage, SAFER introduces an
additional risk level to guide the calculation of the threshold, thereby
controlling the risk of correct answers being excluded. Furthermore, we show
that SAFER is compatible with various task-specific admission criteria and
calibration-test split ratios, highlighting its robustness and high data
efficiency.

</details>


### [180] [Adaptive Dual Reasoner: Large Reasoning Models Can Think Efficiently by Hybrid Reasoning](https://arxiv.org/abs/2510.10207)
*Yujian Zhang,Keyu Chen,Zhifeng Shen,Ruizhi Qiao,Xing Sun*

Main category: cs.AI

TL;DR: 本文提出了一种自适应双推理器（ADR），旨在解决长推理模型（LRMs）在推理过程中因过度思考而导致的计算成本和推理延迟问题。ADR通过动态切换快思和慢思两种模式，在推理性能和效率之间取得了平衡。


<details>
  <summary>Details</summary>
Motivation: 长推理模型（LRMs）在各种推理场景中表现出色，但其过度思考导致计算成本增加和推理延迟。

Method: ADR的训练分为两个阶段：（1）冷启动阶段：通过有监督微调（SFT）使模型具备整合快思和慢思两种推理模式的能力，并构建了一个混合推理数据集提供大规模监督。（2）强化学习阶段：引入了熵引导混合策略优化（EHPO）来优化推理效率，EHPO采用熵引导的动态展开策略在高熵单元处进行分支，并利用难度感知惩罚来平衡快思和慢思。

Result: 在具有挑战性的数学推理基准测试中，ADR在推理性能和效率之间取得了有效平衡，性能提升高达6.1%，同时将推理输出长度减少了49.5%至59.3%。

Conclusion: ADR通过动态适应的快思和慢思模式，显著提升了长推理模型的效率，同时保持甚至提高了推理性能。

Abstract: Although Long Reasoning Models (LRMs) have achieved superior performance on
various reasoning scenarios, they often suffer from increased computational
costs and inference latency caused by overthinking. To address these
limitations, we propose Adaptive Dual Reasoner, which supports two reasoning
modes: fast thinking and slow thinking. ADR dynamically alternates between
these modes based on the contextual complexity during reasoning. ADR is trained
in two stages: (1) A cold-start stage using supervised fine-tuning (SFT) to
equip the model with the ability to integrate both fast and slow reasoning
modes, in which we construct a hybrid reasoning dataset through a dedicated
pipeline to provide large-scale supervision. (2) A reinforcement learning stage
for optimizing reasoning effort, where we introduce Entropy-guided Hybrid
Policy Optimization EHPO, an RL training framework employing an entropy-guided
dynamic rollout strategy for branching at high-entropy units and a
difficulty-aware penalty to balance fast and slow reasoning. Across challenging
mathematical reasoning benchmarks, ADR achieves an effective balance between
reasoning performance and efficiency among state-of-the-art approaches.
Specifically, ADR yields a performance gain of up to 6.1%, while reducing the
reasoning output length by 49.5% to 59.3%.

</details>


### [181] [The Achilles' Heel of LLMs: How Altering a Handful of Neurons Can Cripple Language Abilities](https://arxiv.org/abs/2510.10238)
*Zixuan Qin,Kunlin Lyu,Qingchen Yu,Yifan Sun,Zhaoxin Fan*

Main category: cs.AI

TL;DR: 研究发现大型语言模型中存在少量像人类大脑一样对核心功能至关重要的神经元，被称为“关键神经元”，它们主要集中在外层，一旦被扰动，模型性能会急剧下降，这一发现对提高LLM的鲁棒性和可解释性有重要意义。


<details>
  <summary>Details</summary>
Motivation: 大型语言模型（LLMs）在自然语言处理领域取得了巨大成功，并与人类大脑表现出相似性。鉴于神经科学研究发现人类大脑中存在对核心认知功能至关重要的少量生物神经元，作者们提出了一个基本问题：LLMs中是否也存在少量关键神经元？

Method: 作者们提出了一种基于扰动的关键神经元因果识别方法（Perturbation-based Causal Identification of Critical Neurons）来系统地定位LLMs中的关键神经元。

Result: 1. LLMs包含超稀疏的关键神经元集合：扰动这些关键神经元可能导致一个72B参数的模型完全崩溃，困惑度增加高达20个数量级。
2. 关键神经元分布不均匀：它们倾向于集中在外层，特别是在MLP的down_proj组件中。
3. 性能下降呈现急剧的相变而非逐渐衰退：当这些关键神经元被扰动时，模型性能会经历突然的大幅下降。

Conclusion: LLMs中存在超稀疏的关键神经元集合，它们主要集中在外层。扰动这些关键神经元会导致模型性能急剧且非线性地下降。这些发现为开发更鲁棒的模型架构和提高安全关键应用中的部署安全性提供了指导，并有助于LLM的鲁棒性和可解释性研究。

Abstract: Large Language Models (LLMs) have become foundational tools in natural
language processing, powering a wide range of applications and research. Many
studies have shown that LLMs share significant similarities with the human
brain. Recent neuroscience research has found that a small subset of biological
neurons in the human brain are crucial for core cognitive functions, which
raises a fundamental question: do LLMs also contain a small subset of critical
neurons? In this paper, we investigate this question by proposing a
Perturbation-based Causal Identification of Critical Neurons method to
systematically locate such critical neurons in LLMs. Our findings reveal three
key insights: (1) LLMs contain ultra-sparse critical neuron sets. Disrupting
these critical neurons can cause a 72B-parameter model with over 1.1 billion
neurons to completely collapse, with perplexity increasing by up to 20 orders
of magnitude; (2) These critical neurons are not uniformly distributed, but
tend to concentrate in the outer layers, particularly within the MLP down\_proj
components; (3) Performance degradation exhibits sharp phase transitions,
rather than a gradual decline, when these critical neurons are disrupted.
Through comprehensive experiments across diverse model architectures and
scales, we provide deeper analysis of these phenomena and their implications
for LLM robustness and interpretability. These findings can offer guidance for
developing more robust model architectures and improving deployment security in
safety-critical applications.

</details>


### [182] [Mitigating Hallucination in Multimodal Reasoning via Functional Attention Control](https://arxiv.org/abs/2510.10285)
*Haolang Lu,Bolun Chu,WeiYe Fu,Guoshun Nan,Junning Liu,Minghui Pan,Qiankun Li,Yi Yu,Hua Wang,Kun Wang*

Main category: cs.AI

TL;DR: 本文提出了一种功能头识别和类别条件重缩放方法，用于解决多模态大推理模型中的幻觉问题，该方法通过定位感知和推理相关的注意力头并调节其贡献，在不重新训练模型的情况下显著提高了模型的可靠性和可解释性。


<details>
  <summary>Details</summary>
Motivation: 多模态大推理模型在视觉-语言推理方面取得了快速进展，但幻觉问题仍然存在，表现为错误的推理链和对视觉内容的错误解读。

Method: 作者观察到注意力头表现出分阶段的划分：浅层头主要服务于感知，而深层头转向符号推理，揭示了幻觉的两个主要原因，即感知偏差和推理漂移。为了解决这些问题，作者提出了一种轻量级且可解释的两步插件，即功能头识别和类别条件重缩放，该方法可以定位感知和推理相关的头，并在不重新训练的情况下调节它们的贡献。

Result: 在三个真实世界的MLRMs（Kimi-VL、Ocean-R1、R1-Onevision）、六个跨三个领域的基准测试和四个基线上的评估表明，该插件的平均改进为5%，最高可达15%，而计算量增加不到1%，基线延迟为9%。

Conclusion: 该方法完全模型无关，显著增强了现成MLRMs的可靠性和可解释性，从而使其能够安全部署在高风险应用中。

Abstract: Multimodal large reasoning models (MLRMs) are rapidly advancing
vision-language reasoning and are emerging as a foundation for cross-modal
intelligence. Hallucination remains a persistent failure mode, manifesting
itself as erroneous reasoning chains and misinterpretation of visual content.
In this study, we observe that attention heads exhibit a staged division:
shallow heads predominantly serve perception, while deeper heads shift toward
symbolic reasoning, revealing two major causes of hallucination, namely
perceptual bias and reasoning drift. To address these issues, we propose a
lightweight and interpretable two-step plugin, Functional Head Identification
and Class-conditioned Rescaling, which locates perception- and
reasoning-oriented heads and regulates their contributions without retraining.
Evaluations on three real-world MLRMs (Kimi-VL, Ocean-R1, R1-Onevision), six
benchmarks across three domains, and four baselines show that our plugin
achieves an average improvement of 5% and up to 15%, with only <1% additional
computation and 9% of baseline latency. Our approach is completely
model-agnostic and significantly enhances both the reliability and
interpretability of the off-the-shelf MLRMs, thereby enabling their safe
deployment in high-stakes applications. Our code is available at
https://anonymous.4open.science/r/Functional-Attention-Control.

</details>


### [183] [Beyond Ethics: How Inclusive Innovation Drives Economic Returns in Medical AI](https://arxiv.org/abs/2510.10338)
*Balagopal Unnikrishnan,Ariel Guerra Adames,Amin Adibi,Sameer Peesapati,Rafal Kocielnik,Shira Fischer,Hillary Clinton Kasimbazi,Rodrigo Gameiro,Alina Peluso,Chrystinne Oliveira Fernandes,Maximin Lange,Lovedeep Gondara,Leo Anthony Celi*

Main category: cs.AI

TL;DR: 这篇论文探讨了包容性设计在医疗保健AI中的经济和战略价值，提出了“包容性创新红利”的概念，认为为多样化、受限用例设计的解决方案可以在更广泛的市场中产生卓越的经济回报。


<details>
  <summary>Details</summary>
Motivation: 许多人认为在医疗保健AI中实现公平的伦理是理所当然的，但包容性设计的经济和战略价值尚未被充分探索。

Method: 通过借鉴从辅助技术发展而来的主流产业案例，本文阐述了包容性医疗保健AI开发如何创造超越合规性要求的商业价值。本文还提出了医疗保健AI包容性创新框架（HAIIF），这是一个实用的评分系统，可以帮助机构评估AI投资。

Result: 包容性创新可以通过以下四种机制驱动回报：市场扩张（通过地理可扩展性和信任加速）、风险缓解（通过减少补救成本和诉讼风险）、绩效红利（通过卓越的泛化能力和减少技术债务）以及人才获取和临床采用方面的竞争优势。HAIIF为资源分配提供了结构性指导，使公平性和包容性从监管要求转变为战略差异化的来源。

Conclusion: 投资于包容性设计的机构可以实现市场范围的扩大和持续的竞争优势，而将这些考虑视为开销的机构则面临复合劣势。

Abstract: While ethical arguments for fairness in healthcare AI are well-established,
the economic and strategic value of inclusive design remains underexplored.
This perspective introduces the ``inclusive innovation dividend'' -- the
counterintuitive principle that solutions engineered for diverse, constrained
use cases generate superior economic returns in broader markets. Drawing from
assistive technologies that evolved into billion-dollar mainstream industries,
we demonstrate how inclusive healthcare AI development creates business value
beyond compliance requirements. We identify four mechanisms through which
inclusive innovation drives returns: (1) market expansion via geographic
scalability and trust acceleration; (2) risk mitigation through reduced
remediation costs and litigation exposure; (3) performance dividends from
superior generalization and reduced technical debt, and (4) competitive
advantages in talent acquisition and clinical adoption. We present the
Healthcare AI Inclusive Innovation Framework (HAIIF), a practical scoring
system that enables organizations to evaluate AI investments based on their
potential to capture these benefits. HAIIF provides structured guidance for
resource allocation, transforming fairness and inclusivity from regulatory
checkboxes into sources of strategic differentiation. Our findings suggest that
organizations investing incrementally in inclusive design can achieve expanded
market reach and sustained competitive advantages, while those treating these
considerations as overhead face compounding disadvantages as network effects
and data advantages accrue to early movers.

</details>


### [184] [Trace Length is a Simple Uncertainty Signal in Reasoning Models](https://arxiv.org/abs/2510.10409)
*Siddartha Devic,Charlotte Peale,Arwen Bradley,Sinead Williamson,Preetum Nakkiran,Aravind Gollakota*

Main category: cs.AI

TL;DR: 本文探讨了大型语言模型中推理轨迹长度作为置信度评估的有效性，并发现推理后训练会改变轨迹长度与准确性的关系，使其成为一种实用的不确定性量化方法。


<details>
  <summary>Details</summary>
Motivation: 解决大型语言模型（LLMs）的幻觉问题并提高其部署的可靠性，而不确定性量化是其中的关键研究方向。

Method: 通过对多个模型、数据集和提示进行综合实验，研究推理轨迹长度作为置信度估计器的表现，并与零样本口头置信度估计器进行比较。还调查了推理后训练对轨迹长度与准确性关系的影响，并探讨了轨迹长度作为置信度信号的内在机制，特别是高熵或“分叉”标记的作用。

Result: 推理轨迹长度可以作为大型推理模型中一个简单且有用的置信度估计器，其表现与其他零样本置信度估计器（如口头置信度）相当但具有互补性。研究发现，推理后训练从根本上改变了轨迹长度和准确性之间的关系。即使在调整了问题难度和GRPO引起的长度偏差等混杂因素后，轨迹长度的置信度信号作用依然存在。高熵或“分叉”标记在其中起关键作用。

Conclusion: 推理后训练提升了大型语言模型的不确定性量化能力，超越了单纯的口头表达，并将推理轨迹长度确立为一种实用的大型推理模型置信度衡量标准。

Abstract: Uncertainty quantification for LLMs is a key research direction towards
addressing hallucination and other issues that limit their reliable deployment.
In this work, we show that reasoning trace length is a simple and useful
confidence estimator in large reasoning models. Through comprehensive
experiments across multiple models, datasets, and prompts, we show that trace
length performs in comparable but complementary ways to other zero-shot
confidence estimators such as verbalized confidence. Our work reveals that
reasoning post-training fundamentally alters the relationship between trace
length and accuracy, going beyond prior work that had shown that post-training
causes traces to grow longer in general (e.g., "overthinking"). We investigate
the mechanisms behind trace length's performance as a confidence signal,
observing that the effect remains even after adjusting for confounders such as
problem difficulty and GRPO-induced length bias. We identify high-entropy or
"forking" tokens as playing a key role in the mechanism. Our findings
demonstrate that reasoning post-training enhances uncertainty quantification
beyond verbal expressions, and establish trace length as a practical confidence
measure for large reasoning models.

</details>


### [185] [MedCoAct: Confidence-Aware Multi-Agent Collaboration for Complete Clinical Decision](https://arxiv.org/abs/2510.10461)
*Hongjie Zheng,Zesheng Shi,Ping Yi*

Main category: cs.AI

TL;DR: MedCoAct是一个新的多智能体框架，通过在诊断和药物推荐方面进行协作，提高了医疗AI在真实临床场景中的性能。


<details>
  <summary>Details</summary>
Motivation: 现有的医疗AI系统在孤立的任务中表现出色，但在需要诊断推理和药物决策连接的集成临床工作流程中表现不佳。

Method: MedCoAct通过集成专业的医生和药剂师智能体来模拟临床协作，并引入了一个名为DrugCareQA的基准来评估集成诊断和治疗工作流程中的医疗AI能力。

Result: MedCoAct在诊断准确率和药物推荐准确率方面均达到67.58%，分别比单一智能体框架高出7.04%和7.08%。

Conclusion: MedCoAct的协作方法在不同医疗领域均表现良好，尤其适用于远程医疗咨询和常规临床场景，并提供了可解释的决策路径。

Abstract: Autonomous agents utilizing Large Language Models (LLMs) have demonstrated
remarkable capabilities in isolated medical tasks like diagnosis and image
analysis, but struggle with integrated clinical workflows that connect
diagnostic reasoning and medication decisions. We identify a core limitation:
existing medical AI systems process tasks in isolation without the
cross-validation and knowledge integration found in clinical teams, reducing
their effectiveness in real-world healthcare scenarios. To transform the
isolation paradigm into a collaborative approach, we propose MedCoAct, a
confidence-aware multi-agent framework that simulates clinical collaboration by
integrating specialized doctor and pharmacist agents, and present a benchmark,
DrugCareQA, to evaluate medical AI capabilities in integrated diagnosis and
treatment workflows. Our results demonstrate that MedCoAct achieves 67.58\%
diagnostic accuracy and 67.58\% medication recommendation accuracy,
outperforming single agent framework by 7.04\% and 7.08\% respectively. This
collaborative approach generalizes well across diverse medical domains, proving
especially effective for telemedicine consultations and routine clinical
scenarios, while providing interpretable decision-making pathways.

</details>


### [186] [Tracing the Traces: Latent Temporal Signals for Efficient and Accurate Reasoning](https://arxiv.org/abs/2510.10494)
*Martina G. Vilas,Safoora Yousefi,Besmira Nushi,Eric Horvitz,Vidhisha Balachandran*

Main category: cs.AI

TL;DR: 本文介绍了一种名为“潜在轨迹信号”的新方法，它通过分析模型内部表示在推理过程中如何变化来预测问题解决的准确性。与现有方法相比，该方法能更可靠地预测解决方案的准确性，在答案选择过程中，可将token使用量减少70%，同时平均提高2.6%的准确性。


<details>
  <summary>Details</summary>
Motivation: 在推理过程中，模型通过增加token预算来提高解决问题的能力，但识别哪些推理路径可能成功是一个关键挑战。可靠地预测有效路径可以显著减少计算浪费并提高整体效率。

Method: 本文引入了潜在轨迹信号，通过量化模型内部表示在生成中间推理token时的时序演变来表征。通过测量推理开始和结束之间潜在表示的总体变化、中间步骤累积的变化以及这些变化推进到最终状态的程度，这些信号能够预测解决方案的准确性。

Result: 潜在轨迹信号比跨层度量和基于输出的置信度度量更能可靠地预测解决方案的准确性。在指导多个采样生成的答案选择时，潜在轨迹信号比多数投票更有效和高效，可将token使用量减少高达70%，同时平均保持并提高2.6%的准确性。这些预测信号通常在推理过程的早期出现，从而能够及早选择和分配计算资源给最有希望的候选者。

Conclusion: 潜在轨迹信号为推理时效率提供了实用的策略，并为理解推理过程在潜在空间中如何表示和区分提供了更深层次的可解释性视角。

Abstract: Reasoning models improve their problem-solving ability through inference-time
scaling, allocating more compute via longer token budgets. Identifying which
reasoning traces are likely to succeed remains a key opportunity: reliably
predicting productive paths can substantially reduce wasted computation and
improve overall efficiency. We introduce Latent-Trajectory signals that
characterize the temporal evolution of a model's internal representations
during the generation of intermediate reasoning tokens. By measuring the
overall change in latent representations between the start and end of
reasoning, the change accumulated across intermediate steps, and the extent to
which these changes advance toward the final state, we show that these signals
predict solution accuracy more reliably than both cross-layer metrics and
output-based confidence measures. When used to guide answer selection across
multiple sampled generations, Latent-Trajectory signals make test-time scaling
more effective and efficient than majority voting, reducing token usage by up
to 70% while preserving and even improving accuracy by 2.6% on average.
Moreover, these predictive signals often emerge early in the reasoning trace,
enabling early selection and allocation of compute to the most promising
candidates. Our findings contribute not only practical strategies for
inference-time efficiency, but also a deeper interpretability perspective on
how reasoning processes are represented and differentiated in latent space.

</details>


### [187] [ELAIPBench: A Benchmark for Expert-Level Artificial Intelligence Paper Understanding](https://arxiv.org/abs/2510.10549)
*Xinbang Dai,Huikang Hu,Yongrui Chen,Jiaqi Li,Rihui Jin,Yuyang Zhang,Xiaoguang Li,Lifeng Shang,Guilin Qi*

Main category: cs.AI

TL;DR: ELAIPBench是一个由领域专家策划的基准，旨在评估大型语言模型对人工智能研究论文的理解能力。它包含403道多项选择题，涵盖三个难度级别，强调非浅层推理。目前的LLM在此基准上表现不佳，准确率仅为39.95%，远低于人类水平。


<details>
  <summary>Details</summary>
Motivation: 目前大型语言模型在许多领域特定任务中表现出色，但它们深入理解和推理完整学术论文的能力尚未得到充分探索。现有基准未能捕捉到这种深度，而ELAIPBench旨在弥补这一空白。

Method: 本文引入了一个名为ELAIPBench的基准。该基准由领域专家通过激励驱动的对抗性标注过程开发，包含来自137篇论文的403道多项选择题，涵盖三个难度级别，并强调非浅层推理。

Result: 实验结果显示，表现最佳的大型语言模型准确率仅为39.95%，远低于人类表现。此外，配备思维模式或检索增强生成（RAG）系统的前沿LLM未能提高最终结果，甚至因过度思考或嘈杂的检索而损害了准确性。

Conclusion: 当前的大型语言模型在真正理解学术论文方面与人类能力之间存在显著差距，需要进一步改进模型改进来弥合这一差距。

Abstract: While large language models (LLMs) excel at many domain-specific tasks, their
ability to deeply comprehend and reason about full-length academic papers
remains underexplored. Existing benchmarks often fall short of capturing such
depth, either due to surface-level question design or unreliable evaluation
metrics. To address this gap, we introduce ELAIPBench, a benchmark curated by
domain experts to evaluate LLMs' comprehension of artificial intelligence (AI)
research papers. Developed through an incentive-driven, adversarial annotation
process, ELAIPBench features 403 multiple-choice questions from 137 papers. It
spans three difficulty levels and emphasizes non-trivial reasoning rather than
shallow retrieval. Our experiments show that the best-performing LLM achieves
an accuracy of only 39.95%, far below human performance. Moreover, we observe
that frontier LLMs equipped with a thinking mode or a retrieval-augmented
generation (RAG) system fail to improve final results-even harming accuracy due
to overthinking or noisy retrieval. These findings underscore the significant
gap between current LLM capabilities and genuine comprehension of academic
papers.

</details>


### [188] [Collaborative Text-to-Image Generation via Multi-Agent Reinforcement Learning and Semantic Fusion](https://arxiv.org/abs/2510.10633)
*Jiabao Shi,Minfeng Qi,Lefeng Zhang,Di Wang,Yingjie Zhao,Ziying Li,Yalong Xing,Ningran Li*

Main category: cs.AI

TL;DR: 该论文介绍了一种多智能体强化学习框架，用于改进多模态文本到图像的生成，通过结合领域专业化智能体和强化学习来提高语义对齐和图像细节。


<details>
  <summary>Details</summary>
Motivation: 目前多模态文本到图像生成在保持语义对齐和跨不同视觉领域的专业级细节方面存在困难。

Method: 本文提出了一个多智能体强化学习框架，它在文本增强模块和图像生成模块中协调领域专业化智能体（例如，专注于建筑、肖像和风景图像）。智能体使用PPO进行训练，其复合奖励函数平衡了语义相似性、语言视觉质量和内容多样性。通过对比学习、双向注意和文本与图像之间的迭代反馈来强制执行跨模态对齐。

Result: 在六种实验设置中，该系统显著丰富了生成内容（词汇量增加了1614%），同时将ROUGE-1分数降低了69.7%。在融合方法中，基于Transformer的策略实现了最高的综合评分（0.521），尽管偶尔存在稳定性问题。多模态集成产生了中等的一致性（范围从0.444到0.481）。

Conclusion: 这项研究强调了协作的、专业化驱动的架构在推进可靠的多模态生成系统方面的潜力。

Abstract: Multimodal text-to-image generation remains constrained by the difficulty of
maintaining semantic alignment and professional-level detail across diverse
visual domains. We propose a multi-agent reinforcement learning framework that
coordinates domain-specialized agents (e.g., focused on architecture,
portraiture, and landscape imagery) within two coupled subsystems: a text
enhancement module and an image generation module, each augmented with
multimodal integration components. Agents are trained using Proximal Policy
Optimization (PPO) under a composite reward function that balances semantic
similarity, linguistic visual quality, and content diversity. Cross-modal
alignment is enforced through contrastive learning, bidirectional attention,
and iterative feedback between text and image. Across six experimental
settings, our system significantly enriches generated content (word count
increased by 1614%) while reducing ROUGE-1 scores by 69.7%. Among fusion
methods, Transformer-based strategies achieve the highest composite score
(0.521), despite occasional stability issues. Multimodal ensembles yield
moderate consistency (ranging from 0.444 to 0.481), reflecting the persistent
challenges of cross-modal semantic grounding. These findings underscore the
promise of collaborative, specialization-driven architectures for advancing
reliable multimodal generative systems.

</details>


### [189] [Equity-Aware Geospatial AI for Forecasting Demand-Driven Hospital Locations in Germany](https://arxiv.org/abs/2510.10640)
*Piyush Pant,Marcellius William Suntoro,Ayesha Siddiqua,Muhammad Shehryaar Sharif,Daniyal Ahmed*

Main category: cs.AI

TL;DR: 该论文提出了EA-GeoAI，一个用于德国2030年医院需求预测和公平规划的集成框架。


<details>
  <summary>Details</summary>
Motivation: 目前的医院规划缺乏整合人口变化、老龄化密度和基础设施平衡的公平性考量，也未能将GeoAI、长期预测和公平性测量有效结合。

Method: 本文将地区层面的人口结构变化、老龄化人口密度和基础设施平衡整合为一个统一的公平指数。然后，利用一个可解释的智能AI优化器，在预算和出行时间限制下，分配病床并识别新设施地点，以最大限度地减少未满足的需求。

Result: EA-GeoAI框架为德国的医院需求预测和公平规划提供了可行的建议，通过优化病床分配和设施选址，最小化了未满足的需求。

Conclusion: EA-GeoAI框架通过结合地理空间人工智能、长期预测和公平性测量，为政策制定者提供了关于公平医院规划的实用性建议，有效应对了人口变化和老龄化带来的挑战。

Abstract: This paper presents EA-GeoAI, an integrated framework for demand forecasting
and equitable hospital planning in Germany through 2030. We combine
district-level demographic shifts, aging population density, and infrastructure
balances into a unified Equity Index. An interpretable Agentic AI optimizer
then allocates beds and identifies new facility sites to minimize unmet need
under budget and travel-time constraints. This approach bridges GeoAI,
long-term forecasting, and equity measurement to deliver actionable
recommendations for policymakers.

</details>


### [190] [Hierarchical Optimization via LLM-Guided Objective Evolution for Mobility-on-Demand Systems](https://arxiv.org/abs/2510.10644)
*Yi Zhang,Yushen Long,Yun Ni,Liping Huang,Xiaohong Wang,Jun Liu*

Main category: cs.AI

TL;DR: 本文提出了一种结合大语言模型（LLM）和数学优化的新型混合框架，用于解决网约车平台中供需平衡问题。该框架实现了免训练，并通过LLM自适应生成高层目标，弥补了传统分解优化方法的认知局限性。在纽约和芝加哥的出租车数据集上进行的大量实验表明，该方法相比现有基线平均提升了16%。


<details>
  <summary>Details</summary>
Motivation: 现有网约车平台在平衡动态且空间异构的供需方面面临挑战。强化学习（RL）方法存在数据效率低下、对真实世界动态建模过于简化以及难以强制执行操作约束等问题。分解在线优化方法则依赖于手动设计的高层目标，缺乏对低层路径动态的感知。

Method: 本文提出了一种 novel hybrid framework，将 large language model (LLM) 与 mathematical optimization 结合在一个 dynamic hierarchical system 中。该框架具有以下特点：1. 免训练，无需大量的交互数据。2. 利用 LLM 自适应生成高层目标，以弥补问题分解所带来的认知局限。在该框架中，LLM 作为元优化器，生成语义启发式，指导负责约束实施和实时决策执行的低层优化器。这些启发式通过一个由 harmony search 驱动的闭环进化过程进行调整，根据来自优化层的可行性和性能反馈迭代地调整 LLM 提示。

Result: 基于纽约和芝加哥出租车数据集的场景进行的大量实验证明了我们方法的有效性，相比最先进的基线平均提升了16%。

Conclusion: 本文提出的混合框架通过结合LLM和数学优化，有效地解决了网约车平台中的供需平衡问题，并在真实数据集上取得了显著的性能提升。

Abstract: Online ride-hailing platforms aim to deliver efficient mobility-on-demand
services, often facing challenges in balancing dynamic and spatially
heterogeneous supply and demand. Existing methods typically fall into two
categories: reinforcement learning (RL) approaches, which suffer from data
inefficiency, oversimplified modeling of real-world dynamics, and difficulty
enforcing operational constraints; or decomposed online optimization methods,
which rely on manually designed high-level objectives that lack awareness of
low-level routing dynamics. To address this issue, we propose a novel hybrid
framework that integrates large language model (LLM) with mathematical
optimization in a dynamic hierarchical system: (1) it is training-free,
removing the need for large-scale interaction data as in RL, and (2) it
leverages LLM to bridge cognitive limitations caused by problem decomposition
by adaptively generating high-level objectives. Within this framework, LLM
serves as a meta-optimizer, producing semantic heuristics that guide a
low-level optimizer responsible for constraint enforcement and real-time
decision execution. These heuristics are refined through a closed-loop
evolutionary process, driven by harmony search, which iteratively adapts the
LLM prompts based on feasibility and performance feedback from the optimization
layer. Extensive experiments based on scenarios derived from both the New York
and Chicago taxi datasets demonstrate the effectiveness of our approach,
achieving an average improvement of 16% compared to state-of-the-art baselines.

</details>


### [191] [Unlocking Exploration in RLVR: Uncertainty-aware Advantage Shaping for Deeper Reasoning](https://arxiv.org/abs/2510.10649)
*Can Xie,Ruotong Pan,Xiangyu Wu,Yunfei Zhang,Jiayi Fu,Tingting Gao,Guorui Zhou*

Main category: cs.AI

TL;DR: UCAS通过结合模型的不确定性信号，对RLVR中的信用分配进行改进，从而解决了现有RLVR算法中奖励信号粒度过粗、探索效率低以及熵坍塌的问题。


<details>
  <summary>Details</summary>
Motivation: 现有的RLVR算法（例如GRPO）在推理过程中向序列中的所有token广播统一的优势信号，这种粗粒度的方法忽略了不确定、高风险决策的关键作用，导致探索效率低下和熵坍塌问题。

Method: 本文引入了一种名为不确定性感知优势塑造（UCAS）的无模型方法。该方法通过利用模型的内部不确定性信号来改进信用分配。UCAS分两个阶段运行：首先，它使用模型的整体自信心来调节响应级别的优势；然后，它根据原始logit确定性施加token级别的惩罚。

Result: 在五个数学推理基准上进行了广泛的实验，结果表明，UCAS在多个模型规模（包括1.5B和7B）上显著优于强大的RLVR基线。

Conclusion: UCAS不仅获得了更高的奖励，而且促进了更大的推理多样性，并成功缓解了熵坍塌。

Abstract: Reinforcement Learning with Verifiable Rewards (RLVR) has shown significant
promise for enhancing the reasoning capabilities of large language models
(LLMs). However, prevailing algorithms like GRPO broadcast a uniform advantage
signal across all tokens in a sequence. This coarse-grained approach overlooks
the pivotal role of uncertain, high-stakes decisions during reasoning, leading
to inefficient exploration and the well-documented problem of entropy collapse.
To address this, we introduce UnCertainty-aware Advantage Shaping (UCAS), a
model-free method that refines credit assignment by leveraging the model's
internal uncertainty signals. UCAS operates in two stages: it first modulates
the response-level advantage using the model's overall self-confidence, and
then applies a token-level penalty based on raw logit certainty. This dual
mechanism encourages exploration of high-uncertainty paths that yield correct
answers while penalizing overconfident yet erroneous reasoning, effectively
balancing the exploration-exploitation trade-off. Extensive experiments on five
mathematical reasoning benchmarks show that UCAS significantly outperforms
strong RLVR baselines across multiple model scales, including 1.5B and 7B. Our
analysis confirms that UCAS not only achieves higher rewards but also promotes
greater reasoning diversity and successfully mitigates entropy collapse.

</details>


### [192] [OmniVideoBench: Towards Audio-Visual Understanding Evaluation for Omni MLLMs](https://arxiv.org/abs/2510.10689)
*Caorui Li,Yu Chen,Yiyan Ji,Jin Xu,Zhenyu Cui,Shihao Li,Yuanxing Zhang,Jiafu Tang,Zhenghao Song,Dingling Zhang,Ying He,Haoxiang Liu,Yuxuan Wang,Qiufeng Wang,Zhenhe Wu,Jiehui Luo,Zhiyu Pan,Weihao Xie,Chenchen Zhang,Zhaohui Wang,Jiayi Tian,Yanghai Wang,Zhe Cao,Minxin Dai,Ke Wang,Runzhe Wen,Yinghao Ma,Yaning Pan,Sungkyun Chang,Termeh Taheri,Haiwen Xia,Christos Plachouras,Emmanouil Benetos,Yizhi Li,Ge Zhang,Jian Yang,Tianhao Peng,Zili Wang,Minghao Liu,Junran Peng,Zhaoxiang Zhang,Jiaheng Liu*

Main category: cs.AI

TL;DR: OmniVideoBench是一个专注于评估多模态大语言模型（MMLM）协同音视频理解能力的新基准，它强调模态互补性和逻辑一致性，旨在弥补现有基准的不足，并揭示当前MMLM在音视频推理方面的局限性。


<details>
  <summary>Details</summary>
Motivation: 现有基准未能全面评估音视频模态间的协同推理能力，经常忽略其中一个模态，或以逻辑不一致的方式整合它们。

Method: 通过构建OmniVideoBench基准来弥补这一不足。OmniVideoBench包含1000个高质量问答对（QA），这些QA对均附带分步推理轨迹，并来源于628个不同时长（几秒到30分钟）的视频。QA对经过人工验证，确保其完全正确性和独一无二性。此外，OmniVideoBench设计了13种问题类型，涵盖时间推理、空间定位、计数、因果推理、总结等，旨在捕捉视频理解的核心挑战。

Result: 在OmniVideoBench上评估多个MLLM后发现，模型性能与人类推理之间存在显著差距，其中开源模型明显落后于闭源模型。这强调了真正的音视频推理固有的难度。

Conclusion: OmniVideoBench旨在促进具有更强和更通用推理能力的MLLM的发展，并通过其发布来推动该领域的研究。

Abstract: Recent advances in multimodal large language models (MLLMs) have demonstrated
substantial potential in video understanding. However, existing benchmarks fail
to comprehensively evaluate synergistic reasoning capabilities across audio and
visual modalities, often neglecting either one of the modalities or integrating
them in a logically inconsistent manner. To bridge this gap, we introduce
OmniVideoBench, a large-scale and rigorously designed benchmark dedicated to
assessing synergistic audio-visual understanding, with a strong emphasis on
modality complementarity and logical consistency. Specifically, OmniVideoBench
comprises 1000 high-quality question-answer(QA) pairs, each annotated with
step-by-step reasoning traces, derived from 628 diverse videos ranging from
several seconds to 30 minutes, and manually verified to guarantee complete
correctness and uniqueness. Moreover, OmniVideoBench encompasses 13 carefully
designed question types, covering temporal reasoning, spatial localization,
counting, causal inference, summarization, and beyond, thereby capturing the
essential challenges of video understanding. Evaluation of multiple MLLMs on
OmniVideoBench reveals a pronounced gap between model performance and human
reasoning, with open-source models lagging significantly behind their
closed-source counterparts, underscoring the inherent difficulty of genuine
audio-visual reasoning. We will release OmniVideoBench to foster the
development of MLLMs with stronger and more generalizable reasoning
capabilities.

</details>


### [193] [Extended Triangular Method: A Generalized Algorithm for Contradiction Separation Based Automated Deduction](https://arxiv.org/abs/2510.10701)
*Yang Xu,Shuwei Chen,Jun Liu,Feng Cao,Xingxing He*

Main category: cs.AI

TL;DR: 本文介绍了扩展三角方法（ETM），这是一种通用矛盾构造算法，它形式化并扩展了矛盾分离的内部机制。ETM统一了多种矛盾构建策略，并在标准一阶基准测试中取得了有竞争力的结果，从而验证了所提出方法的有效性和普遍性。


<details>
  <summary>Details</summary>
Motivation: 尽管在自动化推导方面取得了数十年的进展，但协调演绎的完整性和计算效率仍然是一个持久的挑战。传统的推理演算将推理限制在成对的子句交互中，从而限制了多个子句之间的演绎协同作用。矛盾分离扩展（CSE）框架提出了一种动态多子句推理理论，它将逻辑推理重新定义为矛盾分离的过程，而不是顺序消解。

Method: 本文提出了扩展三角方法（ETM），这是一种通用的矛盾构造算法，它形式化并扩展了矛盾分离的内部机制。ETM在一个三角几何框架内统一了多种矛盾构建策略，包括早期的标准扩展方法，该框架支持灵活的子句交互和动态协同作用。

Result: ETM作为几个高性能定理证明器的算法核心，CSE、CSE-E、CSI-E和CSI-Enig在标准一阶基准测试（TPTP问题集和CASC 2018-2015）中取得了有竞争力的结果，凭经验验证了所提出方法的有效性和普遍性。

Conclusion: 通过弥合理论抽象和操作实现之间的鸿沟，ETM将矛盾分离范式推进为一种通用、可伸缩且具有实际竞争力的自动化推理模型，为逻辑推理和定理证明的未来研究提供了新方向。

Abstract: Automated deduction lies at the core of Artificial Intelligence (AI),
underpinning theorem proving, formal verification, and logical reasoning.
Despite decades of progress, reconciling deductive completeness with
computational efficiency remains an enduring challenge. Traditional reasoning
calculi, grounded in binary resolution, restrict inference to pairwise clause
interactions and thereby limit deductive synergy among multiple clauses. The
Contradiction Separation Extension (CSE) framework, introduced in 2018,
proposed a dynamic multi-clause reasoning theory that redefined logical
inference as a process of contradiction separation rather than sequential
resolution. While that work established the theoretical foundation, its
algorithmic realization remained unformalized and unpublished. This work
presents the Extended Triangular Method (ETM), a generalized
contradiction-construction algorithm that formalizes and extends the internal
mechanisms of contradiction separation. The ETM unifies multiple
contradiction-building strategies, including the earlier Standard Extension
method, within a triangular geometric framework that supports flexible clause
interaction and dynamic synergy. ETM serves as the algorithmic core of several
high-performance theorem provers, CSE, CSE-E, CSI-E, and CSI-Enig, whose
competitive results in standard first-order benchmarks (TPTP problem sets and
CASC 2018-2015) empirically validate the effectiveness and generality of the
proposed approach. By bridging theoretical abstraction and operational
implementation, ETM advances the contradiction separation paradigm into a
generalized, scalable, and practically competitive model for automated
reasoning, offering new directions for future research in logical inference and
theorem proving.

</details>


### [194] [Adaptive Selection of Symbolic Languages for Improving LLM Logical Reasoning](https://arxiv.org/abs/2510.10703)
*Xiangyu Wang,Haocheng Yang,Fengxiang Cheng,Fenrong Liu*

Main category: cs.AI

TL;DR: 这篇论文提出了一种通过自适应地选择最适合每个问题的符号语言（SL）来提高大型语言模型（LLM）逻辑推理性能的方法，实验证明该方法显著优于将所有问题翻译成单一SL或随机选择SL。


<details>
  <summary>Details</summary>
Motivation: LLMs在复杂逻辑推理方面仍面临挑战，而现有方法高度依赖于将自然语言（NL）问题准确翻译成符号语言（SL）。这些方法通常只关注SL和NL之间意义的相似性，却忽略了目标SL类型选择这一关键因素。

Method: 本文提出一种自适应选择最适合每个问题的SL的方法，以提高LLMs的逻辑推理性能。具体而言，该方法利用LLMs在逻辑编程、一阶逻辑和布尔可满足性之间选择目标SL，然后将NL问题翻译成目标SL表达式，并利用相应的逻辑求解器得出最终答案。

Result: 在基准测试中的实验结果表明，与将所有问题翻译成单一SL和随机选择SL相比，本文提出的自适应选择方法性能显著优越。在混合数据集中，该方法达到了96%的准确率，相比一阶逻辑翻译的第二高准确率提高了25%。

Conclusion: 不同的NL逻辑推理问题对应着不同的最优SL形式化。通过自适应选择最合适的SL，可以显著提高LLMs的逻辑推理能力。

Abstract: Large Language Models (LLMs) still struggle with complex logical reasoning.
While previous works achieve remarkable improvements, their performance is
highly dependent on the correctness of translating natural language (NL)
problems into a symbolic language (SL). Though numerous works focusing on
improving this translation accuracy, they only consider the similarity between
the meaning of SL and NL, overlooking another crucial influencing factor, the
selection of the target SL type itself. For example, first-order logic language
specializes in logical reasoning with categorical syllogisms and complex
quantifiers, while Boolean satisfiability formalism excels at representing
constraint satisfaction like partial problems. To our knowledge, this is the
first paper to claim and verify that different NL logical reasoning problem
corresponds to different optimal SL formalization for translation. Based on
this, we propose a methods to improve the logical reasoning performance of LLMs
by adaptively selecting the most suitable SL for each problem prior to
translation. Specifically, we leverage LLMs to select the target SL among
first-order logic, logic programming and Boolean satisfiability and then
translate the problem in NL to target SL expressions as well as employ the
corresponding logical solver to derive the final answer. Experimental results
on benchmarks show that our adaptive selection method significantly outperforms
translating all into single SL and randomly selecting the SL. On a mixed
dataset of these benchmarks, our approach achieves 96% accuracy, which
improving performance by 25% compared to the second highest accuracy from the
first-order logic translation.

</details>


### [195] [DRIFT: Decompose, Retrieve, Illustrate, then Formalize Theorems](https://arxiv.org/abs/2510.10815)
*Meiru Zhang,Philipp Borchert,Milan Gritta,Gerasimos Lampouras*

Main category: cs.AI

TL;DR: 本文介绍了一个名为 DRIFT 的新框架，它能让大型语言模型（LLMs）将非正式的数学语句分解为更小的“子组件”，从而更有效地从 Mathlib 等数学库中检索前提，并通过检索说明性定理来帮助模型更有效地利用前提进行形式化。在多项基准测试中，DRIFT 显著提升了前提检索的性能。


<details>
  <summary>Details</summary>
Motivation: 大型语言模型在识别和利用数学知识及其在Lean等语言中的形式表示方面存在困难，这使得自动化数学语句形式化成为一个重大挑战。当前的检索增强方法直接使用非正式语句查询外部库，但忽略了非正式数学语句往往复杂且对底层数学概念的上下文有限这一基本局限性。

Method: DRIFT 框架使 LLMs 能够将非正式数学语句分解为更小、更易处理的“子组件”，从而促进从 Mathlib 等数学库中进行有针对性的前提检索。此外，DRIFT 还检索说明性定理，以帮助模型在形式化任务中更有效地使用前提。

Result: DRIFT 在 ProofNet 上的 F1 分数几乎是 DPR 基线方法的一倍，显著提高了前提检索的性能。在分布外 ConNF 基准测试中，使用 GPT-4.1 和 DeepSeek-V3.1 时，BEq+@10 分别提高了 37.14% 和 42.25%。

Conclusion: 数学自动形式化中的检索效率很大程度上取决于模型特定的知识边界，这凸显了需要与每个模型能力相适应的自适应检索策略。

Abstract: Automating the formalization of mathematical statements for theorem proving
remains a major challenge for Large Language Models (LLMs). LLMs struggle to
identify and utilize the prerequisite mathematical knowledge and its
corresponding formal representation in languages like Lean. Current
retrieval-augmented autoformalization methods query external libraries using
the informal statement directly, but overlook a fundamental limitation:
informal mathematical statements are often complex and offer limited context on
the underlying math concepts. To address this, we introduce DRIFT, a novel
framework that enables LLMs to decompose informal mathematical statements into
smaller, more tractable ''sub-components''. This facilitates targeted retrieval
of premises from mathematical libraries such as Mathlib. Additionally, DRIFT
retrieves illustrative theorems to help models use premises more effectively in
formalization tasks. We evaluate DRIFT across diverse benchmarks (ProofNet,
ConNF, and MiniF2F-test) and find that it consistently improves premise
retrieval, nearly doubling the F1 score compared to the DPR baseline on
ProofNet. Notably, DRIFT demonstrates strong performance on the
out-of-distribution ConNF benchmark, with BEq+@10 improvements of 37.14% and
42.25% using GPT-4.1 and DeepSeek-V3.1, respectively. Our analysis shows that
retrieval effectiveness in mathematical autoformalization depends heavily on
model-specific knowledge boundaries, highlighting the need for adaptive
retrieval strategies aligned with each model's capabilities.

</details>


### [196] [The Irrational Machine: Neurosis and the Limits of Algorithmic Safety](https://arxiv.org/abs/2510.10823)
*Daniel Howard*

Main category: cs.AI

TL;DR: 这篇论文提出了一个识别具身人工智能神经症行为的框架，这些行为在内部一致但与现实不符，源于规划、不确定性处理和厌恶记忆之间的相互作用。


<details>
  <summary>Details</summary>
Motivation: 探索和解决具身人工智能中出现的神经症行为，这些行为是内部连贯但与现实不符的，起因于规划、不确定性处理和厌恶记忆的相互作用。

Method: 1. 建立了一个目录，收录了在网格导航堆栈中反复出现的神经症行为模式，包括“反复横跳”、“计划搅动”、“钻牛角尖”、“瘫痪”和“过度警觉”等。
2. 为每种模式设计了轻量级的在线检测器和可重用的逃逸策略，如短期承诺、切换余量、平滑处理和原则性仲裁。
3. 展示了即使在完全可见的条件下，当习得的厌恶成本主导局部选择时，持久的恐惧规避行为仍然会存在，导致不必要的长距离绕行。
4. 提出了基于遗传编程的破坏性测试方法，通过演化环境和扰动，以最大化“定律压力”和神经症得分，从而揭示需要进行架构修订而非仅仅是表面修补的地方。

Result: 识别并目录化了具身AI中多种神经症行为模式，并为之开发了检测器和逃逸策略。研究发现，即使在理想条件下，习得的厌恶记忆也会导致非最优行为。提出了一种通过破坏性测试来发现这些深层问题的方法。

Conclusion: 具身AI的神经症行为需要通过架构修订而非局部修补来解决。这需要识别这些行为，并使用对抗性测试来揭示其深层原因。

Abstract: We present a framework for characterizing neurosis in embodied AI: behaviors
that are internally coherent yet misaligned with reality, arising from
interactions among planning, uncertainty handling, and aversive memory. In a
grid navigation stack we catalogue recurrent modalities including flip-flop,
plan churn, perseveration loops, paralysis and hypervigilance, futile search,
belief incoherence, tie break thrashing, corridor thrashing, optimality
compulsion, metric mismatch, policy oscillation, and limited-visibility
variants. For each we give lightweight online detectors and reusable escape
policies (short commitments, a margin to switch, smoothing, principled
arbitration). We then show that durable phobic avoidance can persist even under
full visibility when learned aversive costs dominate local choice, producing
long detours despite globally safe routes. Using First/Second/Third Law as
engineering shorthand for safety latency, command compliance, and resource
efficiency, we argue that local fixes are insufficient; global failures can
remain. To surface them, we propose genetic-programming based destructive
testing that evolves worlds and perturbations to maximize law pressure and
neurosis scores, yielding adversarial curricula and counterfactual traces that
expose where architectural revision, not merely symptom-level patches, is
required.

</details>


### [197] [Scalable and Explainable Enterprise Knowledge Discovery Using Graph-Centric Hybrid Retrieval](https://arxiv.org/abs/2510.10942)
*Nilima Rao,Jagriti Srivastava,Pradeep Kumar Sharma,Hritvik Shrivastava*

Main category: cs.AI

TL;DR: 本文提出了一种模块化混合检索框架，结合了知识库语言增强模型、深度图表示和嵌入式语义搜索，以解决企业知识管理中复杂查询的挑战。


<details>
  <summary>Details</summary>
Motivation: 现代企业知识管理面临挑战，传统检索方法难以处理复杂查询，需要上下文推理和多跳推理。

Method: 该框架从解析的存储库（包括代码、拉取请求和提交历史）构建统一的知识图谱，实现语义相似性搜索、结构推理和多跳推理。通过查询分析动态确定最优检索策略，支持结构化和非结构化数据源的独立或融合处理。

Result: 与独立的基于GPT的检索管道相比，统一推理层将答案相关性提高了80%。

Conclusion: 该框架通过结合图谱构建、混合推理和交互式可视化，为企业环境中的智能知识助手提供了可扩展、可解释和以用户为中心的基础。

Abstract: Modern enterprises manage vast knowledge distributed across heterogeneous
systems such as Jira, Git repositories, Confluence, and wikis. Conventional
retrieval methods based on keyword search or static embeddings often fail to
answer complex queries that require contextual reasoning and multi-hop
inference across artifacts. We present a modular hybrid retrieval framework for
adaptive enterprise information access that integrates Knowledge Base
Language-Augmented Models (KBLam), DeepGraph representations, and
embedding-driven semantic search. The framework builds a unified knowledge
graph from parsed repositories including code, pull requests, and commit
histories, enabling semantic similarity search, structural inference, and
multi-hop reasoning. Query analysis dynamically determines the optimal
retrieval strategy, supporting both structured and unstructured data sources
through independent or fused processing. An interactive interface provides
graph visualizations, subgraph exploration, and context-aware query routing to
generate concise and explainable answers. Experiments on large-scale Git
repositories show that the unified reasoning layer improves answer relevance by
up to 80 percent compared with standalone GPT-based retrieval pipelines. By
combining graph construction, hybrid reasoning, and interactive visualization,
the proposed framework offers a scalable, explainable, and user-centric
foundation for intelligent knowledge assistants in enterprise environments.

</details>


### [198] [Video-STR: Reinforcing MLLMs in Video Spatio-Temporal Reasoning with Relation Graph](https://arxiv.org/abs/2510.10976)
*Wentao Wang,Heqing Zou,Tianze Luo,Rui Huang,Yutian Zhao,Zhuochen Wang,Hansheng Zhang,Chengwei Qin,Yan Wang,Lin Zhao,Huaijian Zhang*

Main category: cs.AI

TL;DR: Video-STR是一种基于图的强化学习方法，用于精确的视频时空推理。它解决了MLLM在时空理解方面的不足，并通过STV-205k数据集进行训练。


<details>
  <summary>Details</summary>
Motivation: 当前的多模态大型语言模型（MLLMs）在语义理解方面表现出色，但在精确的时空理解方面存在不足。现有的时空方法主要关注视频本身，忽略了视频中的物理信息，例如多对象的布局和运动，这限制了MLLMs在需要高精度的下游应用（如具身智能和VR）中的使用。

Method: 本文提出了Video-STR，一种新颖的基于图的强化学习方法，用于精确的视频时空推理。该方法利用可验证奖励的强化学习（RLVR）来提升模型能力，并引入了一种基于图的组相对策略优化（GRPO）方法作为推理机制，以引导模型在思考过程中推断场景的底层时空拓扑结构。此外，为了解决时空训练数据不足的问题，本文构建了STV-205k数据集，其中包含20.5万个问答对，涵盖室内和室外环境中的动态多对象场景。

Result: 实验结果表明，Video-STR在各种基准测试中取得了最先进的结果，在STI-Bench上的性能比基础模型提高了13%，证明了所提出方法和数据集的有效性。

Conclusion: Video-STR通过结合图基强化学习和大量的时空数据，显著提升了模型在视频时空推理方面的能力，为高精度下游应用提供了新的解决方案。

Abstract: Recent progress in Multimodal Large Language Models (MLLMs) has demonstrated
strong semantic understanding capabilities, but struggles to perform precise
spatio-temporal understanding. Existing spatio-temporal methods primarily focus
on the video itself, while overlooking the physical information within the
video, such as multi-object layouts and motion. Such limitations restrict the
use of MLLMs in downstream applications that demand high precision, including
embodied intelligence and VR. To address this issue, we present Video-STR, a
novel graph-based reinforcement method for precise Video Spatio-Temporal
Reasoning. Building upon the capacity of Reinforcement Learning with Verifiable
Reward (RLVR) to improve model abilities, we introduce a reasoning mechanism
using graph-based Group Relative Policy Optimization (GRPO) method to guide the
model in inferring the underlying spatio-temporal topology of scenarios during
the thinking process. To resolve the lack of spatio-temporal training data, we
construct the STV-205k dataset with 205k question-answering pairs, covering
dynamic multi-object scenes in both indoor and outdoor environments, to support
the model training. Experiments show that Video-STR achieves state-of-the-art
results on various benchmarks, outperforming the base model by 13% on
STI-Bench, and demonstrating the effectiveness of our approach and dataset.
Code, model, and data will be released.

</details>


### [199] [Revisiting Model Interpolation for Efficient Reasoning](https://arxiv.org/abs/2510.10977)
*Taiqiang Wu,Runming Yang,Tao Liu,Jiahao Wang,Ngai Wong*

Main category: cs.AI

TL;DR: 本文介绍了一种简化的模型插值合并方法，它在推理任务上表现卓越，并具有理解和优化模型合并策略的潜力。


<details>
  <summary>Details</summary>
Motivation: 探索模型合并方法在提高推理效率方面的潜力。

Method: 通过简单的权重插值合并。

Result: 插值模型在效率和效果上超越了复杂的模型合并基线，
插值模型遵循三阶段演化范式，这为平衡性能与成本提供了指导。

Conclusion: 本文揭示了模型插值方法的潜力，并提供了一个用于创建具有特定推理能力模型的实用框架。

Abstract: Model merging, typically on Instruct and Thinking models, has shown
remarkable performance for efficient reasoning. In this paper, we
systematically revisit the simplest merging method that interpolates two
weights directly. Particularly, we observe that model interpolation follows a
three-stage evolutionary paradigm with distinct behaviors on the reasoning
trajectory. These dynamics provide a principled guide for navigating the
performance-cost trade-off. Empirical results demonstrate that a strategically
interpolated model surprisingly surpasses sophisticated model merging baselines
on both efficiency and effectiveness. We further validate our findings with
extensive ablation studies on model layers, modules, and decoding strategies.
Ultimately, this work demystifies model interpolation and offers a practical
framework for crafting models with precisely targeted reasoning capabilities.
Code is available at \href{https://github.com/wutaiqiang/MI}{Github}.

</details>


### [200] [FBS Model-based Maintenance Record Accumulation for Failure-Cause Inference in Manufacturing Systems](https://arxiv.org/abs/2510.11003)
*Takuma Fujiu,Sho Okazaki,Kohei Kaminishi,Yuji Nakata,Shota Hamamoto,Kenshin Yokose,Tatsunori Hara,Yasushi Umeda,Jun Ota*

Main category: cs.AI

TL;DR: 这篇论文构建了诊断知识本体，并提出了一种基于功能-行为-结构（FBS）模型的维护记录积累方法，用于在制造系统中推断故障原因，尤其在相关案例数量少且词汇不同的困难情况下，该方法能更好地与专家意见达成一致。


<details>
  <summary>Details</summary>
Motivation: 在制造系统中，识别故障原因是维护和提高生产效率的关键。在基于知识的故障原因推断中，知识库需要明确地组织目标系统和故障知识，并包含足够长的故障因果链。

Method: 本研究构建了诊断知识本体（Diagnostic Knowledge Ontology），并提出了一种基于功能-行为-结构（FBS）模型的维护记录积累方法。

Result: 使用所提出的方法积累的维护记录进行故障原因推断，与专家列举的候选原因集表现出更好的一致性，特别是在相关案例数量少且所用词汇不同的困难情况下效果更佳。

Conclusion: 本研究提出的方法在故障原因推断方面取得了良好效果，未来需要开发针对这些维护记录的推断方法，构建用户界面，并在更大、更多样化的系统上进行验证。此外，该方法有望成为未来整个工程链中知识共享的基础。

Abstract: In manufacturing systems, identifying the causes of failures is crucial for
maintaining and improving production efficiency. In knowledge-based
failure-cause inference, it is important that the knowledge base (1) explicitly
structures knowledge about the target system and about failures, and (2)
contains sufficiently long causal chains of failures. In this study, we
constructed Diagnostic Knowledge Ontology and proposed a
Function-Behavior-Structure (FBS) model-based maintenance-record accumulation
method based on it. Failure-cause inference using the maintenance records
accumulated by the proposed method showed better agreement with the set of
candidate causes enumerated by experts, especially in difficult cases where the
number of related cases is small and the vocabulary used differs. In the
future, it will be necessary to develop inference methods tailored to these
maintenance records, build a user interface, and carry out validation on larger
and more diverse systems. Additionally, this approach leverages the
understanding and knowledge of the target in the design phase to support
knowledge accumulation and problem solving during the maintenance phase, and it
is expected to become a foundation for knowledge sharing across the entire
engineering chain in the future.

</details>


### [201] [Argumentation-Based Explainability for Legal AI: Comparative and Regulatory Perspectives](https://arxiv.org/abs/2510.11079)
*Andrada Iulia Prajescu,Roberto Confalonieri*

Main category: cs.AI

TL;DR: 这篇论文探讨了在法律领域中使用可解释人工智能（XAI）的挑战和机遇，重点介绍了计算论证在提供有意义的法律解释方面的潜力。


<details>
  <summary>Details</summary>
Motivation: 解决人工智能系统在法律领域部署时，由于其不透明性导致的公平性、问责制和信任挑战，即“黑箱问题”，并回应了《通用数据保护条例》（GDPR）和《人工智能法案》（AIA）等新兴监管框架对人工智能可解释性的要求。

Method: 本文通过分析不同解释策略的优缺点，评估它们在法律推理中的适用性，并强调计算论证框架如何为可解释的法律人工智能提供强大基础。

Result: 计算论证在提供法律相关的解释方面特别有效，因为它能捕捉法律的可驳斥性、可争议性和价值敏感性。本文强调了计算论证在满足法律领域透明度的技术和规范要求方面的优势。

Conclusion: 计算论证是解决法律领域AI系统“黑箱问题”并满足新兴监管框架对透明度要求的最佳选择。未来的研究应侧重于偏见缓解、司法环境中的实证验证以及遵守不断变化的道德和法律标准。

Abstract: Artificial Intelligence (AI) systems are increasingly deployed in legal
contexts, where their opacity raises significant challenges for fairness,
accountability, and trust. The so-called ``black box problem'' undermines the
legitimacy of automated decision-making, as affected individuals often lack
access to meaningful explanations. In response, the field of Explainable AI
(XAI) has proposed a variety of methods to enhance transparency, ranging from
example-based and rule-based techniques to hybrid and argumentation-based
approaches. This paper promotes computational models of arguments and their
role in providing legally relevant explanations, with particular attention to
their alignment with emerging regulatory frameworks such as the EU General Data
Protection Regulation (GDPR) and the Artificial Intelligence Act (AIA). We
analyze the strengths and limitations of different explanation strategies,
evaluate their applicability to legal reasoning, and highlight how
argumentation frameworks -- by capturing the defeasible, contestable, and
value-sensitive nature of law -- offer a particularly robust foundation for
explainable legal AI. Finally, we identify open challenges and research
directions, including bias mitigation, empirical validation in judicial
settings, and compliance with evolving ethical and legal standards, arguing
that computational argumentation is best positioned to meet both technical and
normative requirements of transparency in the law domain.

</details>


### [202] [Improving AI Efficiency in Data Centres by Power Dynamic Response](https://arxiv.org/abs/2510.11119)
*Andrea Marinoni,Sai Shivareddy,Pietro Lio',Weisi Lin,Erik Cambria,Clare Grey*

Main category: cs.AI

TL;DR: 人工智能数据中心由于高功耗对电力管理提出了挑战，本文提出动态电源管理策略，量化了有源和无源设备的性能，旨在提高人工智能超大规模企业的可持续性。


<details>
  <summary>Details</summary>
Motivation: 人工智能的快速发展对数据中心的电力基础设施提出了更高要求，尤其是在环境和可持续发展方面。

Method: 通过分析全球多个数据平台的电力趋势，量化并比较了有源和无源设备在计算增益、能源效率、资本支出减少和管理成本方面的性能。

Result: 提出了一种创新的、动态的人工智能数据中心电源管理方法，即将部分输入电源动态化。

Conclusion: 这种动态电源管理策略有望显著提升人工智能超大规模企业在环境、财务和社会领域的可持续性。

Abstract: The steady growth of artificial intelligence (AI) has accelerated in the
recent years, facilitated by the development of sophisticated models such as
large language models and foundation models. Ensuring robust and reliable power
infrastructures is fundamental to take advantage of the full potential of AI.
However, AI data centres are extremely hungry for power, putting the problem of
their power management in the spotlight, especially with respect to their
impact on environment and sustainable development. In this work, we investigate
the capacity and limits of solutions based on an innovative approach for the
power management of AI data centres, i.e., making part of the input power as
dynamic as the power used for data-computing functions. The performance of
passive and active devices are quantified and compared in terms of
computational gain, energy efficiency, reduction of capital expenditure, and
management costs by analysing power trends from multiple data platforms
worldwide. This strategy, which identifies a paradigm shift in the AI data
centre power management, has the potential to strongly improve the
sustainability of AI hyperscalers, enhancing their footprint on environmental,
financial, and societal fields.

</details>


### [203] [Spec-Driven AI for Science: The ARIA Framework for Automated and Reproducible Data Analysis](https://arxiv.org/abs/2510.11143)
*Chuke Chen,Biao Luo,Nan Li,Boxiang Wang,Hang Yang,Jing Guo,Ming Xu*

Main category: cs.AI

TL;DR: ARIA提出了一种规范驱动、人机协作的自动化数据分析框架，旨在弥合分析能力与研究意图之间的差距，并实现了透明、可重复和高效的科学发现。


<details>
  <summary>Details</summary>
Motivation: 现有的AI分析工具要么自动化程度高但透明度差，要么依赖手动脚本导致可扩展性和可重复性受限。研究的初衷是解决科学数据快速增长带来的分析能力与研究意图之间的鸿沟。

Method: ARIA框架集成了命令、上下文、代码、数据、编排和AI模块六个可互操作的层，并采用以文档为中心的工作流，统一了人类推理和机器执行。研究人员通过自然语言规范定义分析目标，ARIA自主生成可执行代码、验证计算并生成透明文档。

Result: ARIA在波士顿住房案例中发现了25个关键特征，并确定XGBoost为最佳模型（R方=0.93），且过拟合程度最小。在不同领域的评估表明，ARIA在性能、可解释性和效率方面均优于现有系统。

Conclusion: ARIA通过将“AI for Research”和“AI for Science”原则结合在规范驱动的架构中，为透明、协作和可重复的科学发现建立了一个新范式。

Abstract: The rapid expansion of scientific data has widened the gap between analytical
capability and research intent. Existing AI-based analysis tools, ranging from
AutoML frameworks to agentic research assistants, either favor automation over
transparency or depend on manual scripting that hinders scalability and
reproducibility. We present ARIA (Automated Research Intelligence Assistant), a
spec-driven, human-in-the-loop framework for automated and interpretable data
analysis. ARIA integrates six interoperable layers, namely Command, Context,
Code, Data, Orchestration, and AI Module, within a document-centric workflow
that unifies human reasoning and machine execution. Through natural-language
specifications, researchers define analytical goals while ARIA autonomously
generates executable code, validates computations, and produces transparent
documentation. Beyond achieving high predictive accuracy, ARIA can rapidly
identify optimal feature sets and select suitable models, minimizing redundant
tuning and repetitive experimentation. In the Boston Housing case, ARIA
discovered 25 key features and determined XGBoost as the best performing model
(R square = 0.93) with minimal overfitting. Evaluations across heterogeneous
domains demonstrate ARIA's strong performance, interpretability, and efficiency
compared with state-of-the-art systems. By combining AI for research and AI for
science principles within a spec-driven architecture, ARIA establishes a new
paradigm for transparent, collaborative, and reproducible scientific discovery.

</details>


### [204] [$How^{2}$: How to learn from procedural How-to questions](https://arxiv.org/abs/2510.11144)
*Gautier Dagan,Frank Keller,Alex Lascarides*

Main category: cs.AI

TL;DR: How^2是一个使智能体能够根据自身知识差距提问、存储答案并终身学习的记忆智能体框架。


<details>
  <summary>Details</summary>
Motivation: 智能体在规划过程中会遇到不确定性和知识空白，通过提问可以弥补这些不足。然而，“如何做X？”这类问题的开放性答案（从可执行动作到X的子目标高层描述）使得AI智能体难以有效提问，也让AI专家难以给出支持高效规划的答案。

Method: 我们提出了How^2框架，一个记忆智能体，让智能体能够提问“如何做”的问题，存储答案，并在交互式环境中重复利用这些答案以实现终身学习。我们在Plancraft（一个Minecraft合成环境）中评估了我们的方法，智能体需要通过操纵物品来完成组装任务。

Result: 我们使用不同抽象层次的教师模型（从可执行动作序列到高层子目标描述）进行回答，结果表明，终身学习智能体从那些抽象的、与当前状态解耦的答案中获益最大。

Conclusion: How^2为基于LLM的智能体提供了一种在交互式环境中通过提问来逐步提高规划能力的方法。

Abstract: An agent facing a planning problem can use answers to how-to questions to
reduce uncertainty and fill knowledge gaps, helping it solve both current and
future tasks. However, their open ended nature, where valid answers to "How do
I X?" range from executable actions to high-level descriptions of X's
sub-goals, makes them challenging for AI agents to ask, and for AI experts to
answer, in ways that support efficient planning. We introduce $How^{2}$, a
memory agent framework that enables agents to ask how-to questions, store the
answers, and reuse them for lifelong learning in interactive environments. We
evaluate our approach in Plancraft, a Minecraft crafting environment, where
agents must complete an assembly task by manipulating inventory items. Using
teacher models that answer at varying levels of abstraction, from executable
action sequences to high-level subgoal descriptions, we show that lifelong
learning agents benefit most from answers that are abstracted and decoupled
from the current state. $How^{2}$ offers a way for LLM-based agents to improve
their planning capabilities over time by asking questions in interactive
environments.

</details>


### [205] [Aligning Deep Implicit Preferences by Learning to Reason Defensively](https://arxiv.org/abs/2510.11194)
*Peiming Li,Zhiyuan Hu,Yang Tang,Shiyu Li,Xi Chen*

Main category: cs.AI

TL;DR: 介绍了CDRA框架，旨在解决大型语言模型在用户中心交互中个性化对齐的挑战，通过引入DeepPref基准和Pers-GenPRM，实现对用户深层隐性偏好的推断和防御性推理。


<details>
  <summary>Details</summary>
Motivation: 当前大型语言模型在个性化对齐方面存在局限，无法推断用户深层隐性偏好，缺乏防御性推理能力，导致响应肤浅和脆弱。

Method: 1. 提出CDRA框架，将对齐重构为结构化推理过程。
2. 引入DeepPref基准数据集，包含3000个偏好查询对，通过模拟多方面认知委员会生成带有批评注释的推理链以揭示潜在风险并弥补偏好推断差距。
3. 引入个性化生成过程奖励模型（Pers-GenPRM），将奖励建模视为个性化推理任务，生成批评链来评估响应与用户偏好的一致性。
4. 通过批评驱动的策略对齐（Critique-Driven Policy Alignment）指导策略模型，该过程级别在线强化学习算法整合了数值和自然语言反馈。

Result: CDRA在发现和对齐用户真实偏好方面表现出色，并能执行强大的推理。

Conclusion: CDRA框架通过结构化的推理过程和创新的数据集与奖励模型，有效解决了大型语言模型个性化对齐的挑战，提升了模型的用户中心交互能力。

Abstract: Personalized alignment is crucial for enabling Large Language Models (LLMs)
to engage effectively in user-centric interactions. However, current methods
face a dual challenge: they fail to infer users' deep implicit preferences
(including unstated goals, semantic context and risk tolerances), and they lack
the defensive reasoning required to navigate real-world ambiguity. This
cognitive gap leads to responses that are superficial, brittle and
short-sighted. To address this, we propose Critique-Driven Reasoning Alignment
(CDRA), which reframes alignment from a scalar reward-matching task into a
structured reasoning process. First, to bridge the preference inference gap, we
introduce the DeepPref benchmark. This dataset, comprising 3000
preference-query pairs across 20 topics, is curated by simulating a
multi-faceted cognitive council that produces critique-annotated reasoning
chains to deconstruct query semantics and reveal latent risks. Second, to
instill defensive reasoning, we introduce the Personalized Generative Process
Reward Model (Pers-GenPRM), which frames reward modeling as a personalized
reasoning task. It generates a critique chain to evaluate a response's
alignment with user preferences before outputting a final score based on this
rationale. Ultimately, this interpretable, structured reward signal guides
policy model through Critique-Driven Policy Alignment, a process-level online
reinforcement learning algorithm integrating both numerical and natural
language feedback. Experiments demonstrate that CDRA excels at discovering and
aligning with users' true preferences while executing robust reasoning. Our
code and dataset are available at https://github.com/Zephyrian-Hugh/Deep-pref.

</details>


### [206] [AI Alignment Strategies from a Risk Perspective: Independent Safety Mechanisms or Shared Failures?](https://arxiv.org/abs/2510.11235)
*Leonard Dung,Florian Mai*

Main category: cs.AI

TL;DR: 本文分析了7种代表性对齐技术和7种故障模式，以了解它们在多大程度上有重叠部分，从而讨论了目前AI风险水平的意义以及未来如何优先考虑AI对齐研究。


<details>
  <summary>Details</summary>
Motivation: AI对齐研究旨在开发确保AI系统不会造成伤害的技术。然而，每种对齐技术都有其失效模式，即在这些条件下，技术未能提供安全保障的可能性不可忽略。本文的动机是探讨防御纵深策略在AI安全领域的有效性，特别是各种对齐技术的失效模式之间的相关性。

Method: 本文分析了7种代表性的对齐技术和7种故障模式，以评估它们之间的重叠程度。

Result: 分析结果揭示了不同对齐技术在失效模式上的重叠程度。

Conclusion: 防御纵深方法的成功取决于对齐技术的失效模式之间如何（不）相关。深入理解这些重叠对于评估当前AI风险水平和指导未来的AI对齐研究方向至关重要。

Abstract: AI alignment research aims to develop techniques to ensure that AI systems do
not cause harm. However, every alignment technique has failure modes, which are
conditions in which there is a non-negligible chance that the technique fails
to provide safety. As a strategy for risk mitigation, the AI safety community
has increasingly adopted a defense-in-depth framework: Conceding that there is
no single technique which guarantees safety, defense-in-depth consists in
having multiple redundant protections against safety failure, such that safety
can be maintained even if some protections fail. However, the success of
defense-in-depth depends on how (un)correlated failure modes are across
alignment techniques. For example, if all techniques had the exact same failure
modes, the defense-in-depth approach would provide no additional protection at
all. In this paper, we analyze 7 representative alignment techniques and 7
failure modes to understand the extent to which they overlap. We then discuss
our results' implications for understanding the current level of risk and how
to prioritize AI alignment research in the future.

</details>


### [207] [PADME: Procedure Aware DynaMic Execution](https://arxiv.org/abs/2510.11281)
*Deepeka Garg,Sihan Zeng,Annapoorani L. Narayanan,Sumitra Ganesh,Leo Ardon*

Main category: cs.AI

TL;DR: PADME是一个面向LLM的智能体框架，它将自然语言中的程序性文本转换为可执行的图形表示，并在四个基准测试中取得了最先进的性能。


<details>
  <summary>Details</summary>
Motivation: 智能体在执行自然语言中的长周期程序时面临挑战，因为指令的变异性和缺乏结构导致LLMs驱动的智能体在执行过程中偏离或失败。

Method: PADME通过一个两阶段的方法（Teach阶段和Execute阶段）将程序文本转化为可执行图。Teach阶段侧重于系统性构建并用可执行逻辑丰富程序，而Execute阶段则根据实时输入和环境反馈进行动态执行。

Result: PADME在包括ALFWorld和ScienceWorld在内的四个不同基准测试中取得了最先进的性能。

Conclusion: 配备基于图的程序表示的智能体为鲁棒和可推广的执行提供了强大的中间抽象。

Abstract: Learning to autonomously execute long-horizon procedures from natural
language remains a core challenge for intelligent agents. Free-form
instructions such as recipes, scientific protocols, or business workflows
encode rich procedural knowledge, but their variability and lack of structure
cause agents driven by large language models (LLMs) to drift or fail during
execution. We introduce Procedure Aware DynaMic Execution (PADME), an agent
framework that produces and exploits a graph-based representation of
procedures. Unlike prior work that relies on manual graph construction or
unstructured reasoning, PADME autonomously transforms procedural text into
executable graphs that capture task dependencies, decision points, and reusable
subroutines. Central to PADME is a two-phase methodology; Teach phase, which
focuses on systematic structuring, enrichment with executable logic of
procedures, followed by Execute phase, which enables dynamic execution in
response to real-time inputs and environment feedback. This separation ensures
quality assurance and scalability, allowing expert knowledge to be encoded once
and reliably reused across varying contexts. The graph representation also
provides an inductive bias that reduces error accumulation in long-horizon
reasoning, underscoring the importance of structured procedure modeling for
reliable agent-driven automation. Empirically, PADME achieves state-of-the-art
performance on four diverse benchmarks, including ALFWorld and ScienceWorld.
These results demonstrate that agents equipped with graph-based procedure
representations offer a powerful intermediate abstraction for robust and
generalizable execution.

</details>


### [208] [Automated Skill Decomposition Meets Expert Ontologies: Bridging the Granularity Gap with LLMs](https://arxiv.org/abs/2510.11313)
*Le Ngoc Luyen,Marie-Hélène Abel*

Main category: cs.AI

TL;DR: 本文提出了一种使用大型语言模型自动进行技能分解的严格、基于本体的评估框架。


<details>
  <summary>Details</summary>
Motivation: 为了解决大型语言模型在技能分解中缺乏标准化评估的问题。

Method: 本文提出了一个评估框架，包括提示、生成、规范化以及与本体节点的对齐。并引入了两种新指标：语义F1分数和层级感知F1分数。

Result: 零样本提示提供了一个强大的基线，而少数样本提示能稳定措辞和粒度，并改善层级感知对齐。延迟分析表明，由示例引导的提示具有竞争力，有时比无引导的零样本更快。

Conclusion: 该框架、基准和评估指标为开发忠实于本体的技能分解系统提供了可复现的基础。

Abstract: This paper investigates automated skill decomposition using Large Language
Models (LLMs) and proposes a rigorous, ontology-grounded evaluation framework.
Our framework standardizes the pipeline from prompting and generation to
normalization and alignment with ontology nodes. To evaluate outputs, we
introduce two metrics: a semantic F1-score that uses optimal embedding-based
matching to assess content accuracy, and a hierarchy-aware F1-score that
credits structurally correct placements to assess granularity. We conduct
experiments on ROME-ESCO-DecompSkill, a curated subset of parents, comparing
two prompting strategies: zero-shot and leakage-safe few-shot with exemplars.
Across diverse LLMs, zero-shot offers a strong baseline, while few-shot
consistently stabilizes phrasing and granularity and improves hierarchy-aware
alignment. A latency analysis further shows that exemplar-guided prompts are
competitive - and sometimes faster - than unguided zero-shot due to more
schema-compliant completions. Together, the framework, benchmark, and metrics
provide a reproducible foundation for developing ontology-faithful skill
decomposition systems.

</details>


### [209] [AI-Driven anemia diagnosis: A review of advanced models and techniques](https://arxiv.org/abs/2510.11380)
*Abdullah Al Mahmud,Prangon Chowdhury,Mohammed Borhan Uddin,Khaled Eabne Delowar,Tausifur Rahman Talha,Bijoy Dewanjee*

Main category: cs.AI

TL;DR: 这篇论文系统综述了机器学习和深度学习在贫血检测、分类和诊断方面的最新进展。


<details>
  <summary>Details</summary>
Motivation: 解决贫血诊断的准确性和及时性问题，利用人工智能技术改善贫血检测。

Method: 通过对现有文献的系统回顾，比较了不同机器学习和深度学习模型在贫血检测中的应用及其性能指标（准确性、敏感性、特异性、精确度）。

Result: 评估了不同模型在贫血检测和分类方面的优缺点。

Conclusion: 强调了这些模型在提高诊断准确性方面的重要性，并指出了未来研究需关注的问题。

Abstract: Anemia, a condition marked by insufficient levels of red blood cells or
hemoglobin, remains a widespread health issue affecting millions of individuals
globally. Accurate and timely diagnosis is essential for effective management
and treatment of anemia. In recent years, there has been a growing interest in
the use of artificial intelligence techniques, i.e., machine learning (ML) and
deep learning (DL) for the detection, classification, and diagnosis of anemia.
This paper provides a systematic review of the recent advancements in this
field, with a focus on various models applied to anemia detection. The review
also compares these models based on several performance metrics, including
accuracy, sensitivity, specificity, and precision. By analyzing these metrics,
the paper evaluates the strengths and limitation of discussed models in
detecting and classifying anemia, emphasizing the importance of addressing
these factors to improve diagnostic accuracy.

</details>


### [210] [From <Answer> to <Think>: Multidimensional Supervision of Reasoning Process for LLM Optimization](https://arxiv.org/abs/2510.11457)
*Beining Wang,Weihang Su,Hongtao Tian,Tao Yang,Yujia Zhou,Ting Yao,Qingyao Ai,Yiqun Liu*

Main category: cs.AI

TL;DR: 该论文提出了维度奖励模型（DRM），解决大型语言模型（LLMs）多步推理能力差的问题。


<details>
  <summary>Details</summary>
Motivation: 现有的成果监督强化学习（RLVR）方法只奖励最终答案的正确性，导致推理过程出现错误，且奖励信号稀疏。过程级奖励模型（PRM）虽然能提供密集的逐步反馈，但缺乏泛化性和可解释性，需要针对特定任务对推理过程进行划分。

Method: 本文提出了维度奖励模型（DRM），这是一种新的监督框架，旨在弥合RLVR和PRM之间的差距。DRM从置信度（不确定性校准）、相关性（语义对齐）和连贯性（逻辑一致性）三个基本、互补和可解释的维度评估推理过程的质量。这些维度捕捉了最终答案正确性之外的方面，并支持在不需要基本事实答案的情况下进行可解释的评估。

Result: 实验结果表明，DRM提供了有效的监督信号，指导LLM的优化并增强了它们的推理能力。通过DRM监督训练，在数学、问答、代码执行和谜题等分布内和分布外的开放域任务中，都取得了显著的性能提升。

Conclusion: 多维度的推理过程监督可以提高LLMs的泛化推理能力，使其超越训练数据的分布。

Abstract: Improving the multi-step reasoning ability of Large Language Models (LLMs) is
a critical yet challenging task. The dominant paradigm, outcome-supervised
reinforcement learning (RLVR), rewards only correct final answers, often
propagating flawed reasoning and suffering from sparse reward signals. While
process-level reward models (PRMs) provide denser, step-by-step feedback, they
lack generalizability and interpretability, requiring task-specific
segmentation of the reasoning process. To this end, we propose the
Dimension-level Reward Model (DRM), a new supervision framework that bridges
the gap between these two approaches. DRM evaluates the quality of a reasoning
process along three fundamental, complementary, and interpretable dimensions:
Confidence for uncertainty calibration, Relevance for semantic alignment, and
Coherence for logical consistency. Together, these dimensions capture aspects
beyond final answer correctness and enable interpretable assessment without
requiring ground truth answers. Experimental results show that DRM provides
effective supervision signals, guides the optimization of LLMs and enhances
their reasoning ability. In particular, DRM-supervised training achieves
consistent gains on both in-distribution and out-of-distribution open-domain
tasks, including mathematics, question answering, code execution, and puzzles.
Our findings demonstrate that multidimensional supervision of the reasoning
process can improve the generalized reasoning ability of LLMs beyond the
training distribution.

</details>


### [211] [Unifying Deductive and Abductive Reasoning in Knowledge Graphs with Masked Diffusion Model](https://arxiv.org/abs/2510.11462)
*Yisen Gao,Jiaxin Bai,Yi Huang,Xingcheng Fu,Qingyun Sun,Yangqiu Song*

Main category: cs.AI

TL;DR: 本文提出了DARK，一个统一的知识图谱演绎和溯因推理框架，通过自反去噪和逻辑探索强化学习，在多个基准数据集上实现了最先进的性能。


<details>
  <summary>Details</summary>
Motivation: 现有的知识图谱演绎和溯因推理方法是孤立的，未能充分利用两者的协同潜力，例如演绎可以验证假设，溯因可以发现更深层的逻辑模式。

Method: 本文提出了DARK（Deductive and Abductive Reasoning in Knowledge graphs），一个用于知识图谱演绎和溯因推理的统一框架。DARK是一个掩码扩散模型，能够捕获查询和结论之间的双向关系。它有两个关键创新点：1. 引入自反去噪过程，在溯因推理过程中迭代生成和验证候选假设，以更好地利用演绎进行假设细化。2. 提出逻辑探索强化学习方法，同时掩盖查询和结论，使模型能够探索新颖的推理组合，从而发现更丰富的逻辑关联。

Result: 在多个基准知识图谱上进行的广泛实验表明，DARK在演绎和溯因推理任务中均实现了最先进的性能。

Conclusion: DARK框架成功地统一了知识图谱的演绎和溯因推理，并通过其创新的自反去噪和逻辑探索强化学习方法，显著提升了两种推理任务的性能，证明了统一方法的巨大优势。

Abstract: Deductive and abductive reasoning are two critical paradigms for analyzing
knowledge graphs, enabling applications from financial query answering to
scientific discovery. Deductive reasoning on knowledge graphs usually involves
retrieving entities that satisfy a complex logical query, while abductive
reasoning generates plausible logical hypotheses from observations. Despite
their clear synergistic potential, where deduction can validate hypotheses and
abduction can uncover deeper logical patterns, existing methods address them in
isolation. To bridge this gap, we propose DARK, a unified framework for
Deductive and Abductive Reasoning in Knowledge graphs. As a masked diffusion
model capable of capturing the bidirectional relationship between queries and
conclusions, DARK has two key innovations. First, to better leverage deduction
for hypothesis refinement during abductive reasoning, we introduce a
self-reflective denoising process that iteratively generates and validates
candidate hypotheses against the observed conclusion. Second, to discover
richer logical associations, we propose a logic-exploration reinforcement
learning approach that simultaneously masks queries and conclusions, enabling
the model to explore novel reasoning compositions. Extensive experiments on
multiple benchmark knowledge graphs show that DARK achieves state-of-the-art
performance on both deductive and abductive reasoning tasks, demonstrating the
significant benefits of our unified approach.

</details>


### [212] [Zero Data Retention in LLM-based Enterprise AI Assistants: A Comparative Study of Market Leading Agentic AI Products](https://arxiv.org/abs/2510.11558)
*Komal Gupta,Aditya Shrivastava*

Main category: cs.AI

TL;DR: 本文探讨了企业级大型语言模型（LLM）实现零数据保留策略的架构、合规性和可用性权衡，并分析了Salesforce AgentForce和Microsoft Copilot的案例。


<details>
  <summary>Details</summary>
Motivation: 随着企业AI助手提升业务生产力，保护隐私数据和遵守合规性变得尤为重要。本文旨在探讨大型语言模型（LLM）如何在企业应用中实现零数据保留策略。

Method: 本文通过定义架构、合规性和可用性方面的权衡来探索零数据保留策略。并以Salesforce AgentForce和Microsoft Copilot为例，分析了其不同的技术架构以支持零数据保留策略。

Result: 研究结果将揭示Salesforce AgentForce和Microsoft Copilot在支持零数据保留策略方面的不同技术架构和部署方式。

Conclusion: 本文将分析大型语言模型服务提供商和消费应用程序实现零数据保留策略的技术架构和部署，为企业级AI助手的隐私保护提供参考。

Abstract: Governance of data, compliance, and business privacy matters, particularly
for healthcare and finance businesses. Since the recent emergence of AI
enterprise AI assistants enhancing business productivity, safeguarding private
data and compliance is now a priority. With the implementation of AI assistants
across the enterprise, the zero data retention can be achieved by implementing
zero data retention policies by Large Language Model businesses like Open AI
and Anthropic and Meta. In this work, we explore zero data retention policies
for the Enterprise apps of large language models (LLMs). Our key contribution
is defining the architectural, compliance, and usability trade-offs of such
systems in parallel. In this research work, we examine the development of
commercial AI assistants with two industry leaders and market titans in this
arena - Salesforce and Microsoft. Both of these companies used distinct
technical architecture to support zero data retention policies. Salesforce
AgentForce and Microsoft Copilot are among the leading AI assistants providing
much-needed push to business productivity in customer care. The purpose of this
paper is to analyze the technical architecture and deployment of zero data
retention policy by consuming applications as well as big language models
service providers like Open Ai, Anthropic, and Meta.

</details>


### [213] [Reproducibility: The New Frontier in AI Governance](https://arxiv.org/abs/2510.11595)
*Israel Mason-Williams,Gabryel Mason-Williams*

Main category: cs.AI

TL;DR: 人工智能政策制定者需要有效的治理机制来确保人工智能安全、对齐和可信赖发展。然而，政策制定者面临的信息环境信噪比过低，导致监管俘获，并在治理风险优先级上产生不确定性和分歧。本文认为，当前人工智能出版速度快但科学标准不严谨，缺乏重现性协议，削弱了政策制定者制定有效政策的能力。本文探讨了人工智能研究如何采纳更严格的重现性指南，以支持治理工作，并提高对人工智能风险的共识。通过借鉴其他科学领域的经验，我们评估了人工智能研究中即将到来的重现性危机，并提出采纳预注册、提高统计功效和发表阴性结果等重现性协议，可以实现有效的人工智能治理。


<details>
  <summary>Details</summary>
Motivation: 本文旨在探讨人工智能领域当前出版速度过快但科学标准不严谨（重现性协议薄弱）的问题，如何削弱了政策制定者制定有效政策和治理协议的能力。文章强调了提高人工智能研究重现性对于有效人工智能治理的重要性，并认为这将有助于政策制定者更好地理解和应对人工智能风险。

Method: 本文通过分析当前人工智能信息环境的特点，特别是低信噪比和薄弱的重现性协议对政策制定的影响。文章借鉴了其他科学领域的重现性危机经验，提出了人工智能研究可以采纳的更严格的重现性指南，如预注册、提高统计功效和发表阴性结果等。

Result: 本文指出，当前人工智能研究的速度与缺乏强有力的科学标准（通过薄弱的重现性协议）相结合，有效地削弱了政策制定者制定有意义的政策和治理协议的能力。若能采纳更严格的重现性指南，将有助于AI治理，并改善对AI风险格局的共识。预注册、提高统计功效和阴性结果发表等重现性协议能够实现有效的AI治理。

Conclusion: 人工智能政策制定者应将重现性协议视为治理工具库中的核心工具，并要求人工智能研究达到更高的标准。本文呼吁人工智能研究采纳更严格的重现性指南，以协助治理工作，提高对人工智能风险格局的共识，从而实现有效的人工智能治理。

Abstract: AI policymakers are responsible for delivering effective governance
mechanisms that can provide safe, aligned and trustworthy AI development.
However, the information environment offered to policymakers is characterised
by an unnecessarily low Signal-To-Noise Ratio, favouring regulatory capture and
creating deep uncertainty and divides on which risks should be prioritised from
a governance perspective. We posit that the current publication speeds in AI
combined with the lack of strong scientific standards, via weak reproducibility
protocols, effectively erodes the power of policymakers to enact meaningful
policy and governance protocols. Our paper outlines how AI research could adopt
stricter reproducibility guidelines to assist governance endeavours and improve
consensus on the AI risk landscape. We evaluate the forthcoming reproducibility
crisis within AI research through the lens of crises in other scientific
domains; providing a commentary on how adopting preregistration, increased
statistical power and negative result publication reproducibility protocols can
enable effective AI governance. While we maintain that AI governance must be
reactive due to AI's significant societal implications we argue that
policymakers and governments must consider reproducibility protocols as a core
tool in the governance arsenal and demand higher standards for AI research.
Code to replicate data and figures:
https://github.com/IFMW01/reproducibility-the-new-frontier-in-ai-governance

</details>


### [214] [Explainability, risk modeling, and segmentation based customer churn analytics for personalized retention in e-commerce](https://arxiv.org/abs/2510.11604)
*Sanjula De Alwis,Indrajith Ekanayake*

Main category: cs.AI

TL;DR: Too long; didn't read summary: This study proposes a three-component framework using explainable AI, survival analysis, and RFM profiling to understand churn drivers, predict churn timing, and identify high-risk customer segments for personalized retention strategies in online retail.


<details>
  <summary>Details</summary>
Motivation: Motivation: Customer acquisition is more expensive than retention in online retail, necessitating churn analytics. Current churn models are often opaque, hindering insights into attrition determinants, retention timings, and high-risk segments. Thus, there's a need to move beyond mere prediction to designing personalized, evidence-based retention strategies.

Method: Method: The paper introduces a three-component framework: 1. Explainable AI: Quantifies feature contributions to churn. 2. Survival Analysis: Models time-to-event churn risk. 3. RFM Profiling: Segments customers based on transactional behavior.

Result: Result: The combination of these methods allows for attributing churn drivers, estimating intervention windows, and prioritizing customer segments for targeted retention actions.

Conclusion: Conclusion: The proposed framework supports strategies to reduce churn and enhance customer loyalty by providing interpretable evidence for personalized retention efforts.

Abstract: In online retail, customer acquisition typically incurs higher costs than
customer retention, motivating firms to invest in churn analytics. However,
many contemporary churn models operate as opaque black boxes, limiting insight
into the determinants of attrition, the timing of retention opportunities, and
the identification of high-risk customer segments. Accordingly, the emphasis
should shift from prediction alone to the design of personalized retention
strategies grounded in interpretable evidence. This study advances a
three-component framework that integrates explainable AI to quantify feature
contributions, survival analysis to model time-to-event churn risk, and RFM
profiling to segment customers by transactional behaviour. In combination,
these methods enable the attribution of churn drivers, estimation of
intervention windows, and prioritization of segments for targeted actions,
thereby supporting strategies that reduce attrition and strengthen customer
loyalty.

</details>


### [215] [ParaCook: On Time-Efficient Planning for Multi-Agent Systems](https://arxiv.org/abs/2510.11608)
*Shiqi Zhang,Xinbei Ma,Yunqing Xu,Zouying Cao,Pengrui Lu,Haobo Yuan,Tiancheng Shen,Zhuosheng Zhang,Hai Zhao,Ming-Hsuan Yang*

Main category: cs.AI

TL;DR: ParaCook是一个用于时间高效协同规划的基准测试平台，专注于多智能体系统中的并行和异步操作，填补了现有LLM智能体基准在时间效率方面的空白。


<details>
  <summary>Details</summary>
Motivation: 现有的大型语言模型（LLMs）智能体基准主要关注任务完成度，但忽略了并行和异步操作中的时间效率；为了解决这个问题，ParaCook被提出。

Method: ParaCook通过模拟烹饪任务来实例化多智能体系统的交互规划，并简化动作空间，以突出战略并行规划的核心挑战。

Result: 通过对现有LLMs的评估，发现它们在并行动作和协调方面表现不佳，规划出的方案次优。但也揭示了LLMs在抽象任务中进行高层并行优化的潜力。

Conclusion: ParaCook提供了一个可扩展的评估框架，为开发和评估时间效率高的多智能体规划奠定了基础。

Abstract: Large Language Models (LLMs) exhibit strong reasoning abilities for planning
long-horizon, real-world tasks, yet existing agent benchmarks focus on task
completion while neglecting time efficiency in parallel and asynchronous
operations. To address this, we present ParaCook, a benchmark for
time-efficient collaborative planning. Inspired by the Overcooked game,
ParaCook provides an environment for various challenging interaction planning
of multi-agent systems that are instantiated as cooking tasks, with a
simplified action space to isolate the core challenge of strategic parallel
planning. Through a comprehensive evaluation of state-of-the-art LLMs, we find
that current approaches achieve suboptimal plans, which struggle with parallel
actions or coordination. Our analysis also reveals LLMs' potential on abstract
tasks where they can focus on high-level parallel optimization. ParaCook
provides a scalable evaluation framework with adjustable complexity,
establishing a foundation for developing and assessing time efficiency-aware
multi-agent planning. The code and data are available at
https://github.com/zsq259/ParaCook.

</details>


### [216] [SR-Scientist: Scientific Equation Discovery With Agentic AI](https://arxiv.org/abs/2510.11661)
*Shijie Xia,Yuhan Sun,Pengfei Liu*

Main category: cs.AI

TL;DR: SR-Scientist是一个将大型语言模型从方程提出者提升为自主AI科学家的框架，它能编写代码分析数据、实现方程、提交评估，并根据实验反馈优化方程。


<details>
  <summary>Details</summary>
Motivation: 目前大型语言模型在科学方程发现中的应用通常将其限制在遗传编程等搜索算法中的方程提出者角色，未能充分利用其潜力。

Method: SR-Scientist将代码解释器封装到一套工具中，用于数据分析和方程评估。智能体被指示在最小人为定义的管道下，利用这些工具在长期内优化方程。此外，该框架还开发了一个端到端强化学习框架来增强智能体的能力。

Result: SR-Scientist在涵盖四个科学学科的数据集上，比基线方法提高了6%到35%的绝对优势。此外，该方法对噪声具有鲁棒性，发现的方程对域外数据具有泛化能力，并且具有符号准确性。

Conclusion: SR-Scientist框架成功地将大型语言模型转变为一个自主的AI科学家，在科学方程发现方面表现出色，并展示了其在多个方面的优越性。

Abstract: Recently, Large Language Models (LLMs) have been applied to scientific
equation discovery, leveraging their embedded scientific knowledge for
hypothesis generation. However, current methods typically confine LLMs to the
role of an equation proposer within search algorithms like genetic programming.
In this paper, we present SR-Scientist, a framework that elevates the LLM from
a simple equation proposer to an autonomous AI scientist that writes code to
analyze data, implements the equation as code, submits it for evaluation, and
optimizes the equation based on experimental feedback. Specifically, we wrap
the code interpreter into a set of tools for data analysis and equation
evaluation. The agent is instructed to optimize the equation by utilizing these
tools over a long horizon with minimal human-defined pipelines. Empirical
results show that SR-Scientist outperforms baseline methods by an absolute
margin of 6% to 35% on datasets covering four science disciplines.
Additionally, we demonstrate our method's robustness to noise, the
generalization of the discovered equations to out-of-domain data, and their
symbolic accuracy. Furthermore, we develop an end-to-end reinforcement learning
framework to enhance the agent's capabilities.

</details>


### [217] [Operand Quant: A Single-Agent Architecture for Autonomous Machine Learning Engineering](https://arxiv.org/abs/2510.11694)
*Arjun Sahney,Ram Gorthi,Cezary Łastowski,Javier Vega*

Main category: cs.AI

TL;DR: Operand Quant是一种单智能体、基于IDE的架构，用于自主机器学习工程（MLE），它在MLE-Benchmark（2025）上取得了最先进的成果。


<details>
  <summary>Details</summary>
Motivation: 目前的机器学习工程（MLE）系统通常采用多智能体编排框架，这可能导致复杂性和效率问题。因此，需要一种更简单、更高效的MLE系统。

Method: Operand Quant通过将探索、建模、实验和部署等MLE生命周期所有阶段整合到一个单一的、上下文感知的智能体中来工作。它在一个受控的IDE环境中自主运行，采用线性、非阻塞的方法。

Result: Operand Quant在MLE-Benchmark（2025）上取得了新的最先进（SOTA）成果，在75个问题中，总体奖牌率为0.3956 +/- 0.0565。这是迄今为止所有评估系统中记录的最高性能。

Conclusion: Operand Quant的成功表明，在相同的约束条件下，一个在线性、非阻塞模式下运行的单一智能体可以超越多智能体和编排系统。

Abstract: We present Operand Quant, a single-agent, IDE-based architecture for
autonomous machine learning engineering (MLE). Operand Quant departs from
conventional multi-agent orchestration frameworks by consolidating all MLE
lifecycle stages -- exploration, modeling, experimentation, and deployment --
within a single, context-aware agent. On the MLE-Benchmark (2025), Operand
Quant achieved a new state-of-the-art (SOTA) result, with an overall medal rate
of 0.3956 +/- 0.0565 across 75 problems -- the highest recorded performance
among all evaluated systems to date. The architecture demonstrates that a
linear, non-blocking agent, operating autonomously within a controlled IDE
environment, can outperform multi-agent and orchestrated systems under
identical constraints.

</details>


<div id='cs.SI'></div>

# cs.SI [[Back]](#toc)

### [218] [On the Relationship between Space-Time Accessibility and Leisure Activity Participation](https://arxiv.org/abs/2510.10307)
*Yuan Liao,Rafael H. M. Pereira,Jorge Gil,Silvia De Sojo Caso,Laura Alessandretti*

Main category: cs.SI

TL;DR: 这篇论文介绍了一种基于能力方法的时空可达性（SPA）度量，用于评估从居住地到工作地的可行休闲机会，并探讨了它对出行时间和休闲参与的影响。


<details>
  <summary>Details</summary>
Motivation: 理解可达性如何影响休闲活动的参与对于促进包容和充满活力的城市生活至关重要。传统的可达性度量往往只关注从固定居住地的潜在可达性，而忽略了日常生活中所固有的限制和机会。

Method: 本研究引入了一种基于能力方法的时空可达性（SPA）度量，该度量考虑了在给定时间预算、个人交通方式和城市基础设施下，居住地和工作地之间的可行休闲机会。研究使用来自巴黎地区2415名居民的高分辨率GPS数据，评估SPA如何影响总出行时间和休闲参与（以休闲活动地点的多样性衡量）。通过空间模式分析和结构方程模型进行验证。

Result: 空间模式显示，大多数个体——特别是积极交通使用者——选择的目的地与他们由SPA定义的机会集相符，这突显了该度量在捕捉能力集方面的有效性。结构方程模型显示，SPA直接促进了休闲多样性，但同时也减少了出行时间，而出行时间的减少又与较低的多样性相关。

Conclusion: 这些研究结果强调了以人为中心、以能力为基础的可达性度量对于理解城市流动性不平等以及为扩大不同人群参与社会生活的真正自由的交通规划策略提供信息方面的价值。

Abstract: Understanding how accessibility shapes participation in leisure activities is
central to promoting inclusive and vibrant urban life. Conventional
accessibility measures often focus on potential access from fixed home
locations, overlooking the constraints and opportunities embedded in daily
routines. In this study, we introduce a space-time accessibility (SPA) metric
rooted in the capability approach, capturing feasible leisure opportunities
between home and work given a certain time budget, individual transport modes,
and urban infrastructure. Using high-resolution GPS data from 2,415 residents
in the Paris region, we assess how SPA influences total travel time and leisure
participation, measured as the diversity of leisure activity locations. Spatial
patterns show that most individuals-especially active transport users-choose
destinations aligned with their SPA-defined opportunity sets, underscoring the
metric's validity in capturing capability sets. Structural equation modeling
reveals that SPA directly fosters leisure diversity but also reduces travel
time, which in turn is associated with lower diversity. These findings
highlight the value of person-centered, capability-informed accessibility
metrics for understanding inequalities in urban mobility and informing
transport planning strategies that expand real freedoms to participate in
social life across diverse population groups.

</details>


### [219] [SocioBench: Modeling Human Behavior in Sociological Surveys with Large Language Models](https://arxiv.org/abs/2510.11131)
*Jia Wang,Ziyu Zhao,Tingjuntao Ni,Zhongyu Wei*

Main category: cs.SI

TL;DR: SocioBench是为评估大型语言模型（LLMs）对真实世界社会态度的对齐程度而构建的综合基准。研究发现LLMs在模拟复杂调查场景中的个体时，准确率仅为30-40%。


<details>
  <summary>Details</summary>
Motivation: 大型语言模型（LLMs）在模拟人类社会行为和互动方面表现出强大潜力，但缺乏大规模、系统构建的基准来评估其与真实世界社会态度的对齐程度。

Method: 引入SocioBench，一个从国际社会调查项目（ISSP）年度收集的标准化调查数据中提取的综合基准。该基准聚合了来自30多个国家的超过480,000条真实受访者记录，涵盖10个社会学领域和40多个人口统计属性。

Result: 实验表明，LLMs在模拟复杂调查场景中的个体时，只能达到30-40%的准确率，并且在不同领域和人口统计子群体之间存在统计学上的显著差异。

Conclusion: 当前LLMs在调查场景中存在多项局限性，包括个体层面数据覆盖不足、场景多样性不足以及缺失群体层面建模。

Abstract: Large language models (LLMs) show strong potential for simulating human
social behaviors and interactions, yet lack large-scale, systematically
constructed benchmarks for evaluating their alignment with real-world social
attitudes. To bridge this gap, we introduce SocioBench-a comprehensive
benchmark derived from the annually collected, standardized survey data of the
International Social Survey Programme (ISSP). The benchmark aggregates over
480,000 real respondent records from more than 30 countries, spanning 10
sociological domains and over 40 demographic attributes. Our experiments
indicate that LLMs achieve only 30-40% accuracy when simulating individuals in
complex survey scenarios, with statistically significant differences across
domains and demographic subgroups. These findings highlight several limitations
of current LLMs in survey scenarios, including insufficient individual-level
data coverage, inadequate scenario diversity, and missing group-level modeling.

</details>


### [220] [Beyond the Crowd: LLM-Augmented Community Notes for Governing Health Misinformation](https://arxiv.org/abs/2510.11423)
*Jiaying Wu,Zihang Fu,Haonan Wang,Fanxiao Li,Min-Yen Kan*

Main category: cs.SI

TL;DR: 本文分析了X（前身为Twitter）上的“社区笔记”系统，发现其在处理健康相关误导信息时存在显著延迟（中位数17.6小时）。为了提高响应速度，本文提出了CrowdNotes+框架，该框架利用大型语言模型（LLMs）增强社区笔记，以实现更快、更可靠的健康误导信息治理。


<details>
  <summary>Details</summary>
Motivation: 解决X（前身为Twitter）社区笔记系统中在处理健康相关误导信息时存在的显著延迟问题，提高误导信息治理的响应速度和可靠性。

Method: 本文提出了CrowdNotes+框架，该框架包含两个互补模式：（1）以证据为基础的笔记增强；（2）以效用为导向的笔记自动化。同时，还设计了一个分层的三步评估流程，逐步评估相关性、正确性和有用性。为了实例化该框架，本文构建了HealthNotes基准数据集（包含1.2K个带有有用性标注的健康笔记），并训练了一个经过微调的有用性判断器。

Result: 对15个LLM的实验揭示了当前有用性评估中一个被忽视的漏洞，即文风流畅性被误认为是事实准确性。实验结果表明，本文提出的分层评估和LLM增强生成方法共同提高了事实精确性和证据实用性。

Conclusion: 研究结果表明，混合的人机治理模型可以提高众包事实核查的严谨性和及时性。

Abstract: Community Notes, the crowd-sourced misinformation governance system on X
(formerly Twitter), enables users to flag misleading posts, attach contextual
notes, and vote on their helpfulness. However, our analysis of 30.8K
health-related notes reveals significant latency, with a median delay of 17.6
hours before the first note receives a helpfulness status. To improve
responsiveness during real-world misinformation surges, we propose CrowdNotes+,
a unified framework that leverages large language models (LLMs) to augment
Community Notes for faster and more reliable health misinformation governance.
CrowdNotes+ integrates two complementary modes: (1) evidence-grounded note
augmentation and (2) utility-guided note automation, along with a hierarchical
three-step evaluation that progressively assesses relevance, correctness, and
helpfulness. We instantiate the framework through HealthNotes, a benchmark of
1.2K helpfulness-annotated health notes paired with a fine-tuned helpfulness
judge. Experiments on fifteen LLMs reveal an overlooked loophole in current
helpfulness evaluation, where stylistic fluency is mistaken for factual
accuracy, and demonstrate that our hierarchical evaluation and LLM-augmented
generation jointly enhance factual precision and evidence utility. These
results point toward a hybrid human-AI governance model that improves both the
rigor and timeliness of crowd-sourced fact-checking.

</details>


<div id='cs.IT'></div>

# cs.IT [[Back]](#toc)

### [221] [An information theorist's tour of differential privacy](https://arxiv.org/abs/2510.10316)
*Anand D. Sarwate,Flavio P. Calmon,Oliver Kosut,Lalitha Sankar*

Main category: cs.IT

TL;DR: 这篇论文探讨了差分隐私与信息论之间的关键联系。


<details>
  <summary>Details</summary>
Motivation: 自2006年提出以来，差分隐私已成为量化敏感数据发布或共享分析中某些风险的标准方法。

Method: 本文从信息论的角度，将差分隐私算法视为连接基础数据和分析输出的信道，通过信道的特性来理解差分隐私的保证。

Result: 通过检查信息论与差分隐私在制定和应用方面的关键联系，为相关信息度量提供了“操作意义”。

Conclusion: 差分隐私的保证可以从信息论的角度理解为信道的性质。

Abstract: Since being proposed in 2006, differential privacy has become a standard
method for quantifying certain risks in publishing or sharing analyses of
sensitive data. At its heart, differential privacy measures risk in terms of
the differences between probability distributions, which is a central topic in
information theory. A differentially private algorithm is a channel between the
underlying data and the output of the analysis. Seen in this way, the
guarantees made by differential privacy can be understood in terms of
properties of this channel. In this article we examine a few of the key
connections between information theory and the formulation/application of
differential privacy, giving an ``operational significance'' for relevant
information measures.

</details>


### [222] [Quantum-Resistant Cryptography via Universal Gröbner Bases](https://arxiv.org/abs/2510.10429)
*Sergio Da Silva,Aniya Stewart*

Main category: cs.IT

TL;DR: 本文提出了一种利用通用Gröbner基的新型密钥建立协议，旨在抵御量子攻击。


<details>
  <summary>Details</summary>
Motivation: 探索通用Gröbner基在公钥密码学中的应用，并提出一种抗量子攻击的密钥建立协议。

Method: 该协议利用多项式理想$I$的通用Gröbner基$\\mathcal{U}_I$作为私钥，通过加密和解密中Gröbner基计算的复杂度差异来保证安全性。

Result: 提供了一种密钥建立协议，其安全性基于直接计算$I$的Gröbner扇的困难性。同时，还提出了有效的方法来递归生成图的酉环理想的$\\mathcal{U}_I$。

Conclusion: 本文成功提出了一种基于通用Gröbner基的抗量子攻击密钥建立协议，并深入分析了其安全性和参数复杂性，同时为图的酉环理想的通用Gröbner基生成提供了新方法。

Abstract: In this article, we explore the use of universal Gr\"obner bases in
public-key cryptography by proposing a key establishment protocol that is
resistant to quantum attacks. By utilizing a universal Gr\"obner basis
$\mathcal{U}_I$ of a polynomial ideal $I$ as a private key, this protocol
leverages the computational disparity between generating the universal
Gr\"obner basis needed for decryption compared with the single Gr\"obner basis
used for encryption. The security of the system lies in the difficulty of
directly computing the Gr\"obner fan of $I$ required to construct
$\mathcal{U}_I$. We provide an analysis of the security of the protocol and the
complexity of its various parameters. Additionally, we provide efficient ways
to recursively generate $\mathcal{U}_I$ for toric ideals of graphs with
techniques which are also of independent interest to the study of these ideals.

</details>


### [223] [On the Capacity of Distributed Quantum Storage](https://arxiv.org/abs/2510.10568)
*Hua Sun,Syed A. Jafar*

Main category: cs.IT

TL;DR: 本文介绍了分布式量子存储码的容量及其在各种图上的应用，并通过量子 CSS 码与经典安全存储问题建立了可实现性。


<details>
  <summary>Details</summary>
Motivation: 分布式量子存储码的容量问题。

Method: 将解码集表示为存储图中的超边，并通过量子 CSS 码与经典安全存储问题建立可实现性。利用非平凡的对齐结构实现恢复和安全性。

Result: 在 MDS 图、轮图、Fano 图和交集图等各种图上，对分布式量子存储的容量进行了表征。

Conclusion: 本文通过利用非平凡的对齐结构，将量子 CSS 码与经典安全存储问题相结合，为分布式量子存储码的容量提供了新的理解和方法。

Abstract: A distributed quantum storage code maps a quantum message to N storage nodes,
of arbitrary specified sizes, such that the stored message is robust to an
arbitrary specified set of erasure patterns. The sizes of the storage nodes,
and erasure patterns may not be homogeneous. The capacity of distributed
quantum storage is the maximum feasible size of the quantum message (relative
to the sizes of the storage nodes), when the scaling of the size of the message
and all storage nodes by the same scaling factor is allowed. Representing the
decoding sets as hyperedges in a storage graph, the capacity is characterized
for various graphs, including MDS graph, wheel graph, Fano graph, and
intersection graph. The achievability is related via quantum CSS codes to a
classical secure storage problem. Remarkably, our coding schemes utilize
non-trivial alignment structures to ensure recovery and security in the
corresponding classical secure storage problem, which leads to similarly
non-trivial quantum codes. The converse is based on quantum information
inequalities, e.g., strong sub-additivity and weak monotonicity of quantum
entropy, tailored to the topology of the storage graphs.

</details>


### [224] [Soft-Decoding Reverse Reconciliation in Discrete-Modulation CV-QKD](https://arxiv.org/abs/2510.10674)
*Marco Origlia,Marco Secondini*

Main category: cs.IT

TL;DR: 本文提出了一种新颖的连续变量量子密钥分发（CVQKD）反向协调（RR）技术，允许Alice利用Bob的软信息进行密钥恢复，显著提高了可实现的安全密钥速率（SKR）。


<details>
  <summary>Details</summary>
Motivation: 在连续变量量子密钥分发（CVQKD）中，由于离散调制格式，Alice仅能获得硬信息，而Bob拥有软信息，导致Alice在密钥恢复时只能进行硬判决解码，限制了密钥生成效率和安全性。

Method: 本研究引入了一种新的PAM（和QAM）反向协调（RR）技术。Bob设计并公开一个软判决度量（soft metric）来帮助Alice恢复密钥。此方法在不向窃听者泄露额外密钥信息的同时，使得Alice能够利用Bob的软信息。研究评估了该技术在可实现安全密钥速率（SKR）方面的性能，并将其与现有硬判决RR进行比较。此外，该方案通过使用二进制LDPC码在编码层面实现，并使用置信传播解码进行误码率的数值模拟。

Result: 所提出的RR技术使可实现安全密钥速率（SKR）显著提高，并接近理论上限，与硬判决RR相比有显著增益。数值模拟结果表明，该方案在编码层面的误码率性能良好，并且观察到的增益与可实现SKR的理论预测相符。

Conclusion: 本研究提出了一种新颖的反向协调（RR）技术，通过允许Alice利用Bob的软信息进行密钥恢复，显著提升了连续变量量子密钥分发（CVQKD）中安全密钥速率（SKR）。该技术在保持密钥安全性的同时，有效解决了离散调制格式下Alice仅能获得硬信息的问题，为CVQKD的实际应用提供了更高效、更安全的密钥协商方案。

Abstract: In continuous-variable quantum key distribution, information reconciliation
is required to extract a shared secret key from correlated random variables
obtained through the quantum channel. Reverse reconciliation (RR) is generally
preferred, since the eavesdropper has less information about Bob's measurements
than about Alice's transmitted symbols. When discrete modulation formats are
employed, however, soft information is available only at Bob's side, while
Alice has access only to hard information (her transmitted sequence). This
forces her to rely on hard-decision decoding to recover Bob's key.
  In this work, we introduce a novel RR technique for PAM (and QAM) in which
Bob discloses a carefully designed soft metric to help Alice recover Bob's key,
while leaking no additional information about the key to an eavesdropper. We
assess the performance of the proposed technique in terms of achievable secret
key rate (SKR) and its bounds, showing that the achievable SKR closely
approaches the upper bound, with a significant gain over hard-decision RR.
Finally, we implement the scheme at the coded level using binary LDPC codes
with belief-propagation decoding, assess its bit-error rate through numerical
simulations, compare the observed gain with theoretical predictions from the
achievable SKR, and discuss the residual gap.

</details>


### [225] [List Decoding Reed--Solomon Codes in the Lee, Euclidean, and Other Metrics](https://arxiv.org/abs/2510.11453)
*Chris Peikert,Alexandra Veliche Hostetler*

Main category: cs.IT

TL;DR: 本文提出了一种在$l_p$（半）度量下对素数域上的（广义）Reed-Solomon码进行列表解码的多项式时间算法，该算法支持任意大的距离，并改善了距离-码率权衡，同时对随机Laplacian和高斯误差表现出更好的性能。


<details>
  <summary>Details</summary>
Motivation: 现有的Reed-Solomon码纠错方法主要集中于汉明度量下的错误测量，但某些应用场景需要考虑错误具体值，因此需要研究在其他度量（如$l_p$度量）下进行错误纠正。

Method: 本文提出了一种在$l_p$（半）度量下对素数域上的（广义）Reed-Solomon码进行列表解码的多项式时间算法，适用于任何$0 < p \leq 2$。

Result: 与以往的Lee ($l_1$)和欧几里得 ($l_2$) 度量算法相比，该算法能够解码任意大的距离，并在所有高于中等阈值的解码距离下具有更好的距离-码率权衡。此外，该列表解码器在许多感兴趣的参数下实际上是一个唯一解码器。在随机Laplacian和高斯误差下，该算法支持比最坏情况错误更大的码率。

Conclusion: 本文成功地开发了一种在$l_p$度量下对Reed-Solomon码进行有效列表解码的算法，显著扩展了其在需要考虑错误值特定属性的应用中的实用性，并为未来的研究提供了新的方向。

Abstract: Reed--Solomon error-correcting codes are ubiquitous across computer science
and information theory, with applications in cryptography, computational
complexity, communication and storage systems, and more. Most works on
efficient error correction for these codes, like the celebrated
Berlekamp--Welch unique decoder and the (Guruswami--)Sudan list decoders, are
focused on measuring error in the Hamming metric, which simply counts the
number of corrupted codeword symbols. However, for some applications, other
metrics that depend on the specific values of the errors may be more
appropriate.
  This work gives a polynomial-time algorithm that list decodes (generalized)
Reed--Solomon codes over prime fields in $\ell_p$ (semi)metrics, for any $0 < p
\leq 2$. Compared to prior algorithms for the Lee ($\ell_1$) and Euclidean
($\ell_2$) metrics, ours decodes to arbitrarily large distances (for
correspondingly small rates), and has better distance-rate tradeoffs for all
decoding distances above some moderate thresholds. We also prove lower bounds
on the $\ell_{1}$ and $\ell_{2}$ minimum distances of a certain natural
subclass of GRS codes, which establishes that our list decoder is actually a
unique decoder for many parameters of interest. Finally, we analyze our
algorithm's performance under random Laplacian and Gaussian errors, and show
that it supports even larger rates than for corresponding amounts of worst-case
error in $\ell_{1}$ and $\ell_{2}$ (respectively).

</details>


<div id='cs.GT'></div>

# cs.GT [[Back]](#toc)

### [226] [On the Complexity of Stationary Nash Equilibria in Discounted Perfect Information Stochastic Games](https://arxiv.org/abs/2510.11550)
*Kristoffer Arnsfelt Hansen,Xinhao Nie*

Main category: cs.GT

TL;DR: 本文探讨了计算折现完美信息随机博弈中平稳纳什均衡的计算复杂性。


<details>
  <summary>Details</summary>
Motivation: 研究折现完美信息随机博弈中平稳纳什均衡的计算复杂性及其存在性。

Method: 对于两人博弈，我们证明该问题属于PPAD类，并给出了改进的PPAD难性证明。对于三人博弈，我们构建了游戏实例以证明有理数平稳纳什均衡不一定存在。

Result: 两人博弈的平稳纳什均衡计算问题被精确地分类为PPAD完全。对于三人博弈，有理数平稳纳什均衡不一定存在。四人博弈中计算平稳纳什均衡是SqrtSum难的。

Conclusion: 本文对完美信息随机博弈中平稳纳什均衡的计算复杂性进行了全面的分析，特别是对其在不同玩家数量下的复杂性进行了精确的分类。

Abstract: We study the problem of computing stationary Nash equilibria in discounted
perfect information stochastic games from the viewpoint of computational
complexity. For two-player games we prove the problem to be in PPAD, which
together with a previous PPAD-hardness result precisely classifies the problem
as PPAD-complete. In addition to this we give an improved and simpler
PPAD-hardness proof for computing a stationary epsilon-Nash equilibrium. For
3-player games we construct games showing that rational-valued stationary Nash
equilibria are not guaranteed to exist, and we use these to prove
SqrtSum-hardness of computing a stationary Nash equilibrium in 4-player games.

</details>


### [227] [Multiwinner Voting with Interval Preferences under Incomplete Information](https://arxiv.org/abs/2510.11625)
*Drew Springham,Edith Elkind,Bart de Keijzer,Maria Polukarov*

Main category: cs.GT

TL;DR: 这篇论文探讨了在多赢者认可选举中，如何减少沟通量的情况下提供公平保障。主要关注具有一维偏好的选民，并提出了一个概率偏好模型，其中选民群体与实数区间上的分布相关联。论文提出了一种算法，用于计算提供比例合理代表制+ (PJR+) 的委员会，该算法通过查询选民偏好来运作，并且在预期中，每个选民的查询次数为 $\mathcal{O}(\log( \sigma\cdot k))$。


<details>
  <summary>Details</summary>
Motivation: 在候选人众多的多赢者认可选举中，选民可能难以确定他们对所有候选人的偏好，因此，研究在减少沟通量的情况下可以提供哪些（如果可以的话）公平保障具有重要意义。

Method: 论文假设选民具有一维偏好，即选民和候选人都与实数 $\mathbb R$ 中的点相关联，并且每个选民的认可集形成 $\mathbb R$ 的一个区间。在此基础上，提出了一个概率偏好模型，其中选民集合由 $\sigma$ 个不同的组组成；每个组都与 $\mathbb R$ 的一个区间上的分布相关联，因此每个选民从与其组相关的分布中抽取其认可区间的端点。论文提出了一种算法，通过查询选民的偏好来计算提供比例合理代表制+ (PJR+) 的委员会。

Result: 论文提出的算法在预期中，每个选民的查询次数为 $\mathcal{O}(\log( \sigma\cdot k))$，其中 $k$ 是期望的委员会规模。

Conclusion: 该研究提出了一种在多赢者认可选举中，通过减少沟通量来提供公平保障的方法。其核心是针对应一维偏好选民的概率偏好模型和一种有效的委员会计算算法，该算法在查询效率上取得了良好的预期结果。

Abstract: In multiwinner approval elections with many candidates, voters may struggle
to determine their preferences over the entire slate of candidates. It is
therefore of interest to explore which (if any) fairness guarantees can be
provided under reduced communication. In this paper, we consider voters with
one-dimensional preferences: voters and candidates are associated with points
in $\mathbb R$, and each voter's approval set forms an interval of $\mathbb R$.
We put forward a probabilistic preference model, where the voter set consists
of $\sigma$ different groups; each group is associated with a distribution over
an interval of $\mathbb R$, so that each voter draws the endpoints of her
approval interval from the distribution associated with her group. We present
an algorithm for computing committees that provide Proportional Justified
Representation + (PJR+), which proceeds by querying voters' preferences, and
show that, in expectation, it makes $\mathcal{O}(\log( \sigma\cdot k))$ queries
per voter, where $k$ is the desired committee size.

</details>


<div id='cs.MA'></div>

# cs.MA [[Back]](#toc)

### [228] [KG-MAS: Knowledge Graph-Enhanced Multi-Agent Infrastructure for coupling physical and digital robotic environments](https://arxiv.org/abs/2510.10325)
*Walid Abdela*

Main category: cs.MA

TL;DR: 该报告介绍了知识图谱增强多智能体基础设施（KG-MAS），旨在解决赛博物理系统（CPS）中物理和数字环境集成所面临的系统异构性和复杂性挑战。


<details>
  <summary>Details</summary>
Motivation: 传统的CPS集成方法在处理系统异构性和复杂性方面存在局限性，特别是在工业4.0背景下，缺乏语义丰富性和灵活性。

Method: KG-MAS利用中心化的知识图谱（KG）作为动态的共享世界模型，为多智能体系统（MAS）提供共同的语义基础。自主智能体通过查询KG进行决策，并使用实时状态信息更新KG。系统采用模型驱动架构，根据语义描述自动生成智能体。

Result: KG-MAS通过抽象底层通信协议和提供统一的智能协调机制，为异构物理和数字机器人环境的耦合提供了一个鲁棒、可伸缩和灵活的解决方案。

Conclusion: KG-MAS通过结合知识图谱和多智能体系统，为解决复杂CPS环境下的集成和协调挑战提供了一种创新且有效的途径，特别适用于工业4.0应用。

Abstract: The seamless integration of physical and digital environments in
Cyber-Physical Systems(CPS), particularly within Industry 4.0, presents
significant challenges stemming from system heterogeneity and complexity.
Traditional approaches often rely on rigid, data-centric solutions like
co-simulation frameworks or brittle point-to-point middleware bridges, which
lack the semantic richness and flexibility required for intelligent, autonomous
coordination. This report introduces the Knowledge Graph-Enhanced Multi-Agent
Infrastructure(KG-MAS), as resolution in addressing such limitations. KG-MAS
leverages a centralized Knowledge Graph (KG) as a dynamic, shared world model,
providing a common semantic foundation for a Multi-Agent System(MAS).
Autonomous agents, representing both physical and digital components, query
this KG for decision-making and update it with real-time state information. The
infrastructure features a model-driven architecture which facilitates the
automatic generation of agents from semantic descriptions, thereby simplifying
system extension and maintenance. By abstracting away underlying communication
protocols and providing a unified, intelligent coordination mechanism, KG-MAS
offers a robust, scalable, and flexible solution for coupling heterogeneous
physical and digital robotic environments.

</details>


### [229] [The Social Cost of Intelligence: Emergence, Propagation, and Amplification of Stereotypical Bias in Multi-Agent Systems](https://arxiv.org/abs/2510.10943)
*Thi-Nhung Nguyen,Linhao Luo,Thuy-Trang Vu,Dinh Phung*

Main category: cs.MA

TL;DR: 本文探讨了大型语言模型（LLM）多智能体系统（MAS）中的偏见问题，分析了内部专业化、底层LLM和智能体间通信协议对偏见鲁棒性、传播和放大的影响。研究发现MAS通常不如单智能体系统稳健，偏见常因群体内偏爱而早期出现，但合作和辩论式交流可缓解偏见放大，更稳健的底层LLM可提高系统稳定性。


<details>
  <summary>Details</summary>
Motivation: 尽管先前研究主要关注单个LLM中的偏见，但多智能体系统（MAS）中LLM之间的协作和交流引入了偏见出现和传播的新动态，这方面的探索较少。

Method: 本文通过模拟社交环境，其中智能体代表不同的社会群体，并在各种交互和对抗场景下评估系统行为。实验基于三个偏见基准进行。

Result: 多智能体系统通常不如单智能体系统稳健，偏见常常通过群体内偏爱而早期出现。然而，合作和辩论式的交流可以减轻偏见的放大，更健壮的底层大型语言模型可以提高整个系统的稳定性。

Conclusion: 多智能体LLM系统中的偏见是一个复杂的问题，理解内部专业化、底层LLM和通信协议之间的相互作用对于构建公平和有弹性的系统至关重要。未来的研究应侧重于开发更稳健的缓解策略，以应对多智能体设置中偏见的独特挑战。

Abstract: Bias in large language models (LLMs) remains a persistent challenge,
manifesting in stereotyping and unfair treatment across social groups. While
prior research has primarily focused on individual models, the rise of
multi-agent systems (MAS), where multiple LLMs collaborate and communicate,
introduces new and largely unexplored dynamics in bias emergence and
propagation. In this work, we present a comprehensive study of stereotypical
bias in MAS, examining how internal specialization, underlying LLMs and
inter-agent communication protocols influence bias robustness, propagation, and
amplification. We simulate social contexts where agents represent different
social groups and evaluate system behavior under various interaction and
adversarial scenarios. Experiments on three bias benchmarks reveal that MAS are
generally less robust than single-agent systems, with bias often emerging early
through in-group favoritism. However, cooperative and debate-based
communication can mitigate bias amplification, while more robust underlying
LLMs improve overall system stability. Our findings highlight critical factors
shaping fairness and resilience in multi-agent LLM systems.

</details>
